id,authorId,author,authorUrl,authorType,authorHeadline,title,type,url,excerpt,voteupCount,commentCount,zfavCount,createdTime,updatedTime,content
664346177,354a091a04e0c20269578c0064f8d2a4,AI技能研究所,https://api.zhihu.com/people/354a091a04e0c20269578c0064f8d2a4,people,程序员的进阶之旅,什么是<em>人工智能</em>？如何学习人工智能？,article,https://api.zhihu.com/articles/664346177,作为一个 985 本硕计算机的<em>人工智能</em>经验开发者来谈谈我对人工智能的理解，全文 3000 余字都是干货，建议收藏、点赞后观看。一句话，<em>人工智能</em>，是机器能够具有与人类思维相关的认知功能的能力。 作为一个长期在<em>人工智能</em>行业挣扎的经验者，有必要来回答这个问题，给新入行、想入行的同学一些经验分享,275,5,0,1698742852,1698743073,"<p data-pid=""8TrX6zJu""><b>作为一个 985 本硕计算机的人工智能经验开发者来谈谈我对人工智能的理解，全文 3000 余字都是干货，建议收藏、点赞后观看。</b></p><h3><b>一句话，人工智能，是机器能够具有与人类思维相关的认知功能的能力。</b></h3><p data-pid=""1l-OCghp""><b>作为一个长期在人工智能行业挣扎的经验者，有必要来回答这个问题，给新入行、想入行的同学一些经验分享，本片内容涉及广、涵盖多，建议收藏、点赞、关注三连后慢慢食用</b></p><p data-pid=""CNKcKh2i"">人类和机器的渊源，缠绵古今。如果没有借助、发现、发明、使用机器，我们人类这个物种注定不会走的太远。从近代来说，<span class=""nolink"">农业机械化</span>、汽车、高铁、建筑设备等等，都是机器。这些机器长期以来帮助我们改进我们的生活状态和品质。</p><p data-pid=""8bw-0YKn"">另一方面，人类是矛盾的，即倾向于使用更便捷的机器，又害怕的担心机器的智能发展飞快。二十世纪的理论家，如<span class=""nolink"">计算机科学家</span>和数学家<span class=""nolink"">艾伦·图灵</span>，设想了一个机器可以比人类更快地<span class=""nolink"">执行功能</span>的未来，就是<span class=""nolink"">人工智能时代</span>。</p><p data-pid=""dMvOD7kb"">自计算机在 20 世纪 70 年代开始普及以来，计算机的功能越来越复杂，计算能力越来越强，由此展开对人工智能这一愿景的实现。</p><p data-pid=""CQVdi-y4""><b>人工智能是机器执行与人类思维相关的认知功能的能力，例如感知、推理、学习、与环境交互、解决问题，甚至发挥创造力的未来世界的愿景。</b></p><h2>目前的人工智能是什么？</h2><p data-pid=""byi-MLVO"">人工智能 (AI) 是计算机科学的一个广泛分支，涉及构建能够执行通常需要<span class=""nolink"">人类智能</span>的任务的智能机器。虽然人工智能是一门具有多种方法的跨学科科学，但尤其是机器学习和深度学习的进步正在为科技行业的几乎每个领域带来范式转变。</p><ul><li data-pid=""iomcggU3""><b>将人工智能称之为机器可以具有人类思维相关认知能力的愿景</b></li><li data-pid=""JaWeQge3"">目前解决的方式是通过机器学习的方法来逼近人工智能这一个愿景</li><li data-pid=""MlAZujkf"">其中深度学习是机器学习中目前效果较好且最火热的一个技术分支</li></ul><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-e53e8c38dac2ba80e1d7b0fecaa7caec_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-e53e8c38dac2ba80e1d7b0fecaa7caec""/></figure><p data-pid=""9jIbbEs-"">让我们看一个人工智能驱动产品的例子——Amazon Echo，Amazon Echo 是一款使用亚马逊开发的<span class=""nolink"">虚拟助理</span>人工智能技术 Alexa 的<span class=""nolink"">智能音箱</span>。Amazon Alexa 能够进行语音交互、播放音乐、设置闹钟、播放有声读物以及提供新闻、天气、体育和交通报告等实时信息。<br/> </p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-ef259fa0f5337c381ba1baf599c1d1d2_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-ef259fa0f5337c381ba1baf599c1d1d2""/></figure><h2>人工智能的等级</h2><p data-pid=""ArwCflEF"">目前就人工智能的发展趋势来看，可以把人工智能划分为三个层级，<b>弱人工智能（ANI），<span class=""nolink"">通用人工智能</span>（AGI）和超级人工智能（ASI）</b>三类。</p><p data-pid=""Er9pC193""><b>弱人工智能（ANI）：</b>是迄今为止成功完成的人工智能技术。 ANI(Artificial Narrow Intelligence)被设计出来用来执行单个任务并且以目标为导向。 ANI 非常有能力完成其编程要完成的特定任务。 ANI 的一些示<span class=""nolink"">例是语音</span>助<span class=""nolink"">手、面部</span>识别或驾驶汽车。</p><p data-pid=""q5DnhC1m""><b>通用人工智能（AGI）：</b>是具有通用人工思维的智能机器，机器可以模仿人类的智能和行为，并具有从数据中学习并应用其智能来解决任何问题的能力。<span class=""nolink"">人工通用情报</span>可以在任何给定情况下以类似于人类的方式思考、理解和行动，目前最火热的 ChatGPT 正属于这个阶段。</p><p data-pid=""gc9McL3U""><b>超级人工智能（ASI）：</b>可以变得自我意识并超越人类能力和智能的假想，距离目前这个阶段还很遥远。</p><p data-pid=""jeHRUBJ_""><b>说道这里就需要谈一下最近在知乎<span class=""nolink"">知学堂</span>上的一门课《<span class=""nolink"">程序员的 AI 大模型进阶</span>之旅</b>》；</p><a data-draft-node=""block"" data-draft-type=""edu-card"" data-edu-card-id=""1702729766845812736""></a><p data-pid=""QUs4n8n9""><b>上面的链接就是公开课的链接，<span class=""nolink"">完全免费</span>的科普课程。添加课程之后一定一定一定要添加助教小姐姐的微信，可以私聊助教领取今年最火最热的大模型学习资源！</b></p><p data-pid=""mfi0kfZw"">目前入行人工智能需要从机器学习、深度学习知识点出发，来学习相关技术，才能从事相关人工智能行业。下面简单来说说如何入手这些技术，从而可以达到入行人工智能的程度。</p><h2>什么是机器学习？</h2><p data-pid=""qR1QHaMg"">机器学习是计算机科学的一门学科，它使用计算机算法和分析来构建可以解决业务问题的<span class=""nolink"">预测模型</span>。</p><p data-pid=""Ed-2Vj0T"">根据<span class=""nolink"">麦肯锡公司</span>的说法，机器学习基于可以从数据中学习的算法，而无需依赖基于规则的编程。</p><p data-pid=""X4umApON""><b>「如果计算机程序在 T 中的任务中的性能（按 P 测量）随着经验 E 的提高而提高，则可以说它可以从关于某类任务 T 和性能测量 P 的经验 E 中学习。」</b></p><p data-pid=""vP5fmamO"">所以你看，机器学习有很多定义。但它到底是如何运作的呢？</p><h2>机器学习如何工作？</h2><p data-pid=""6kX2x4mO"">机器学习访问大量数据（结构化和非结构化）并从中学习以预测未来。它通过使用多种算法和技术从数据中学习。下图显示了机器如何从数据中学习。</p><p data-pid=""cybJ09t2"">上面一张图就完全展示了机器学习是如何工作的，如果想要自学机器学习的朋友，可以参考这几篇内容：</p><p class=""ztext-empty-paragraph""><br/></p><a href=""https://www.zhihu.com/question/27468261/answer/3165416828"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic3.zhimg.com/v2-c51b70700d40a993928fe7cfd3cf8712_l.jpg"" data-image-width=""720"" data-image-height=""659"" class=""internal"">如何自学机器学习Machine Learning？</a><a href=""https://www.zhihu.com/question/504465921/answer/3159529591"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic3.zhimg.com/v2-57f6868b901ffc798c7f5ed4c2149396_qhd.jpg"" data-image-width=""275"" data-image-height=""183"" class=""internal"">入门机器学习，哪些书籍值得看？</a><h2><br/>机器学习的类型</h2><p data-pid=""n6A877yi"">机器学习算法主要分为三大类：</p><h3>1. 监督学习</h3><p data-pid=""NPvNNSG0"">在<b>监督学习</b>中，数据已经被标记，这意味着你知道目标变量。使用这种学习方法，<b>系统可以根据过去的数据预测未来的结果。它要求至少为模型提供输入和输出变量才能对其进行训练。</b></p><p data-pid=""CCqGxCGm"">下面是监督学习方法的一个例子。该算法是使用狗和猫的标记数据进行训练的。经过训练的模型可以预测新图像是猫还是狗。<br/> </p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-15f873ebbf21a2084aa71dbec30c29c7_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-15f873ebbf21a2084aa71dbec30c29c7""/></figure><p data-pid=""HmsToZAq""> 监督学习的一些例子包括<b>线性回归、逻辑回归、<a href=""https://www.zhihu.com/search?q=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3172224248%7D"" class=""internal"">支持向量机</a>、朴素贝叶斯和决策树。</b></p><h3>2.无监督学习</h3><p data-pid=""Z1lHrlI9""><b>无监督学习算法利用未标记的数据自行从数据中发现模式。</b>该系统能够从提供的输入数据中识别隐藏的特征。一旦数据更具可读性，模式和相似性就会变得更加明显。</p><p data-pid=""_X9Vgy9t"">下面是一个使用未标记数据训练模型的无监督学习方法的示例。在这种情况下，数据由不同的车辆组成。该模型的目的是对每种车辆进行分类。<br/> </p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-744d62942f6e1086365ba7d7a8857425_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-744d62942f6e1086365ba7d7a8857425""/></figure><p data-pid=""EvTT2MrY"">无监督学习的一些示例包括 k 均值聚类、<span class=""nolink"">层次聚类</span>和异常检测。</p><h3>3. 强化学习</h3><p data-pid=""i6gc4V7U"">强化学习的目标是训练智能体在不确定的环境中完成任务。代理从环境中接收观察结果和奖励，并向环境发送操作。奖励衡量行动在完成任务目标方面的成功程度。</p><p data-pid=""hIyAfKJ5"">下面的示例展示了如何训练机器识别形状。<br/> </p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-d5130cb2a09befd1aaabec84b34f66d9_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-d5130cb2a09befd1aaabec84b34f66d9""/></figure><p data-pid=""z0ZjOqDR"">强化学习算法的示例包括 Q 学习和深度 Q 学习<span class=""nolink"">神经网络</span></p><h2>什么是深度学习？</h2><p data-pid=""O4atfnvV"">深度学习是机器学习的一个子集，它处理受人脑结构和功能启发的算法。深度学习算法可以处理大量结构化和<span class=""nolink"">非结构化数据</span>。深度学习的核心概念在于<span class=""nolink"">人工神经网络</span>，它使机器能够做出决策。</p><p data-pid=""CEEW_gjy""><b>深度学习与机器学习之间的主要区别在于数据呈现给机器的方式。机器学习算法通常需要结构化数据，而深度学习网络则在多层人工神经网络上工作。</b></p><p data-pid=""6CVfEMuA"">这是一个简单的神经网络的样子：<br/> </p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-603b6cc91ce0bfc251e844ae7270a312_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-603b6cc91ce0bfc251e844ae7270a312""/></figure><p data-pid=""5qRFvjKF""><b>该网络有一个输入层，用于接受数据的输入。<span class=""nolink"">隐藏层</span>用于从数据中查找任何隐藏的特征。然后输出层提供预期的输出。</b></p><p data-pid=""Khnaryhe"">这是一个使用大量未标记的眼睛视网膜数据的神经网络的示例。<span class=""nolink"">网络模型</span>根据这些数据进行训练，以确定一个人是否患有<span class=""nolink"">糖尿病视网膜病变</span>。<br/> </p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-6c70a579fa441fa0d99880e25e574a3b_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-6c70a579fa441fa0d99880e25e574a3b""/></figure><p data-pid=""JAD5Etpn"">现在我们已经了解了什么是深度学习，让我们看看它是如何工作的。</p><h2>深度学习如何工作？</h2><ol><li data-pid=""UeR5QZl9""><b>计算加权和。</b></li><li data-pid=""KTtgElHu""><b>计算出的权重总和作为输入传递给<span class=""nolink"">激活函数</span>。</b></li><li data-pid=""tKf3fKFz""><b>激活函数将「输入的加权和」作为函数的输入，添加偏差，并决定是否应该激发神经元。</b></li><li data-pid=""r-3k_oQI""><b>输出层给出预测输出。</b></li><li data-pid=""4yYReMmj""><b>将模型输出与实际输出进行比较。训练神经网络后，模型使用反向传播方法来提高网络的性能。<a href=""https://www.zhihu.com/search?q=%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3172224248%7D"" class=""internal"">成本函数</a>有助于降低错误率。</b></li></ol><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-65b1dabb5dd4e11e36867dd93d2b1d1e_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-65b1dabb5dd4e11e36867dd93d2b1d1e""/></figure><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-46de51a5281936cebdf957b91fa97a90_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-46de51a5281936cebdf957b91fa97a90""/></figure><p data-pid=""JNjutX1t"">在下面的示例中，深度学习和神经网络用于识别车牌上的号码。许多国家都使用这种技术来识别违规者和超速车辆。<br/> </p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-6f3bfdacb409997ae1f9474ccc15f634_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-6f3bfdacb409997ae1f9474ccc15f634""/></figure><p data-pid=""2XkQPXC-"">想要入门深度学习或者找深度学习的相关资料，不如看看我写的这一篇文章，回答了深度学习相关资源、图书；以及学习下方法和路线。</p><a href=""https://www.zhihu.com/question/36675272/answer/3149048176"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic3.zhimg.com/v2-e31bfbce487a66e4b6165111f287c4c2_qhd.jpg"" data-image-width=""275"" data-image-height=""183"" class=""internal"">有哪些优秀的深度学习入门书籍？需要先学习机器学习吗？</a><p data-pid=""7WZfGt76"">本文作者：@TopGeeky<br/> </p>"
666172193,354a091a04e0c20269578c0064f8d2a4,AI技能研究所,https://api.zhihu.com/people/354a091a04e0c20269578c0064f8d2a4,people,程序员的进阶之旅,入门<em>人工智能</em>需要学习哪些基础知识？,article,https://api.zhihu.com/articles/666172193,我当初研究生在学习<em>人工智能</em>的时候，就是因为一些基础知识掌握的不行，走了很多弯路！在硬生生走的过程中，我慢慢入门了<em>人工智能</em>，积累了一些经验，为了不让大家再重蹈我的老路，这个回答我就好好给大家分享一下！ <em>人工智能</em>是 计算机科学的一个分支，它是一个很大的方向。从<em>人工智能</em>的研究范围就可见一斑,153,7,0,1699604986,1708683952,"<p data-pid=""ND-ou-Gs""><b>我当初研究生在学习人工智能的时候，就是因为一些基础知识掌握的不行，走了很多弯路！</b></p><p data-pid=""b7V59GAb"">在硬生生走的过程中，我慢慢入门了人工智能，积累了一些经验，为了不让大家再重蹈我的老路，这个回答我就好好给大家分享一下！</p><p data-pid=""8lw-eydL"">人工智能是<span class=""nolink"">计算机科学</span>的一个分支，它是一个很大的方向。</p><p data-pid=""Uh7baUME"">从人工智能的研究范围就可见一斑，它是一门研究如何使计算机能够模拟且实现<span class=""nolink"">人类智能</span>的学科。</p><p data-pid=""65Ea_faE"">直白点说就是，它通过模拟人的认知过程和思维意识，使计算机具有类似人的智力水平，去做人可以做的事情。</p><p data-pid=""4p3sKv1B"">要达成这个目的，要做方方面面的努力，这又使得人工智能产生了很多的分支。<br/> </p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-635224cd74234b3a1a8a3c3e08073e5d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""720"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb"" width=""720"" data-original=""https://pic2.zhimg.com/v2-635224cd74234b3a1a8a3c3e08073e5d_r.jpg"" data-original-token=""v2-635224cd74234b3a1a8a3c3e08073e5d""/></figure><p data-pid=""VtBN85PW"">比如如何模拟人的认知过程和思维意识，帮助计算机能像人类那样思考，就产生了「<b>机器学习</b>」这门计算机理论。</p><p data-pid=""2MOL4xpA"">通过训练数据和算法模型让机器具有人工智能的方法，比如大家都知道的深度学习，是机器学习的其中一个研究方向，它是使用<span class=""nolink"">神经网络</span>模拟人类大脑的工作方式。</p><p data-pid=""tvwbv-xe"">比如如何让计算机和人对话，这就需要计算机能够理解人类语言的含义，并进行回复，这就产生了 NLP 「<b><a href=""https://www.zhihu.com/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">自然语言处理</a></b>」。</p><p data-pid=""QbL4LmwI"">用来研究计算机与人类自然语言之间的交互，比如一些智能客服，问答系统，手机上常用的 Hi Siri。</p><p data-pid=""f3G_yu4B"">比如如何让计算机具备感知视觉信息，理解它看到的东西，就产生了「<b><a href=""https://www.zhihu.com/search?q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">计算机视觉</a></b>」。</p><p data-pid=""jFDZOgVo"">从图像或视频中提取有用的特征，进行识别、分析和理解，现在应用在视频监控、自动驾驶、<a href=""https://www.zhihu.com/search?q=%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">医学影像</a>诊断等方面。</p><p data-pid=""tZeHNdtt"">比如通过<a href=""https://www.zhihu.com/search?q=%E6%99%BA%E8%83%BD%E4%BD%93&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">智能体</a>与环境的交互学习最优行为的「强化学习」，比如改进人与机器之间交互方式的「<b><a href=""https://www.zhihu.com/search?q=%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">人机交互</a></b>」，比如...</p><p data-pid=""LhMiKnO5"">这些都是人工智能领域中的研究方向。<br/> </p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-a6fb882ea416e648593d0021060867ac_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""604"" data-rawheight=""330"" class=""origin_image zh-lightbox-thumb"" width=""604"" data-original=""https://pic1.zhimg.com/v2-a6fb882ea416e648593d0021060867ac_r.jpg"" data-original-token=""v2-a6fb882ea416e648593d0021060867ac""/></figure><p data-pid=""mdEtOlgI"">随着人工智能的快速发展，ChatGPT、<span class=""nolink"">GPT-4</span> 等新产品和新技术的发布，再次让人工智能变的火热，在<a href=""https://www.zhihu.com/search?q=%E5%A4%B8%E5%85%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">夸克</a>发布的《2023 高考志愿》报告中，人工智能相关专业的关注度上升最快，我很高兴大家能够去关注 AI，尤其是在当今这个时代，AI 正在成为各行各业的核心驱动力。</p><p data-pid=""PGWDMsny"">我一直认为在当今，人人都应该去了解一下 AI 技术，借着这股技术热潮助力自己，当然我也听很多同学说有这个想法，但是不知道怎么去了解，正好最近<b>「知乎知学堂」</b>旗下<b>「AGI 课堂</b>」推出的<b>「程序员的 AI 大模型进阶之旅」</b>公开课，我建议大家去看一下，邀请的都是圈内的技术大佬解读最前沿的技术，只有两天的课程。</p><p data-pid=""fO0QkdQ_"">通过这个课好好了解像我们这样的普通人如何做 GPT 浪潮中的超级个体，一定<b>别忘了添加助教老师微信可以免费领 AI 大模型资料，不要白不要！</b></p><a data-draft-node=""block"" data-draft-type=""edu-card"" data-edu-card-id=""1706347135925227520""></a><p data-pid=""bGTVYmI8"">不同的研究领域侧重点各不相同，需要的基础知识也是不同的。</p><p data-pid=""RAZc0-hJ"">你拿<b>机器学习</b>来说，它需要的基础知识：</p><p data-pid=""HtDNBIPe"">1、<a href=""https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">数学基础</a></p><p data-pid=""sLSt1gVb"">像微积分、线性代数、概率论与数理统计，用来理解和计算机器学习中算法的<a href=""https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">数学原理</a>与推导，以及<a href=""https://www.zhihu.com/search?q=%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">优化方法</a>。</p><p data-pid=""rnTwRNPI"">2、编程基础</p><p data-pid=""aBQPgWVm"">掌握编程语言，用来实现机器学习的算法，比如 Python、R、C++ 等。</p><p data-pid=""H4KqnVkG"">3、<a href=""https://www.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">数据结构与算法基础</a></p><p data-pid=""7647_p51"">机器学习算法中使用了很多的数据结构和算法，了解常用的数据结构与算法能更好的理解和实现机器学习算法和模型。</p><p data-pid=""Peo6MAXO"">你像<b>自然语言处理</b>，它需要的基础知识：</p><p data-pid=""k3vajtTT"">1、数学基础</p><p data-pid=""xgxUdqgU"">微积分、线性代数、概率论与数理统计这些内容，有助于理解 NLP 模型以及学会用它们来处理文本数据，</p><p data-pid=""gWU7FsLe"">2、编程基础</p><p data-pid=""2FCh9io7"">掌握编程语言，常见的是 Python、C++ 这些可以用来编写和运行程序。</p><p data-pid=""CG5prqs3"">3、数据结构与算法基础。</p><p data-pid=""5cxlVEF7"">数据结构和算法对于处理和分析文本数据非常重要，掌握常见的数据结构与算法能让自己写出更高效的 NLP 算法和模型。</p><p data-pid=""_eGfX9uh"">4、语言学基础</p><p data-pid=""vutDjxM_"">这个是学习自然语言处理必须的，了解基本的语言学概念和语言结构，比如像语法、句法、语义，对于自然语言处理来说是很重要的。</p><p data-pid=""6U_yKSfj"">再者像计算机视觉，它需要的基础知识：依然是数学基础、编程基础、数据结构与算法基础以外，你需要额外具有<a href=""https://www.zhihu.com/search?q=%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">数字图像处理基础</a>，熟悉数字图像技术处理的基本技术。</p><p data-pid=""ZtbiiEiu"">因为篇幅原因，再多就不列了。</p><p data-pid=""hwo5bjPW"">你可以看到，人工智能的每个研究方向具体要做的内容不同，具体要求可能也会有所不同，但还是存在着交叉和重叠的知识内容。</p><p data-pid=""yc4GK-LS"">也就是<b>数学基础、编程基础、数据结构与算法</b>。</p><p data-pid=""pSqWq2JY"">这些也是学习人工智能所需要的前置知识，最好在你学习某个具体方向之前要快速学一下，只需要学习我们能用到的就好。</p><p data-pid=""bPqq-Y97""><b>不要求到精通的程度，但最少你要了解</b>，起码在后续的学习中碰到这个知识，就算你不熟，也知道可以去哪里找到这个知识学。<br/> </p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-fba26896be35ca435d9c788c366fb6c1_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image"" data-original-token=""v2-fba26896be35ca435d9c788c366fb6c1""/></figure><h2>数学基础</h2><p data-pid=""AS6XJ7MN"">数学对于人工智能的学习至关重要，我们需要理解和应用相关的<a href=""https://www.zhihu.com/search?q=%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">模型算法</a>，有了数学基础，可以帮助我们理解算法模型背后的数学原理，以及后续在训练模型或者<a href=""https://www.zhihu.com/search?q=%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">评估模型</a>时涉及的计算过程。</p><p data-pid=""bsehoWWk"">大家也不要害怕，主要的就是微积分、线性代数、<a href=""https://www.zhihu.com/search?q=%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">概率论与数理统计</a>，这都是大学中学过的数学课程。</p><p data-pid=""ZnqPJmVS"">1、微积分</p><p data-pid=""h5hqQSnh"">微积分是高数中的内容，重点是在<a href=""https://www.zhihu.com/search?q=%E5%BE%AE%E5%88%86&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">微分</a>方面，重点看一下极限、导数、<a href=""https://www.zhihu.com/search?q=%E5%81%8F%E5%AF%BC%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">偏导数</a>、梯度。</p><p data-pid=""nWjrhgjL"">2、线性代数</p><p data-pid=""TgCiLoZ7"">线性代数对于人工智能的学习很重要，涉及到很多<a href=""https://www.zhihu.com/search?q=%E7%9F%A9%E9%98%B5&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">矩阵</a>的运算，重点在向量、矩阵、线性方程组、特征值、<a href=""https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">特征向量</a>。</p><p data-pid=""4s59Fu0k"">3、概率论与数理统计</p><p data-pid=""33pnSk5J"">人工智能中很多算法涉及到概率论与数理统计中的内容，比如<a href=""https://www.zhihu.com/search?q=%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">最大似然估计</a>，高斯分布。</p><p data-pid=""4qMWgjhK"">这里需要看概率分布(正态分布、均匀分布、<a href=""https://www.zhihu.com/search?q=%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">伯努利分布</a>)、抽样分布(t 分布、<a href=""https://www.zhihu.com/search?q=%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">卡方分布</a>)、统计量(均值、方差、置信区间)、<a href=""https://www.zhihu.com/search?q=%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">假设检验</a>等。</p><p data-pid=""XNQ4kP5f"">看着挺多，其实都是学过的内容，重新复习一下就想起来。</p><h2>编程基础</h2><p data-pid=""vtnErCrS"">编程这个的重要性就不必多说了吧，不会编程啥也干不了。</p><p data-pid=""2Pe8MQIO"">刚开始你就先掌握 Python 就好了，Python 具有完善的人工智能生态系统，很多模型的代码都是基于 Python 实现的。</p><p data-pid=""Pm6NaFx-"">各种配套的第三方库和工具也很完善，比如强大的数据处理库 Numpy、Pandas，比如丰富的<a href=""https://www.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">数据可视化</a>库 Matplotlib、Seaborn。</p><p data-pid=""_NhcRMpY"">如何学习 Python，提高 Python 的编程能力，不是本文的重点，可以看下面这个回答：</p><a href=""https://www.zhihu.com/question/553628459/answer/2689874048"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic4.zhimg.com/v2-c11a85e9ec73b235adb557b7de9235b3_l.jpg"" data-image-width=""300"" data-image-height=""300"" class=""internal"">怎样提高自己的 Python 编程能力？</a><p data-pid=""WJB2Bq2r""><br/>后续等你到了一个更高的阶段，应该也会用到 C++，它是一种<a href=""https://www.zhihu.com/search?q=%E7%BC%96%E8%AF%91%E5%9E%8B%E8%AF%AD%E8%A8%80&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">编译型语言</a>，可以<a href=""https://www.zhihu.com/search?q=%E7%9B%B4%E6%8E%A5%E8%AE%BF%E9%97%AE&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">直接访问</a>和控制底层硬件以及内存，进行精细化的<a href=""https://www.zhihu.com/search?q=%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">内存管理</a>和优化，在处理大规模数据时至关重要。</p><p data-pid=""vo46SVXL"">关于 C++ 的学习路线，我先给大家放在下面，需要的时候可以回头来看：<br/></p><a href=""https://zhuanlan.zhihu.com/p/435927070"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic4.zhimg.com/v2-c6d1bf41be595f4697e082a6ace6033f_qhd.jpg"" data-image-width=""1360"" data-image-height=""579"" class=""internal"">Rocky0429：这才是你最想要的 C++ 学习路线</a><h2>数据结构与算法</h2><p data-pid=""LiBfqhCp"">在本科阶段，数据结构与算法就是最重要的计算机基础课之一，不管是后续考研还是找工作都很重要，没想到吧，在人工智能的学习中，数据结构与算法依然重要。</p><p data-pid=""h5bkp961"">比如常见的社交网络分析，需要使用<a href=""https://www.zhihu.com/search?q=%E5%9B%BE%E7%AE%97%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">图算法</a>处理和分析复杂的图结构数据，使用<a href=""https://www.zhihu.com/search?q=%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">搜索算法</a>解决<a href=""https://www.zhihu.com/search?q=%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">推荐系统</a>问题，或者使用<a href=""https://www.zhihu.com/search?q=%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">排序算法</a>生成最终的推荐结果。</p><p data-pid=""T8REbT5O"">对于学习人工智能的同学，或者想以后走研究这条路，数据结构与算法一定要<a href=""https://www.zhihu.com/search?q=%E5%A6%82%E8%87%82%E4%BD%BF%E6%8C%87&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3171801327%7D"" class=""internal"">如臂使指</a>。</p><p data-pid=""6r57r_Yq"">数据结构主要就是数组、链表、栈、队列和树等。</p><p data-pid=""2VBuZ1em"">算法重要的就是排序算法、搜索算法、图等。</p><p data-pid=""oPX8ImiA"">如果你想详细的学习数据结构与算法，可以<span class=""nolink"">看这里</span>：</p><a href=""https://zhuanlan.zhihu.com/p/582109772"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic3.zhimg.com/v2-d8dbf58ad1ac56aeecfe35f5bd86ce2e_qhd.jpg"" data-image-width=""1370"" data-image-height=""681"" class=""internal"">Rocky0429：这才是你最想要的数据结构与算法学习路线</a><p data-pid=""MOaSOKGh""><br/>最低要求就是大家一定要掌握原理，至于暂时写不出来，也没关系。</p><p data-pid=""sz899us0"">写在最后</p><p data-pid=""taM4sPuf"">把前置技能花点时间准备好，等你具体研究哪个方向，再去学对应方向的知识。</p><p data-pid=""ynMWhCod"">当然不管你是出于什么目的想要学习人工智能，不管以后你是不是要从事相关行业，我都希望你能在有时间去学习的时候，多多了解一下 AI，<b>「知乎知学堂」</b>旗下<b>「AGI 课堂</b>」的这个<b>「程序员的 AI 大模型进阶之旅</b>」公开课一定可以帮助到你！</p><p data-pid=""vYK5Edxn"">两天的课程，找到可以借助 AI 帮助自己破局的方法，做 GPT 浪潮中的超级个体！</p><a data-draft-node=""block"" data-draft-type=""edu-card"" data-edu-card-id=""1706347058565484544""></a><p data-pid=""DUQNXC64"">之前看到一句很有意思的对话：</p><p data-pid=""5pGICUCd"">问</p><p data-pid=""PgIM3Fej"">人工智能的巅峰是什么？</p><p data-pid=""GJEO2UHi"">答</p><p data-pid=""evAvS2jJ"">人工智能的巅峰时 AI 们集体讨论该如何解决人类问题。</p><p data-pid=""N2418nQY"">想象一下，如果 AI 们来到知乎提出这样的问题：「学习人类需要哪些基础知识」，那会是怎样一副场景？</p><p data-pid=""4oSAsaGq"">画面太美，难以想象。</p><p data-pid=""N_cdG28J"">本文作者：@Rocky0429</p>"
70687533,bb0e6449d94f40250780a94349e3c250,zhongyian,https://api.zhihu.com/people/bb0e6449d94f40250780a94349e3c250,people,,<em>人工智能</em>专业劝退贴。,article,https://api.zhihu.com/articles/70687533,尤其对于前两届的小白鼠们。 2、朋友说：他的亲戚家小孩，对<em>人工智能</em>非常感兴趣。 我希望大家明白一点，现在的<em>人工智能</em>是弱人工智能，远远不像小朋友们想象中的那么高大上，你的兴趣不能简简单单的来自于几部科幻电影,1729,400,2040,1561442171,1561457730,"<p data-pid=""Npv3YdWu"">本来我是没打算写这么个东西的，虽然已经给我推荐了好几次人工智能专业要学啥？想学人工智能专业去哪个学校好？这样类似的问题，我都没有回答，因为恐怕我一张嘴就是劝退。可是现在身边有朋友咨询我说家里亲戚的小孩对人工智能专业感兴趣，问我怎么样，叹一口气，趁着大部分报考还没结束，我谈一谈我的看法，希望能对大家有用，多少为大家提供一些思路吧。</p><p data-pid=""wIjc5941"">1、人工智能课程设置到底怎样？</p><p data-pid=""3PR8Iyt_"">这个问题知乎聊的比较多了，我就不献丑了，但是总体上我是认同：<b>你要实在想报考，985以上高校考虑一下，其他的慎重</b>。目前cv、nlp取得不错结果的算法，深度学习这一卦的占主流，你想做人工智能，先看看你想报考的学校能买得起多少GPU吧。当然了，本科阶段有可能不涉及到这些，只是想让大家明白，<b>并不是这么多高校都有经验、师资来做这些的，尤其对于前两届的小白鼠们。</b></p><p data-pid=""IdhxMBbs"">2、朋友说：他的亲戚家小孩，对人工智能非常感兴趣。</p><p data-pid=""1QG9gLRn"">我希望大家明白一点，现在的人工智能是弱人工智能，远远不像小朋友们想象中的那么高大上，你的<b>兴趣不能简简单单的来自于几部科幻电影。</b>我不否认智能时代会到来，但我相信绝不会是你读大学的这四年，尤其现在工业界所做的算法，大量的数据处理类脏活累活，甚至就是规则，根本没美帝大片那么高级。alpha go下围棋是赢了人类高手，但那是因为数据的积累和计算力的提升。虽然这些也算是广义上人工智能研究范畴，但是和目前大家一窝蜂的想学的人工智能算法是两回事，你可以思考一下，为啥神经网络上世纪就有，可是直到最近才爆火？也就是说，目前你学习的人工智能远远不是“智能”，只是模式匹配加一定的参数优化。我不是否定大家的兴趣，而是<b>希望能够了解一些后再下判断，否则最后支持你学习下去的就不一定是兴趣了，是孤勇。</b></p><p data-pid=""q71IvI_4"">3、另一种思路：先学数学、物理这样的基础学科，打好理论基础，再转人工智能。</p><p data-pid=""9OhTxeGu"">这种想法我认为有两个问题：a. 答主一个朋友本科C9物理实验班，研究生转到到Top2做算法算法，据和他交流：大三大四补修了很多计算机核心课程，一边补课一边补代码挺痛苦的，同时虽然做算法的老师挺喜欢要数学系的学生，但是保研考研的主流还是计算机专业老本行的学生，他的同学好多想转计算机或金融，但是最后成功率不足10%。b. 这类基础学科之所以经久不衰真的是因为很多学科的本质就是数学，逻辑思考很重要，尤其到了博士阶段。但是说句糙话“你大爷永远是你大爷”，数学物理这类学科真不是谁都能学的，尤其到了科研的时候，你课堂学的那点数学完全不够看。你<b>高中数学、物理学的好，分数考的高和你适合这类基础学科完全是两个概念，</b>尤其这类学科怎么也得到了研究生阶段才能体现出积累的价值吧。所以一句话，你觉得自己<b>真的优秀且喜欢，大胆的上，基础学科需要真正优秀的人才，只是我估计我等大部分普通人，还是可远观不可亵玩，高岭之花慎重采撷，</b>需要考虑：如果万一没研究生读了，没给你在另一行发光发热的机会，你要找啥工作呢？</p><p data-pid=""9nEYIthS"">4、人工智能就业好。</p><p data-pid=""ZfWQgqqK"">目前的就业形势呢，算法的确比其他很多专业要好，最起码薪资水平还是在线的。但是如果按照算法本身时序的前后来比呢？恐怕是2019年校招不如2018年。可以随便查查一些帖子，什么2019真的是AI寒冬吗？等等（可参考头条2019算法劝退邮件）。这还只是研究生，如果未来人工智能在本科阶段就可以批量生产，难以想象四年以后的求职市场竞争得多激烈。<b>个人认为人工智能目前泡沫有些大，不止本科要培养，研究生还有一堆其他专业转来的，不是特别看好“全民AI”这件事。</b>其实也接触过一些业界的人，普遍反应的是，公司有坑，也有人投简历，但是实在很难match，这样说的话，其实倒不能说供过于求，只是供需不匹配。而<b>新开设的人工智能专业能不能供需匹配，不好说。</b></p><p class=""ztext-empty-paragraph""><br/></p><p data-pid=""gAhcvArY"">最后再墨迹一段吧，其实我不是不看好人工智能这个方向，这个方向肯定是未来发展的核心竞争力，也是国家大力扶持的要弯道超车的力量，个人也是做这块的，还是能体会到算法对于某些行业的提升的，我只是怕小朋友们对市场盲目乐观，轻易入行。对人工智能感兴趣，<b>建议本科选计算机相关专业，</b>本科教育本来就是基础教育，研究生做人工智能来得及，大部分做算法的人都是这类专业，并且计算机专业<b>进可攻退可守</b>，能学人工智能最好，不能还可以做一枚程序员。</p>"
342522102,1b72d70b702b3920638f0235d380ebd8,忆臻,https://api.zhihu.com/people/1b72d70b702b3920638f0235d380ebd8,people,,斯坦福大学——<em>人工智能</em>本科4年课程清单,article,https://api.zhihu.com/articles/342522102,一位在行业内工作了几年的斯坦福<em>人工智能</em>”师兄“就根据自己的工作和学习经验，为入学的师弟师妹们送出了一份大礼：<em>人工智能</em>的本科4年课程清单，希望想要了解<em>人工智能</em>的新生能够靠着这份指路图,1601,0,7527,1609994870,1609994891,"<p></p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-c8a7e64798002833822b321da91524c2_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""900"" data-rawheight=""383"" class=""origin_image zh-lightbox-thumb"" width=""900"" data-original=""https://pic3.zhimg.com/v2-c8a7e64798002833822b321da91524c2_r.jpg"" data-original-token=""v2-c8a7e64798002833822b321da91524c2""/></figure><blockquote data-pid=""v5mCAhQG"">来自 | 大数据文摘   出品 编译 | 笪洁琼<br/></blockquote><p data-pid=""WB_DxANh""><b>注：文末附【深度学习与自然语言处理】交流群，最近赶ACL，比较忙，很多同学加了没有回过期了，可以重新加一下，备注好的一定会回复，敬请谅解。</b></p><p data-pid=""40blx3Zn"">相信每个入行人工智能的老手，对自己过往的几年学习生涯都或多或少会有一些遗憾：如果我当年先从基本概念入手就好了，如果我当年把核心算法吃的更透一点就好了……</p><p data-pid=""RaCTgVki"">最近，一位在行业内工作了几年的斯坦福人工智能”师兄“就根据自己的工作和学习经验，为入学的师弟师妹们送出了一份大礼：人工智能的本科4年课程清单，希望想要了解人工智能的新生能够靠着这份指路图，少走弯路。</p><p data-pid=""bNv6PcT7"">这位大方的学长名叫Mihail Eric，本人也是一位妥妥的“学霸”。在斯坦福大学的NLP研究组里，与Christopher Manning, Percy Liang, Christopher Potts（三位巨佬）一起做研究，度过了3年非常充实的研究时光，收获颇多。同时Eric向ACL\EMNLP\NLP for AI work投稿论文，均已发表，目前正在担任ACL的审稿人，也是业内人工智能公司Alexa AI的工程师。</p><p data-pid=""tn5he022"">作为已经毕业的学长，Eric离开学校已经有一段时间了，总结了一下自己的学习和工作经历，Eric为自己的人工智能和机器学习的职业生涯，设计一个完整的4年制人工智能本科学位基础课程。这些课程是为AI和CS领域的萌新提供的，虽然是萌新定位，但Eric觉得这些课程直到今天都还在用，是基础必会的技能。</p><h2><b>第1年，打基础</b></h2><h3>假设你没有CS的经验，这一年的大部分时间应该用在CS和机器学习的核心概念和算法上，推荐的课程有：<br/></h3><p data-pid=""szm0WS8A"">1.编程基础，这是AI从业人员必需具备的软件工程学的基本技能。课程为CS106B：</p><p data-pid=""Lpwl_e56""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs106b/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs106b/</span><span class=""ellipsis""></span></a></p><p data-pid=""gk63V-C4"">2.计算机系统，这门课的重点在于计算机的基础设计和构建，尤其是学习软件编译的时候，运行一个程序会发生什么，以及程序是如何在内存中运行的。课程为CS107：</p><p data-pid=""Rs6XKXih""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs107/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs107/</span><span class=""ellipsis""></span></a></p><p data-pid=""LycOgn4Y"">3.算法概论，这门课包含了计算机算法背后的数学和理论基础，比如最优搜索算法和动态编程，以及如何分析这些算法的内存和优缺点。课程为CS161：</p><p data-pid=""_MYoYVTz""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs161/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs161/</span><span class=""ellipsis""></span></a></p><p data-pid=""gnivnpJP"">4.概率论，概率论和统计学是机器学习算法中的核心，尤其是分析数据在实战里很重要。课程为CS109：</p><p data-pid=""cLgFv-H9""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs109/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs109/</span><span class=""ellipsis""></span></a></p><p data-pid=""VTOs9hZd"">5.线性代数，如何计算矩阵和向量，线性方程组，最小二乘法，这些都是机器学习需要用到的数学基础。课程为EE103：</p><p data-pid=""YRm6jx0J""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/ee103/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">ee103/</span><span class=""ellipsis""></span></a></p><p data-pid=""2r9oRc29"">6.多维微积分，调试函数的梯度，反向传播以及机器学习，这些都是经常用到的。课程为向量微积分（工程师）：</p><p data-pid=""CTSNOrZP""><a href=""https://link.zhihu.com/?target=https%3A//www.coursera.org/learn/vector-calculus-engineers"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://www.</span><span class=""visible"">coursera.org/learn/vect</span><span class=""invisible"">or-calculus-engineers</span><span class=""ellipsis""></span></a></p><p data-pid=""FoZK0u_K""><b> 第2年，从系统的基础知识中开始慢慢探索</b></p><h3><b>AI本科的第二年，至少要明白人工智能的一些原理，应该用什么样的理论去解决问题，还需要加强对计算机系统运行的理解。</b><br/></h3><p data-pid=""IWHmlP03"">1.人工智能概论，这门课包括了人工智能领域所运用的研究，比如搜索，游戏，逻辑以及图像还有机器学习算法的应用。课程CS221：</p><p data-pid=""0tOZBhh3""><a href=""https://link.zhihu.com/?target=https%3A//stanford-cs221.github.io/spring2020/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">stanford-cs221.github.io</span><span class=""invisible"">/spring2020/</span><span class=""ellipsis""></span></a></p><p data-pid=""G_uP2fMt"">2.编译器，主要是讲编译器背后的设计和理论，你学了之后，至少应该了解一个编译器是如何构建的，还有编译器的模块化组件，也需要了解。如果你对语义识别感兴趣，可以好好琢磨一下编译器的设计和传统的自然语言处理堆栈之间的相似之处，非常有趣。课程CS143：</p><p data-pid=""NDhBu6OG""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs143/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs143/</span><span class=""ellipsis""></span></a></p><p data-pid=""z_V8WIQg"">3.数据库原理，主要讲数据库管理系统背后的原则，比如关系数据模型、索引、模式等主题，但凡你想成为数据专家或者机器学习工程师，就必须要懂得数据库的原理。课程CS145：</p><p data-pid=""3umwSKZu""><a href=""https://link.zhihu.com/?target=https%3A//cs145-fa19.github.io/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">cs145-fa19.github.io/</span><span class=""invisible""></span></a></p><p data-pid=""1_d25rJR"">4.并行计算，并行计算这门课会讲Apache Spark到GPU这些系统背后的原理，课程CS149：</p><p data-pid=""TOxbZPbM""><a href=""https://link.zhihu.com/?target=http%3A//cs149.stanford.edu/fall19/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">cs149.stanford.edu/fall</span><span class=""invisible"">19/</span><span class=""ellipsis""></span></a></p><p data-pid=""dObYHkDa"">5.操作系统，你如果想要擅长系统编程，就一定要上这门课，这门课是讲如何从头开始搭建一个操作系统，不仅需要设计系统，还需要明白如何调试和代码管理。如果你不明白，你手下的人敲完：sudo rm -rf /*就撤了，你该如何拯救你的代码？课程CS140：</p><p data-pid=""rV0o3lz_""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/~ouster/cgi-bin/cs140"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/~ouste</span><span class=""invisible"">r/cgi-bin/cs140</span><span class=""ellipsis""></span></a></p><p data-pid=""xyoBU1sb""><b> 第3年，开启提升课程</b><br/></p><p data-pid=""bxOExyWC"">这个时候应该开始学高级课程，开始NLP，BA，CV这些方向的研究，</p><p data-pid=""rBLjxo-G"">1.机器学习，监督学习和模型训练的概念，比如偏差、方差，正则化以及模型选择，这些内容看上去简单，实际上每个AI从业者天天都在用的。课程CS229：</p><p data-pid=""GkhWUFDe""><a href=""https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">cs229.stanford.edu/</span><span class=""invisible""></span></a></p><p data-pid=""Jic_G6V6"">2.凸优化，这门课运用很广，比如统计学，机器学习，信号处理和其它使用凸优化的领域，虽然现在有不少问题都是非凸化的，但是你最好还是要懂背后的逻辑。课程EE364A：</p><p data-pid=""eX0PtAaH""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/ee364a/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">ee364a/</span><span class=""ellipsis""></span></a></p><p data-pid=""NeCVLv1E"">3.概率图模型，像CV和NLP就会经常用到，所以还是需要了解。课程CS228：</p><p data-pid=""OFD46i1I""><a href=""https://link.zhihu.com/?target=https%3A//cs.stanford.edu/~ermon/cs228/index.html"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">cs.stanford.edu/~ermon/</span><span class=""invisible"">cs228/index.html</span><span class=""ellipsis""></span></a></p><p data-pid=""xF4k4dwV"">4.数据挖掘，大数据与数据挖掘，这门课涵盖了处理大型数据集的技术方法，会运用到推荐算法、聚类以及大规模的数据集计算与分析，要知道每天产生的数量还是比较大的。课程CS246：</p><p data-pid=""qBheYSpF""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs246/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs246/</span><span class=""ellipsis""></span></a></p><p data-pid=""VRDKFX6V"">5.NLP，自然语言处理，让机器懂得文本数据的理论和时间，而且还会在这门课里学到传统自然语言处理，老师会教如何用深度学习技术来处理这些。课程CS224N：</p><p data-pid=""2Sw6HeIh""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs224n/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs224n/</span><span class=""ellipsis""></span></a></p><p data-pid=""RP01WqoE"">6.基于CV的卷积神经网络，基本包含了深度学习背后的理论，比如CV模型就经常运用，只要学过人工智能课程的人，没错，是来自李飞飞教授的CS231N吧。课程CS231N：</p><p data-pid=""QU9__BVf""><a href=""https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">cs231n.stanford.edu/</span><span class=""invisible""></span></a></p><h3><b> 第4年，同学你该开始打比赛了</b><br/></h3><p data-pid=""Q1-cRSNe"">经过前面3年课程的训练，你应该对计算机系统和人工智能概念、应用有了清楚的理解，找到你比较感兴趣的方向，拿起数据集，就要开始自己跑模型，做数据分析，调参还有解决Bug。想要成为一个真正的人工智能专家，不仅得学，还得上手练。</p><p data-pid=""kzxy6XNt"">1.做研究项目，有的学校会提供这类课程，在这门课里，你需要深入研究整个项目的情况。课程CS341：</p><p data-pid=""cm24GtKg""><a href=""https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs341/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">web.stanford.edu/class/</span><span class=""invisible"">cs341/</span><span class=""ellipsis""></span></a></p><p data-pid=""sLHDWI46"">2.参加课题研究，主动去找研究生学长学姐，做他们的助理，将基础知识再过一遍，也可以选择自己开一个课题项目，进行研究，主要是让你有一个项目经验。</p><p data-pid=""z9BDkonY"">3.大厂实习，如果你要是时间管理的好，可以考虑在课余时间去AI公司实习，一般大厂都有这种3-6个月的实习机会，在实习里，不仅让你了解书本上的基础知识，还能使用基础知识进行落地的运用，这种应该算比较好的实战机会了。</p><p data-pid=""6Z9-Q5jp"">以上就是作为一个人工智能从业者的斯坦福毕业生Eric，送给AI萌新的4年规划安排，如果你觉得上面的课程比较难，可以适当调整，当然也欢迎你与文摘菌分享你的学习之路。</p><p data-pid=""89snfMG8"">就像歌词所说“一代人终将老去，但总有人正年轻。“AI之路任重且道远，不管是不是萌新，只要你还愿意学习，就一直在路上，加油！！！</p><p data-pid=""hHobsDc9"">参考素材：<a href=""https://link.zhihu.com/?target=https%3A//www.mihaileric.com/posts/complete-artificial-intelligence-undergraduate-course-plan/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://www.</span><span class=""visible"">mihaileric.com/posts/co</span><span class=""invisible"">mplete-artificial-intelligence-undergraduate-course-plan/</span><span class=""ellipsis""></span></a></p><p data-pid=""itul5Wxc""><b>机器学习算法-自然语言处理交流群</b></p><p data-pid=""Y3olhNSv"">已建立机器学习算-自然语言处理微信交流群！想要进交流群进行学习的同学，可以直接加我的微信号：<b>HIT_NLP</b>。加的时候备注一下：<b>知乎+学校+昵称 （不加备注不会接受同意，望谅解）</b>，即可。然后我们就可以拉你进群了。群里已经有非得多国内外高校同学，交流氛围非常好。</p><h2><b>推荐阅读</b></h2><p data-pid=""NeLOMWV0""><a href=""https://zhuanlan.zhihu.com/p/342247527"" class=""internal"">超过500个附代码的AI/机器学习/深度学习/计算机视觉/NLP项目</a></p><p data-pid=""ZArHvtbX""><a href=""https://zhuanlan.zhihu.com/p/342188618"" class=""internal"">Awesome Transformer for Vision Resources List库</a></p><p data-pid=""amrrWd_4""><a href=""https://zhuanlan.zhihu.com/p/342178401"" class=""internal"">2020 Top10计算机视觉论文总结：论文，代码，解读，还有demo视频！</a></p><p data-pid=""mUudN4ao""><a href=""https://zhuanlan.zhihu.com/p/341987450"" class=""internal"">摘要数据整理仓库，6个数据集！</a></p><p data-pid=""p2pyv3v4""><a href=""https://zhuanlan.zhihu.com/p/341660943"" class=""internal"">156个参考文献！Visual Transformer 调研survey</a></p><p data-pid=""J1SvFW9T""><a href=""https://zhuanlan.zhihu.com/p/341588674"" class=""internal"">NLP生成任务痛点！58页generation评价综述</a></p><p data-pid=""yK28FPSr""><a href=""https://zhuanlan.zhihu.com/p/341558474"" class=""internal"">机器学习画图模板ML Visuals更新</a></p><p data-pid=""TNCsxBBD""><a href=""https://zhuanlan.zhihu.com/p/341476148"" class=""internal"">谷歌最新28页高效 Transformer 模型综述</a></p><p data-pid=""qDaSj53x""><a href=""https://zhuanlan.zhihu.com/p/341394324"" class=""internal"">Papers with Code 2020 全年回顾</a></p><p data-pid=""DHg3OJxo""><a href=""https://zhuanlan.zhihu.com/p/341350164"" class=""internal"">最新14页《图神经网络可解释性》综述论文</a></p><p data-pid=""ZMdgpnZR""><a href=""https://zhuanlan.zhihu.com/p/341232760"" class=""internal"">陶大程等人编写！最新41页深度学习理论综述</a></p><p data-pid=""7mXZ_qg1""><a href=""https://zhuanlan.zhihu.com/p/302409233"" class=""internal"">使用PyTorch时，最常见的4个错误</a></p><p data-pid=""eUyo_hMZ""><a href=""https://zhuanlan.zhihu.com/p/330213537"" class=""internal"">加拿大蒙特利尔大学助理教授刘邦招收2021/2022年博士生</a></p><p data-pid=""P9pS655I""><a href=""https://zhuanlan.zhihu.com/p/335717109"" class=""internal"">【EMNLP2020】基于动态图交互网络的多意图口语语言理解框架</a></p><p data-pid=""f0Dd6uay""><a href=""https://zhuanlan.zhihu.com/p/338256656"" class=""internal"">一文搞懂 PyTorch 内部机制</a></p><p data-pid=""gtqL21Rx""><a href=""https://zhuanlan.zhihu.com/p/341212550"" class=""internal"">AAAI 2021论文接收列表放出!!!</a></p>"
165192306,7c2ba06784a34b43ea47a98702c3164a,感言敢说医学僧,https://api.zhihu.com/people/7c2ba06784a34b43ea47a98702c3164a,people,,浅谈对于“<em>人工智能</em>”的理解,article,https://api.zhihu.com/articles/165192306,将会是人类智慧的“容器”。<em>人工智能</em>可以对人的意识、思维的信息过程的模拟。<em>人工智能</em>不是人的智能，但能像人那样思考、也可能超过人的智能。 二、 未来能否<em>人工智能</em>是否能够取代人类,112,3,233,1596069802,1596069802,"<p data-pid=""2koreMvg""><b>浅谈对于“人工智能”的理解和评论</b></p><p data-pid=""oF3bLYsk""><b>一、</b> <b>人工智能的定义(素材来源于百度百科）</b></p><p data-pid=""f1FfHqX1"">人工智能（Artificial Intelligence），<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E8%258B%25B1%25E6%2596%2587/3079091"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">英文</a>缩写为AI。它是<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25A0%2594%25E7%25A9%25B6/1883844"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">研究</a>、<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%25BC%2580%25E5%258F%2591/9400971"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">开发</a>用于<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%25A8%25A1%25E6%258B%259F/7698898"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">模拟</a>、<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%25BB%25B6%25E4%25BC%25B8/7834264"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">延伸</a>和扩展人的<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2599%25BA%25E8%2583%25BD/66637"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">智能</a>的理论、方法、技术及应用系统的一门新的技术科学。人工智能是<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E8%25AE%25A1%25E7%25AE%2597%25E6%259C%25BA"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">计算机</a>科学的一个分支，它企图了解智能的实质，并生产出一种新的能以<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25BA%25BA%25E7%25B1%25BB%25E6%2599%25BA%25E8%2583%25BD/2287229"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">人类智能</a>相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25B8%2593%25E5%25AE%25B6%25E7%25B3%25BB%25E7%25BB%259F/267819"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">专家系统</a>等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类<a href=""https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2599%25BA%25E6%2585%25A7/129438"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">智慧</a>的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。</p><p data-pid=""DV5LiWHr""><b>二、</b> <b>未来能否人工智能是否能够取代人类？</b></p><p data-pid=""7giZR1mY""><b>注：本节思路是从工业革命或科技革命带来的体力劳动、脑力劳动角度分析人类劳动迭代的变化</b></p><p data-pid=""wvBnGT_T"">从事我们医学技术类专业相关性的的医学生谈及人工智能流行这样的一句评价， “人工智能所带来的医疗技术变革对于医学技术的冲击当轮到医学检验技术时，医学影像技术还会远吗？”。说起人工智能必然要提起四次工业革命和技术革命，第一次工业革命带来了远超人类力量的以蒸汽机为代表的机器，将从事复杂体力劳动的人解放出来；第二次工业革命以电气化为代表的工业革命，人类完成了机器的进一步革新，带来了远超人类速度的以汽车为代表的工业产品将马车和人力车夫从简单性的体力劳动中解放出来；第三次工业革命以计算机为代表的科技革命，高效的信息系统将人类的复杂脑力中解放出来，人类的劳动更为井然有序，冗杂的劳动程序和人员进一步被淘汰,万物置身于互联网这个网络空间中，创造出了比人类沟通方式更为高效、便捷、打破时间和空间距离的大网—互联网，此时互联网只是一个虚拟的空间；第四次科技革命到来时（21世纪-）将互联网这张网从虚拟世界中物理化，变成了一个外部空间，基于万物互联所构成的物联网，在这个物联网所构成的外部世界里，机器变得更加智能化，变得比人类更为智能，机器智能诊断技术将医学技术人员从复杂的脑力劳动中解放出来，其智能程度远远超过人类。</p><p data-pid=""AU4vnCgh""><b>三、</b> <b>转换思路：淘汰不可避免，这是人类社会劳动效率发展的缩影</b></p><p data-pid=""oH9vnCoB"">人工智能会替代人类中智能的部分，加强机器的认知功能，但其智慧程度不如</p><p data-pid=""BFOGgb0n"">人类—创造力。人类的社会的发展不断的将人类从繁重的体力劳动和脑力劳动（简单操作性，或者认知程序性较为简单的人类智慧功能）中解放出来，从而带动社会效率的整体进步。就如工业革命时代的汽车，人力车夫永远跑不过汽车；就如科技革命时代的计算机，人类的记忆功能永远比不上电脑。人类社会向着空间和时间距离的压缩，空间上的距离不再是距离，但是这些科技的成果的背后都是人类的创造…………………</p>"
85863388,b6a959e485deafff18131b8f889130c2,创者张大纬,https://api.zhihu.com/people/b6a959e485deafff18131b8f889130c2,people,一个肤浅的人和一个有趣的灵魂。,<em>人工智能</em>的利弊,article,https://api.zhihu.com/articles/85863388,"以后我们的汽车也可以该变成无人驾驶的汽车。<em>人工智能</em>增进了人与人之间的距离。同时,<em>人工智能</em>使我们的生活更方便。 目前人工智能已经为人类创造出了非常可观的经济效益，<em>人工智能</em>可以代替人类做大量人类不想做、不能做的工作",375,3,586,1570665447,1588568349,"<p data-pid=""RiZJ0IWB"">随着科学技术和互联网的发展，地球已经变成了一个小小的地球村，人工智能领域也迅速发展，特别是在中国“2025智造”提出后，国内的人工智能领域也掀起一段热潮。面对发展如此迅速的人工智能，既有利，也有弊。随着科技的发展,我们的生活变得越来越方便。然而任何事物都有它的两面性。当然人工智能也不例外。</p><p data-pid=""TWRPdORk""><br/>一方面，人工智能给我们带来了非常多的便捷。就拿我们日常都不离手的手机来说，一开始我们只能用手机发送消息和打电话,但是现在它可以上网和不在你身边的人聊天。除此外也可以拿来拍照，相当于一个照相机，还可以拿来打游戏等等。智能，不仅仅在这些很小的东西上体现，据报道，以后我们的汽车也可以该变成无人驾驶的汽车。人工智能增进了人与人之间的距离。同时,人工智能使我们的生活更方便。</p><p data-pid=""Gi3MBc5C""><br/>目前人工智能已经为人类创造出了非常可观的经济效益，人工智能可以代替人类做大量人类不想做、不能做的工作，而且机器犯错误的概率比人低，并且能够持续工作，大大的提升工作效率，节约了大量的成本，未来的人工智能可能还会代替人类工作，代替人类做家务，帮助人类学习，甚至可以照顾老人和小孩，实时监护人类的健康，生病了直接给人来治疗，延长人类的寿命，让人类的生活变得越来越美好。</p><p data-pid=""IheMc9MX""><br/>科技的发展是一把双刃剑，这是已经谈论很久的问题。当然，也有很多人会认为人工智能可能是我们的灾难。最近,许多科幻片都在讲诉我们关于人工智能之间的事情，其中很大部分都在讲智能机器人失去控制,并且有了自己的思维想杀死人类，他们来统治世界。</p><p data-pid=""xOLR_Wy5""><br/>人类将来的某一天会不会被各种各样人工智能机器人所代替，人类未来会不会成为机器人的奴役？</p><p data-pid=""RJnOiE_J""><br/>汽车的发明颠覆了传统的马车行业，人工智能的发展同样也将颠覆许多行业。机器人代替了许多人类的工作将导致大量的人口失业，机器新的学习速度远远快于人类，在一期国内热播的最强大脑里有一期人机大战中，最终小度（机器人的名字）战胜了选手，如果未来的某一天，机器人变成像电影《机械姬》中有意识的机器人，那么人类随时会变成机器人的奴隶，同时，人工智能面临着技术失控的危险，霍金曾发出警告，人类面临一个不确定的未来，先进的人工智能设备能够独立思考，并适应环境变化，它们未来或将成为导致人类灭亡的终结者！</p><p data-pid=""_JFLT0I9""><br/>我个人认为，人工智能是人类最受益的。因为我们有了人工智能后，我们的生活变得更有趣和方便，智能机器人的事情也许不会发生。我认为我们不应该只是担心事情会不会发生，而是需要开发更好的产品以及处理人工智能可能会发生的事情。未雨绸缪的事情从小我们就知道，所以我相信，我们能研究出他们，那一定会有克制的方法，而不是科幻电影里面的无法控制，但是所有的科幻电影的结局都是无论再怎么坏的影响，都会有比较好的结果，所以，最后的我们凡是都要往好的地方想。</p><p data-pid=""si5PtCuM""><br/>人工智能改变了人们的生活，我们对人工智能应加以好的利用，同时要避免带来的弊端，人工智能与人类、与社会、与自然和谐相处，这样才能长远的发展。</p><p data-pid=""XV53sbiZ"">来源: 奇点</p>"
375549477,0d5f195831e5e99bf216291b05f553a0,算法进阶,https://api.zhihu.com/people/0d5f195831e5e99bf216291b05f553a0,people,,一文概览<em>人工智能</em>(AI)发展历程,article,https://api.zhihu.com/articles/375549477,以一个清晰的脉络呈现出来，以此展望<em>人工智能</em>（<em>AI</em>）未来的趋势。 一、人工智能简介 1.1 <em>人工智能</em>研究目的 人工智能（Artificial Intelligence，AI）研究目的是通过探索智慧的实质,476,5,1058,1622037715,1690867415,"<p data-pid=""FYE32GDE"">如同蒸汽时代的蒸汽机、电气时代的发电机、信息时代的计算机和互联网，人工智能（AI）正赋能各个产业，推动着人类进入智能时代。本文从介绍人工智能及主要的思想派系，进一步系统地梳理了其发展历程、标志性成果并侧重其算法思想介绍，将这段 60余年几经沉浮的历史，以一个清晰的脉络呈现出来，以此<b>展望人工智能（AI）未来的趋势</b>。</p><h2><b>一、人工智能简介</b></h2><h2><b>1.1 人工智能研究目的</b></h2><p data-pid=""ix2uSnnh"">人工智能（Artificial Intelligence，AI）研究目的是通过探索智慧的实质，扩展人类智能——促使智能主体会听（语音识别、机器翻译等）、会看（图像识别、文字识别等）、会说（语音合成、人机对话等）、会思考（人机对弈、专家系统等）、会学习（知识表示，机器学习等）、会行动（机器人、自动驾驶汽车等）。一个经典的AI定义是：<b>“ 智能主体可以理解数据及从中学习，并利用知识实现特定目标和任务的能力。(A system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation)”</b></p><h2><b>1.2 人工智能的学派</b></h2><p data-pid=""dBRDORXg"">在人工智能的发展过程中，不同时代、学科背景的人对于智慧的理解及其实现方法有着不同的思想主张，并由此衍生了不同的学派，影响较大的学派及其代表方法如下：</p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-4e9fc833512fdf7cb375418f7d3184b5_b.jpg"" data-rawwidth=""1082"" data-rawheight=""601"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1082"" data-original=""https://pic2.zhimg.com/v2-4e9fc833512fdf7cb375418f7d3184b5_r.jpg"" data-original-token=""v2-4e9fc833512fdf7cb375418f7d3184b5""/></figure><p class=""ztext-empty-paragraph""><br/></p><p data-pid=""O-1W4QIi"">其中，<b>符号主义及联结主义</b>为主要的两大派系： </p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-7e585fdf1e09ec905d7c8e42e9047556_b.jpg"" data-rawwidth=""1240"" data-rawheight=""434"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic3.zhimg.com/v2-7e585fdf1e09ec905d7c8e42e9047556_r.jpg"" data-original-token=""v2-7e585fdf1e09ec905d7c8e42e9047556""/></figure><p class=""ztext-empty-paragraph""><br/></p><ul><li data-pid=""IT4cyX5G"">“符号主义”（Symbolicism），又称逻辑主义、计算机学派，认为认知就是通过对有意义的表示符号进行推导计算，并将学习视为逆向演绎，主张用显式的公理和逻辑体系搭建人工智能系统。如用决策树模型输入业务特征预测天气： </li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-a6f8456a57504f68106dc8fa4d869895_b.jpg"" data-rawwidth=""743"" data-rawheight=""514"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""743"" data-original=""https://pic2.zhimg.com/v2-a6f8456a57504f68106dc8fa4d869895_r.jpg"" data-original-token=""v2-a6f8456a57504f68106dc8fa4d869895""/></figure><ul><li data-pid=""M3r1nlEN"">“联结主义”（Connectionism），又叫仿生学派，笃信大脑的逆向工程，主张是利用数学模型来研究人类认知的方法，用神经元的连接机制实现人工智能。如用神经网络模型输入雷达图像数据预测天气： </li></ul><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-6251dc6d82906bfd8add4c8a142b8e6b_b.jpg"" data-rawwidth=""685"" data-rawheight=""345"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""685"" data-original=""https://pic4.zhimg.com/v2-6251dc6d82906bfd8add4c8a142b8e6b_r.jpg"" data-original-token=""v2-6251dc6d82906bfd8add4c8a142b8e6b""/></figure><h2><b>二、人工智能发展史</b></h2><p data-pid=""0OcKmhAO"">从始至此，人工智能(AI)便在充满未知的道路探索，曲折起伏，我们可将这段发展历程大致划分为5个阶段期：</p><ul><li data-pid=""awtxB0VG"">起步发展期：1943年—20世纪60年代</li><li data-pid=""4Dlxm-yL"">反思发展期：20世纪70年代</li><li data-pid=""xQlXUl_s"">应用发展期：20世纪80年代</li><li data-pid=""A6NnF0uD"">平稳发展期：20世纪90年代—2010年</li><li data-pid=""AyZAeiKj"">蓬勃发展期：2011年至今</li></ul><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-fdbb40fecf633fc304a342d2ccf51c68_b.jpg"" data-rawwidth=""1200"" data-rawheight=""1016"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1200"" data-original=""https://pic1.zhimg.com/v2-fdbb40fecf633fc304a342d2ccf51c68_r.jpg"" data-original-token=""v2-fdbb40fecf633fc304a342d2ccf51c68""/></figure><h2><b>2.1 起步发展期：1943年—20世纪60年代</b></h2><p data-pid=""F7h5j_nj""><b>人工智能概念的提出后，发展出了符号主义、联结主义(神经网络)，相继取得了一批令人瞩目的研究成果，如机器定理证明、跳棋程序、人机对话等，掀起人工智能发展的第一个高潮。</b></p><ul><li data-pid=""J6FnCsGT"">1943年，美国神经科学家麦卡洛克（Warren McCulloch）和逻辑学家皮茨（Water Pitts）提出神经元的数学模型，这是现代人工智能学科的奠基石之一。  </li><li data-pid=""aBI58WBT"">1950年，艾伦·麦席森·图灵（Alan Mathison Turing）提出“图灵测试”（测试机器是否能表现出与人无法区分的智能），让机器产生智能这一想法开始进入人们的视野。 </li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-a229a48e4569981d9a0baa1fe032492d_b.jpg"" data-rawwidth=""1220"" data-rawheight=""686"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1220"" data-original=""https://pic2.zhimg.com/v2-a229a48e4569981d9a0baa1fe032492d_r.jpg"" data-original-token=""v2-a229a48e4569981d9a0baa1fe032492d""/></figure><ul><li data-pid=""DOJkfxH0"">1950年，克劳德·香农（Claude Shannon）提出计算机博弈。  </li><li data-pid=""DIjkjy9q"">1956年，达特茅斯学院人工智能夏季研讨会上正式使用了人工智能（artificial intelligence，AI）这一术语。这是人类历史上第一次人工智能研讨，标志着人工智能学科的诞生。  </li><li data-pid=""XcCJPjlJ"">1957年,弗兰克·罗森布拉特（Frank Rosenblatt）在一台IBM-704计算机上模拟实现了一种他发明的叫做“感知机”（Perceptron）的神经网络模型。  </li></ul><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-56dd1bbe82c94f7f3d738cf6b3c47d1e_b.jpg"" data-rawwidth=""1240"" data-rawheight=""618"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic3.zhimg.com/v2-56dd1bbe82c94f7f3d738cf6b3c47d1e_r.jpg"" data-original-token=""v2-56dd1bbe82c94f7f3d738cf6b3c47d1e""/></figure><p data-pid=""hGCrfVsB""> 感知机可以被视为一种最简单形式的前馈式人工神经网络，是一种二分类的线性分类判别模型，其输入为实例的特征向量想（x1,x2...），神经元的激活函数f为sign，输出为实例的类别（+1或者-1），模型的目标是要将输入实例通过超平面将正负二类分离。  </p><ul><li data-pid=""PysaFjT5"">1958年，David Cox提出了logistic regression。</li></ul><blockquote data-pid=""ERZcDjSC""> LR是类似于感知机结构的线性分类判别模型，主要不同在于神经元的激活函数f为sigmoid，模型的目标为(最大似然)极大化正确分类概率。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-a5fbd778c9ad888fae6fd643476e0ad6_b.jpg"" data-rawwidth=""485"" data-rawheight=""323"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""485"" data-original=""https://pic3.zhimg.com/v2-a5fbd778c9ad888fae6fd643476e0ad6_r.jpg"" data-original-token=""v2-a5fbd778c9ad888fae6fd643476e0ad6""/></figure><ul><li data-pid=""RnrEYzEv"">1959年，Arthur Samuel给机器学习了一个明确概念：Field of study that gives computers the ability to learn without being explicitly programmed.（机器学习是研究如何让计算机不需要显式的程序也可以具备学习的能力）。  </li><li data-pid=""khbv-Rsr"">1961年，Leonard Merrick Uhr 和 Charles M Vossler发表了题目为A Pattern Recognition Program That Generates, Evaluates and Adjusts its Own Operators 的模式识别论文，该文章描述了一种利用机器学习或自组织过程设计的模式识别程序的尝试。  </li><li data-pid=""8Akb3WZg"">1965年，古德（I. J. Good）发表了一篇对人工智能未来可能对人类构成威胁的文章，可以算“AI威胁论”的先驱。他认为机器的超级智能和无法避免的智能爆炸最终将超出人类可控范畴。后来著名科学家霍金、发明家马斯克等人对人工智能的恐怖预言跟古德半个世界前的警告遥相呼应。  </li><li data-pid=""Srep8pK9"">1966 年，麻省理工学院科学家Joseph Weizenbaum 在 ACM 上发表了题为《ELIZA-a computer program for the study of natural language communication between man and machine》文章描述了ELIZA 的程序如何使人与计算机在一定程度上进行<b>自然语言对话</b>成为可能，ELIZA 的实现技术是通过关键词匹配规则对输入进行分解，而后根据分解规则所对应的重组规则来生成回复。  </li><li data-pid=""Urw4qOjk"">1967年，Thomas等人提出K最近邻算法（The nearest neighbor algorithm）。  </li></ul><blockquote data-pid=""7wLOhzqK""> KNN的核心思想，即给定一个训练数据集，对新的输入实例Xu，在训练数据集中找到与该实例最邻近的K个实例，以这K个实例的最多数所属类别作为新实例Xu的类别。</blockquote><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-0d055ae38d48675e641022ce178398c3_b.jpg"" data-rawwidth=""1240"" data-rawheight=""1071"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-0d055ae38d48675e641022ce178398c3_r.jpg"" data-original-token=""v2-0d055ae38d48675e641022ce178398c3""/></figure><ul><li data-pid=""ML-AxnL_"">1968年，爱德华·费根鲍姆（Edward Feigenbaum）提出首个专家系统DENDRAL，并对知识库给出了初步的定义，这也孕育了后来的第二次人工智能浪潮。该系统具有非常丰富的化学知识，可根据质谱数据帮助化学家推断分子结构。</li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-da228c21fac39a291f87d12b0863de95_b.jpg"" data-rawwidth=""1080"" data-rawheight=""439"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-da228c21fac39a291f87d12b0863de95_r.jpg"" data-original-token=""v2-da228c21fac39a291f87d12b0863de95""/></figure><p data-pid=""PG3jMwhM"">专家系统（Expert Systems）是AI的一个重要分支，同自然语言理解，机器人学并列为AI的三大研究方向。它的定义是使用人类专家推理的计算机模型来处理现实世界中需要专家作出解释的复杂问题，并得出与专家相同的结论，可视作“知识库(knowledge base)”和“推理机(inference machine)” 的结合。  </p><ul><li data-pid=""GvlO7GMP"">1969年，“符号主义”代表人物马文·明斯基（Marvin Minsky）的著作《感知器》提出对XOR线性不可分的问题：单层感知器无法划分XOR原数据，解决这问题需要引入更高维非线性网络（MLP,  至少需要两层），但多层网络并无有效的训练算法。这些论点给神经网络研究以沉重的打击，神经网络的研究走向长达10年的低潮时期。</li></ul><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-b787321714493a634eb63efcf2e88420_b.jpg"" data-rawwidth=""1212"" data-rawheight=""626"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1212"" data-original=""https://pic1.zhimg.com/v2-b787321714493a634eb63efcf2e88420_r.jpg"" data-original-token=""v2-b787321714493a634eb63efcf2e88420""/></figure><h2><b>2.2 反思发展期：20世纪70年代</b></h2><p data-pid=""1PElwAYq""><b>人工智能发展初期的突破性进展大大提升了人们对人工智能的期望，人们开始尝试更具挑战性的任务，然而计算力及理论等的匮乏使得不切实际目标的落空，人工智能的发展走入低谷。</b></p><ul><li data-pid=""ruGDM_Cm"">1974年，哈佛大学沃伯斯(Paul Werbos)博士论文里，首次提出了通过误差的反向传播(BP)来训练人工神经网络，但在该时期未引起重视。</li></ul><blockquote data-pid=""8ZY_EDtM""> BP算法的基本思想不是（如感知器那样）用误差本身去调整权重，而是用误差的导数（梯度）调整。通过误差的梯度做反向传播，更新模型权重,  以下降学习的误差，拟合学习目标，实现&#39;网络的万能近似功能&#39;的过程。</blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-cf4ececf831b160bb3a69f5d581bf876_b.jpg"" data-rawwidth=""815"" data-rawheight=""447"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""815"" data-original=""https://pic3.zhimg.com/v2-cf4ececf831b160bb3a69f5d581bf876_r.jpg"" data-original-token=""v2-cf4ececf831b160bb3a69f5d581bf876""/></figure><ul><li data-pid=""_lyfsTQX"">1975年，马文·明斯基(Marvin Minsky)在论文《知识表示的框架》(A Framework for Representing Knowledge)中提出用于人工智能中的知识表示学习框架理论。  </li><li data-pid=""pPGTZXdW"">1976年，兰德尔·戴维斯（Randall Davis）构建和维护的大规模的知识库，提出使用集成的面向对象模型可以提高知识库（KB）开发、维护和使用的完整性。  </li><li data-pid=""wydNOK48"">1976年，斯坦福大学的肖特利夫(Edward H. Shortliffe)等人完成了第一个用于血液感染病的诊断、治疗和咨询服务的医疗专家系统MYCIN。  </li><li data-pid=""O6fHUzcX"">1976年，斯坦福大学的博士勒纳特发表论文《数学中发现的人工智能方法——启发式搜索》，描述了一个名为“AM”的程序，在大量启发式规则的指导下开发新概念数学，最终重新发现了数百个常见的概念和定理。  </li><li data-pid=""hWzXYXHR"">1977年，海斯·罗思(Hayes. Roth)等人的基于逻辑的机器学习系统取得较大的进展，但只能学习单一概念，也未能投入实际应用。  </li><li data-pid=""Ze5htyXq"">1979年，汉斯·贝利纳（Hans Berliner）打造的计算机程序战胜双陆棋世界冠军成为标志性事件。(随后，基于行为的机器人学在罗德尼·布鲁克斯和萨顿等人的推动下快速发展，成为人工智能一个重要的发展分支。格瑞·特索罗等人打造的自我学习双陆棋程序又为后来的强化学习的发展奠定了基础。)  </li></ul><h2><b>2.3 应用发展期：20世纪80年代</b></h2><p data-pid=""P7gCzg_M""><b>人工智能走入应用发展的新高潮。专家系统模拟人类专家的知识和经验解决特定领域的问题，实现了人工智能从理论研究走向实际应用、从一般推理策略探讨转向运用专门知识的重大突破。而机器学习(特别是神经网络)探索不同的学习策略和各种学习方法，在大量的实际应用中也开始慢慢复苏。</b></p><ul><li data-pid=""5_5tzV9N"">1980年，在美国的卡内基梅隆大学(CMU)召开了第一届机器学习国际研讨会，标志着机器学习研究已在全世界兴起。  </li><li data-pid=""NtAZR7kb"">1980年，德鲁·麦狄蒙（Drew McDermott）和乔恩·多伊尔（Jon Doyle）提出非单调逻辑，以及后期的机器人系统。  </li><li data-pid=""YJICrGsF"">1980年，卡耐基梅隆大学为DEC公司开发了一个名为XCON的专家系统，每年为公司节省四千万美元，取得巨大成功。  </li><li data-pid=""dJorBONw"">1981年，保罗（R.P.Paul）出版第一本机器人学课本，“Robot Manipulator：Mathematics，Programmings and Control”，标志着机器人学科走向成熟。  </li><li data-pid=""ZdqMM3n1"">1982年，马尔（David Marr）发表代表作《视觉计算理论》提出计算机视觉（Computer Vision）的概念，并构建系统的视觉理论，对认知科学（CognitiveScience）也产生了很深远的影响。  </li><li data-pid=""z8a4gvRE"">1982年，约翰·霍普菲尔德（John Hopfield） 发明了霍普菲尔德网络，这是最早的RNN的雏形。霍普菲尔德神经网络模型是一种单层反馈神经网络（神经网络结构主要可分为前馈神经网络、反馈神经网络及图网络），从输出到输入有反馈连接。它的出现振奋了神经网络领域，在人工智能之机器学习、联想记忆、模式识别、优化计算、VLSI和光学设备的并行实现等方面有着广泛应用。 </li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-80d12508254dd8aed3ab6708cc936185_b.jpg"" data-rawwidth=""850"" data-rawheight=""625"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""850"" data-original=""https://pic2.zhimg.com/v2-80d12508254dd8aed3ab6708cc936185_r.jpg"" data-original-token=""v2-80d12508254dd8aed3ab6708cc936185""/></figure><ul><li data-pid=""k-77EyF1"">1983年，Terrence Sejnowski, Hinton等人发明了玻尔兹曼机（Boltzmann Machines），也称为随机霍普菲尔德网络，它本质是一种无监督模型，用于对输入数据进行重构以提取数据特征做预测分析。  </li><li data-pid=""pARgh1Pg"">1985年，朱迪亚·珀尔提出贝叶斯网络(Bayesian network)，他以倡导人工智能的概率方法和发展贝叶斯网络而闻名，还因发展了一种基于结构模型的因果和反事实推理理论而受到赞誉。  </li></ul><blockquote data-pid=""_9ReNlaq""> 贝叶斯网络是一种模拟人类推理过程中因果关系的不确定性处理模型，如常见的朴素贝叶斯分类算法就是贝叶斯网络最基本的应用。 </blockquote><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-b879625acd95e9c125d81c967f24cbe1_b.jpg"" data-rawwidth=""800"" data-rawheight=""494"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic2.zhimg.com/v2-b879625acd95e9c125d81c967f24cbe1_r.jpg"" data-original-token=""v2-b879625acd95e9c125d81c967f24cbe1""/></figure><p data-pid=""EoA9kUdr"">贝叶斯网络拓朴结构是一个有向无环图(DAG)，通过把某个研究系统中涉及的随机变量，根据是否条件独立绘制在一个有向图中，以描述随机变量之间的条件依赖，用圈表示随机变量(random variables)，用箭头表示条件依赖(conditional dependencies)就形成了贝叶斯网络。 对于任意的随机变量，其联合概率可由各自的局部条件概率分布相乘而得出。如图中b依赖于a(即：a-&gt;b)，c依赖于a和b，a独立无依赖，根据贝叶斯定理有 P(a,b,c) = P(a)*P(b|a)*P(c|a,b) </p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-f2ca1ef23b299c216eb96765393ead63_b.jpg"" data-rawwidth=""260"" data-rawheight=""254"" data-size=""normal"" data-caption="""" class=""content_image"" width=""260"" data-original-token=""v2-f2ca1ef23b299c216eb96765393ead63""/></figure><ul><li data-pid=""zEXCVtzc"">1986年，罗德尼·布鲁克斯(Brooks)发表论文《移动机器人鲁棒分层控制系统》，标志着基于行为的机器人学科的创立，机器人学界开始把注意力投向实际工程主题。  </li><li data-pid=""hPiYdtgx"">1986年，辛顿(Geoffrey Hinton)等人先后提出了多层感知器(MLP)与反向传播（BP）训练相结合的理念（该方法在当时计算力上还是有很多挑战，基本上都是和链式求导的梯度算法相关的），这也解决了单层感知器不能做非线性分类的问题，开启了神经网络新一轮的高潮。 </li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-51edebbb2259d0f3b8ec667c76b68f65_b.jpg"" data-rawwidth=""1240"" data-rawheight=""698"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic2.zhimg.com/v2-51edebbb2259d0f3b8ec667c76b68f65_r.jpg"" data-original-token=""v2-51edebbb2259d0f3b8ec667c76b68f65""/></figure><ul><li data-pid=""_vLTcFrf"">1986年，昆兰（Ross Quinlan）提出ID3决策树算法。  </li></ul><blockquote data-pid=""ZIqdaJAR""> 决策树模型可视为多个规则(if, then)的组合，与神经网络黑盒模型截然不同是，它拥有良好的模型解释性。 </blockquote><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-b5edd21a2b88739ada3c9f4ac2994490_b.jpg"" data-rawwidth=""752"" data-rawheight=""552"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""752"" data-original=""https://pic1.zhimg.com/v2-b5edd21a2b88739ada3c9f4ac2994490_r.jpg"" data-original-token=""v2-b5edd21a2b88739ada3c9f4ac2994490""/></figure><p data-pid=""xveRLK-t"">ID3算法核心的思想是通过自顶向下的贪心策略构建决策树：根据信息增益来选择特征进行划分（信息增益的含义是 引入属性A的信息后，数据D的不确定性减少程度。也就是信息增益越大，区分D的能力就越强)，依次递归地构建决策树。  </p><ul><li data-pid=""MOgxmqTq"">1989年，George Cybenko证明了“万能近似定理”（universal approximation theorem）。简单来说，多层前馈网络可以近似任意函数，其表达力和图灵机等价。这就从根本上消除了Minsky对神经网络表达力的质疑。</li></ul><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-538228b6f0ca45ae7b358cd8eab76ede_b.jpg"" data-rawwidth=""784"" data-rawheight=""613"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""784"" data-original=""https://pic3.zhimg.com/v2-538228b6f0ca45ae7b358cd8eab76ede_r.jpg"" data-original-token=""v2-538228b6f0ca45ae7b358cd8eab76ede""/></figure><p data-pid=""bI2CdDWs"">“万能近似定理”可视为神经网络的基本理论：⼀个前馈神经⽹络如果具有线性层和⾄少⼀层具有 “挤压” 性质的激活函数（如 sigmoid 等），给定⽹络⾜够数量的隐藏单元，它可以以任意精度来近似任何从⼀个有限维空间到另⼀个有限维空间的 borel 可测函数。  </p><ul><li data-pid=""i-vuG1wp"">1989年，LeCun (CNN之父) 结合反向传播算法与权值共享的卷积神经层发明了卷积神经网络（Convolutional Neural Network，CNN），并首次将卷积神经网络成功应用到美国邮局的手写字符识别系统中。</li></ul><blockquote data-pid=""h-usPD8i""> 卷积神经网络通常由输入层、卷积层、池化（Pooling）层和全连接层组成。卷积层负责提取图像中的局部特征，池化层用来大幅降低参数量级(降维)，全连接层类似传统神经网络的部分，用来输出想要的结果。 </blockquote><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-04ca66f198c1e79aa2947618e1371f58_b.jpg"" data-rawwidth=""1106"" data-rawheight=""761"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1106"" data-original=""https://pic1.zhimg.com/v2-04ca66f198c1e79aa2947618e1371f58_r.jpg"" data-original-token=""v2-04ca66f198c1e79aa2947618e1371f58""/></figure><h2><b>2.4 平稳发展期：20世纪90年代—2010年</b></h2><p data-pid=""wQriHSgx""><b>由于互联网技术的迅速发展，加速了人工智能的创新研究，促使人工智能技术进一步走向实用化，人工智能相关的各个领域都取得长足进步。在2000年代初，由于专家系统的项目都需要编码太多的显式规则，这降低了效率并增加了成本，人工智能研究的重心从基于知识系统转向了机器学习方向。</b></p><ul><li data-pid=""OG2hMggV"">1995年，Cortes和Vapnik提出联结主义经典的支持向量机(Support Vector Machine)，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。</li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-7c0d7ec41ec823366287c59fc864c63d_b.jpg"" data-rawwidth=""1240"" data-rawheight=""757"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic2.zhimg.com/v2-7c0d7ec41ec823366287c59fc864c63d_r.jpg"" data-original-token=""v2-7c0d7ec41ec823366287c59fc864c63d""/></figure><p data-pid=""uChnCt3X"">支持向量机（Support Vector Machine, SVM）可以视为在感知机基础上的改进，是建立在统计学习理论的VC维理论和结构风险最小原理基础上的广义线性分类器。与感知机主要差异在于：1、感知机目标是找到一个超平面将各样本尽可能分离正确(有无数个)，SVM目标是找到一个超平面不仅将各样本尽可能分离正确，还要使各样本离超平面距离最远(只有一个最大边距超平面)，SVM的泛化能力更强。2、对于线性不可分的问题，不同于感知机的增加非线性隐藏层，SVM利用核函数，本质上都是实现特征空间非线性变换，使可以被线性分类。</p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-9ededeb8abe521e365699dc7242827c4_b.jpg"" data-rawwidth=""922"" data-rawheight=""812"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""922"" data-original=""https://pic1.zhimg.com/v2-9ededeb8abe521e365699dc7242827c4_r.jpg"" data-original-token=""v2-9ededeb8abe521e365699dc7242827c4""/></figure><ul><li data-pid=""0qxmI-xm"">1995年， Freund和schapire提出了 AdaBoost (Adaptive Boosting)算法。AdaBoost采用的是Boosting集成学习方法——串行组合弱学习器以达到更好的泛化性能。另外一种重要集成方法是以随机森林为代表的Bagging并行组合的方式。以“偏差-方差分解”分析，Boosting方法主要优化偏差，Bagging主要优化方差。</li></ul><blockquote data-pid=""a9pCvdOd""> Adaboost迭代算法基本思想主要是通过调节的每一轮各训练样本的权重(错误分类的样本权重更高)，串行训练出不同分类器。最终以各分类器的准确率作为其组合的权重，一起加权组合成强分类器。 </blockquote><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-9c8c727bf3be55b967eaededdde036df_b.jpg"" data-rawwidth=""702"" data-rawheight=""372"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""702"" data-original=""https://pic4.zhimg.com/v2-9c8c727bf3be55b967eaededdde036df_r.jpg"" data-original-token=""v2-9c8c727bf3be55b967eaededdde036df""/></figure><ul><li data-pid=""CLvuYuMo"">1997年国际商业机器公司（简称IBM）深蓝超级计算机战胜了国际象棋世界冠军卡斯帕罗夫。深蓝是基于暴力穷举实现国际象棋领域的智能，通过生成所有可能的走法，然后执行尽可能深的搜索，并不断对局面进行评估，尝试找出最佳走法。  </li><li data-pid=""Z47Dywmr"">1997年，Sepp Hochreiter 和 Jürgen Schmidhuber提出了长短期记忆神经网络(LSTM)。  </li></ul><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-c4e5676cd69b0348c63ffcac29c35415_b.jpg"" data-rawwidth=""1240"" data-rawheight=""466"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic2.zhimg.com/v2-c4e5676cd69b0348c63ffcac29c35415_r.jpg"" data-original-token=""v2-c4e5676cd69b0348c63ffcac29c35415""/></figure><p data-pid=""BJ08x2Oo"">LSTM是一种复杂结构的循环神经网络（RNN），结构上引入了遗忘门、输入门及输出门：输入门决定当前时刻网络的输入数据有多少需要保存到单元状态，遗忘门决定上一时刻的单元状态有多少需要保留到当前时刻，输出门控制当前单元状态有多少需要输出到当前的输出值。这样的结构设计可以解决长序列训练过程中的梯度消失问题。  </p><ul><li data-pid=""HjNSZyBg"">1998年，万维网联盟的蒂姆·伯纳斯·李(Tim Berners-Lee)提出语义网(Semantic  Web)的概念。其核心思想是：通过给万维网上的文档（如HTML）添加能够被计算机所理解的语义(Meta data)，从而使整个互联网成为一个基于语义链接的通用信息交换媒介。换言之，就是构建一个能够实现人与电脑无障碍沟通的智能网络。  </li><li data-pid=""aT75Dzhd"">2001年，John Lafferty首次提出条件随机场模型（Conditional random field，CRF）。 CRF是基于贝叶斯理论框架的判别式概率图模型，在给定条件随机场P ( Y ∣ X ) 和输入序列x，求条件概率最大的输出序列y *。在许多自然语言处理任务中比如分词、命名实体识别等表现尤为出色。  </li><li data-pid=""4t5OysEv"">2001年，布雷曼博士提出随机森林（Random Forest）。 随机森林是将多个有差异的弱学习器(决策树)Bagging并行组合，通过建立多个的拟合较好且有差异模型去组合决策，以优化泛化性能的一种集成学习方法。多样差异性可减少对某些特征噪声的依赖，降低方差（过拟合），组合决策可消除些学习器间的偏差。  </li></ul><blockquote data-pid=""rgydHgyK""> 随机森林算法的基本思路是对于每一弱学习器(决策树)有放回的抽样构造其训练集，并随机抽取其可用特征子集，即以训练样本及特征空间的多样性训练出N个不同的弱学习器，最终结合N个弱学习器的预测（类别或者回归预测数值），取最多数类别或平均值作为最终结果。 </blockquote><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-8f80442ad6eca93e6e11c82b48263e27_b.jpg"" data-rawwidth=""595"" data-rawheight=""460"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""595"" data-original=""https://pic4.zhimg.com/v2-8f80442ad6eca93e6e11c82b48263e27_r.jpg"" data-original-token=""v2-8f80442ad6eca93e6e11c82b48263e27""/></figure><ul><li data-pid=""_e4QrqBD"">2003年，David Blei, Andrew Ng和 Michael I. Jordan于2003年提出LDA（Latent Dirichlet Allocation）。</li></ul><blockquote data-pid=""q9aFi8En""> LDA是一种无监督方法，用来推测文档的主题分布，将文档集中每篇文档的主题以概率分布的形式给出，可以根据主题分布进行主题聚类或文本分类。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-b5b349f42db9f9472e153723ec4c9e9a_b.jpg"" data-rawwidth=""850"" data-rawheight=""524"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""850"" data-original=""https://pic3.zhimg.com/v2-b5b349f42db9f9472e153723ec4c9e9a_r.jpg"" data-original-token=""v2-b5b349f42db9f9472e153723ec4c9e9a""/></figure><ul><li data-pid=""TTqMTBpJ"">2003年，Google公布了3篇大数据奠基性论文，为大数据存储及分布式处理的核心问题提供了思路：非结构化文件分布式存储（GFS）、分布式计算（MapReduce）及结构化数据存储（BigTable），并奠定了现代大数据技术的理论基础。 </li></ul><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-b80bb89b291d545624552928e316c5bc_b.jpg"" data-rawwidth=""816"" data-rawheight=""639"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""816"" data-original=""https://pic1.zhimg.com/v2-b80bb89b291d545624552928e316c5bc_r.jpg"" data-original-token=""v2-b80bb89b291d545624552928e316c5bc""/></figure><ul><li data-pid=""pWFnDadd"">2005 年，波士顿动力公司推出一款动力平衡四足机器狗，有较强的通用性，可适应较复杂的地形。  </li><li data-pid=""33I_tXG0"">2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念（Deeping Learning），开启了深度学习在学术界和工业界的浪潮。2006年也被称为深度学习元年，杰弗里·辛顿也因此被称为深度学习之父。  </li></ul><blockquote data-pid=""tql7esqA""> 深度学习的概念源于人工神经网络的研究，它的本质是使用多个隐藏层网络结构，通过大量的向量计算，学习数据内在信息的高阶表示。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-aa1096df4b04824dba62eb8c67344376_b.jpg"" data-rawwidth=""640"" data-rawheight=""640"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic3.zhimg.com/v2-aa1096df4b04824dba62eb8c67344376_r.jpg"" data-original-token=""v2-aa1096df4b04824dba62eb8c67344376""/></figure><ul><li data-pid=""G3t0l_Pm"">2010年，Sinno Jialin Pan和 Qiang Yang发表文章《迁移学习的调查》。</li></ul><blockquote data-pid=""hgcT2wH9""> 迁移学习(transfer learning)通俗来讲，就是运用已有的知识（如训练好的网络权重）来学习新的知识以适应特定目标任务，核心是找到已有知识和新知识之间的相似性。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-bb02d30958961262eff777aa88ad42c2_b.jpg"" data-rawwidth=""1024"" data-rawheight=""586"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1024"" data-original=""https://pic3.zhimg.com/v2-bb02d30958961262eff777aa88ad42c2_r.jpg"" data-original-token=""v2-bb02d30958961262eff777aa88ad42c2""/></figure><h2><b>2.5 蓬勃发展期：2011年至今</b></h2><p data-pid=""mX0cwfNF""><b>随着大数据、云计算、互联网、物联网等信息技术的发展，泛在感知数据和图形处理器等计算平台推动以深度神经网络为代表的人工智能技术飞速发展，大幅跨越了科学与应用之间的技术鸿沟，诸如图像分类、语音识别、知识问答、人机对弈、无人驾驶等人工智能技术实现了重大的技术突破，迎来爆发式增长的新高潮。</b></p><ul><li data-pid=""O3vgQQuh"">2011年，IBM Watson问答机器人参与Jeopardy回答测验比赛最终赢得了冠军。Waston是一个集自然语言处理、知识表示、自动推理及机器学习等技术实现的电脑问答（Q&amp;A）系统。  </li><li data-pid=""k1WngDhA"">2012年，Hinton和他的学生Alex Krizhevsky设计的AlexNet神经网络模型在ImageNet竞赛大获全胜，这是史上第一次有模型在 ImageNet 数据集表现如此出色，并引爆了神经网络的研究热情。  </li></ul><blockquote data-pid=""89lyYMem""> AlexNet是一个经典的CNN模型，在数据、算法及算力层面均有较大改进，创新地应用了Data Augmentation、ReLU、Dropout和LRN等方法，并使用GPU加速网络训练。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-cd26342b8cede7ab4cde5315065f3fa6_b.jpg"" data-rawwidth=""944"" data-rawheight=""480"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""944"" data-original=""https://pic3.zhimg.com/v2-cd26342b8cede7ab4cde5315065f3fa6_r.jpg"" data-original-token=""v2-cd26342b8cede7ab4cde5315065f3fa6""/></figure><ul><li data-pid=""ol_jFCfW"">2012年，谷歌正式发布谷歌知识图谱Google Knowledge Graph），它是Google的一个从多种信息来源汇集的知识库，通过Knowledge Graph来在普通的字串搜索上叠一层相互之间的关系，协助使用者更快找到所需的资料的同时，也可以知识为基础的搜索更近一步，以提高Google搜索的质量。</li></ul><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-fd739ec9217e0de5930e693e809d386f_b.jpg"" data-rawwidth=""1240"" data-rawheight=""698"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-fd739ec9217e0de5930e693e809d386f_r.jpg"" data-original-token=""v2-fd739ec9217e0de5930e693e809d386f""/></figure><p class=""ztext-empty-paragraph""><br/></p><blockquote data-pid=""XLnuKVyJ""> 知识图谱是结构化的语义知识库，是符号主义思想的代表方法，用于以符号形式描述物理世界中的概念及其相互关系。其通用的组成单位是RDF三元组(实体-关系-实体)，实体间通过关系相互联结，构成网状的知识结构。</blockquote><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-056d8a437c163f87d1d4679639b67a34_b.jpg"" data-rawwidth=""913"" data-rawheight=""353"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""913"" data-original=""https://pic1.zhimg.com/v2-056d8a437c163f87d1d4679639b67a34_r.jpg"" data-original-token=""v2-056d8a437c163f87d1d4679639b67a34""/></figure><ul><li data-pid=""jEWJVNxc"">2013年，Durk Kingma和Max Welling在ICLR上以文章《Auto-Encoding Variational Bayes》提出变分自编码器（Variational Auto-Encoder，VAE）。</li></ul><blockquote data-pid=""xhfSkxjk""> VAE基本思路是将真实样本通过编码器网络变换成一个理想的数据分布，然后把数据分布再传递给解码器网络，构造出生成样本，模型训练学习的过程是使生成样本与真实样本足够接近。 </blockquote><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-d5892a66f5d42d01374000a6af8f6045_b.jpg"" data-rawwidth=""634"" data-rawheight=""244"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""634"" data-original=""https://pic2.zhimg.com/v2-d5892a66f5d42d01374000a6af8f6045_r.jpg"" data-original-token=""v2-d5892a66f5d42d01374000a6af8f6045""/></figure><ul><li data-pid=""d9NuHXOJ"">2013年，Google的Tomas Mikolov 在《Efficient Estimation of Word Representation in Vector Space》提出经典的 Word2Vec模型用来学习单词分布式表示，因其简单高效引起了工业界和学术界极大的关注。 </li></ul><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-5bd1bb430318210df294efe4745b3fd4_b.jpg"" data-rawwidth=""682"" data-rawheight=""388"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""682"" data-original=""https://pic1.zhimg.com/v2-5bd1bb430318210df294efe4745b3fd4_r.jpg"" data-original-token=""v2-5bd1bb430318210df294efe4745b3fd4""/></figure><p class=""ztext-empty-paragraph""><br/></p><blockquote data-pid=""TOzj0loh""> Word2Vec基本的思想是学习每个单词与邻近词的关系，从而将单词表示成低维稠密向量。通过这样的分布式表示可以学习到单词的语义信息，直观来看，语义相似的单词的距离相近。 </blockquote><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-cab9314496d1ca19d085fde8f8f13ad3_b.jpg"" data-rawwidth=""1240"" data-rawheight=""654"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-cab9314496d1ca19d085fde8f8f13ad3_r.jpg"" data-original-token=""v2-cab9314496d1ca19d085fde8f8f13ad3""/></figure><p data-pid=""im9KwcY2""> Word2Vec网络结构是一个浅层神经网络（输入层-线性全连接隐藏层-&gt;输出层），按训练学习方式可分为CBOW模型(以一个词语作为输入，来预测它的邻近词)或Skip-gram模型 (以一个词语的邻近词作为输入，来预测这个词语)。 </p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-0d8da46867bfec269f6ffa4769415961_b.jpg"" data-rawwidth=""720"" data-rawheight=""371"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""720"" data-original=""https://pic2.zhimg.com/v2-0d8da46867bfec269f6ffa4769415961_r.jpg"" data-original-token=""v2-0d8da46867bfec269f6ffa4769415961""/></figure><ul><li data-pid=""ed-t1PTv"">2014年，聊天程序“尤金·古斯特曼”（Eugene Goostman）在英国皇家学会举行的“2014图灵测试”大会上，首次“通过”了图灵测试。  </li><li data-pid=""TJw3zjm4"">2014年，Goodfellow及Bengio等人提出生成对抗网络（Generative Adversarial Network，GAN），被誉为近年来最酷炫的神经网络。  </li></ul><blockquote data-pid=""khPprXQ4""> GAN是基于强化学习(RL)思路设计的，由生成网络(Generator, G)和判别网络(Discriminator, D)两部分组成， 生成网络构成一个映射函数G: Z→X（输入噪声z, 输出生成的伪造数据x）, 判别网络判别输入是来自真实数据还是生成网络生成的数据。在这样训练的博弈过程中，提高两个模型的生成能力和判别能力。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-ef4fe61ad1a28256abef242f3889ba06_b.jpg"" data-rawwidth=""1063"" data-rawheight=""295"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1063"" data-original=""https://pic3.zhimg.com/v2-ef4fe61ad1a28256abef242f3889ba06_r.jpg"" data-original-token=""v2-ef4fe61ad1a28256abef242f3889ba06""/></figure><ul><li data-pid=""pwczthdz"">2015年，为纪念人工智能概念提出60周年，深度学习三巨头LeCun、Bengio和Hinton(他们于2018年共同获得了图灵奖)推出了深度学习的联合综述《Deep learning》。</li></ul><blockquote data-pid=""Xh7Z59Ym""> 《Deep learning》文中指出深度学习就是一种特征学习方法，把原始数据通过一些简单的但是非线性的模型转变成为更高层次及抽象的表达，能够强化输入数据的区分能力。通过足够多的转换的组合，非常复杂的函数也可以被学习。 </blockquote><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-debf8c5bcd93ae1770dbb04030aafc27_b.jpg"" data-rawwidth=""1240"" data-rawheight=""478"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-debf8c5bcd93ae1770dbb04030aafc27_r.jpg"" data-original-token=""v2-debf8c5bcd93ae1770dbb04030aafc27""/></figure><ul><li data-pid=""OO01JfUO"">2015年，Microsoft Research的Kaiming He等人提出的残差网络（ResNet）在ImageNet大规模视觉识别竞赛中获得了图像分类和物体识别的优胜。</li></ul><blockquote data-pid=""ZYi41QCE""> 残差网络的主要贡献是发现了网络不恒等变换导致的“退化现象（Degradation）”，并针对退化现象引入了 “快捷连接（Shortcut connection）”，缓解了在深度神经网络中增加深度带来的梯度消失问题。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-b98a13a7300cc12c0c4c336fe6aef002_b.jpg"" data-rawwidth=""680"" data-rawheight=""360"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""680"" data-original=""https://pic3.zhimg.com/v2-b98a13a7300cc12c0c4c336fe6aef002_r.jpg"" data-original-token=""v2-b98a13a7300cc12c0c4c336fe6aef002""/></figure><ul><li data-pid=""Qnvuo1yR"">2015年，谷歌开源TensorFlow框架。它是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning）算法的编程实现，其前身是谷歌的神经网络算法库DistBelief。  </li><li data-pid=""TiNizp9M"">2015年，马斯克等人共同创建OpenAI。它是一个非营利的研究组织，使命是确保通用人工智能 (即一种高度自主且在大多数具有经济价值的工作上超越人类的系统）将为全人类带来福祉。其发布热门产品的如：OpenAI Gym，GPT等。  </li><li data-pid=""CoVC9a0E"">2016年，谷歌提出联邦学习方法，它在多个持有本地数据样本的分散式边缘设备或服务器上训练算法，而不交换其数据样本。  </li></ul><blockquote data-pid=""CKEeyPfe""> 联邦学习保护隐私方面最重要的三大技术分别是： 差分隐私 ( Differential Privacy )、同态加密 ( Homomorphic Encryption )和 隐私保护集合交集 ( Private Set Intersection )，能够使多个参与者在不共享数据的情况下建立一个共同的、强大的机器学习模型，从而解决数据隐私、数据安全、数据访问权限和异构数据的访问等关键问题。 </blockquote><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-a6c38507ea78d369be2f97d9b4c80049_b.jpg"" data-rawwidth=""1240"" data-rawheight=""598"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic2.zhimg.com/v2-a6c38507ea78d369be2f97d9b4c80049_r.jpg"" data-original-token=""v2-a6c38507ea78d369be2f97d9b4c80049""/></figure><ul><li data-pid=""Sdb0kRL_"">2016年，AlphaGo与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜。</li></ul><blockquote data-pid=""oQTNpReq""> AlphaGo是一款围棋人工智能程序，其主要工作原理是“深度学习”，由以下四个主要部分组成：策略网络（Policy Network）给定当前局面，预测并采样下一步的走棋；快速走子（Fast rollout）目标和策略网络一样，但在适当牺牲走棋质量的条件下，速度要比策略网络快1000倍；价值网络（Value Network）估算当前局面的胜率；蒙特卡洛树搜索（Monte Carlo Tree Search）树搜索估算每一种走法的胜率。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-4964b2a2af182e19857f4ec09c50ee0e_b.jpg"" data-rawwidth=""1240"" data-rawheight=""457"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic3.zhimg.com/v2-4964b2a2af182e19857f4ec09c50ee0e_r.jpg"" data-original-token=""v2-4964b2a2af182e19857f4ec09c50ee0e""/></figure><p data-pid=""BKpsT2ov"">在2017年更新的AlphaGo Zero，在此前的版本的基础上，结合了强化学习进行了自我训练。它在下棋和游戏前完全不知道游戏规则，完全是通过自己的试验和摸索，洞悉棋局和游戏的规则，形成自己的决策。随着自我博弈的增加，神经网络逐渐调整，提升下法胜率。更为厉害的是，随着训练的深入，AlphaGo Zero还独立发现了游戏规则，并走出了新策略，为围棋这项古老游戏带来了新的见解。</p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-6d8522eea18dc6a4f5d79ddd34f823b7_b.jpg"" data-rawwidth=""1240"" data-rawheight=""851"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-6d8522eea18dc6a4f5d79ddd34f823b7_r.jpg"" data-original-token=""v2-6d8522eea18dc6a4f5d79ddd34f823b7""/></figure><ul><li data-pid=""1iDMT_A7"">2017年，中国香港的汉森机器人技术公司（Hanson Robotics）开发的类人机器人索菲亚，是历史上首个获得公民身份的一台机器人。索菲亚看起来就像人类女性，拥有橡胶皮肤，能够表现出超过62种自然的面部表情。其“大脑”中的算法能够理解语言、识别面部，并与人进行互动。  </li><li data-pid=""03W6PvOl"">2018年，Google提出论文《Pre-training of Deep Bidirectional Transformers for Language Understanding》并发布Bert(Bidirectional Encoder Representation from Transformers)模型，成功在 11 项 NLP 任务中取得 state of the art 的结果。  </li></ul><blockquote data-pid=""jctyLE4U""> BERT是一个预训练的语言表征模型，可在海量的语料上用无监督学习方法学习单词的动态特征表示。它基于Transformer注意力机制的模型，对比RNN可以更加高效、能捕捉更长距离的依赖信息，且不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的masked language model（MLM），以致能生成深度的双向语言表征。 </blockquote><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-5bb90b334ffc9c8c5836f4b3ab302f9a_b.jpg"" data-rawwidth=""1238"" data-rawheight=""492"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1238"" data-original=""https://pic3.zhimg.com/v2-5bb90b334ffc9c8c5836f4b3ab302f9a_r.jpg"" data-original-token=""v2-5bb90b334ffc9c8c5836f4b3ab302f9a""/></figure><ul><li data-pid=""E_QhmuiL"">2019年， IBM宣布推出Q System One，它是世界上第一个专为科学和商业用途设计的集成通用近似量子计算系统。  </li><li data-pid=""gBNJwsst"">2019年，香港 Insilico Medicine 公司和多伦多大学的研究团队实现了重大实验突破，通过深度学习和生成模型相关的技术发现了几种候选药物，证明了 AI 发现分子策略的有效性，很大程度解决了传统新药开发在分子鉴定困难且耗时的问题。  </li><li data-pid=""4KM2r1bo"">2020年，Google与Facebook分别提出SimCLR与MoCo两个无监督学习算法，均能够在无标注数据上学习图像数据表征。两个算法背后的框架都是对比学习（contrastive learning），对比学习的核心训练信号是图片的“可区分性”。  </li><li data-pid=""LrNB-VFe"">2020年，OpenAI开发的文字生成 (text generation) 人工智能GPT-3，它具有1,750亿个参数的自然语言深度学习模型，比以前的版本GPT-2高100倍，该模型经过了将近0.5万亿个单词的预训练，可以在多个NLP任务（答题、翻译、写文章）基准上达到最先进的性能。  </li><li data-pid=""bHJYo-QA"">2020年，马斯克的脑机接口（brain–computer interface, BCI）公司Neuralink举行现场直播，展示了植入Neuralink设备的实验猪的脑部活动。  </li><li data-pid=""J7DDGuLT"">2020年，谷歌旗下DeepMind的AlphaFold2人工智能系统有力地解决了蛋白质结构预测的里程碑式问题。它在国际蛋白质结构预测竞赛（CASP）上击败了其余的参会选手，精确预测了蛋白质的三维结构，准确性可与冷冻电子显微镜（cryo-EM）、核磁共振或 X 射线晶体学等实验技术相媲美。  </li><li data-pid=""pz5z5hD9"">2020年，中国科学技术大学潘建伟等人成功构建76个光子的量子计算原型机“九章”，求解数学算法“高斯玻色取样”只需200秒，而目前世界最快的超级计算机要用6亿年。  </li><li data-pid=""MRx-iWyX"">2021年，OpenAI提出两个连接文本与图像的神经网络：DALL·E 和 CLIP。DALL·E 可以基于文本直接生成图像，CLIP 则能够完成图像与文本类别的匹配。  </li><li data-pid=""hxu19urK"">2021年，德国Eleuther人工智能公司于今年3月下旬推出开源的文本AI模型GPT-Neo。对比GPT-3的差异在于它是开源免费的。  </li><li data-pid=""jOqe3wWl"">2021年，美国斯坦福大学的研究人员开发出一种用于打字的脑机接口（brain–computer interface, BCI），这套系统可以从运动皮层的神经活动中解码瘫痪患者想象中的手写动作，并利用递归神经网络（RNN）解码方法将这些手写动作实时转换为文本。相关研究结果发表在2021年5月13日的Nature期刊上，论文标题为“High-performance brain-to-text communication via handwriting”。 </li></ul><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-ea57e098a88fbf11a2929e53b19748cf_b.jpg"" data-rawwidth=""544"" data-rawheight=""782"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""544"" data-original=""https://pic4.zhimg.com/v2-ea57e098a88fbf11a2929e53b19748cf_r.jpg"" data-original-token=""v2-ea57e098a88fbf11a2929e53b19748cf""/></figure><ul><li data-pid=""8n-oFQOk""> 2021年，AlphaFold 2 能很好地预判蛋白质与分子结合的概率，为我们展示了<b>人工智能驱动自然学科</b>研究的无限潜力；</li></ul><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-dc75cc03567821973fb004490ee2577c_b.jpg"" data-rawwidth=""1440"" data-rawheight=""495"" data-size=""normal"" data-caption="""" data-default-watermark-src=""https://pic3.zhimg.com/v2-99822936d5c55794549bc53f861c2972_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""1440"" data-original=""https://pic1.zhimg.com/v2-dc75cc03567821973fb004490ee2577c_r.jpg"" data-original-token=""v2-dec56a81daed4e7fb084333143385b35""/></figure><ul><li data-pid=""Jq9dZZ1j"">2022年，ChatGPT出来，AI的想象瞬时就开始爆了...未来已来！</li></ul><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-331eeaa1df22da8a73f99b9eab6db1d3_b.jpg"" data-rawwidth=""1560"" data-rawheight=""946"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1560"" data-original=""https://pic4.zhimg.com/v2-331eeaa1df22da8a73f99b9eab6db1d3_r.jpg"" data-original-token=""v2-2587ab12df658112aed0af555a47fee0""/></figure><h2><b>三、AI 未来趋势</b></h2><p data-pid=""dmH-bAif"">人工智能有三个要素：数据、算力及算法，数据即是知识原料，算力及算法提供“计算智能”以学习知识并实现特定目标。  人工智能60多年的技术发展，可以归根为<b>算法、算力及数据层面</b>的发展，那么在可以预见的未来，人工智能发展将会出现怎样的趋势呢？</p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-e04ede5a540b8de5e7b58afde8f5a641_b.jpg"" data-rawwidth=""590"" data-rawheight=""224"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""590"" data-original=""https://pic2.zhimg.com/v2-e04ede5a540b8de5e7b58afde8f5a641_r.jpg"" data-original-token=""v2-e04ede5a540b8de5e7b58afde8f5a641""/></figure><p class=""ztext-empty-paragraph""><br/></p><h2><b>3.1 数据层面</b></h2><p data-pid=""H3BqzRbk"">数据是现实世界映射构建虚拟世界的基本要素，随着数据量以指数形式增长，开拓的虚拟世界的疆土也不断扩张。不同于AI算法开源，关键数据往往是不开放的，数据隐私化、私域化是一种趋势，数据之于AI应用，如同流量是互联网的护城河，有核心数据才有关键的AI能力。 </p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-159b2c188345db3e317f65a14cf75eba_b.jpg"" data-rawwidth=""608"" data-rawheight=""527"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""608"" data-original=""https://pic3.zhimg.com/v2-159b2c188345db3e317f65a14cf75eba_r.jpg"" data-original-token=""v2-159b2c188345db3e317f65a14cf75eba""/></figure><p class=""ztext-empty-paragraph""><br/></p><h2><b>3.2 算力层面</b></h2><blockquote data-pid=""w2IBA9LW""> 推理就是计算（reason is nothing but reckoning）  --托马斯.霍布斯  </blockquote><p data-pid=""HTyTYH4R"">计算是AI的关键，自2010年代以来的深度学习浪潮，很大程度上归功于计算能力的进步。</p><ul><li data-pid=""0eV7AmCm"">量子计算发展</li></ul><p data-pid=""Y8IoTnJT"">在计算芯片按摩尔定律发展越发失效的今天，计算能力进步的放慢会限制未来的AI技，量子计算提供了一条新量级的增强计算能力的思路。随着量子计算机的量子比特数量以指数形式增长，而它的计算能力是量子比特数量的指数级，这个增长速度将远远大于数据量的增长，为数据爆发时代的人工智能带来了强大的硬件基础。 </p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-2f6db207d73aa136c954c0e98c57cf48_b.jpg"" data-rawwidth=""1240"" data-rawheight=""698"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic1.zhimg.com/v2-2f6db207d73aa136c954c0e98c57cf48_r.jpg"" data-original-token=""v2-2f6db207d73aa136c954c0e98c57cf48""/></figure><p class=""ztext-empty-paragraph""><br/></p><ul><li data-pid=""VyEaV3S9"">边缘计算发展</li></ul><p data-pid=""ry-U5PxK"">边缘计算作为云计算的一种补充和优化，一部分的人工智能正在加快速度从云端走向边缘，进入到越来越小的物联网设备中。而这些物联网设备往往体积很小，为此轻量机器学习（TinyML）受到青睐，以满足功耗、延时以及精度等问题。</p><ul><li data-pid=""80WItkuW"">类脑计算发展</li></ul><p data-pid=""H6_Y9-YP"">以类脑计算芯片为核心的各种类脑计算系统，在处理某些智能问题以及低功耗智能计算方面正逐步展露出优势。类脑计算芯片设计将从现有处理器的设计方法论及其发展历史中汲取灵感，在计算完备性理论基础上结合应用需求实现完备的硬件功能。同时类脑计算基础软件将整合已有类脑计算编程语言与框架，实现类脑计算系统从“专用”向“通用”的逐步演进。</p><ul><li data-pid=""ZcshGG7r"">人工智能计算中心成为智能化时代的关键基础设施</li></ul><p data-pid=""MxYsce2B"">人工智能计算中心基于最新人工智能理论，采用领先的人工智能计算架构，是融合公共算力服务、数据开放共享、智能生态建设、产业创新聚集的“四位一体”综合平台，可提供算力、数据和算法等人工智能全栈能力，是人工智能快速发展和应用所依托的新型算力基础设施。未来，随着智能化社会的不断发展，人工智能计算中心将成为关键的信息基础设施，推动数字经济与传统产业深度融合，加速产业转型升级，促进经济高质量发展。</p><h2><b>3.3 算法层面</b></h2><ul><li data-pid=""LkGkyd3B"">机器学习自动化(AutoML)发展</li></ul><p data-pid=""J3IC-Dvd"">自动化机器学习（AutoML）解决的核心问题是：在给定数据集上使用哪种机器学习算法、是否以及如何预处理其特征以及如何设置所有超参数。随着机器学习在许多应用领域取得了长足的进步，这促成了对机器学习系统的不断增长的需求，并希望机器学习应用可以自动化构建并使用。借助AutoMl、MLOps技术，将大大减少机器学习人工训练及部署过程，技术人员可以专注于核心解决方案。 </p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-dc0bbec8840a24a5f32beee20277761c_b.jpg"" data-rawwidth=""794"" data-rawheight=""346"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""794"" data-original=""https://pic1.zhimg.com/v2-dc0bbec8840a24a5f32beee20277761c_r.jpg"" data-original-token=""v2-dc0bbec8840a24a5f32beee20277761c""/></figure><p class=""ztext-empty-paragraph""><br/></p><ul><li data-pid=""3nPr25EK"">向分布式隐私保护方向演进</li></ul><p data-pid=""2S6s3NyA"">当前全球多个国家和地区已出台数据监管法规，如HIPAA（美国健康保险便利和责任法案）、GDPR（欧盟通用数据保护条例），《数据安全法》、《<a href=""https://www.zhihu.com/search?q=%E4%B8%AA%E4%BA%BA%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E6%B3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2290601207%7D"" class=""internal"">个人隐私保护法</a>》等，通过严格的法规限制多机构间隐私数据的交互。分布式隐私保护机器学习(联邦学习)通过加密、分布式存储等方式保护机器学习模型训练的输入数据，是打破数据孤岛、完成多机构联合训练建模的可行方案。</p><ul><li data-pid=""gQDGFiL3"">数据和机理融合</li></ul><p data-pid=""9lgMiDY_"">AI模型的发展是符合简单而美的定律的。从数据出发的建模从数据中总结规律，追求在实践中的应用效果。从机理出发的建模以基本物理规律为出发点进行演绎，追求简洁与美的表达。</p><p data-pid=""Wq5QouC_"">一个好的、主流的的模型，通常是高度总结了数据规律并切合机理的，是“优雅”的，因为它触及了问题的本质。就和科学理论一样，往往简洁的，没有太多补丁，而这同时解决了收敛速度问题和泛化问题。</p><ul><li data-pid=""ED1hqkr2"">神经网络模型结构发展</li></ul><p data-pid=""rod5NHqd"">神经网络的演进一直沿着<b>模块化+层次化</b>的方向，不断把多个承担相对简单任务的模块组合起来。</p><p data-pid=""9ONcaU5b"">神经网络结构通过较低层级模块侦测基本的特征，并在较高层级侦测更高阶的特征，无论是多层前馈网络，还是卷积神经网络，都体现了这种模块性(近年Hinton提出的“胶囊”（capsule）网络就是进一步模块化发展)。因为我们处理的问题（图像、语音、文字）往往都有天然的模块性，学习网络的模块性若匹配了问题本身内在的模块性，就能取得较好的效果。</p><p data-pid=""0hZT8SfS"">层次化并不仅仅是网络的拓扑叠加，更重要的是学习算法的升级，仅仅简单地加深层次可能会导致BP网络的梯度消失等问题。</p><ul><li data-pid=""F_QXQvtv"">多学派方法融合发展</li></ul><p data-pid=""lH6mfqUI"">通过多学派方法交融发展，得以互补算法之间的优势和弱点。如 1）贝叶斯派与神经网络融合，Neil Lawrence组的Deep Gaussian process, 用简单的概率分布替换神经网络层。2）符号主义、集成学习与神经网络的融合，周志华老师的深度随机森林。3)  符号主义与神经网络的融合：将知识库(KG)融入进神经网络，如GNN、知识图谱表示学习。4)  神经网络与强化学习的融合，如谷歌基于DNN+强化学习实现的Alpha Go 让AI的复杂任务表现逼近人类。</p><ul><li data-pid=""C4lPgWxb"">基于大规模无(自)监督预训练发展</li></ul><blockquote data-pid=""4LrTKmG7""> If intelligence is a cake, the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL)    -- Yann Lecun  </blockquote><p data-pid=""iEkg7YGs"">监督学习需要足够的带标签数据，然而人工标注大量数据既耗时又费力，在一些领域(如医学领域)上几乎不太可能获得足量的标注数据。通过大规模无(自)监督预训练方法利用现实中大量的无标签数据是一个研究的热点，<b>AI大模型拥有超大规模参数、巨量训练数据，通过模型的巨量化可以提高人工智能的通用属性，并降低人工智能的应用门槛。</b>如GPT-3的出现激发了对大规模自监督预训练方法继续开展探索和研究。未来，基于大规模图像、语音、视频等多模态数据的跨语言的自监督预训练模型将进一步发展，并不断提升模型的认知、推理能力。</p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-d9cb951a152bc5cf72ee3fe8a89e732d_b.jpg"" data-rawwidth=""936"" data-rawheight=""720"" data-size=""normal"" data-caption="""" data-default-watermark-src=""https://pic2.zhimg.com/v2-79642b0c21d7fcdba981f54a8ec782b1_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""936"" data-original=""https://pic2.zhimg.com/v2-d9cb951a152bc5cf72ee3fe8a89e732d_r.jpg"" data-original-token=""v2-26cffda01efd5759bd2f2488cc2729cf""/></figure><ul><li data-pid=""gZyzhrUQ"">基于因果学习方法发展</li></ul><p data-pid=""fq4d1_lh"">当前人工智能模型大多关注于数据特征间相关性，而相关性与更为本源的因果关系并不等价，可能导致预测结果的偏差，对抗攻击的能力不佳，且模型往往缺乏可解释性。另外，模型需要独立同分布(i.i.d.)假设(现实很多情况，i.i.d.的假设是不成立的)，若测试数据与训练数据来自不同的分布，统计学习模型往往效果不佳，而因果推断所研究的正是这样的情形：如何学习一个可以在不同分布下工作、蕴含因果机制的因果模型(Causal Model)，并使用因果模型进行干预或反事实推断。</p><ul><li data-pid=""GY6LZ2qx"">可解释性AI (XAI)发展</li></ul><p data-pid=""XLLX88CF"">可解释的人工智能有可能成为未来机器学习的核心，随着模型变得越来越复杂，确定简单的、可解释的规则就会变得越来越困难。一个可以解释的AI（Explainable AI, 简称XAI）意味着AI运作的透明，便于人类对于对AI监督及接纳，以保证算法的公平性、安全性及隐私性。 </p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-4de17934dda094b30dddc34e0f2b5cc0_b.jpg"" data-rawwidth=""1240"" data-rawheight=""698"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic1.zhimg.com/v2-4de17934dda094b30dddc34e0f2b5cc0_r.jpg"" data-original-token=""v2-4de17934dda094b30dddc34e0f2b5cc0""/></figure><p class=""ztext-empty-paragraph""><br/></p><h2><b>后记</b></h2><p data-pid=""lULX7_Lr"">随着数据、算力及算法取得不断的突破，人工智能可能进入一个永恒的春天。 本文主要从技术角度看待AI趋势多少是片面的，虽然技术是第一生产力，有着自身的发展规律，但不可忽视的是技术是为需求市场所服务的，技术结合稳定的市场需求才能得到长足的发展。</p><p class=""ztext-empty-paragraph""><br/></p><hr/><p class=""ztext-empty-paragraph""><br/></p><a data-draft-node=""block"" data-draft-type=""ad-link-card"" data-ad-id=""fee_0d5f195831e5e99bf216291b05f553a0""></a><p data-pid=""rApL594C"">文章首发于“算法进阶”，公众号阅读原文可访问<a href=""https://link.zhihu.com/?target=https%3A//github.com/aialgorithm/Blog"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Github博客</a> </p><p></p>"
350927537,81fb7713c8c0eac1e0fc396e80b4e5b9,百事可乐,https://api.zhihu.com/people/81fb7713c8c0eac1e0fc396e80b4e5b9,people,,<em>人工智能</em>是否会取代人类？,article,https://api.zhihu.com/articles/350927537,我们的时代也正在不断地进步。如今，<em>人工智能</em>已经逐渐遍及我们生活的角角落落。但就“<em>人工智能</em>也许会在将来的某一天取代人类”这一话题的存疑，我持有否定的态度。 <em>人工智能</em>确实给人们的生活带来了很多的便利,106,8,181,1613446564,1613446564,"<p></p><p data-pid=""mquW6t-b"">随着新兴科技的快速发展，我们的时代也正在不断地进步。如今，人工智能已经逐渐遍及我们生活的角角落落。但就“人工智能也许会在将来的某一天取代人类”这一话题的存疑，我持有否定的态度。</p><p data-pid=""MldGj0YS"">人工智能确实给人们的生活带来了很多的便利，但它目前的发展还无法达到能够取代人类的这个高度。为什么我会这样说呢？因为，在我看来，人类的语言是复杂的，是多变的，也是深奥的。首先，说它复杂，其实是因为我们人类在说话的时候，有了语境的大前提、肢体语言的衬托，亦或是环境的渲染等帮助，我们的语言实际上省去了很多隐含的东西。就比如说：“下次还去这家蛋糕店。”其实这句话就隐含了“这个蛋糕很好吃”等信息。而人工智能虽然也很强大，但它却无法解码这些语言背后所隐藏的信息。人工AI目前能做的，只是简单的按照系统的步骤去解码人们为它输入的各类程序，继而实行操作，并不能对这些程序进行判断和分析。其次，说它多变是由于随着时间的推移，在各种因素的影响之下，人类的语言也会被赋予各种不同的理解。过渡到人工AI的层面上来说，人工智能所缺乏的其实是一种学习能力。这里所说的学习能力指的是学习“不再犯错”的这一能力。“不再犯错”举个例子，就是说，95后的人说：“哎呀，你好意思吗你，挣这么多钱。”其实说话者本人是想传达对对方赚钱能力的肯定和赞美之意，但如果放在00后面前，这句话就会被理解成：“你什么意思啊，别人不该挣那么多钱吗？”这从另一个角度来说，也就是我们生活中常有的“错意”现象。我们人类如果第一次遇到这种情况，就会记住自己这一次的误解，下一次就会去避免；而人工智能它不具备此“不再犯错”的能力。最后，说它深奥，在于从古至今，前人为我们这些后人留下了很多的经典著作。就拿《诗经》来说，里面的很多诗词，都被后人过度解读，我们人类甚至还将错就错的将这些诗词运用到我们日常的谈话，交流，亦或是写作中，即所谓的“引用”和“用典”。人工AI目前还无法做到解读这些语言更深层次所要传达的意思。</p><p data-pid=""SnMIBvWu"">综上所述，也就证实了科学家们的观点：“人工智能发展的最高高度就是能够和人类对话。”可见，能够达到这一高度仍旧是还有很长的路要走的。</p><p data-pid=""m1pmlYbO"">所以说，“人工智能将来会取代人类”这一说法的实现距离我们还是很遥远的。因此，我们也没必要过度的去担心智能化时代会给我们带来的威胁。</p>"
128986471,229fe918fd567e48c0363e99386580eb,bbtt,https://api.zhihu.com/people/229fe918fd567e48c0363e99386580eb,people,database developer ｜ENTJ,浅谈<em>人工智能</em>领域各个方向,article,https://api.zhihu.com/articles/128986471,再后来国家政策、资源一边倒的倾向AI，高校不管师资力量都逐渐开设<em>人工智能</em>学院，2019入坑AI的，后面找算法工作的，顶会+竞赛top排名+985硕是标配，在2020年AI下行趋势非常明显,294,22,794,1586594214,1644684284,"<h2><b>一.前言</b></h2><p data-pid=""chnfe4wu"">最近面临着读研选方向的问题，研究生选方向也算一件很重要的事情，这几天咨询了很多大佬，算是理清了AI大致的方向</p><p data-pid=""1cv2FtRS"">写这篇文章是想分享一下理解的宏观上的AI，算不上是科普文（每个人都有自己的理解吧），希望对即将 选导师、选实验室、选研究方向的同学们，对转行、自学的朋友萌 有所帮助~~</p><p data-pid=""lycO1svj"">适用读者：对AI领域感兴趣的、想加入这股浪潮（入坑）、同时不太清楚各个子方向同学</p><p data-pid=""s2b9OON5"">首先附上诚意满满的彩图 ^ ^</p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-aebbe1555788ffe7e9ef7b5987f98d35_b.jpg"" data-size=""normal"" data-rawwidth=""1994"" data-rawheight=""1463"" class=""origin_image zh-lightbox-thumb"" width=""1994"" data-original=""https://pic2.zhimg.com/v2-aebbe1555788ffe7e9ef7b5987f98d35_r.jpg"" data-original-token=""v2-3464424976dd255139864943509af427""/><figcaption>AI子方向</figcaption></figure><h2>有没有觉得这个良心配图好好看哇！！活泼个性而不失严谨！！！</h2><p data-pid=""wbi9MVA7"">那就点个关注吧！！</p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-caeef292b37b1dbd18727c93f5400fd9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""500"" data-rawheight=""500"" class=""origin_image zh-lightbox-thumb"" width=""500"" data-original=""https://pic2.zhimg.com/v2-caeef292b37b1dbd18727c93f5400fd9_r.jpg"" data-original-token=""v2-3b834bf9a94412191647546aad2fedd1""/></figure><h2>二.算法层面</h2><p data-pid=""FkaWm3K-"">AI离不开数据，同时也离不开算法，在算法层面大致有：<b>机器学习ML</b>（machine learning），<b>深度学习DL</b>（Deep learning），<b>强化学习RL</b>（Reinforcement learning），在早期的人工智能阶段，AI≈ 统计学 + 数据挖掘 + 算力 + 计算机科学，也可以简单的理解成在数据上的概率游戏，到2012年左右，随着算力的提升，反向传播让深度学习进入了大众的视野，2016年运用强化学习的alpha go打败李世石，AI这个词开始火爆起来，相信后续的历史大家也就很清楚了，也是深度学习不断颠覆的时代，视觉领域的yolo家族，CNN，RCNN家族不断迭代进化，强化学习的DQN，NLP领域的2018年谷歌提出的Bert等等</p><h2>三.子领域层面</h2><p data-pid=""w_8RjW6r"">AI个人理解的主要的方向有 <b>计算机视觉</b>、<b>自然语言</b>、<b>语音</b>、<b>知识图谱</b>这几大方向</p><p data-pid=""F4b40XDL"">搭建一个完整的AI，需要它的 <b>视觉模块</b>，<b>听觉模块</b>，<b>语言模块</b>，<b>记忆推理认知模块</b>，当然还有<b>躯体模块</b>等（机器手臂、传感器），所以AI也相应的分为了以下几个子领域，<i>它们之间也有重叠和相互需要的部分</i></p><p data-pid=""IEhQUIH2""><b>计算机视觉 CV</b>：computer vision，可以简单的理解<b>智能体的视觉模块</b>，是研究机器如何看的科学，也是工业界AI应用场景最多的领域（不过好像加入的人有点多...），主要处理图像的识别、理解，目标跟踪、检测等问题</p><p data-pid=""Jyd9Mwim"">CV的子方向有：目标检测、目标识别、图像分类、图像分割、图像内容理解、姿态估计、SLAM（定位和地图构建）等等，比如自拍的美颜、p大进入校园刷的人脸识别、手机的指纹解锁、高端一点的自动驾驶（虽然还有很长一段路要走），安防，这些都涉及计算机视觉 CV</p><p data-pid=""EXeqa6Rq""><b>自然语言处理 NLP</b>：Natural Language Processing，可以简单的理解<b>智能体的语言模块</b>，涉及语言的理解、认知的科学</p><p data-pid=""gTJd6JA3"">主要的范畴和子方向有：中文自动分词、文本分类、文本情感分析、问答QA、对话Dialog、信息检索 Information retrieval、信息摘要 Information extraction、机器翻译等，比如用的apple的Sir、Google翻译、百度的搜索...这些都涉及NLP</p><p data-pid=""uXPZYhc3"">相比于CV，NLP典型应用场景较少，范围窄而复杂度更高，不过cv和nlp也要相互联系的地方，个人觉得也是未来研究的热点方向，比如一个人走到一个地方环视一周，然后用语言描述出来，cv再根据描述重新构建场景，这个叫场景重建，再比如从艺术家的画中分析表达的情感态度，这也算二者结合的地方</p><p data-pid=""6kuGnh2q""><b>语音 VC</b>：Voice Recognition，可以简单的理解<b>智能体的听觉模块</b>，也算一个模式识别的问题，解决的是从多个声音（有许多噪声）中筛选出你想要的声音，比如微信聊天的语音识别，比如英语流利说的发音打分都要用到语音VC识别的相关算法</p><p data-pid=""pqiHMm_0""><b>知识图谱 KG</b>：Knowledge Graph，可以算作一个独立的方向，个人感觉和记忆理解这块比较接近，可以简单的理解<b>智能体的记忆推理认知模块</b>，也就是知识库的图数据呈现，是一种语义网络，我们这里把它单独划为一个子方向，把实体用关系串联起来，在知识库文本中挖掘语义，知识推理，使得AI更具有可解释性，也是非常重要的方向</p><blockquote data-pid=""MH8xWlLk"">总结一下，科学家想要搭建智能体，要分别搭建它的视觉、语言、听觉、记忆推理认知等模块，而这些模型也分别对应自己的领域和方向</blockquote><p class=""ztext-empty-paragraph""><br/></p><h2>四.工业界需求大方向</h2><p data-pid=""a7qK5bS-"">由于其他方向不太了解，就没有写了，今后有新认识会更新</p><p data-pid=""z3CJTEDq"">对应NLP方向来说，真正能给老板带来实实在在收益的技术，老板才会招兵买马组建人才部门，目前互联网大厂落地场景可以分为以下四类（问答还在快速发展，其他三个业务比较成熟稳定）：</p><p data-pid=""cZ6UzI6q""><b>搜索、问答、推荐系统、计算广告</b></p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-666531ba8d060bc0722fac871fa5acd6_b.jpg"" data-size=""normal"" data-rawwidth=""1213"" data-rawheight=""592"" class=""origin_image zh-lightbox-thumb"" width=""1213"" data-original=""https://pic3.zhimg.com/v2-666531ba8d060bc0722fac871fa5acd6_r.jpg"" data-original-token=""v2-10e50801890ea35f6e582446afc7490b""/><figcaption>工业界落地场景</figcaption></figure><p data-pid=""55lwKT5U""><b>搜索</b>：说大一点，众所周知的百度搜索引擎，Google搜索，说小一点，基本上每个成熟的网页、APP都需要自己的搜索功能</p><p data-pid=""QIKPTKtg""><b>推荐</b>：靠个性化推荐打出一片天地的字节跳动，根据用户偏好推荐相应的内容，阿里的淘宝推荐界面、美团根据用户口味喜好推荐外卖等等，知乎推荐你喜欢看的文章等等，应用场景也是相当广阔</p><p data-pid=""g7mW6Wby""><b>计算广告</b>：据说Facebook 90%以上的收入都是来自于广告，广告和推荐有交集，根据广告金主霸霸的需求，不断调整广告出现位置，提高广告点击率等等</p><p data-pid=""Frz7Tr2N""><b>智能问答</b>：这里的问答是任务式问答，类似智能客服，不管是淘宝智能商家还是超市选购，还是医院挂号的问答机器，还是银行智能客服，问答这块的未来需求会很大！！所以提前划分到经典落地三大类：广告 搜索 还有推荐。不信可以过个三年回来考古！！个人理解~~</p><h2>五.最后一点点</h2><p data-pid=""5SklRZZj"">根据历史的发展，在2010~2015入坑AI的，算是‘第一批吃螃蟹’的人，他们收获爆炸红利是应得的，那个时候懂一点决策树、svm跑跑minist就是大神；在2016~2018入坑AI的，算是嗅觉敏锐的那一批人，那个时候自学西瓜书和Andrew wu机器学习、花书是标配；再后来国家政策、资源一边倒的倾向AI，高校不管师资力量都逐渐开设人工智能学院，2019入坑AI的，后面找算法工作的，顶会+竞赛top排名+985硕是标配，在2020年AI下行趋势非常明显，并且前人已经几乎透支了AI红利，工业界算法岗爆满，头条官方劝退，学术界2016年后少有突破性框架、不断呼吁可解释模型的局面下，还要硬着头皮加入AI吗？这不仅需要高学历高工程能力高智商，还需要有自己的独特理解和对AI的那份执著</p><p data-pid=""T8ECRsgu"">这篇文章希望对不管是准备弃坑转开发的、即将入坑的、还是已经入坑的小伙伴们在AI大致的理解上有帮助！！</p><p data-pid=""KmvZ4DUW"">（完结撒花）</p>"
31479646,09483d2f20303d65c93be7151e9eccd7,人邮异步社区,https://api.zhihu.com/people/09483d2f20303d65c93be7151e9eccd7,people,官方微信：异步社区，人民邮电出版社创办的IT专业图书社区,AI经典书单：入门<em>人工智能</em>该读哪些书？,article,https://api.zhihu.com/articles/31479646,即日至12月30日 NO.1<em>人工智能</em>科普类：人工智能科普、人工智能哲学 《智能的本质》斯坦福、伯克利客座教授30年AI研究巅峰之作 《科学+遇见<em>人工智能</em>》李开复、张亚勤、张首晟等20余位科学家与投资人共同解读AI革命,429,10,2747,1511846961,1670731077,"<p data-pid=""qGWtrcXY"">快问快答：学习人工智能该读哪些书可以快速入门呢？我的答案是多读经典书。方向对了即使慢点，总会走向成功的终点。而该读哪些书，我带来了五份经典书单。</p><p data-pid=""ghF7F7sj"">人工智能有多火，相信铺天盖地的新闻已经证实了这一点，不可否认，我们已经迎来了人工智能的又一次高潮。与前几次人工智能的飞跃相比，这一次人工智能突破将软件算法、高并发硬件系统以及大数据有机地结合在一起，进而将人工智能推向了最接近人类智能的制高点。</p><p data-pid=""Ve5Qzk_a"">我在招聘网站上搜索人工智能相关的岗位，这些岗位的涉及到的技术领域包含：</p><p data-pid=""wODufami"">算法、深度学习、机器学习、自然语言处理、数据结构、Tensorflow、Python 、数据挖掘、搜索开发、spider开发、神经网络、视觉度量、图像识别、语音识别、推荐系统、系统算法、图像算法、数据分析、贝叶斯方法、概率编程、计算机数学、数据仓库、matlab建模等关键词，基本涵盖了现阶段人工智能细分领域的人才结构。</p><p data-pid=""qpNqOMtc"">将上面的岗位涉及到的岗位和技术划分为四大类，就形成了今天的五份书单：</p><p data-pid=""VTSgogY9""><b>活动：关注微信公众号：异步图书，并在微信公众号后台回复文字“知乎”，截图发到后台即有机会活得图书一本！数量：10个，时间：即日至12月30日</b></p><p data-pid=""nnDAq7LI""><b>NO.1人工智能科普类：人工智能科普、人工智能哲学</b></p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-0dc1ef13231927eeb222e7663d58de67_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic4.zhimg.com/v2-0dc1ef13231927eeb222e7663d58de67_r.jpg"" data-original-token=""v2-0dc1ef13231927eeb222e7663d58de67""/></figure><p data-pid=""8J0Xezaz"">《智能的本质》斯坦福、伯克利客座教授30年AI研究巅峰之作</p><p data-pid=""erYF-kAj"">《科学+遇见人工智能》李开复、张亚勤、张首晟等20余位科学家与投资人共同解读AI革命，机器学习教父Tom Mitchell力荐</p><p data-pid=""_aF0Zt1M"">《人工智能时代》从人工智能的历史、现状、未来，工业机器人、商业机器人、家用机器人、机器翻译、机器学习等人工智能应用领域依次介绍了人工智能发展前景。</p><p data-pid=""USh2hX8K"">《人工智能简史》 跟着图灵、冯•诺依曼、香农、西蒙、纽维尔、麦卡锡、明斯基等人工智能的先驱们重走人工智能之路，站在前人的肩膀上，看人工智能的三生三世，鉴以往才能知未来。 </p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-8c3636297fba1d6e296e4a3c1ab3fe81_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic2.zhimg.com/v2-8c3636297fba1d6e296e4a3c1ab3fe81_r.jpg"" data-original-token=""v2-8c3636297fba1d6e296e4a3c1ab3fe81""/></figure><p data-pid=""wkLRqmrp"">《科技之巅》源自麻省理工学院的科技创新预言圣经 掌握未来数年全球科技发展趋势和商业化潜力。</p><p data-pid=""CuptC2yN"">《科技之巅2》全球多领域20位行业专家深度解读强化学习、基因疗法2.0、自动驾驶货车、量子计算等新科技新产业。</p><p data-pid=""yh-FTUz3"">《数学之美（第2版）》浪潮之巅 文明之光 硅谷之谜 大学之路作者吴军博士作品 新版增加大数据和机器学习内容 文津图书奖获奖书 央视新闻推荐的学科敲门砖。</p><p data-pid=""g2I32iiW"">《技术的潜能》 美国知名战略咨询管理专家马库斯教授力作！为非技术人士了解技术影响力而写的一本简明读本！</p><p data-pid=""YeIvB4nK""><b>NO.2人工智能深度学习类：深度学习、Tensorflow</b></p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-ecf92978f622503003c3c2127bf46ccf_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic4.zhimg.com/v2-ecf92978f622503003c3c2127bf46ccf_r.jpg"" data-original-token=""v2-ecf92978f622503003c3c2127bf46ccf""/></figure><p data-pid=""qZtMUuCT"">《深度学习》AI圣经，深度学习领域奠基性的经典畅销书 特斯拉CEO埃隆·马斯克等国内外众多专家推荐！</p><p data-pid=""47S6mayO"">《深度学习精要（基于R语言）》基于R语言实战,使用无监督学习建立自动化的预测和分类模型</p><p data-pid=""pE6dsXqC"">《TensorFlow技术解析与实战》包揽TensorFlow1.1的新特性 人脸识别 语音识别 图像和语音相结合等热点一应俱全 李航 余凯等人工智能领域专家倾力推荐!</p><p data-pid=""Hka075FX"">《TensorFlow机器学习项目实战》第二代机器学习实战指南，提供深度学习神经网络等项目实战，有效改善项目速度和效率。</p><p data-pid=""1QCdD4We""><b>NO.3人工智能机器学习类：Python、机器学习、数据科学。</b></p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-065bb1bd7c5bbde00378edf76612e9c2_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic3.zhimg.com/v2-065bb1bd7c5bbde00378edf76612e9c2_r.jpg"" data-original-token=""v2-065bb1bd7c5bbde00378edf76612e9c2""/></figure><p data-pid=""9GDU28Jm"">《Python机器学习实践指南》 结合了机器学习和Python 语言两个热门的领域，通过利用两种核心的机器学习算法来用Python 做数据分析。</p><p data-pid=""WbRimA-K"">《Python机器学习——预测分析核心算法》 从算法和Python语言实现的角度，认识机器学习。</p><p data-pid=""wK5j2Gq4"">《机器学习实践应用》阿里机器学习专家力作，实战经验分享，基于阿里云机器学习平台，针对7个具体的业务场景，搭建了完整的解决方案。</p><p data-pid=""j_1fqURJ"">《NLTK基础教程——用NLTK和Python库构建机器学习应用》绍如何通过NLTK库与一些Python库的结合从而实现复杂的NLP任务和机器学习应用。</p><figure data-size=""normal""><img src=""https://pic1.zhimg.com/v2-7ccad2f2db644309ea9be89129e4d5d8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic1.zhimg.com/v2-7ccad2f2db644309ea9be89129e4d5d8_r.jpg"" data-original-token=""v2-7ccad2f2db644309ea9be89129e4d5d8""/></figure><p data-pid=""bdd0X_MN"">《Microsoft Azure机器学习和预测分析》  了解新的微软Azure Machine Learning服务 掌握高效构建和部署预测模型的实用技能。</p><p data-pid=""PxynG_l_"">《机器学习与数据科学（基于R的统计学习方法）》 为数据科学家提供了一些在统计学习领域会用到的工具和技巧。</p><p data-pid=""Kf4jJ_Eb"">《机器学习Web应用》 eBay公司EU Analytics部门负责人Davide Cervellin作序推荐，全面Python机器学习的图书 学会在Web下构建机器学习系统的权威指南。</p><p data-pid=""yg4Jh-OW"">《实用机器学习》 使用R语言引导读者掌握机器学习实战 顺利针对新问题 新数据选择和使用机器学习算法。</p><p class=""ztext-empty-paragraph""><br/></p><p data-pid=""c_9LNMXP""><b>NO.4人工智能算法策略类：算法、神经网络、自然语言处理、推荐系统、系统算法、图像算法、贝叶斯、概率编程、数学算法等。</b></p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-b81362da75237946668b6b40bc9930ae_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic3.zhimg.com/v2-b81362da75237946668b6b40bc9930ae_r.jpg"" data-original-token=""v2-b81362da75237946668b6b40bc9930ae""/></figure><p data-pid=""wWp4r9EV"">《神经网络算法与实现——基于Java语言》 完整地演示了使用Java开发神经网络的过程，既有非常基础的实例也有高级实例。</p><p data-pid=""81qqlz0C"">《趣学算法》 50 多个实例循展示算法的设计、实现、复杂性分析及优化过程 培养算法思维 带您感受算法之美。</p><p data-pid=""8Vg89ikY"">《算法谜题》 Google、Facebook等一流IT公司算法面试必备，经典算法谜题合集。</p><p data-pid=""NVwwo7NJ"">《Python算法教程》  精通Python基础算法 畅销书Python基础教程作者力作。</p><figure data-size=""normal""><img src=""https://pic3.zhimg.com/v2-0b152aea9509aecfd3f581a9ba991ac6_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic3.zhimg.com/v2-0b152aea9509aecfd3f581a9ba991ac6_r.jpg"" data-original-token=""v2-0b152aea9509aecfd3f581a9ba991ac6""/></figure><p data-pid=""ILQ7vPzD"">《编程之法：面试和算法心得》程序员面试宝典 笔试金典 CSDN访问量过千万的博客结构之法算法之道博主July著作。</p><p data-pid=""HF7l7rjc"">《趣题学算法》 一本有趣的、易学的、实用的，帮助读者快速入门应用的算法书。</p><p data-pid=""E6YQ2Kw7"">《Java遗传算法编程》 遗传算法设计 机器学习人工智能 来自Java专家的声音 用遗传算法解决类似旅行商的经典问题。</p><p data-pid=""YhBuPUx_"">《算法学习与应用从入门到精通》 320个实例、753分钟视频、5个综合案例、74个技术解惑，一本书的容量，讲解了入门类、范例类和项目实战类三类图书的内容。</p><p class=""ztext-empty-paragraph""><br/></p><p data-pid=""ZVRdYQ0s"">NO.5人工智能时间图像和视觉识别类：图像识别、语音识别、自然语言处理、matlab建模工程。</p><figure data-size=""normal""><img src=""https://pic4.zhimg.com/v2-b91f919323ad21f61b66244ae12f3ee7_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic4.zhimg.com/v2-b91f919323ad21f61b66244ae12f3ee7_r.jpg"" data-original-token=""v2-b91f919323ad21f61b66244ae12f3ee7""/></figure><p data-pid=""blRRk6Ta"">《OpenCV和Visual Studio图像识别应用开发》无人驾驶人脸识别基础技术 用OpenCV实现图像处理应用 计算机视觉编程实战手册。</p><p data-pid=""Vjjn0zIF"">《人脸识别原理及算法——动态人脸识别系统研究》 介绍了动态场景下的人脸识别方法，该方法综合应用了人脸定位、人脸识别、视频处理等算法。</p><p data-pid=""KJSWqjVD"">《精通Python自然语言处理》 用Python开发令人惊讶的NLP项目 自然语言处理任务 掌握利用Python设计和构建给予NLP的应用的实践。</p><p data-pid=""Ks2XsrS1"">《Python自然语言处理》基于Python编程语言和NLTK，自然语言处理领域的一本实用入门指南。</p><figure data-size=""normal""><img src=""https://pic2.zhimg.com/v2-514a8c1ddc056b332ee6e4ac4d391ea1_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""800"" data-rawheight=""270"" class=""origin_image zh-lightbox-thumb"" width=""800"" data-original=""https://pic2.zhimg.com/v2-514a8c1ddc056b332ee6e4ac4d391ea1_r.jpg"" data-original-token=""v2-514a8c1ddc056b332ee6e4ac4d391ea1""/></figure><p data-pid=""ME61bngI"">《贝叶斯方法：概率编程与贝叶斯推断》 机器学习 人工智能 数据分析从业者的技能基础 国际杰出机器学习专家余凯博士 腾讯专家研究员岳亚丁博士推荐。</p><p data-pid=""mgAGkHw0"">《贝叶斯思维：统计建模的Python学习法》Think Stats和Think Python图书作者重磅出击 数据分析师 数据工程师 数据科学家案头常备。</p><p data-pid=""tQ42qBrl"">《概率编程实战》人工智能领域的先驱、美国加州大学伯克利分校教授Stuart Russell作序推荐！一本不可思议的Scala概率编程实战书籍！</p><p data-pid=""F7EBsK_o"">《自己动手写神经网络》 机器学习与人工智能参考书 基于Java语言撰写。</p><p data-pid=""iqdtTJfn"">未经授权不可转载，转载请联系公众号，异步社区</p><p></p><p></p>"
