id,authorId,author,authorUrl,authorType,authorHeadline,title,type,url,excerpt,voteupCount,commentCount,zfavCount,createdTime,updatedTime,content
664346177,354a091a04e0c20269578c0064f8d2a4,AI技能研究所,https://api.zhihu.com/people/354a091a04e0c20269578c0064f8d2a4,people,程序员的进阶之旅,什么是人工智能？如何学习人工智能？,article,https://api.zhihu.com/articles/664346177,作为一个 985 本硕计算机的<em>人工智能</em>经验开发者来谈谈我对人工智能的理解，全文 3000 余字都是干货，建议收藏、点赞后观看。一句话，<em>人工智能</em>，是机器能够具有与人类思维相关的认知功能的能力。 作为一个长期在<em>人工智能</em>行业挣扎的经验者，有必要来回答这个问题，给新入行、想入行的同学一些经验分享,278,5,0,1698742852,1698743073,作为一个 985 本硕计算机的人工智能经验开发者来谈谈我对人工智能的理解，全文 3000 余字都是干货，建议收藏、点赞后观看。一句话，人工智能，是机器能够具有与人类思维相关的认知功能的能力。作为一个长期在人工智能行业挣扎的经验者，有必要来回答这个问题，给新入行、想入行的同学一些经验分享，本片内容涉及广、涵盖多，建议收藏、点赞、关注三连后慢慢食用人类和机器的渊源，缠绵古今。如果没有借助、发现、发明、使用机器，我们人类这个物种注定不会走的太远。从近代来说，农业机械化、汽车、高铁、建筑设备等等，都是机器。这些机器长期以来帮助我们改进我们的生活状态和品质。另一方面，人类是矛盾的，即倾向于使用更便捷的机器，又害怕的担心机器的智能发展飞快。二十世纪的理论家，如计算机科学家和数学家艾伦·图灵，设想了一个机器可以比人类更快地执行功能的未来，就是人工智能时代。自计算机在 20 世纪 70 年代开始普及以来，计算机的功能越来越复杂，计算能力越来越强，由此展开对人工智能这一愿景的实现。人工智能是机器执行与人类思维相关的认知功能的能力，例如感知、推理、学习、与环境交互、解决问题，甚至发挥创造力的未来世界的愿景。目前的人工智能是什么？人工智能 (AI) 是计算机科学的一个广泛分支，涉及构建能够执行通常需要人类智能的任务的智能机器。虽然人工智能是一门具有多种方法的跨学科科学，但尤其是机器学习和深度学习的进步正在为科技行业的几乎每个领域带来范式转变。将人工智能称之为机器可以具有人类思维相关认知能力的愿景目前解决的方式是通过机器学习的方法来逼近人工智能这一个愿景其中深度学习是机器学习中目前效果较好且最火热的一个技术分支让我们看一个人工智能驱动产品的例子——Amazon Echo，Amazon Echo 是一款使用亚马逊开发的虚拟助理人工智能技术 Alexa 的智能音箱。Amazon Alexa 能够进行语音交互、播放音乐、设置闹钟、播放有声读物以及提供新闻、天气、体育和交通报告等实时信息。 人工智能的等级目前就人工智能的发展趋势来看，可以把人工智能划分为三个层级，弱人工智能（ANI），通用人工智能（AGI）和超级人工智能（ASI）三类。弱人工智能（ANI）：是迄今为止成功完成的人工智能技术。 ANI(Artificial Narrow Intelligence)被设计出来用来执行单个任务并且以目标为导向。 ANI 非常有能力完成其编程要完成的特定任务。 ANI 的一些示例是语音助手、面部识别或驾驶汽车。通用人工智能（AGI）：是具有通用人工思维的智能机器，机器可以模仿人类的智能和行为，并具有从数据中学习并应用其智能来解决任何问题的能力。人工通用情报可以在任何给定情况下以类似于人类的方式思考、理解和行动，目前最火热的 ChatGPT 正属于这个阶段。超级人工智能（ASI）：可以变得自我意识并超越人类能力和智能的假想，距离目前这个阶段还很遥远。说道这里就需要谈一下最近在知乎知学堂上的一门课《程序员的 AI 大模型进阶之旅》；上面的链接就是公开课的链接，完全免费的科普课程。添加课程之后一定一定一定要添加助教小姐姐的微信，可以私聊助教领取今年最火最热的大模型学习资源！目前入行人工智能需要从机器学习、深度学习知识点出发，来学习相关技术，才能从事相关人工智能行业。下面简单来说说如何入手这些技术，从而可以达到入行人工智能的程度。什么是机器学习？机器学习是计算机科学的一门学科，它使用计算机算法和分析来构建可以解决业务问题的预测模型。根据麦肯锡公司的说法，机器学习基于可以从数据中学习的算法，而无需依赖基于规则的编程。「如果计算机程序在 T 中的任务中的性能（按 P 测量）随着经验 E 的提高而提高，则可以说它可以从关于某类任务 T 和性能测量 P 的经验 E 中学习。」所以你看，机器学习有很多定义。但它到底是如何运作的呢？机器学习如何工作？机器学习访问大量数据（结构化和非结构化）并从中学习以预测未来。它通过使用多种算法和技术从数据中学习。下图显示了机器如何从数据中学习。上面一张图就完全展示了机器学习是如何工作的，如果想要自学机器学习的朋友，可以参考这几篇内容：如何自学机器学习Machine Learning？入门机器学习，哪些书籍值得看？机器学习的类型机器学习算法主要分为三大类：1. 监督学习在监督学习中，数据已经被标记，这意味着你知道目标变量。使用这种学习方法，系统可以根据过去的数据预测未来的结果。它要求至少为模型提供输入和输出变量才能对其进行训练。下面是监督学习方法的一个例子。该算法是使用狗和猫的标记数据进行训练的。经过训练的模型可以预测新图像是猫还是狗。 监督学习的一些例子包括线性回归、逻辑回归、支持向量机、朴素贝叶斯和决策树。2.无监督学习无监督学习算法利用未标记的数据自行从数据中发现模式。该系统能够从提供的输入数据中识别隐藏的特征。一旦数据更具可读性，模式和相似性就会变得更加明显。下面是一个使用未标记数据训练模型的无监督学习方法的示例。在这种情况下，数据由不同的车辆组成。该模型的目的是对每种车辆进行分类。 无监督学习的一些示例包括 k 均值聚类、层次聚类和异常检测。3. 强化学习强化学习的目标是训练智能体在不确定的环境中完成任务。代理从环境中接收观察结果和奖励，并向环境发送操作。奖励衡量行动在完成任务目标方面的成功程度。下面的示例展示了如何训练机器识别形状。 强化学习算法的示例包括 Q 学习和深度 Q 学习神经网络什么是深度学习？深度学习是机器学习的一个子集，它处理受人脑结构和功能启发的算法。深度学习算法可以处理大量结构化和非结构化数据。深度学习的核心概念在于人工神经网络，它使机器能够做出决策。深度学习与机器学习之间的主要区别在于数据呈现给机器的方式。机器学习算法通常需要结构化数据，而深度学习网络则在多层人工神经网络上工作。这是一个简单的神经网络的样子： 该网络有一个输入层，用于接受数据的输入。隐藏层用于从数据中查找任何隐藏的特征。然后输出层提供预期的输出。这是一个使用大量未标记的眼睛视网膜数据的神经网络的示例。网络模型根据这些数据进行训练，以确定一个人是否患有糖尿病视网膜病变。 现在我们已经了解了什么是深度学习，让我们看看它是如何工作的。深度学习如何工作？计算加权和。计算出的权重总和作为输入传递给激活函数。激活函数将「输入的加权和」作为函数的输入，添加偏差，并决定是否应该激发神经元。输出层给出预测输出。将模型输出与实际输出进行比较。训练神经网络后，模型使用反向传播方法来提高网络的性能。成本函数有助于降低错误率。在下面的示例中，深度学习和神经网络用于识别车牌上的号码。许多国家都使用这种技术来识别违规者和超速车辆。 想要入门深度学习或者找深度学习的相关资料，不如看看我写的这一篇文章，回答了深度学习相关资源、图书；以及学习下方法和路线。有哪些优秀的深度学习入门书籍？需要先学习机器学习吗？本文作者：@TopGeeky
664339575,354a091a04e0c20269578c0064f8d2a4,AI技能研究所,https://api.zhihu.com/people/354a091a04e0c20269578c0064f8d2a4,people,程序员的进阶之旅,人工智能主要有哪些领域？,article,https://api.zhihu.com/articles/664339575,答主是<em>人工智能</em>领域的在读博士生，平时也会跟我老婆（文科生）解释<em>人工智能</em>的各种应用，比如什么 人脸识别技术，或者是 ChatGPT 这类型的技术。通常我不会深入到技术细节，因为确实会很难懂，所以我会很很多生活中的案例来解释。当我们提到<em>人工智能</em>（<em>AI</em>），很多人可能首先会想到一些很「科幻」的场景,113,3,0,1698741501,1708690959,答主是人工智能领域的在读博士生，平时也会跟我老婆（文科生）解释人工智能的各种应用，比如什么人脸识别技术，或者是 ChatGPT 这类型的技术。通常我不会深入到技术细节，因为确实会很难懂，所以我会很很多生活中的案例来解释。当我们提到人工智能（AI），很多人可能首先会想到一些很「科幻」的场景，或者就是日常生活中常用的语音助手，比如手机里的「小爱同学」或者智能家居中的「天猫精灵」。这些都是 AI 的应用层面，但其实 AI 的世界要比这些更为广阔和深入。先让我们了解 AI 的基础层。基础层好比人的心脏和大脑，为 AI 提供运行的基本能力。假设你家里的电视机通过 AI 智能推荐系统向你推荐了一部火爆的电视剧，那么这背后就是机器学习的功劳。这个系统会根据你过去看过什么，以及其他类似你的观众喜欢什么，来为你做个性化的推荐。还有一些技术，比如自然语言处理（NLP），让这些助手能够理解你的语言。举个例子，如果你告诉「小爱同学」：「明天早上八点叫我起床」，它能理解你的意思，并在第二天早上准时叫你起床。那么，让我们转向应用层。应用层是 AI 如何用基础层的能力去解决现实问题，也就是说，它是 AI「上岗工作」的地方。比如，在电商平台上，AI 可以通过分析你的购物记录和浏览习惯，为你推荐可能会感兴趣的商品。在医疗领域，AI 可以协助医生读取和分析 X 光片或 MRI 图像，更准确地诊断病情。在交通方面，AI 可以控制智能红绿灯，根据路况自动调整红绿灯的时间，减少拥堵。总结一下，基础层是 AI 能「听得懂、看得见、学得会」的地方，而应用层则是 AI 用这些能力去完成具体任务的场所。在电视剧推荐的例子里，机器学习和数据分析就是基础层的代表。而当 AI 用这个推荐能力去帮你找到感兴趣的电视剧，或者在电商平台、医疗诊断和交通管理等方面帮你解决问题时，这就是应用层的表现。其实对于基础层和应用层来说，他们的内容远比上述提到的要多得多。 上图来自一篇学术论文，总体来说，人工智能会分为两大层，基础层 Infrastructure Layer 和应用层 Application Layer。还是用现实生活来举例子，比如打开电视看电视节目前，需要有电视信号塔、卫星、电缆等来传输信号。这些都是「基础设施」，没有它们，电视节目就无法传输。当你打开电视并选择一个节目或频道来观看时，你实际上是在与应用层进行交互。这里的应用层包括了电视节目、用户界面、遥控器等。这两个很明显的区别就是基础层的东西你是很难直接感受的，比如说信号塔的位置在哪里，卫星的信道是什么，因为这些都是已经部署好的，你也没有必要去了解这里面的细节。而应用层就显得直观多了，比如各个电视节目，你可以通过遥控器来随便选，可以看网络直播，也可以看电影点播。这个跟人工智能的基础层和应用层是非常相似的，基础层是人工智能的「引擎」，负责处理与运算、数据存储和其他基本任务。这一层包括硬件（如服务器、GPU、TPU 等）、底层软件（如操作系统、数据库管理系统）、网络（云计算资源、数据中心等）以及基础算法（机器学习算法、深度学习网络等）。应用层则更关注人工智能如何与实际应用相结合。这一层会包括特定领域的应用程序、用户界面、高级数据分析和业务逻辑等。它通常是普通用户最直接接触到的部分比如刷脸支付，语音翻译等等。下面的内容是针对于对于人工智能想要有更加深入的技术方面的理解的朋友，泛泛而谈的话基础层和应用层就可以覆盖大多数的内容了。其次，如果对于人工智能感兴趣的话，从 GPT 这种每个人都可以使用的软件是非常推荐的，因为它比起其他的人工智能应用更加的简单和有趣，而知乎知学堂开设的这门人工智能介绍课，就非常的适合作为入门课程进行人工智能的全面了解和学习，这对于不管是工作还是学习都会是受益良多的。确实，将人工智能（AI）的结构细分为更多层次可以提供更深入的理解。在基础层（Infrastructure Layer）和应用层（Application Layer）之间，通常还存在三个额外的层次：感知层（Perception Layer）、认知层（Cognitive Layer）和决策层（Decision-Making Layer）。这些层次起到桥梁作用，它们处理从基础设施到最终应用之间更为复杂和高级的任务。这个分层模型就像是家里的电路系统。你可以想象高压电流从发电站开始，但在到达你的家里供电之前，需要经过几个步骤来转换电流。这些转换步骤有点像是人工智能中的感知层、认知层和决策层。简单地说，就像电流需要被「整理」和「调整」以便在家里使用一样，人工智能的不同层也有类似的角色。感知层就像是电流的「接收器」，负责收集信息；认知层是「处理器」，负责理解这些信息；最后，决策层就像是「执行器」，根据收到的信息做出决策。这些层一起工作，确保人工智能能够从基础设施到最终应用（比如一个手机 app 或医疗诊断工具）都能顺畅运行。这种层级结构是层层递进的：底层的稳定性和效能直接影响上层的性能表现。从基础层的角度看，硬件组件（如芯片）提供了必要的计算能力。这一层是整个体系的基础，因为没有强大的计算力，复杂的人工智能算法就无法高效运行。而这些算法，通常由机器学习、深度学习等技术实现，是实现底层决策的关键。感知层随后接入，它就像我们的感官，负责从外界收集信息。这一层用于处理来自传感器、图像、声音等的原始数据，并将其转化为对后续层有用的信息。进一步来说，认知层像是一个「大脑」，对来自感知层的信息进行更高级的处理和解释。这里可能涉及自然语言处理、图像识别和数据分析等一系列复杂任务。最后，决策层则基于认知层的输出做出相应的行动或决策。这可以是驾驶汽车、生成文本或者进行医疗诊断等。整个体系的高性能运作依赖于这些层之间的协调和优化。就像一个精密的机器，每个部件都必须精确地运转，才能确保整体的高效和准确。这种综合性也显示了人工智能跨多个学科和领域的潜力，从计算机科学和工程到认知科学、社会科学甚至伦理学，都有其应用和影响。如果我们将这种层次化的人工智能视角应用到我们自身和社会中，可能会发现一些令人惊奇的相似之处。就像人工智能从基础层到应用层需要精密协调，我们的社会也需要各个组成部分—从个体到社群，再到更大的社会制度—齐心协力，以创造一个更加和谐、高效的环境。最终，人工智能不仅是一种技术革命，更是一面镜子，反映出我们如何组织、理解和优化复杂系统的多维能力。本文作者：@平凡
696836937,5d466e863fe977c11a8251e25fba9229,蕾奥规划,https://api.zhihu.com/people/5d466e863fe977c11a8251e25fba9229,people,行动规划+运营咨询,识途 | 人工智能要回归造物本质,article,https://api.zhihu.com/articles/696836937,和关乎人类生存与生活的紧密度。这也表明，在“<em>人工智能</em>+”赋能千行百业的时代里，规划、建筑行业应当更为谨慎。 面对我国从城市建设转而步入城市运营时代的这一节点，<em>人工智能</em>与城市物质空间的融合发展一直备受瞩目,0,0,0,1715252516,1715252516,规划、建筑的百年魅力在于附着于土地、依托于现实的重度，和关乎人类生存与生活的紧密度。这也表明，在“人工智能+”赋能千行百业的时代里，规划、建筑行业应当更为谨慎。面对我国从城市建设转而步入城市运营时代的这一节点，人工智能与城市物质空间的融合发展一直备受瞩目。本期视频，深圳大学建筑与城市规划学院研究员、本原设计研究中心副主任、趣城工作室创始人兼主持设计师、深港城市/建筑双城双年展学术委员会主任张宇星教授将与您分享：“人工智能要回归造物本质”。在没有无限的能源之前，要想在三维空间里面去实现人工智能，这几乎是一件不可能的事。—— 张宇星深圳大学建筑与城市规划学院研究员本原设计研究中心副主任趣城工作室创始人兼主持设计师深港城市/建筑双城双年展学术委员会主任讨论人工智能，首先需要设定边界，原本在二维空间里讨论的理论，一旦将讨论拓展到三维空间，就会变成科幻。因为想象都是科幻，是非常遥远的，更不用说建造领域。在二维里面，计算人的行为或者是整个建造系统，人工智能可以介入，但是涉及到具体造物本身是很困难的，即使局部可以去实现人工智能，它的功效比也是非常低的，换而言之这并不合算。现在有一个误区，关于建筑学这个领域，如果天天跟着搞科技的那帮人去玩高科技，我们永远追不上他们。最后发现，不如回到人本身，人的生命体其实就是一个人工智能，何必再培养人工智能？我们最终还是要回到建筑学。关于城市规划的一些原点问题，基础是不能被抛弃的——我们的研究、设计对象非常重、非常坚固、也非常稳定，迭代速度特别慢，和人的关系又特别紧密，而这些东西都是其他学科没有的。所以，大家一方面要跟上科技的步伐，另一方面也要回到学科的本质。「识途」从「城市蕾好」的科普精髓中孕育而生，是对城市探索旅程的深化与延伸。作为原栏目衍生的子系列，我们将继续沿着城乡与社会发展的轨迹前行，通过专访深入触达不同行业和领域，洞察智慧城市运营的每一环节，结合“人、地、财、产、政、文、技、数”城市发展的八大要素，探寻现在正在发生的，以及未来可能发生的。如果城市是书，漫步便是阅读的艺术。在「识途」中，我们不仅仅是在漫步，每一期节目都将通过专访中的一瞥，带你窥见那些各个行业领军人物、专家学者的城市足迹，深入讨论他们的专业知识，以及对城市高质量发展运营的洞察与远见，和大家一同阅读，一同识途。第20期：公园带来的城市逆袭第19期：5大公园场景，你不知道的流量密码第18期：魔都“1000颗树” ，给一个种的理由第17期：智慧互联如何形象你的生活？第16期：公园的1000种玩法
696838063,1a3031d3eb0898a3c2d1aa05766d923b,21dB声学人,https://api.zhihu.com/people/1a3031d3eb0898a3c2d1aa05766d923b,people,,三星宣布收购胎儿超声人工智能软件公司Sonio,article,https://api.zhihu.com/articles/696838063,为妇产科超声开发了IT解决方案和AI助理功能，协助医疗专业人员评估和记录超声检查。 Sonio Detect是FDA 510(k)批准的<em>人工智能</em>助手，利用深度学习网络确保更高的实时图像质量,0,0,0,1715253017,1715253017,三星电子旗下子公司三星麦迪逊 (Samsung Medison) 近日宣布签署协议，收购胎儿超声人工智能软件公司Sonio SAS 100%的股份。Sonio成立于2020年，旨在通过医疗进步提高全球妇女和儿童的健康，为妇产科超声开发了IT解决方案和AI助理功能，协助医疗专业人员评估和记录超声检查。Sonio Detect是FDA 510(k)批准的人工智能助手，利用深度学习网络确保更高的实时图像质量，并通过更高的临床准确性来提高护理质量。交易完成后，Sonio仍将是一家总部位于法国的独立公司，并将继续独立进行商业增长，同时其产品和服务将与所有超声设备制造商保持兼容。与Sonio世界一流的人工智能专业知识相结合，三星麦迪逊领先的超声系统将有助于将突破性的人工智能增强工作流程推向市场，并加速患者治疗结果和产前护理质量的创新。“我们对Sonio团队在孕产妇护理领域所取得的成就表示极大的尊重和钦佩。通过收购Sonio，三星麦迪逊将继续履行我们的承诺，用技术改善人们的生活质量。”三星麦迪逊首席执行官Yong Kwan Kim表示。“与Sonio的合作将汇集一流的超声人工智能技术和报告功能，为产前超声检查带来范式转变。”Sonio首席执行官Cécile Brosset表示：“Samsung Medison成熟的全球超声业务与Sonio先进的人工智能相结合，为双方创造了令人兴奋的增长机会。” “我们发现三星麦迪逊是一个令人惊叹、值得信赖的合作伙伴，可以帮助我们追求和加速我们的路线图和使命。除了与Samsung Medison密切合作外，作为一家独立公司，Sonio将继续在全球范围内推进医疗报告技术和诊断软件，包括医疗保健服务不足的领域。”信息源于：三星官网 关于我们 21dB声学人是中国科学院声学研究所苏州电声产业化基地旗下科技媒体，专注于声学新技术、音频测试与分析、声学市场调研、声学创业孵化、知识产权服务等。 合作推广 稿件投稿｜创业支持｜知产服务｜ 技术转化请发送需求至以下邮箱，我们将派专人与您联系21db@ioasonic.com 版权声明 文中所有图片和文字版权归21dB声学人所有如需转载或媒体合作，请与我们联系
696807729,ea903910d5059b14f74a26feef464ccf,青峰君说,https://api.zhihu.com/people/ea903910d5059b14f74a26feef464ccf,people,商业&amp;财经老司机,生成式人工智能岗位需求激增，市场迎来新机遇,article,https://api.zhihu.com/articles/696807729,随着<em>人工智能</em>应用的爆发，生成式人工智能（AIGC）的招聘市场正变得异常火爆。数据显示，今年一季度，生成式<em>人工智能</em>相关职位需求同比增长超过三倍。这一增长趋势不仅反映了行业需求的激增,1,0,0,1715244231,1715244231,随着人工智能应用的爆发，生成式人工智能（AIGC）的招聘市场正变得异常火爆。数据显示，今年一季度，生成式人工智能相关职位需求同比增长超过三倍。这一增长趋势不仅反映了行业需求的激增，也揭示了技术发展和市场应用的迅速扩展。生成式人工智能岗位的薪酬水平也显著提高。据了解，AIGC相关岗位的平均招聘年薪达到40万元以上，其中自然语言处理专家、图像算法工程师和架构师等岗位的年薪甚至超过50万元。这种较高的薪酬水平是生成式人工智能相关岗位火爆的重要原因之一。生成式人工智能岗位需求的激增不仅限于科技企业，还蔓延到了其他行业。例如，在一家连锁餐饮企业，技术部门负责人表示，企业急需利用生成式人工智能技术来更精准地响应客户评价，这反映了生成式AI技术在商业应用中的广泛需求。这一趋势的背后，是生成式人工智能技术的快速发展及其在多个领域的广泛应用。从底层算力、芯片设计，到模型训练、商业落地，生成式AI技术在各环节都展现出强大的潜力和价值。同时，由于行业较新，很多企业都在寻找具备相关技能的人才，这也导致了人才稀缺和招聘难度的增加。综上所述，生成式人工智能岗位需求的激增不仅反映了技术发展的趋势，也为求职者提供了新的机遇。随着技术的不断进步和应用领域的扩展，生成式人工智能将继续成为招聘市场的重要领域，吸引更多的人才涌入。（数据支持：天眼查）
696789520,6a35e8a72b811bfd21d72fce0bf1201e,华商学习网,https://api.zhihu.com/people/6a35e8a72b811bfd21d72fce0bf1201e,people,,北京大学人工智能AI研修班,article,https://api.zhihu.com/articles/696789520,北京大学<em>人工智能</em>AI研修班，是引领未来科技趋势的精英聚集地。在这里，学员们将深入探索人工智能的前沿技术和应用，与国内外顶尖学者和行业专家交流互动，拓宽视野，启迪思维,1,0,0,1715240380,1715240380,北京大学人工智能AI研修班，是引领未来科技趋势的精英聚集地。在这里，学员们将深入探索人工智能的前沿技术和应用，与国内外顶尖学者和行业专家交流互动，拓宽视野，启迪思维。研修班课程涵盖机器学习、深度学习、自然语言处理等核心领域，注重理论与实践相结合，让学员们不仅掌握理论知识，更能将AI技术应用于实际问题解决中。通过系统的学习和实践，学员们将提升自己在人工智能领域的专业素养，为未来的职业发展奠定坚实基础，共同推动人工智能技术的创新与发展。本文由华商学习网整理，感谢支持！
696678092,3e9a8873e6eec602731b1ba11fe56d1c,贺天行,https://api.zhihu.com/people/3e9a8873e6eec602731b1ba11fe56d1c,people,NLP研究,清华交叉信息研究院人工智能方向博士生招生（2025年入学）,article,https://api.zhihu.com/articles/696678092,注意这个夏令营只接受大三本科生。我目前计划主攻的方向有AI小镇（Social Simulacra），用AI做游戏（AI for gaming），和AI安全（AI Safety）。感兴趣的同学可以在夏令营报名的时候pick我哦（<em>人工智能</em>和自然语言处理方向）,210,15,168,1715204474,1715204954,我将于今年（2024）下半年入职清华叉院，我会远程参加招生夏令营（https://admission.iiis.tsinghua.edu.cn/）。注意这个夏令营只接受大三本科生。我目前计划主攻的方向有AI小镇（Social Simulacra），用AI做游戏（AI for gaming），和AI安全（AI Safety）。感兴趣的同学可以在夏令营报名的时候pick我哦（人工智能和自然语言处理方向）。我的个人主页：Tianxing He （UW Postdoc，MIT PhD）下面我会解释一下这些方向。 AI小镇：我们如何更加高效地用AI agent来模拟一个接近真实的人类社会沙盒？这种模拟能带来什么样的应用（工作训练，政府政策等）？如何搭建相关的dataset，benchmark，或者interface？出自斯坦福的AI小镇AI和游戏：AI能为游戏设计和游戏开发带来哪些变革？你是否想自己亲手做一个游戏demo？反过来，游戏引擎（如Unity）能如何帮助AI的研究？AI安全：AI有哪些新颖的应用？是否有手段去攻击这些应用？反过来，怎么去防御这些攻击？ 结语：如果想发邮件联系我的话，请写明为什么想和我合作，以及目前想做什么样的项目或者科研，当然还有你为什么有能力和相关经验来做这些事。谢谢！我的email: cloudygooseg at gmail.com，我uw邮箱的email也会看。
696644230,ce18d576388c4604dd91107368f85cb6,Qs.Zhang张拳石,https://api.zhihu.com/people/ce18d576388c4604dd91107368f85cb6,people,张拳石 上海交大副教授 博导 可解释性机器学习 招博士后,人工智能疯了，学无道，富且贵焉，耻也,article,https://api.zhihu.com/articles/696644230,2023年当大部分人撤离了刷榜的阵地之后，<em>人工智能</em>越来越疯了，也要把我逼疯了，或许有一天我会以其他的形式留在学术界，而不是陪着大家演戏。我希望严肃的面对这个世界,391,41,237,1715173184,1715185315,"2023年当大部分人撤离了刷榜的阵地之后，人工智能越来越疯了，也要把我逼疯了，或许有一天我会以其他的形式留在学术界，而不是陪着大家演戏。我希望严肃的面对这个世界，无奈台面上很多论文放弃了起码的准则，论文背后的学者、舆论、科研攻略一齐编织着皇帝的新衣。在多少交流场合，面向不同的学者的各式论文，我常常两句话打断你的报告，然后用5秒钟说出这一流派的根本问题，而你立马用10秒钟回复自己的无能为力，惊愕于我揭开整个方向的新衣，但是依然坚持你们中了很多论文而且，档次还是顶级。原谅我难以融入你们的默契。闭上嘴的南郭先生总是比抚起琴的阮籍看上去权威一些，但是耻也。就让我说说最近读到的顶级刊物的顶级论文。论文1：关于深度学习中某个问题有个思考A，针对这个思考A提出了验证性实验（或验证性指标）B，基于前面的实验得到了新的insight C，然后提出算法D，解决了C的问题。但是A于B之间没有五服以内的直接的亲缘（数学）关系，A于C之间也没有讨论相同范畴的事物，D在算法上并没有直接对C中的问题进行证明建模的讨论，而以一种无心插柳柳成荫的形式解决了C。论文2：做了个实验，构造了复杂任务A，然后在A的基础上做了些调整，构造了一个不太复杂的任务B，然后发现神经网络模型果然在相对简单的任务上性能更好一些（然后提出不同角度、不同指标论证这个结论），原因是B中给出了一些“捷径特征”——然后实验中果然发现了神经网络学到了这些捷径，这就是成果。论文3：我们把这个模型在A、B、C、D、E等等不同任务上做了测试，发现此模型在A、B、C上新能比较好，而没有能力建模D、E等任务。记得16年，我审稿过程中，类似的实验发现被多个审稿人联合拒稿，理由是这些结论都是众所周知的，没有创新，但是这样的论文2024年却中了oral。论文4：我发现了反直觉现象，把在A上训练一下，然后在B上微调一下，效果真好。然后A跟B看上去没有什么关系。论文5：我发现了一个结论C。但是C究竟是什么，好像说不清楚，似是而非，没有数学描述，没有定义的边界。motivation的哲学，实验设计凭直觉，得出的结论。。。论文6：是一篇理论，看上去挺唬人了。然后经验上大概30%的论文都可以证明出明显的错误。还有些论文十多年都没有真正影响过应用。为什么不给自己立一些原则呢？当看到在炼丹框架下的方法论很难得到根本性的突破，为什么不阻止自己做这些类型呢？说说XAI方向：工程性post-hoc explanation of DNNs无法给出严谨的解释，无法在重大应用中给出绝对的、可靠的评估，已经是七八年以来的共识了，为什么在大量position papers发表以后，大量工程性解释算法还是持续地发表呢？为什么不静下来想一想踏踏实实的核心问题呢？暂且不提创新性，有些论文的结论是correct，有些论文的结论是wrong，但是很大一部分论文not even wrong（可能一些人不清楚这是什么意思）。当无法对一个现象或结论的边界范围做出数学上的描述时，能不能先不急于把这个结论公布到学术界。可以只有实验而没有证明，但是对论题本身是不是要有个清晰的界定。对上述原则的坚持，在审稿过程中甚至常常成为拒稿的原因。何去何从我记得最早在2023年10月的时候，就跟一些同学和同事聊到过我的一个预测，后面也是不是提及。当一个领域的最顶级刊物中30%-50%以上的论文无法清晰地表达出其所得结论的范畴时，或者无法以任何形式（甚至没有资格）作为另一篇论文的研究基础时，这个领域的口碑就会急转直下，因为大家会发现很大一部分研究不会直接为领域的发展做出贡献，哪怕形式上的贡献。这里，甚至可以不考虑因为性能不济而不被引用的问题，而是很多论文中似是而非的结论根本无法被清晰地提炼表达出来，给出一个明确的指引。我认为这个时间点在2026年以前。大家都说很多工程性实验论文可以给出一些启发式的insights，为未来的扎实理论突破做铺垫。这是一个美好的幻想。除了一些及其浮夸的无法严格验证其普适性的现象孤例以外，绝大部分insights与2016年的认知并没有本质的拓展。大家在忙着各种事儿，忙着在规则体系内寻找向上的路径，但是对这个规则没有质疑和反思。这样的现象遍布全球，科举文化下尤甚。看到一个扩张的时代，就如当年的大炼钢铁，大家都清楚领域中在做什么，但是没有人高声说哪怕一句话，不断地新建、扩张、捷报频传，等着2026的到来。世上何曾存在过什么荣耀，不滑稽自欺已是千难万难。Qs.Zhang张拳石：这两年，我究竟做了些什么（2021-2023）Qs.Zhang张拳石：神经网络可解释性研究中常被忽略的几个根本问题Qs.Zhang张拳石：AI从技术到科学：神经网络中的概念符号涌现的发现与证明Qs.Zhang张拳石：证明神经网络精细决策逻辑可以严格解释为符号化等效交互概念Qs.Zhang张拳石：敢问深度学习路在何方，从统一12种提升对抗迁移性的算法说起Qs.Zhang张拳石：神经网络可解释性：正本清源，论统一14种输入重要性归因算法Qs.Zhang张拳石：可解释性理论系列：反思深度学习，去伪存真、合众归一198 赞同 · 16 评论文章211 赞同 · 17 评论文章220 赞同 · 17 评论文章上交大张拳石：深度学习可解释性，从百家争鸣到合众归一​mp.weixin.qq.com/s/KXdSwv_ypx1l2AIHrVb-3A​mp.weixin.qq.com/s/KXdSwv_ypx1l2AIHrVb-3A​mp.weixin.qq.com/s/KXdSwv_ypx1l2AIHrVb-3A​mp.weixin.qq.com/s/KXdSwv_ypx1l2AIHrVb-3AQs.Zhang张拳石：ICLR 2022 Oral论文中得分排名前五的高分论文“发现并证明神经网络表征瓶颈”（得分10,8,8,8）923 赞同 · 25 评论文章995 赞同 · 25 评论文章1030 赞同 · 25 评论文章1067 赞同 · 26 评论文章Qs.Zhang张拳石：神经网络的博弈交互解释性（一）：前言，漂在零丁洋里的体系268 赞同 · 20 评论文章275 赞同 · 20 评论文章前言，漂在零丁洋里的体系博弈交互概念、定义、定理、推论、与计算动机：建模知识，连接性能背景基础Shapley value双变元博弈交互多变元博弈交互，及其近似计算多阶博弈交互相关定理与推论自然语言交互树博弈交互与知识表达的关探索中低阶博弈交互所建模的视觉概念及泛化能力探索高阶博弈交互所建模的视觉概念神经网络对纹理概念的建模相比形状概念更具有弹性博弈交互与对抗攻击的关系，推导证明与实验证明博弈交互与对抗迁移性的负相关关系证明多个前人迁移性增强算法可近似归纳解释为对博弈交互的抑制交互损失函数与迁移性的增强博弈交互与泛化能力的关系，推导证明与实验探索交互强度与泛化能力的关系证明Dropout对交互强度的抑制交互强度损失函数与泛化能力的提升从博弈交互层面解释对抗鲁棒性对抗攻击在多阶博弈交互上的效用从知识构成的层面探索对抗训练提升鲁棒性的原因去芜存菁：解释并萃取多个前人防御算法中公共的有效机理神经网络对抗迁移性：从神农尝百草到精炼与萃取完善Shapley value理论体系，建模并学习基准值在博弈交互体系内，对“美”提出一个假设性建模可解释性核心——神经网络的知识表达瓶颈博弈交互与神经网络知识表征发现并理论解释神经网络的表达瓶颈突破表达瓶颈及探究不同交互复杂度下的表达能力敢问深度学习路在何方，从统一12种提升对抗迁移性的算法说起神经网络可解释性：正本清源，论统一14种输入重要性归因算法对智能模型中概念涌现的证明数学证明神经网络中符号化概念涌现的现象可解释的哈萨尼网络通过博弈交互 某某某某某某通过博弈交互 某某某某某某"
695641297,95f2e2be27f19827ffe78b7d18877f0d,AIDD Pro,https://api.zhihu.com/people/95f2e2be27f19827ffe78b7d18877f0d,people,人工智能与药物研发领域发展探索者,使用量子增强人工智能设计新型 KRAS 抑制剂,article,https://api.zhihu.com/articles/695641297,Computer (Zapata AI) 与Insilico Medicine、多伦多大学和圣裘德儿童研究医院合作，在使用量子增强<em>人工智能</em> (<em>AI</em>) 设计新型 KRAS 抑制剂方面取得了重大进展,1,0,1,1715168668,1715168668,"使用量子增强人工智能设计新型 KRAS 抑制剂图片来源：iStock、Bartlomej WroblewskiZapata Computer (Zapata AI) 与Insilico Medicine、多伦多大学和圣裘德儿童研究医院合作，在使用量子增强人工智能 (AI) 设计新型 KRAS 抑制剂方面取得了重大进展。该研究利用在 16 量子位 IBM 量子设备上运行的生成式 AI 模型产生了 100 万种潜在的候选药物。通过算法过滤和人工评估，这分子被缩减为15个，之后进行了合成和基于细胞的分析测试。图 1. 用于开发 KRAS 配体的量子-经典混合框架示意图。初始阶段主要是数据挖掘和制作用于模型训练的数据集。作者从文献中提取了经过实验验证的 650 种针对 KRAS 蛋白的抑制剂。通过应用 STONED-SELFIES 算法，得出每个已识别化合物的类似物，从而扩充了约 850,000 个化合物集合。使用针对 KRAS 蛋白的 REAL 配体库进行虚拟筛选，确定了前 250,000 个候选化合物，最终形成了一个包含 100 多万个分子的数据集，用于训练生成模型。完成模型训练后，作者将利用经典长短期记忆（Long Short-Term Memory ） LSTM 模型和量子回路天生机（Quantum Circuit Born Machines (QCBM) 作为基础生成框架，创建靶向 KRAS 的新分子。LSTM 网络处理包含配体化学结构的序列数据，而 QCBM 则根据 LSTM 生成样本的质量进行训练，创建复杂的高维概率分布。组合工作流程利用 Chemistry42 作为奖励函数，鼓励创造结构多样化和可合成的分子。KARS-难以开发的靶点KARS是RAS 基因家族中的一员。由于RAS 基因家族经常在人类癌症中发生突变，导致直接靶向 RAS 蛋白的研究总是失败。最近有科研人员研发了直接靶向的KRAS G12C 抑制剂-sotorasib（AMG 510）和 adagrasib（MRTX849），改变了 KRAS ""不可成药 ""的观点。然而，这两种抑制剂作为单药的临床疗效却并不持久。什么是量子回路天生机量子回路天生机（QCBM）是一种具有生成能力无监督的变分量子算法。QCBM 与通常依赖经典神经网络的经典机器学习模型不同，它还具有叠加和纠缠的固有量子特性。与传统机器学习的方法相比qcbm的泛化能力，生成新的、有效的样本方面的能力更强，但量子信息处理在数据加载和可训练性等方面仍有许多不足。因此，作者取其精华去其糟粕，提出了一个新的量子-经典生成模型。图 2. 用于药物发现应用的量子增强生成模型。(A) 结合量子回路天生机（QCBM）和长短期记忆（LSTM）的混合模型。该模型使用来自量子硬件（quantum hardware）的先验样本进行迭代训练。(B) 将先验样本整合到 LSTM 架构中的方法。分子信息（SELFIES 编码）和量子数据通过加法或并集的方式进行合并。然后将得到的样本 X ′ (t) 输入 LSTM 单元。(C) 量子先验组件被描述为 QCBM，每个训练历时从量子硬件生成样本，并使用 Chemistry42 或局部滤波器计算的奖励值 P(x) = Softmax(R(x)) 进行训练。(D) 实验样本选择过程：从每个模型中抽取 100 万个化合物样本，包括经典样本（通过 vanilla LSTM）、量子样本（量子硬件上的 QCBM）和模拟样本（经典硬件上的量子模拟）。这些样本经过 Chemistry42 的评估，筛选出不适合药理学目的的化合物，并根据对接得分（PLI 分数）对剩余化合物进行排序。随后，选出 15 个新化合物进行合成。结论作者介绍了一种旨在发现特定分子靶标的新配体的量子-经典混合算法。其方法巧妙地将作为先验分布的量子回路天生机（QCBM）与经典的长短期记忆（LSTM）模型结合在一起，并通过旨在促进类药物小分子生成的奖励函数来加强这种结合。作者通过对两组不同任务的计算评估，对进行了基准测试，结果显示作者的模型生成高质量分子的能力更高，但是与经典方法相比对接部分的得分略低。此外，作者还研究了量子比特数量对先验分布建模的影响，并观察到生成高质量分子的建模成功率与所涉及的量子比特数量大致呈线性相关。作者推出了一种量子-经典混合算法。虽然该算法使用的量子比特数量不多，并且没有任何纠错功能，连接性也有限，但是目前的该算法的性能已经超越了传统算法，这预示着更先进的量子计算机和更好的量子-经典算法在未来的药物发现中有着巨大的潜力。随着量子比特数量的增加、保真度的提高、纠错能力的增强以及连接性的增强，量子计算在药物发现中的应用前景会逐渐成为计算和实验科学的新风潮。参考资料：Vakili M G, Gorgulla C, Nigam A K, et al. Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS[J]. arXiv preprint arXiv:2402.08210, 2024.版权信息本文系AIDD Pro接受的外部投稿，文中所述观点仅代表作者本人观点，不代表AIDD Pro平台，如您发现发布内容有任何版权侵扰或者其他信息错误解读，请及时联系AIDD Pro (请添加微信号sixiali_fox59)进行删改处理。本文为原创内容，未经授权禁止转载，授权后转载亦需注明出处。有问题可发邮件至sixiali@stonewise.cn"
696396356,47767cbcbfe7a85006b1a4d45202ca08,西湖大学<em>人工智能</em>,https://api.zhihu.com/people/47767cbcbfe7a85006b1a4d45202ca08,people,Westlake AI 官方发布,面向全球本硕学生，西湖大学人工智能暑期公开课开启报名,article,https://api.zhihu.com/articles/696396356,机器学习、深度学习、数据科学； 2. <em>AI</em> 核心应用：计算机视觉、语音语言处理、机器人、数据挖掘； 3. <em>AI</em> for Science：AI 生命科学、<em>AI</em> 生物医学、AI 学科交叉。 *报名通道现已开启,14,2,18,1715071579,1715071579,"与AI科学家面对面，你最想问什么？是复杂算法背后的秘密，还是未来技术的趋势？是人工智能如何改变世界，还是AI职业发展策略？2024年7月28日至8月11日，西湖大学工学院将开设为期2周的人工智能暑期公开课，12位人工智能与数据科学领域的资深教授将来到西湖大学云谷校区，邀请本科生、硕士生共同探讨人工智能发展的最前沿。听他们讲讲，当今世界上最先进的AI大模型、类人学习，以及AI在医疗、金融、教育等各个领域的应用；还有机会走进西湖大学人工智能相关实验室或产业转化基地，感受AI在多个领域的萌芽和发展，实地了解一项科技成果从“实验室”走向“应用场”的全过程。西湖大学人工智能暑期公开课主要涵盖的研究方向包括：1. AI 基础研究：机器学习、深度学习、数据科学；2. AI 核心应用：计算机视觉、语音语言处理、机器人、数据挖掘；3. AI for Science：AI 生命科学、AI 生物医学、AI 学科交叉。*报名通道现已开启，具体报名方式详见文末人工智能暑期公开课首批12位导师介绍人工智能与数据科学领域是西湖大学工学院重点建设的方向。工学院AI分支目前已有20位PI（特聘研究员、博导），他们来自世界知名的实验室和科研机构，除了组建实验室进行AI相关研究外，也承担本科生、博士生的教学任务。2024年西湖大学人工智能暑期公开课，首批12位导师阵容正式发布。首发阵容包含西湖大学AI方向2位讲席教授，和来自国内外高校的10位资深教授，他们将为学生们带来AI 基础研究、AI 核心应用、AI for Science等方向的前沿新知。*以下是首批12位导师介绍（按姓氏拼音排序）程涛教授伦敦大学学院程涛教授是伦敦大学学院地理信息学教授，博士生导师，图灵研究所研究员，大数据分析SpaceTimeLab的创始人和主任。她的研究兴趣包括人工智能和大数据、网络复杂性、城市分析（建模、预测、聚类、可视化和模拟），及其在交通、商业、健康、社交以及犯罪和自然灾害预防等方面的应用。她在英国和欧盟获得了2500多万英镑的研究经费，与英国的多个政府机构和企业有深度合作。她发表了300多篇研究论文，并获得了众多国际最佳论文奖。何瑜岚教授伦敦国王学院何瑜岚为英国伦敦国王学院（King’s College London）计算机系教授，2021年入选英国AI人才计划，获Turing AI Fellow称号。何瑜岚长期从事自然语言处理领域的研究工作，在这些领域中发表相关论文超过200篇。研究方向包括机器阅读理解、模型可解释性和可信人工智能以及NLP在医疗、金融和教育领域里的应用。她的研究获得了多项奖项，包括SWSA十年奖及CIKM时间检验奖。受邀担任自然语言处理领域的国际顶级会议EMNLP2020的程序委员会主席、AACL2022大会主席及多个国际顶级会议的（资深）区域主席。何瑜岚主持了24项科研项目，总价值超过600万英镑，资助方包括英国工程及物理科学基金、英国创新科技局、英国皇家工程学院，欧盟地平线2020及欧盟第七框架。曾担任华为欧洲区自然语言处理领域顾问，目前担任伦敦证券交易所集团顾问，对大规模语言模型相关业务提供建议。2020年入选清华大学-中国工程院知识智能联合研究中心、清华大学人工智能研究院与北京智源人工智能研究院发布的人工智能全球女性Top100榜单，并获得AI 2020最具影响力学者提名奖。金耀初教授西湖大学金耀初，西湖大学人工智能讲席教授，欧洲科学院院士、国际电气和电子工程师协会会士（IEEE Fellow）。曾任芬兰科学院与芬兰国家创新局“芬兰杰出教授”、德国联邦教育与研究部“洪堡人工智能教席教授”。金耀初教授已出版专著5部，在多个IEEE汇刊及CVPR、NeurIPS、ICLR及ACM MM等学术会议发表论文500余篇，获美国、欧盟和日本专利9项。多次获“IEEE进化计算汇刊优秀论文奖”及“IEEE 计算智能杂志优秀论文奖”。曾任《IEEE认知与发育系统汇刊》主编，IEEE计算智能学会副理事长，两次担任IEEE 杰出演讲人。任2016 IEEE 计算智能系列研讨会总主席、2020 IEEE 进化计算大会主席等。长期从事人工智能与计算智能的理论、算法和工程应用研究，特别是数据驱动的优化、多目标优化，演化机器学习，安全与隐私保护的机器学习与优化、图神经网络组合优化、演化发育通用人工智能及形态发育自组织机器人等。Josef Kittler教授萨里大学Josef Kittler教授为英国皇家工程院院士、国际模式识别协会前主席，英国萨里大学计算机与电子工程学院杰出教授。Josef Kittler教授是模式识别研究领域享有盛誉的国际顶级学者，研究领域主要包括模式识别、生物特征识别、机器学习和人工智能。其合著著作《模式识别：一种统计学方法》是模式识别研究领域的经典著作之一。迄今为止，教授共发表论文1000余篇，学术引用近七万次，承担了数十个英国和欧盟政府资助的研究课题。Josef Kittler教授还于1986年创立了英国萨里大学计算机视觉、语音和信号处理中心（CVSSP），该中心是目前英国最大的计算机视觉和人工智能研究中心，在计算机视觉研究领域名列英国第一。李子青教授西湖大学李子青（Stan Z. Li, IEEE Fellow），西湖大学人工智能讲席教授，曾任微软亚洲研究院lead researcher、中科院自动化所模式识别国家重点实验室资深研究员。发表论文500余篇，著作10部，Google Scholar引用65000余次，World Scientist and University Rankings 2024 计算机学科中国区排名第2。他领导开发了世界上第一个实时人脸识别系统，设计研发了多个国家级人脸识别系统，并落地实施应用。当前负责实施科技部“新一代人工智能”重大项目2项、国家自然科学基金区域重点项目（AI+生命科学）1项。实验室研究包括两大方向：AI基础研究，包括图/序列/多模态表征学习、自监督学习、生成模型、预训练方法；AI for Science研究，包括AI+生命科学、AI+合成生物学等。刘兵教授伊利诺伊大学芝加哥分校刘兵，伊利诺伊大学芝加哥分校杰出教授，于爱丁堡大学获得人工智能博士学位。ACM Fellow，IEEE Fellow，主要从事语义分析、观点挖掘、数据挖掘、机器学习以及自然语言处理等方面的研究，是语义分析、观点挖掘研究领域的开创者之一。刘兵教授有两篇论文在KDD会议中获得“10-year test-of-time”奖项，其工作曾被纽约时报头版报道。刘兵教授还担任ACM SIGKDD Chair，DMKD Action Editor，TWEB、KAIS等期刊的Associate Editor。他曾在2013-2017年担任ACM SIGKDD主席，并担任众多重要数据挖掘会议的项目主席，也是2018年ACM SIGKDD创新奖得主。Marios M. Polycarpou教授塞浦路斯大学Marios M. Polycarpou教授为欧洲科学院院士、IEEE Fellow、IFAC Fellow、塞浦路斯科学文学和艺术学院院士，现担任塞浦路斯大学KIOS智能系统与网络研究中心主任，是塞浦路斯大学电气与计算机工程系创始成员，英国帝国理工学院荣誉教授，《Proceedings of the IEEE》《Annual Reviews in Control》《Foundations and Trends in Systems and Control》编辑委员会成员。他的教学和研究领域包括智能系统与网络，自适应与协同控制系统，计算智能，故障诊断与分布式处理等。发表超过300篇学术期刊，专辑和学术会议论文，并与他人合作撰写专著7本。他是IEEE和IFAC的会士，曾担任IEEE计算智能学会主席以及IEEE Transactions on Neural Networks and Learning Systems杂志主编，获2016年IEEE神经网络先驱奖，先后参加了60多个欧洲和美国的专业机构和工业的研究项目，包括欧洲研究理事会（ERC）高级拨款、ERC协同拨款和欧盟团队计划。申强教授亚伯大学申强教授是英国皇家工程院院士，威尔士科学院院士、理事会成员，英国亚伯大学副校长兼商学与物理科学学部主任。曾连续两次被任命为英国高校及研究院卓越研究评估框架（REF2014：2008-2014 和 REF2021：2014-2021）的计算机科学和信息学评委会成员。在众多国际会议任主席或发表主题演讲，并长期担任英国计算智能指导委员会主席。撰写两部研究专著和450多篇论文，是2024 IEEE 模糊系统先驱奖获得者。申强教授的研究包括：计算智能、不确定性学习与推理、模式识别、数据建模与分析及其在智能决策支持中的应用。沈春华教授浙江大学沈春华教授，本科毕业于南京大学、后在阿德莱德大学获得博士学位。机器视觉领军学者。现任浙江大学计算机学院、计算机辅助设计与图形学国家重点实验室求是讲席教授。2012年，沈春华获得澳大利亚研究委员会的未来学者称号。此外，他担任澳大利亚研究委员会机器人视觉卓越中心的科研负责人。2020年，沈春华获得澳大利亚科研终身成就奖，是工程与计算机类奖项的5名获选人之一。据计算机排名网站 CSRanking 显示，沈春华是过去10年间在计算机视觉方向3大顶级会议发表论文最多的在澳学者。沈春华教授主要研究领域为目标检测、图像分割等方向。杨先一教授圭尔夫大学杨先一教授于1987年毕业于北京大学技术物理系，1990年获中国科学院生物物理硕士学位；1996年12月获美国休斯敦大学电子与计算机工程第二个硕士学位；1999年6月获加拿大阿尔伯塔大学电机与计算机工程博士学位。现任加拿大圭尔夫大学高级机器人与智能系统实验室主任。杨先一教授成功地提出了一个“受生物启发的神经网络系统（Biologically Inspired Neural Network Framework）”，该系统可用于各种机器人系统的实时环境感知、路径规划、目标跟踪和控制，并发表了一系列相关研究论文。他被认为是“将受生物启发的方法用于机器人学与控制系统”的先驱者之一。杨教授的研究方向包括机器人、人工智能、传感器和信号处理、多传感器融合、无线传感器网络、智能控制和计算神经科学。张孟杰教授惠灵顿维多利亚大学张孟杰教授是新西兰皇家科学院院士（Fellow of RSNZ），IEEE会士/院士（Fellow of IEEE），IEEE杰出讲席教授（IEEE Distinguished Lecturer）。现任新西兰惠灵顿维多利亚大学学术委员会委员，科学研究委员会成员，奖学金评定委员会委员；惠灵顿维多利亚大学工学部副部长兼工程与主计算机学院科学研究委员会主席，进化计算研究中心主任，计算机科学首席教授，博士生导师；惠灵顿维多利亚大学人工智能、机器学习、大数据及进化计算学科及科学研究牵头人。主要研究领域为人工智能、机器学习与大数据，尤其是在进化计算与学习（遗传编程、粒子群以及学习分类系统）、特征提取、选择、构造和变换及高维降维、计算机视觉与图形处理、作业车间调度和资源分配，多目标优化，数据不平衡分类和缺失数据分类，自动进化深度学习和迁移学习。张峥教授亚马逊云科技上海人工智能研究院张峥教授是亚马逊云科技资深首席科学家、亚马逊云科技上海人工智能研究院院长，本科毕业于复旦大学电子工程系，后获得美国伊利诺伊大学厄巴纳-香槟分校（UIUC）博士学位。他曾任上海纽约大学计算机终身教授，研究领域为深度学习、人工智能、高性能大容量计算和存储系统，多次获国际学术会议论文奖，是开源深度学习平台MXNet和DGL的共同创始人和顾问。张峥教授在大规模分布式计算理论与实践、及其与机器学习的交叉领域被公认为经验丰富的世界级专家。报名通道现已正式开启面向全球遴选优秀学生公开课时间2024年7月28日至8月11日申请截止时间2024年6月15日17时整申请方式访问下方链接，申请参加西湖大学人工智能暑期公开课。https://www.wjx.top/vm/ryK8VrQ.aspx#审核及录取工学院资格审查小组组织评审，根据报名情况择优录取50人。遴选结果将于2024年6月28日前通过邮件另行通知，电子邮箱、手机号码等本人联系方式请保持通讯通畅。费用说明本次人工智能暑期公开课免收学费。针对报名成功并通过评审的申请者，主办方将承担其城市间交通费用、住宿费及餐食。主办方将为完成2周全部公开课课程的学员发放证书。联系方式电话：0571-87381209邮箱：summercamp_se@westlake.edu.cn*公开课详细课程内容及日程安排敬请关注西湖大学人工智能知乎号的后续通知"
696248231,b25aca5efe1523a2ba8984f89efacf10,<em>AI</em>技术通,https://api.zhihu.com/people/b25aca5efe1523a2ba8984f89efacf10,people,,推荐9个人工智能平台,article,https://api.zhihu.com/articles/696248231,推荐9个<em>人工智能</em>平台 多年来，人工智能(<em>AI</em>)覆盖了我们生活的方方面面。从企业到日常生活，<em>人工智能</em>技术的使用越来越频繁。为了最大限度挖掘<em>人工智能</em>的潜力，需要充分了解目前主流的人工智能平台,1,0,0,1715004451,1715004451,推荐9个人工智能平台多年来，人工智能(AI)覆盖了我们生活的方方面面。从企业到日常生活，人工智能技术的使用越来越频繁。为了最大限度挖掘人工智能的潜力，需要充分了解目前主流的人工智能平台。了解哪些工具可用以及它们提供的功能可以帮助您在选择满足您需求的人工智能平台时做出明智的决定。在这篇博客中，我们将分享9个人工智能网站和程序，这些网站和程序为商业开发和个人开源。加快工作流程的9大人工智能平台1. Google谷歌人工智能平台是一个全面的套件，供企业和开发人员快速探索数据集。谷歌发布了Google AI，利用其开放资源和人工智能工具解决问题，包括自然语言处理、计算机视觉和机器学习。它为企业和开发人员提供机器学习(ML)工具以及从预训练的模型到定制的ML解决方案。此外，凭借其自动ML工具，它变得非常容易创建，部署和管理自己的AI模型。2. TensorFlowTensorFlow是一个开源平台，你可以从中获得免费的资源，从而彻底改变机器学习体验。它为设计和构建深度学习模型提供了通用的服务。使用TensorFlow ai平台，公司或开发人员也可以轻松地跨多个平台部署，也可以在多种设备上运行。TensorFlow生态系统拥有强大的资源，可以促进行业和领域的研究和应用。不仅如此，它还支持机器学习算法，包括用于复杂预测分析的神经网络。在人工智能的支持下，TensorFlow可以推动创新并提高性能。3. Microsoft Azure微软Azure提供了一套全面的云服务，可以满足你的所有业务需求。微软Azure为人工智能、分析、物联网等提供解决方案。此外，它允许企业使用其人工智能工具和框架在全球范围内构建、部署和管理应用程序。其次，它具有极高的安全性和多层保护，以便在Microsoft软件和第三方软件关于系统的灵活性方面提供最大的便利。使用Azure平台，你可以根据自身条件进行构建，在任何设备上进行部署来推动你的开发。同时，Azure有助于提高效率、降低成本，并在行业或领域获取竞争优势。4. OpenAIOpenAI平台提供了强大的模型，如GPT-4。这些模型经过良好的训练，可以理解和生成类似人类创作的文本，从而为跨多个领域的通用应用程序提供支持。OpenAI的模型具有翻译语言、回答问题、撰写文章等功能。它不仅提供了强大的预训练模型，而且还提供详细注释的文档，以帮助你理解、实施和充分利用这些AI工具。无论你是人工智能技术的新手还是经验丰富的开发人员，你都能很快上手。除此之外，它强调安全和公平，制定严格的政策来防止滥用。为研究、开发和部署提供坚实的基础，OpenAI平台可以是一个游戏规则的改变者。5. NVIDIANVIDIA的AI平台是加速计算领域的创新。NVIDIA AI可以提供业界领先的解决方案，包括机器学习、深度学习和数据分析。借助GPU和CUDA-X AI软件等强大的生态系统。NVIDIA AI可帮助你将数百万个数据集转化为引人注目的见解。此外，它致力于构建安全、可扩展且易于使用的解决方案，使NVIDIA成为满足人工智能和数据科学需求的可靠合作伙伴。6. H2O.aiH2O是一款用于数据分析的开源软件，可以让你做出更快的决定。它支持大多数机器学习任务，从快速数据摄取和数据转换到高级模型构建和深刻的模型解释。有了它，你可以一次在多台计算机上构建数千个模型。H2O还提供了自动 ML功能供开发人员使用。H2O.ai是一个出色的平台。可以提供一种确保人工智能模型的公平性、可解释性和可问责性的综合方法。H2O鼓励你构建和部署人工智能解决方案，不仅可以提供结果，还可以解释决策。7. Amazon Web Services亚马逊网络服务(AWS)允许你通过机器学习更快地进行创新。它提供了一套机器学习服务和云基础设施和框架，帮助开发人员为其行业和领域创建复杂而智能的应用程序。使用AWS，你可以从训练有素的人工智能服务中选择计算机语言、建议和预测。甚至可以使用它的SageMaker构建、训练和部署自己的模型。它提供了各种灵活性、可扩展性和低成本的服务，其深度学习AMI和高性能计算实例使其成为处理复杂机器学习任务的绝佳途径。借助AWS，你可以利用人工智能技术的强大功能来提升业务水平，并达到更有效的结果。8. DataRobotDataRobot平台可以有效地开发、部署和监控其人工智能和机器学习模型。DataRobot的自动ML和自动实时功能简化了数据建模过程，使开发人员都可以使用它。该平台还提供模型管理和监视工具，确保您的模型稳健可靠。有了DataRobot的人工智能云，无论数据的大小或复杂程度如何，你都能得到一个灵活的、可扩展的解决方案。9. FotorFotor是一个人工智能平台，你可以用它来增强你的图形设计。Fotor提供了一系列人工智能服务，可以帮助你改变创作艺术的方式。也可以使用它的人工智能照片编辑器来使你的图像更有创意。在这个人工智能工具的帮助下，艺术品触手可及。它最突出的特点是人工智能图像生成器，这是一个人工智能程序，只需根据提示输入，就可以获取生成的图像。在机器学习工具的支持下，Fotor彻底改变了我们创作艺术的方式。结论在这篇博客中，分享了9个最好的人工智能平台，它们彻底改变我们的工作和生活方式。借助人工智能程序，你的业务将提升到一个新的水平，更高效部署、管理和分析数据集。无论你是初学者还是经验丰富的开发人员，都可以尝试这些AI平台。希望这篇博客对你有帮助!本文转载自 雪兽软件
696249536,215be2baf2544fb5ab51128f9a3f594e,我和脂肪作斗争,https://api.zhihu.com/people/215be2baf2544fb5ab51128f9a3f594e,people,&lt;190,人工智能来袭！合并报表能否实现“一键合并”,article,https://api.zhihu.com/articles/696249536,目前主要负责集团公司的财务报表合并工作。 恰逢目前<em>人工智能</em>热度较高，本人又喜欢研究新事物新热点，因此打算从<em>人工智能</em>的方向去研究如何通过自动化的方式，完成繁杂的合并报表工作,2,0,0,1715006643,1715006643,本人从事审计工作7年，财务工作5年，目前主要负责集团公司的财务报表合并工作。恰逢目前人工智能热度较高，本人又喜欢研究新事物新热点，因此打算从人工智能的方向去研究如何通过自动化的方式，完成繁杂的合并报表工作。本集团目前分为两个分部，可以分别叫做事业群A和事业群B（以下简称A和B），A的下属公司有10个，其中有个最主要的C，C的下属公司有70多家，为集团最主要的孙公司，负责全国各地、各个业态的销售单元。因此集团合并需要首先完成C的合并，然后再分别完成A和B的合并，最后是集团合并，因此整个合并需要3个层级。再说说具体业务，集团为贸易公司，主要通过C的下级单位HK与HZ进行外部采购，再批发给A和C的下级子公司A2-A10、C2-C70，这些子公司分布在全国各地，通过门店进行对外销售。主要业务包括内部批发与采购，内部资金归集与下划，内部管理咨询、内部租赁与物业服务等等，业务链条相对简单。目前集团通过国内财务软件公司新上线合并报表系统，可实现不同层级报表合并，通过【合并计算】功能，实现不同抵消规则的抵消，也可以通过手工勾选方式，将未自动抵消的往来、交易、现金流等进行手工抵消，最后将抵消数据反写到差额表，进行数据汇总，实现报表半自动化合并。集团架构但目前在合并过程中存在一些问题，影响合并报表自动化效率：（1）因内部交易存在时间差异、入账不同步等因素，需要手工勾选大量内部交易；（2）内部核算不规范，挂的客商不准确，抵消分录不完整；（3）内部对账差异大，超过自动生成抵消分录阈值；（4）内部租赁业务需要手工填写内部抵消抵消模板；（5）增量权益抵消无法实现，需要手工抵消；（5）特殊权益交易无法实现，也需要手工抵消。存在以上问题，导致无法实现快速高效完成报表合并。那么问题来了，现在人工智能不断升级迭代，是否能实现合并报表的自动化操作呢？先Mark一下这个问题，自己也在不断调研与积累相关知识，看看多久能实现报表自动化，一键合并。@2024年5月6日
696249676,3007a61156a3b3f23733e05ba4a2b42b,英国学术科研资讯,https://api.zhihu.com/people/3007a61156a3b3f23733e05ba4a2b42b,people,,全奖博士丨伦敦 Meta 基础人工智能研究 (FAIR) 团队招聘全奖博士,article,https://api.zhihu.com/articles/696249676,Meta正在寻找一名博士生加入其在伦敦的基础<em>人工智能</em>研究(FAIR)团队。我们致力于通过在科学方法和技术上取得根本性的进步来推动<em>人工智能</em>领域的发展，以帮助我们与世界互动和理解世界,6,0,4,1715005053,1715005053,"岗位简介Meta正在寻找一名博士生加入其在伦敦的基础人工智能研究(FAIR)团队。我们致力于通过在科学方法和技术上取得根本性的进步来推动人工智能领域的发展，以帮助我们与世界互动和理解世界。在伦敦，我们专门研究自然语言处理、3D计算机视觉和强化学习。对于这个职位，我们正在寻找一个对进一步了解大型基础模型并使其更安全、更准确有热情的人。博士职位为伦敦大学学院的博士生，为期4年，将与伦敦大学学院人工智能中心合作，由该实验室的学者和FAIR的研究科学家共同监督。本着FAIR的总体使命精神，我们希望Meta和学术合作伙伴的博士们的工作都是开放的科学和开源的。岗位职责研究推进智能系统的科学和技术的发展。研究以了解基础模型所学习的机制。通过出版物影响相关研究领域的进展。与现有研究项目的团队合作，提高生产力。最低资格要求掌握机器学习方面的知识。掌握一门编程语言。在雇佣时必须在就业国获得工作许可，并在就业期间保持持续的工作许可。申请人需要单独申请并被伦敦大学学院录取，并在伦敦大学学院申请表中提及他们的FAIR申请。优先录用条件技术领域的本科或硕士学位(或同等学历)，如计算机科学，应用数学；或同等的实践经验。有基于机器学习和/或深度学习方法的系统构建经验。通过实习、工作经验、编码竞赛或开源贡献展示研究和软件工程师经验。熟悉Python, Lua, C++， C, C#和/或Java有机器学习、人工智能、计算机视觉、自然语言处理、计算神经科学、优化、计算机科学、统计学、应用数学或数据科学相关领域的出版物或经验。关于MetaMeta构建的技术帮助人们联系、寻找社区和发展业务。当Facebook于2004年推出时，它改变了人们联系的方式。Messenger、Instagram和WhatsApp等应用程序进一步增强了全球数十亿人的能力。现在，Meta正在从2D屏幕转向增强现实和虚拟现实等沉浸式体验，以帮助构建社交技术的下一次进化。选择与我们一起在Meta建立职业生涯的人们将帮助塑造一个未来，这个未来将带领我们超越今天数字连接所带来的可能性——超越屏幕的限制，距离的限制，甚至物理规则。Meta致力于在招聘过程中为残疾、长期疾病、精神健康状况或虔诚的宗教信仰、神经分化或需要怀孕相关支持的候选人提供合理的住宿支持。如果您需要帮助，请联系：accommodations-ext@fb.com申请链接https://www.metacareers.com/jobs/288971014265574/"
696231765,b5379f1a6ed18db1d60c50fdf71b469b,DeepTech深科技,https://api.zhihu.com/people/b5379f1a6ed18db1d60c50fdf71b469b,people,科学、技术、产业。,上海交大团队研发通用人工智能，解决传统蛋白质工程难题,article,https://api.zhihu.com/articles/696231765,在一定程度上打破了传统方法面临的瓶颈，利用 AI 来设计和改造蛋白质，逐渐成为该领域的大势所趋。 自主研发蛋白质设计通用<em>人工智能</em>，实现从序列到功能的精准蛋白预测 据介绍，在 AI 蛋白质设计领域,5,0,0,1714997481,1714997488,"“作为一名基础科研人员，当我第一次看到我们研发的蛋白质工程通用人工智能技术，实现面向功能的蛋白序列设计，并被湿实验验证成功之时，心中涌起的激动是无与伦比的。”上海交通大学自然科学研究院&物理与天文学院&药学院特聘教授洪亮表示。图 | 洪亮（来源：洪亮）他进一步解释道，这意味着以前需要靠专家经验和大量实验试错的蛋白质工程，现在可以通过通用人工智能进行定向设计，从而数倍乃至数十倍地减少时间和经济成本。此外，由于该模型具有通用性，对各领域都适用，决定了它将大大加速我国生物制造业、合成生物学、生物医药等领域的发展，帮助我国企业与国际头部公司进行良性互动与竞争。相关论文以《蛋白质工程与轻量级图去噪神经网络》（Protein Engineering with Lightweight Graph Denoising Neural Networks）为题发表在 Journal of Chemical Information and Modeling 上[1]。上海交通大学自然科学研究院助理研究员周冰心博士为第一作者，洪亮教授担任通讯作者。图 | 相关论文（来源：Journal of Chemical Information and Modeling）现如今，洪亮和团队开发的蛋白质设计通用人工智能 AccelProtein™，通过 AI+ 计算的“干实验”与高效的“湿实验”协同闭环迭代，解决了传统蛋白质工程中研发时间长、成本高、上位组合差等核心问题，为体外检测、合成生物学等领域提供了数十款性能优异的蛋白质产品。利用通用人工智能设计蛋白质，已成为蛋白质工程领域的大势所趋众所周知，蛋白质是生命系统的基础，在细胞、组织和器官中扮演着重要角色。除了它所拥有的生物学意义，蛋白质对于众多行业应用来说也至关重要，具有广泛的市场价值。例如，在生物医学领域，可以作为药物靶点和治疗剂；在化学工程领域，能充当各种反应的关键催化剂。不过，自然界的蛋白质，通常需要经过工程改造，提高它的活性、热稳定性、对极端 PH 环境和恶劣溶剂的耐受性等多种指标之后，才能在各类工业应用中获得应用。而利用传统蛋白质设计需要经历长达数年的实验研究，不仅耗时耗力、成本较大，也愈发不能满足许多工业应用中重要蛋白质的改造要求。近年来，深度学习技术的发展，在一定程度上打破了传统方法面临的瓶颈，利用 AI 来设计和改造蛋白质，逐渐成为该领域的大势所趋。自主研发蛋白质设计通用人工智能，实现从序列到功能的精准蛋白预测据介绍，在 AI 蛋白质设计领域，洪亮已有多年研究经验。他本科和硕士分别毕业于中国科学技术大学和香港中文大学的物理系，博士时期在美国阿克伦大学高分子科学系从事蛋白质生物物理方面的机制研究。在美国橡树岭国家实验室完成博士后研究后，他来到上海交通大学，通过将实验和计算生物学方法进行结合的方式，继续对蛋白质的性能进行研究。“其实这些研究都属于‘后解释’的范畴。换言之就是，对蛋白质的一些物理机制进行解释，比如它的运动形态和各种热力学参数如何影响其功能的发挥。”洪亮解释说。2020 年，AlphaFold 的问世为洪亮开启 AI 蛋白质设计研究打造了一个契机。“用户只需向 AlphaFold 输入蛋白质序列，就能得到准确的结构预测，这对于整个分子生物学领域来说非常震撼。但 AlphaFold 只解决了从序列到结构的问题，没有解决结构到功能的问题，我们想做一套打通结构到功能的通用人工智能，彻底打破传统蛋白质工程方法的禁锢。”他说。因此，他开始带领团队做 AI 蛋白质设计方面的研究，并在 2021 年开发了一套基于预训练的蛋白质设计的通用人工智能 AccelProtein™ ——与 AlphaFold 预测结构不同，AccelProtein™ 开创性地实现了从序列直达功能的精准蛋白质设计。具体来说，该课题组通过预训练方法，让 AccelProtein™ 学习自然界已知的所有蛋白质序列和结构特征，并探索与理解自然界中蛋白质序列与功能的映射规律，从而开发出一套能够高效地设计出稳定性好、活性高、功能性强的 AI 蛋白质设计通用大模型。那么，该模型如何实现精准的蛋白质设计？据洪亮介绍，自然界已知的具有完整氨基酸序列的蛋白质有几亿条，这些蛋白质的氨基酸序列以存在即合理的方式排列着。在掌握这些序列以后，该团队采用双重任务学习方法：一方面，帮助大模型在经过预训练学习以后，掌握满足蛋白质序列排布的语言规则，另一方面，通过所构建的亿量级蛋白质标签数据库，为蛋白质打上标签，进一步提升模型精度，从而提供精准、高效地蛋白质设计，大大降低试错成本。和同类通用人工智能模型相比，AccelProtein™ 主要具备如下优势。其一，架构优势。采用几何深度学习方法对模型架构进行简化，能在保证模型精度的同时降低模型参数，便于进行大规模预训练和推理。其二，策略优势。利用小样本乃至零样本学习方法，提高大模型的工程泛化能力，帮助它在仅有少数湿实验数据的情况下实现蛋白质性能优化，极大地提高了蛋白质设计的效率——以往需要 2~5 年才能完成的项目，在 AccelProtein™ 的支持下只需要 2~6 个月即可完成。其三，数据优势。通过与国内多家科研院所和企业的合作，获得了丰富全面的高精度蛋白质序列数据，尤其是一些高热、低温或强酸强碱环境下的数据。此外，该课题组还开发了其他几种 AI 蛋白质通用大模型，并取得了可与 Google、Meta 等国际团队推出的同类成果相媲美的成绩。根据美国哈佛大学医学院创立的蛋白质突变性质预测榜单 ProteinGym，洪亮团队提出的大模型夺得非检索方法排名第一的桂冠，并在总榜前十名的排名中占据一半席位。其中，预测真核蛋白的大模型排名第一，预测原核蛋白的大模型排名第二，预测人类蛋白的大模型排名第三[2]。（来源：ProteinGym 榜单）如上所说，在整个蛋白质设计过程中，通用人工智能可在不需要或仅有少数湿实验数据的条件下，完成对蛋白质改造的赋能。这是否意味着，生物实验在其中已经没有发挥作用的空间？对此，洪亮持否定看法。他认为，首先，AI 在优化特定蛋白时，还需要湿实验来指导和调整方向。其次，生物学家也能够通过湿实验提出更多典型的科学问题，便于大模型团队基于这些问题开发定制化的大模型，从而实现批量的蛋白质设计。创办 AI 蛋白质设计公司，已完成十余项蛋白质产品交付正是基于在 AI 蛋白质设计领域取得的成果，洪亮于 2021 年创办了上海天鹜科技有限公司。后者已经在不到三年时间里，完成了十余款蛋白质设计项目的成果交付，并已获得数千万元 Pre-A 轮融资，投资机构包括耀途资本、金沙江资本等。据了解，目前该公司的服务范围已拓展至创新药、体外诊断、合成生物学等多个行业领域。当下及未来，该课题组也在尝试拓展与更多科研院所和企业之间的合作，希望能在蛋白质工程这一赛道，打出全国最好、世界最优的标志。在洪亮看来，虽然中国的生物制药行业目前已然具备强大的实力，但在全球整个产品链条中的利润比仍然较低。原因在于，缺乏良好的设计上游产品的能力，以至于在短时间内无法实现“破局”。“毕竟国际企业所拥有的设计能力，是在过去一百年来经过大量的科研探索和实验数据积累，以及数不清的人才积淀的基础上才产生的。但如今有了蛋白质通用人工智能，我们就可以不走国际企业的这条发展道路，直接利用 AI 来实现‘换道超车’。”洪亮表示。可以想见，一旦这条横穿跑道的道路被走通，我国就能在合成生物学和生物医药领域，和国际企业展开一场全新的竞争。参考资料：1.Zhou, B., Zheng, L., Wu, B., Tan, Y., Lv, O., Yi, K., ... & Hong, L. (2023). Protein engineering with lightweight graph denoising neural networks.Journal of Chemical Information and Modeling.2.https://proteingym.org/benchmarks运营/排版：何晨龙"
694580674,aedc64baed83dd8ecebc1f703487b577,xexz,https://api.zhihu.com/people/aedc64baed83dd8ecebc1f703487b577,people,,人工智能,article,https://api.zhihu.com/articles/694580674,这片RAM的数据线接ROM的地址线，这就是计算机。（控制表） 如果是两片RAM这样接，这就是‘<em>人工智能</em>’:-)。（反馈表）,0,0,0,1714053913,1714053913,一片ROM的数据线接另一片RAM的地址线，这片RAM的数据线接ROM的地址线，这就是计算机。（控制表）如果是两片RAM这样接，这就是‘人工智能’:-)。（反馈表）
666037172,cc4430d7cd6879a36da45b4d2b24656e,思宁,https://api.zhihu.com/people/cc4430d7cd6879a36da45b4d2b24656e,people,好看的皮囊千篇一律，有趣的灵魂万里挑一,人工智能,article,https://api.zhihu.com/articles/666037172,近年来，有不法分子开始利用 <em>AI</em> 技术进行牟利，例如利用&#34;AI换脸&#34;冒充熟人诈骗，制作发布假新闻混淆视听、赚取流量等。此外&#34;<em>AI</em>换脸&#34;&#34;AI换声&#34;还存在法律风险，容易造成肖像权、隐私权、著作权、声音权的侵犯,1,0,1,1712983021,1712983021,"随着技术的迭代升级，人工智能迎来了跨越式发展，AI技术除了用于短视频创作等娱乐性的软件外，也被广泛应用于医疗、新闻传播、生物多样性监测等领域，虽然AI技术让我们的生活更加丰富，但不分人群、不设限制的运用，必然会产生一系列的问题。近年来，有不法分子开始利用 AI 技术进行牟利，例如利用""AI换脸""冒充熟人诈骗，制作发布假新闻混淆视听、赚取流量等。此外""AI换脸""""AI换声""还存在法律风险，容易造成肖像权、隐私权、著作权、声音权的侵犯。因此""AI 应用""需要多方约束规范，在源头端进一步加强公民个人信息保护，在技术端进一步普及数字水印鉴伪等技术的利用；除了法律法规的完善和监管的发力，也期待头部互联网平台能提高社会责任感，让未来的人工智能更加健康规范。"
696154210,13871b8ea367f2712f0d8b2c08620c88,博而雅科技,https://api.zhihu.com/people/13871b8ea367f2712f0d8b2c08620c88,people,专注脑机交互应用研发,人工智能如此强大，靠的是什么？,article,https://api.zhihu.com/articles/696154210,实现对数据的自动分析和处理。随着计算机和<em>人工智能</em>行业的不断进步，智能家居、语音助手、辅助驾驶……渐渐融入我们的衣食住行，<em>人工智能</em>成为生活中不可或缺的一部分。 而<em>人工智能</em>领域的爆发式进步增长,1,0,0,1714977209,1714977209,在过去的几十年里，计算机行业经历了从最初的电子管计算机到如今的超级计算机的飞速发展。随着计算能力的提升，我们逐渐进入了大数据时代，数据量呈现出爆炸式增长。而如何有效地处理和分析这些数据，成为了摆在我们面前的一大挑战。人工智能技术的出现，为我们提供了一种解决方案。它可以通过模拟人类的思维过程，实现对数据的自动分析和处理。随着计算机和人工智能行业的不断进步，智能家居、语音助手、辅助驾驶……渐渐融入我们的衣食住行，人工智能成为生活中不可或缺的一部分。而人工智能领域的爆发式进步增长，离不开神经网络模型这项关键技术支撑。神经网络模型，顾名思义，是模拟人脑神经元连接方式而构建的一种计算模型。它由大量的“神经元”（即神经网络中的节点）相互连接而成，每个神经元接收来自其他神经元的信号，并根据一定的规则进行处理后输出信号给下一层神经元。这种处理过程类似于人脑中的神经元传递信息，使得神经网络能够模拟人类的思维过程。神经网络模型的核心在于其强大的学习和适应能力。通过训练，神经网络可以调整神经元之间的连接权重，从而优化自身的性能。这种训练过程通常涉及大量的数据输入和反复迭代，使得神经网络能够逐渐学习到数据的内在规律和模式。神经网络模型的发展历史可以追溯到上世纪40年代，当时心理学家Warren McCulloch和数学家Walter Pitts提出了第一个基于生物神经元的计算模型。然而，由于计算能力的限制，神经网络的研究进展缓慢。直到20世纪80年代，随着计算机技术的突破，神经网络才重新受到关注。进入21世纪，随着大数据时代的到来和计算能力的飞速提升，神经网络模型迎来了发展的黄金时期。2006年，加拿大教授Geoffrey Hinton提出了“深度学习”的概念，为神经网络的发展指明了方向。随后，卷积神经网络（CNN）、循环神经网络（RNN）等新型神经网络模型相继问世，并在图像识别、语音识别等领域取得了突破性进展。如今，神经网络模型已经广泛应用于各个领域，成为人工智能技术的核心之一。在图像识别领域，神经网络模型通过训练，对图像像素的分析和处理，实现对图像的自动分类和识别出图片中的物体、场景和情感等信息，为自动驾驶、人脸识别等领域提供了强大的技术支持。在语音识别和自然语言处理方面，神经网络模型可以通过对语音信号的分析和处理，实现对语音的自动识别和转换，使得计算机能够理解和生成人类语言，从而实现智能客服、语音助手、机器翻译等实用功能。此外，神经网络模型还可以帮助我们更好地理解和处理复杂庞大的数据，为决策制定提供精准的分析支持，提高我们的科研工作效率，例如在一些高精尖的专业领域发挥着重要作用，如金融分析、市场预测、医学影像分析、能源管理等。尽管神经网络模型在人工智能领域的成就已经不容小觑，但目前而言其最大价值还远远没有实现，真正“完美”的模型仍未出现，这是因为模型开发是一项具有十分高难度的挑战。例如，模型的训练需要大量的数据和计算资源，而数据的获取和处理往往是一项艰巨的任务；并且，神经网络的“黑箱”特性使得其决策过程缺乏透明度，难以解释和信任；此外，如何保证模型的鲁棒性（强健性或抗干扰性）和泛化能力，如何避免模型的过拟合和欠拟合等问题，都需要我们进行深入研究和探索；当然，我们对人类自身神经系统的了解也有局限。未来，随着算力提升和数据不断增加，我们希望能够逐渐解决这些挑战。首先，我们可以训练更加复杂、更加精准的神经网络模型，以便大大提升模型的准确性和效率；其次，新型的神经网络架构和算法将进一步提高模型的效率和性能，降低对数据和计算资源的需求；第三，可解释性人工智能（XAI）的研究将帮助我们理解神经网络的决策过程，增强人们对AI技术的信任和接受度；与其他技术相结合，形成更加智能化的系统，例如，与物联网相结合，实现对各种设备的智能控制和管理，与区块链技术相结合，实现更加安全、可靠的数据处理和交换。总之，随着计算机和人工智能行业的不断发展，神经网络模型还将会有诸多突破和变革，我们期待着未来神经网络模型能够为我们带来更多的惊喜和改变。
690774979,868f5b2f824ce68147ee67ded3f9d47a,<em>Ai</em>翻唱-桥安,https://api.zhihu.com/people/868f5b2f824ce68147ee67ded3f9d47a,people,,探索人工智能的奥秘,article,https://api.zhihu.com/articles/690774979,自我修正。 <em>人工智能</em>的分类 人工智能可以分为两大类：窄AI（Narrow AI）和通用AI（General AI）。 窄AI 窄AI，也称为弱AI，是指设计用来执行特定任务的智能系统,1,0,2,1712247806,1712247806,在21世纪的科技浪潮中，人工智能（AI）已经成为一个不可忽视的力量。从智能手机到自动驾驶汽车，从虚拟助手到医疗诊断，AI的应用无处不在，极大地改变了我们的生活和工作方式。本文将为您揭开人工智能的神秘面纱，带您一探究竟。 什么是人工智能？人工智能，简称AI，是指由人造系统所表现出来的智能。它是计算机科学的一个分支，旨在创建能够执行需要人类智能的任务的机器和软件。这些任务包括但不限于学习（获取信息并根据信息对其进行规则化以达到结论）、推理（使用规则达到近似或确定的结论）和自我修正。人工智能的分类人工智能可以分为两大类：窄AI（Narrow AI）和通用AI（General AI）。窄AI窄AI，也称为弱AI，是指设计用来执行特定任务的智能系统。比如语音识别、图像识别、推荐系统等。这些系统在其专门的领域表现出色，但它们并不具备超出预设任务范围的智能。通用AI通用AI，也称为强AI，是指具有广泛的认知能力，能够在多种任务和环境中表现出与人类相似智能的系统。目前，强AI仍然是科学研究中的一个目标，尚未实现。人工智能的工作原理人工智能系统通常通过机器学习（Machine Learning）来获得智能。机器学习是一种数据分析方法，它使计算机系统能够通过经验自动改进性能。其中，深度学习（Deep Learning）是机器学习的一个子领域，它使用类似于人脑神经网络的结构来学习复杂的模式。人工智能的应用人工智能已经渗透到我们生活的方方面面：- 医疗领域：AI可以辅助医生进行疾病诊断、个性化治疗计划制定等。- 金融服务：在金融领域，AI用于风险管理、交易、个人理财助手等。- 自动驾驶：AI技术是自动驾驶汽车的核心技术之一，它使汽车能够感知周围环境并做出决策。- 智能家居：AI让家居设备更加智能化，如智能音箱、智能照明系统等。- 娱乐产业：AI在游戏设计、电影特效制作等领域发挥着重要作用。人工智能的挑战与未来尽管人工智能带来了许多便利，但它也面临着一些挑战，如隐私保护、道德伦理、就业影响等。未来，我们需要在发展AI技术的同时，确保其可持续、安全和负责任地使用。随着技术的不断进步，人工智能的未来充满了无限可能。我们期待着AI能够在更多领域发挥作用，为人类社会带来更多的福祉和进步。
688809452,dd20e924e9c60af44628a517c5e13c9e,顽强的豆芽菜,https://api.zhihu.com/people/dd20e924e9c60af44628a517c5e13c9e,people,有话直说,人工智能：塑造未来的科技巨浪,article,https://api.zhihu.com/articles/688809452,我们需要认真对待<em>人工智能</em>的潜在风险，并采取有效的应对策略来确保<em>人工智能</em>技术的健康发展。 六、人工智能发展的政策建议 6.1 加强<em>人工智能</em>的监管与立法 随着人工智能技术的快速发展,0,0,0,1711326663,1711326663,一、人工智能的概述1.1 定义与背景人工智能，作为21世纪最具革命性的科技浪潮，正以前所未有的速度改变着我们的世界。它涵盖了机器学习、深度学习、自然语言处理等多个技术领域，旨在模拟、延伸和扩展人的智能，实现机器能够像人类一样进行思考和决策。随着计算机技术的飞速发展，人工智能已经从最初的简单算法发展到如今的复杂系统，成为推动社会进步的重要力量。回顾人工智能的发展历程，我们可以清晰地看到其背后的技术积淀和不断突破。从早期的逻辑推理程序，到后来的专家系统，再到现在的深度学习，人工智能的技术基础日益坚实。同时，随着大数据、云计算等技术的普及，人工智能的应用场景也越来越广泛，从自动驾驶到医疗健康，从金融科技到智能家居，人工智能正在逐步渗透到我们生活的方方面面。然而，人工智能的快速发展也带来了一系列挑战和机遇。一方面，数据安全与隐私保护成为亟待解决的问题。随着人工智能技术的广泛应用，个人信息的泄露和滥用风险也在不断增加。另一方面，人工智能的伦理问题也日益凸显。例如，自动驾驶汽车在面临紧急情况时应该如何选择？这是一个需要我们深入思考和探讨的问题。尽管如此，人工智能的发展仍然充满了无限的可能性和机遇。它不仅可以提高生产效率、改善生活质量，还可以推动产业升级、创造新的就业机会。正如著名科学家霍金所说：“人工智能的发展将改变我们的生活方式和工作方式，甚至可能改变我们的思维方式。”因此，我们应该积极拥抱人工智能这一科技巨浪，共同探索其未来的无限可能。1.2 发展历程与现状人工智能的发展历程可谓波澜壮阔，从早期的逻辑推理程序到如今的深度学习，其技术不断突破，应用领域日益广泛。在过去的几十年里，人工智能经历了从符号主义到连接主义的转变，再到如今的数据驱动阶段。随着计算能力的提升和大数据的涌现，人工智能得以快速发展，并在许多领域取得了显著成果。以自动驾驶为例，近年来，随着深度学习技术的发展，自动驾驶技术取得了重大突破。特斯拉等公司的自动驾驶汽车已经在部分道路上实现了商业化运营，极大地提高了交通效率和安全性。此外，在医疗健康领域，人工智能也在辅助诊断、药物研发等方面发挥着越来越重要的作用。例如，IBM的Watson肿瘤助手已经能够帮助医生分析病理切片，提高诊断准确率。然而，人工智能的发展也面临着诸多挑战。随着技术的不断进步，人工智能的伦理问题日益凸显。例如，在自动驾驶汽车面临紧急情况时，如何做出合理的决策成为了一个亟待解决的问题。此外，数据安全与隐私保护也是人工智能发展中不可忽视的问题。随着人工智能技术的广泛应用，个人数据的收集和处理成为了一个重要环节，如何保障个人数据的安全和隐私成为了一个亟待解决的问题。展望未来，人工智能的发展将更加迅速和广泛。随着技术的不断创新和突破，人工智能将在更多领域发挥重要作用。同时，随着人工智能与人类社会的深度融合，我们也需要更加关注其带来的挑战和风险。正如著名科学家霍金所说：“人工智能的全面发展将带来人类的终结。”因此，我们需要在推动人工智能发展的同时，加强对其的监管和立法，确保其健康、可持续地发展。二、人工智能的技术基础2.1 机器学习机器学习作为人工智能的核心技术之一，近年来取得了显著的进展。它基于统计学和计算机科学的交叉学科，通过训练大量的数据，让计算机能够自动学习和改进，从而实现对新数据的预测和决策。机器学习在各个领域都有广泛的应用，如自动驾驶、医疗健康、金融科技等。以自动驾驶为例，机器学习在自动驾驶车辆中发挥着至关重要的作用。通过训练大量的道路数据和交通场景，自动驾驶车辆可以识别行人、车辆、交通信号等，并自主决策行驶路线和速度。据相关数据显示，使用机器学习技术的自动驾驶车辆已经实现了在特定场景下的完全自动驾驶，大大提高了道路安全和交通效率。机器学习还在医疗健康领域发挥着重要作用。例如，通过训练大量的医疗图像数据，机器学习可以帮助医生快速准确地识别病变部位和病情，提高诊断的准确性和效率。此外，机器学习还可以帮助医生制定个性化的治疗方案，提高治疗效果和患者的生活质量。然而，机器学习也面临着一些挑战和问题。其中，数据的质量和标注问题是机器学习的重要瓶颈之一。如果数据存在噪声或标注不准确，将会严重影响机器学习模型的训练效果和性能。此外，机器学习模型的泛化能力也是一个重要的问题，即模型能否在未见过的数据上进行准确的预测和决策。为了解决这些问题，研究者们提出了许多方法和技术。例如，数据增强技术可以通过对原始数据进行变换和扩展，增加数据的多样性和数量，从而提高模型的泛化能力。此外，迁移学习技术也可以利用已有的知识和模型，加速新模型的训练和提高性能。这些技术的发展将进一步推动机器学习在各个领域的应用和发展。总之，机器学习作为人工智能的核心技术之一，正在深刻地改变着我们的生活和工作方式。未来，随着技术的不断创新和突破，机器学习将在更多领域发挥重要作用，为人类社会的发展和进步做出更大的贡献。2.2 深度学习深度学习作为人工智能领域的重要分支，近年来取得了显著的进展。它基于神经网络模型，通过模拟人脑神经元的连接方式，构建出复杂的网络结构，从而实现对大量数据的高效处理和分析。深度学习的应用广泛，不仅在图像识别、语音识别等领域取得了突破，还在自然语言处理、推荐系统等方面发挥着重要作用。以图像识别为例，深度学习通过训练大量的图像数据，使神经网络能够自动提取图像中的特征，进而实现对图像的分类、识别等任务。据相关数据显示，深度学习在图像识别领域的准确率已经达到了惊人的水平，甚至超过了人类的识别能力。这一突破性的进展，不仅为图像识别领域带来了巨大的变革，也为其他领域的发展提供了新的思路和方法。深度学习的发展离不开大数据的支持。随着数据量的不断增长，深度学习模型的训练效果也得到了显著提升。同时，计算机硬件的快速发展也为深度学习的训练提供了强大的计算能力。这些因素共同推动了深度学习在各个领域的应用和发展。然而，深度学习也面临着一些挑战和问题。例如，深度学习模型的训练需要大量的数据和计算资源，这使得其在实际应用中受到了一定的限制。此外，深度学习模型的可解释性较差，难以解释模型内部的运行机制和决策过程。这些问题需要我们在未来的研究中加以解决和完善。总之，深度学习作为人工智能领域的重要分支，已经在多个领域取得了显著的进展。它的发展不仅推动了人工智能技术的进步，也为各个领域的创新和发展提供了新的思路和方法。未来，随着技术的不断发展和完善，深度学习将会在更多领域发挥重要作用，为人类社会的发展带来更多的机遇和挑战。2.3 自然语言处理自然语言处理（NLP）作为人工智能领域的一个重要分支，近年来取得了显著的进展。它旨在让计算机能够理解和处理人类语言，实现人机交互的自然和流畅。随着深度学习技术的发展，NLP在各个领域的应用越来越广泛。在医疗健康领域，NLP的应用尤为突出。例如，通过NLP技术，医生可以自动分析患者的症状和病史，提高诊断的准确性和效率。据研究，使用NLP技术辅助诊断，可以将医生的诊断准确率提高10%以上。此外，NLP还可以帮助医生从大量的医学文献中快速筛选出有用的信息，为临床决策提供有力支持。在金融领域，NLP也发挥着重要作用。例如，通过NLP技术，金融机构可以自动分析大量的金融新闻和报告，提取有用的信息来辅助投资决策。此外，NLP还可以用于智能客服和风险控制等方面，提高金融服务的效率和质量。然而，NLP技术的发展也面临着一些挑战。例如，语言的复杂性和多样性使得NLP技术的实现难度很大。此外，NLP技术的准确性和可靠性也需要不断提高。为了解决这些问题，研究者们不断探索新的算法和模型，以提高NLP技术的性能和应用范围。正如著名计算机科学家吴恩达所说：“自然语言处理是人工智能的皇冠上的明珠。”随着技术的不断进步和应用领域的不断拓展，NLP必将在未来发挥更加重要的作用，为人类社会的发展带来更多的机遇和挑战。三、人工智能在各领域的应用3.1 自动驾驶自动驾驶作为人工智能技术在交通领域的杰出代表，正逐渐改变我们的出行方式。随着传感器技术、计算机视觉和机器学习等关键技术的不断进步，自动驾驶汽车已经能够在特定场景下实现高度自主驾驶。例如，Waymo作为谷歌母公司Alphabet的自动驾驶项目，已经在多个城市进行了商业化运营，其自动驾驶出租车服务已经累计行驶了数百万英里，证明了自动驾驶技术的可行性和安全性。自动驾驶的广泛应用将带来诸多益处。首先，它能够有效减少因人为失误导致的交通事故，据统计，每年全球约有120万人因交通事故而死亡，其中许多事故是由于驾驶员的疏忽或疲劳驾驶造成的。自动驾驶汽车通过精确的传感器和算法，能够更准确地感知周围环境，做出快速而准确的决策，从而大大降低事故发生率。此外，自动驾驶还有助于提高交通效率，减少拥堵现象。根据麦肯锡的预测，到2030年，自动驾驶汽车将减少全球交通拥堵造成的经济损失约50%。自动驾驶汽车能够实时获取道路信息和车辆数据，通过智能调度和优化行驶路线，实现更高效的交通流动。然而，自动驾驶技术的发展也面临着一些挑战。其中，数据安全和隐私保护是亟待解决的问题。自动驾驶汽车需要收集大量的道路和车辆数据来进行训练和优化，这些数据可能包含个人隐私信息。因此，在推动自动驾驶技术发展的同时，必须加强对数据安全和隐私保护的监管，确保个人信息不被滥用。此外，自动驾驶汽车的普及还需要解决法律和伦理问题。例如，当自动驾驶汽车面临紧急情况时，应该如何做出决策以最大程度地保护乘客和其他道路使用者的安全？这需要我们在技术发展的同时，不断完善相关法律法规和伦理准则，为自动驾驶汽车的商业化运营提供坚实的法律保障。综上所述，自动驾驶作为人工智能技术在交通领域的杰出代表，具有巨大的潜力和价值。然而，要实现自动驾驶技术的广泛应用和商业化运营，还需要克服诸多挑战。通过加强技术研发、完善法律法规和伦理准则、加强数据安全和隐私保护等措施，我们有信心推动自动驾驶技术的健康发展，为人类创造更美好的未来。3.2 医疗健康在医疗健康领域，人工智能的应用已经取得了显著的成果。例如，通过深度学习和大数据分析，人工智能可以辅助医生进行疾病诊断和治疗方案制定。据研究，使用人工智能辅助诊断肺癌的准确率已经超过了传统方法。此外，人工智能还可以帮助医生进行手术操作，提高手术的精确度和安全性。例如，在心脏手术中，人工智能可以通过分析心脏图像数据，为医生提供最佳的手术路径和方案，从而减少手术风险和提高手术成功率。人工智能在医疗健康领域的应用不仅提高了医疗服务的效率和质量，还降低了医疗成本。通过自动化和智能化的方式，人工智能可以减少医疗资源的浪费和人力成本，从而为患者提供更加优质和高效的医疗服务。此外，人工智能还可以帮助医疗机构进行精细化管理和优化资源配置，提高医疗服务的整体水平和竞争力。然而，人工智能在医疗健康领域的应用也面临着一些挑战和伦理问题。例如，数据安全和隐私保护是人工智能应用中需要重点关注的问题之一。在医疗健康领域，患者的个人信息和医疗数据是非常敏感的，必须得到充分的保护。此外，人工智能在辅助诊断和治疗过程中，也可能会出现误判和误诊的情况，需要医生和患者共同承担风险。因此，在医疗健康领域应用人工智能时，需要充分考虑其优势和不足，并制定相应的应对策略。同时，还需要加强人工智能的监管和立法，确保其应用符合伦理和法律要求，保障患者的权益和安全。3.3 金融科技金融科技作为人工智能应用的重要领域之一，正在深刻改变着金融行业的生态和格局。随着大数据、云计算、区块链等技术的不断发展，金融科技在风险管理、客户服务、产品创新等方面展现出巨大的潜力。在风险管理方面，人工智能技术的应用使得金融机构能够更准确地评估客户的信用状况，实现风险定价的精细化。例如，基于深度学习的信用评分模型，可以通过分析客户的消费记录、社交网络等多维度数据，为客户提供个性化的信用评估服务，有效降低了信贷风险。在客户服务方面，金融科技通过智能客服、智能投顾等应用，提升了客户服务的效率和质量。智能客服能够实时响应客户的咨询需求，提供24小时不间断的服务；智能投顾则能够根据客户的风险偏好和投资目标，为客户提供个性化的投资建议，帮助客户实现资产的优化配置。此外，金融科技还在产品创新方面发挥着重要作用。通过运用人工智能技术，金融机构能够开发出更加符合市场需求和客户偏好的金融产品。例如，基于机器学习的量化交易模型，能够帮助投资者更准确地把握市场趋势，实现资产的增值保值。正如阿里巴巴集团创始人马云所说：“未来的金融将是科技驱动的金融。”金融科技的发展将不断推动金融行业的创新和变革，为经济社会发展注入新的动力。四、人工智能带来的挑战与机遇4.1 数据安全与隐私保护随着人工智能技术的快速发展，数据安全与隐私保护问题日益凸显。在人工智能应用过程中，大量个人数据被收集、存储和处理，这些数据包括个人身份信息、行为习惯、健康状况等敏感信息。一旦这些数据泄露或被滥用，将对个人隐私造成严重威胁，甚至可能引发社会信任危机。以近年来频发的数据泄露事件为例，据统计，仅2022年全球就发生了数千起数据泄露事件，涉及数亿用户的个人信息。这些事件不仅给受害者带来了巨大的经济损失和精神压力，也严重损害了企业的声誉和信任度。因此，加强数据安全与隐私保护已成为人工智能发展不可或缺的一环。为了应对这一挑战，我们需要从多个层面入手。首先，政府应制定和完善相关法律法规，明确数据收集、存储和使用的规范和标准，加大对违法行为的处罚力度。同时，企业也应承担起保护用户数据的责任，加强内部管理和技术防范，确保用户数据的安全性和隐私性。此外，我们还需要借助先进的技术手段来加强数据安全与隐私保护。例如，通过采用加密技术、匿名化处理、差分隐私等技术手段，可以有效保护用户数据不被泄露和滥用。同时，我们也需要加强人工智能技术的研发和应用，推动人工智能与数据安全、隐私保护等领域的深度融合，为数据安全与隐私保护提供更加全面和有效的解决方案。正如著名计算机科学家、图灵奖获得者约翰·霍普克罗夫特所说：“隐私是信息时代的公民权利。”在人工智能快速发展的今天，我们更需要重视数据安全与隐私保护问题，确保人工智能技术的健康发展与社会的和谐稳定。4.2 人工智能的伦理问题人工智能的伦理问题已成为全球关注的焦点。随着技术的快速发展，人工智能在各个领域的应用越来越广泛，同时也带来了一系列伦理挑战。例如，在自动驾驶领域，当车辆面临无法避免的碰撞时，应该如何做出决策？是保护乘客的安全还是尽量减少对行人的伤害？这涉及到伦理道德的权衡和取舍。此外，人工智能在医疗领域的应用也引发了关于数据隐私和患者权益的讨论。如何确保患者的个人信息不被滥用，同时保障医疗决策的公正性和准确性，是亟待解决的问题。为了应对这些伦理挑战，我们需要建立完善的伦理框架和监管机制。首先，政府应制定相关法律法规，明确人工智能技术的使用范围和限制，确保技术的合规性和安全性。其次，企业和研究机构应加强自律，遵循伦理原则，确保人工智能技术的健康发展。此外，我们还需要加强公众教育和意识提升，让更多人了解人工智能的伦理问题，并参与到讨论和解决中来。总之，人工智能的伦理问题不容忽视。我们需要从多个层面出发，加强监管、自律和公众参与，共同推动人工智能技术的健康发展，为人类社会带来更大的福祉。 近年来，人工智能的伦理问题引发了广泛的讨论。例如，在2018年，亚马逊因其招聘人工智能系统存在性别偏见而备受争议。该系统在评估简历时，更倾向于推荐男性候选人，这引发了关于算法公平性和透明度的质疑。此外，人工智能在决策过程中可能存在的歧视和偏见问题也不容忽视。例如，某些算法可能基于历史数据做出决策，而这些数据可能包含不公平的偏见，从而导致不公平的结果。为了解决这些问题，我们需要建立更加完善的伦理框架和监管机制。首先，我们需要对算法进行审查和测试，确保其公平性和透明度。其次，我们需要建立独立的监管机构，对人工智能技术的使用进行监督和评估。此外，我们还需要加强公众教育和意识提升，让更多人了解人工智能的伦理问题，并参与到讨论和解决中来。正如著名科学家爱因斯坦所说：“科技是一种强大的工具，但使用它的人必须了解它的局限性和可能的后果。”因此，我们必须认真对待人工智能的伦理问题，加强监管和自律，确保技术的健康发展，为人类社会带来更大的福祉。4.3 人工智能带来的就业机会随着人工智能技术的快速发展，它正逐渐渗透到各个行业，为就业市场带来了前所未有的机遇。据国际数据公司（IDC）预测，到2025年，全球人工智能相关岗位将增至2300万个，其中新增的就业机会将主要来自于人工智能技术的研发、应用和维护。这一趋势不仅为科技行业带来了大量的就业机会，同时也为传统行业提供了转型升级的路径。以金融行业为例，人工智能的应用使得金融服务更加智能化、个性化。智能投顾、智能风控等新型金融业态应运而生，为金融行业带来了大量的就业机会。据毕马威发布的报告，到2025年，全球金融行业将有超过30%的工作岗位被人工智能取代，但同时也会有大量新的就业机会涌现。此外，人工智能还催生了众多新兴职业，如数据科学家、机器学习工程师、人工智能伦理顾问等。这些职业不仅需要具备深厚的技术背景，还需要具备创新思维和跨学科知识。因此，对于有志于从事人工智能领域的年轻人来说，他们可以通过不断学习和实践，提升自己的技能水平，抓住这些新兴的就业机会。正如著名经济学家约瑟夫·熊彼特所说：“创新是经济发展的根本动力。”人工智能作为第四次工业革命的核心技术之一，正以其强大的创新力推动着就业市场的变革。因此，我们应该积极拥抱人工智能带来的就业机遇，不断提升自身技能水平，以适应未来社会的发展需求。五、人工智能的未来展望5.1 技术创新与突破在人工智能领域，技术创新与突破是推动其持续发展的关键动力。近年来，随着深度学习、强化学习等技术的快速发展，人工智能在语音识别、图像识别、自然语言处理等领域取得了显著进展。例如，在语音识别领域，通过采用深度学习技术，语音识别的准确率得到了大幅提升，已经接近甚至超越了人类水平。这种技术创新不仅改善了用户体验，也为人工智能在智能家居、医疗等领域的应用提供了更多可能性。此外，人工智能在算法优化、数据处理等方面也取得了重要突破。例如，通过引入注意力机制、自注意力机制等新型算法，人工智能在处理复杂任务时的性能得到了显著提升。同时，随着大数据技术的发展，人工智能可以处理的数据量也大幅增加，从而提高了其预测和决策的准确性。这些技术创新与突破为人工智能在自动驾驶、医疗健康等领域的应用提供了坚实的技术支撑。然而，技术创新与突破也面临着诸多挑战。随着人工智能技术的不断发展，其所需的计算资源、数据资源等成本也在不断增加。此外，人工智能技术的可解释性、鲁棒性等问题也亟待解决。因此，未来的技术创新需要更加注重算法优化、资源利用等方面的研究，以实现更高效、更可靠的人工智能技术。正如著名科学家爱因斯坦所说：“想象力比知识更重要。”在人工智能领域，技术创新与突破需要充分发挥想象力和创新精神，不断探索新的技术路线和应用场景。只有这样，我们才能更好地应对未来的挑战和机遇，推动人工智能技术的持续发展和广泛应用。5.2 人工智能与人类社会的融合人工智能与人类社会的融合正在逐渐深化，这种融合不仅改变了我们的生活方式，也重塑了社会结构和经济形态。随着技术的不断进步，人工智能已经渗透到各个领域，从医疗、教育到交通、娱乐，无所不在。例如，在医疗领域，人工智能通过深度学习和大数据分析，可以帮助医生更准确地诊断疾病，提高治疗效率。在教育领域，人工智能可以个性化地推荐学习资源，帮助学生更高效地学习。这些应用不仅提高了人们的生活质量，也推动了社会的进步。然而，人工智能与人类社会的融合也带来了一系列挑战。一方面，随着人工智能的普及，许多传统职业可能面临被取代的风险，这会导致一部分人失业。另一方面，人工智能的发展也引发了数据安全和隐私保护等问题。因此，在推动人工智能与人类社会的融合过程中，我们需要关注这些问题，并采取相应的措施来应对。为了促进人工智能与人类社会的和谐融合，我们需要加强人工智能的监管与立法，确保技术的发展符合社会的利益。同时，我们还需要推动人工智能的产学研合作，加快技术的创新和应用。此外，培养人工智能领域的人才队伍也是至关重要的，只有拥有足够的人才储备，才能推动人工智能技术的持续发展。正如著名科学家霍金所说：“人工智能的发展要么带来人类历史上最好的事情，要么带来最坏的事情。”因此，我们需要审慎地对待人工智能的发展，既要充分利用其带来的机遇，也要积极应对其带来的挑战。只有这样，我们才能实现人工智能与人类社会的和谐融合，共同创造一个更加美好的未来。5.3 人工智能的潜在风险与应对策略人工智能的潜在风险不容忽视。随着技术的快速发展，我们面临着数据安全与隐私保护的严峻挑战。例如，根据一项研究，近年来因人工智能技术泄露的个人数据数量呈指数级增长。这些泄露的数据可能被用于恶意目的，如身份盗窃或欺诈行为。此外，人工智能的决策过程也可能存在偏见和歧视，这可能导致不公平的结果。例如，某些招聘算法已被发现对特定性别或种族的候选人存在偏见。因此，我们需要采取应对策略来减轻这些风险。首先，加强数据安全和隐私保护是至关重要的。企业和政府应加大投入，研发更先进的数据加密技术和隐私保护算法，确保个人数据不被滥用。此外，建立严格的数据监管机制也是必要的，以确保数据在合法、公正、透明的环境下使用。同时，我们还需要提高公众对数据安全和隐私保护的意识，教育他们如何保护自己的个人信息。其次，解决人工智能决策过程中的偏见和歧视问题也是关键。这需要我们建立更加公正和透明的算法开发过程，确保算法的公平性和公正性。此外，对算法进行定期审查和更新也是必要的，以确保其适应不断变化的社会环境和需求。同时，我们还需要建立有效的监管机制，对算法的使用进行监督和评估，确保其不会对个人和社会造成不公平的影响。正如著名科学家霍金所说：“人工智能的全面发展将带来人类历史上最大的变革。它将改变我们的生活方式，甚至改变我们是谁。但除非我们学会如何避免危险，否则我们可能无法控制它。”因此，我们需要认真对待人工智能的潜在风险，并采取有效的应对策略来确保人工智能技术的健康发展。六、人工智能发展的政策建议6.1 加强人工智能的监管与立法随着人工智能技术的快速发展，其在各个领域的应用越来越广泛，但同时也带来了一系列的问题和挑战。为了保障人工智能技术的健康发展，加强人工智能的监管与立法显得尤为重要。 首先，加强人工智能的监管可以有效避免技术滥用和误用。近年来，人工智能技术在人脸识别、智能监控等领域的应用越来越广泛，但同时也引发了一些争议。例如，一些商家滥用人工智能技术收集用户信息，侵犯用户隐私；一些政府机构过度依赖人工智能技术，导致决策失误等问题。因此，加强人工智能的监管可以有效规范技术的使用，保障公众利益。 其次，加强人工智能的立法可以为技术的发展提供法律保障。目前，人工智能技术在全球范围内仍处于快速发展阶段，相关的法律法规还不够完善。因此，加强人工智能的立法可以为技术的发展提供法律保障，规范各方的行为，促进技术的健康发展。 此外，加强人工智能的监管与立法还可以促进技术的创新和发展。在监管和立法的推动下，人工智能技术将更加注重安全性和可靠性，推动技术的创新和发展。同时，加强监管和立法也可以为人工智能技术的商业化应用提供更加稳定的环境，吸引更多的投资和支持。 总之，加强人工智能的监管与立法是保障人工智能技术健康发展的重要措施。通过加强监管和立法，可以有效避免技术滥用和误用，为技术的发展提供法律保障，促进技术的创新和发展。同时，也需要各方共同努力，推动人工智能技术的健康发展，为人类社会的进步和发展做出更大的贡献。6.2 推动人工智能的产学研合作推动人工智能的产学研合作对于促进人工智能技术的快速发展和应用至关重要。产学研合作能够汇聚各方资源，形成合力，共同推动人工智能技术的创新和应用。通过加强产学研合作，可以加速人工智能技术的研发进程，提高技术应用的效率和效果。以自动驾驶为例，产学研合作在自动驾驶技术的研发和应用中发挥了重要作用。高校和研究机构在自动驾驶算法和传感器技术方面进行了深入研究，为自动驾驶技术的发展提供了理论支持。同时，企业则将这些研究成果应用于实际产品中，通过不断的试验和改进，推动了自动驾驶技术的快速发展。这种产学研合作模式不仅加速了自动驾驶技术的研发进程，还提高了技术应用的可靠性和安全性。此外，产学研合作还能够促进人工智能技术的广泛应用。通过合作，企业可以将高校和研究机构的研究成果转化为实际产品，满足市场需求，推动人工智能技术在各个领域的应用。例如，在医疗健康领域，产学研合作推动了人工智能在医学影像分析、疾病诊断和治疗等方面的应用，提高了医疗服务的效率和质量。然而，推动人工智能的产学研合作也面临一些挑战。首先，各方之间的合作需要建立在相互信任的基础上，需要加强沟通和协调。其次，产学研合作需要投入大量的人力、物力和财力，需要各方共同承担风险和成本。因此，需要建立有效的合作机制和合作模式，促进各方的积极参与和合作。总之，推动人工智能的产学研合作是促进人工智能技术快速发展和应用的重要途径。通过加强合作，可以汇聚各方资源，形成合力，共同推动人工智能技术的创新和应用。同时，也需要建立有效的合作机制和合作模式，促进各方的积极参与和合作，共同应对人工智能发展面临的挑战和机遇。6.3 培养人工智能领域的人才队伍在人工智能的浪潮中，培养一支高素质的人才队伍显得尤为重要。随着技术的飞速发展，人工智能领域对人才的需求日益旺盛。据预测，到2025年，全球人工智能领域的人才缺口将达到数百万。因此，加强人工智能领域的人才培养，不仅有助于推动科技进步，更是应对未来社会经济发展挑战的关键。为了培养高质量的人工智能人才，我们需要构建一个完善的教育体系。这包括从基础教育阶段就开始培养学生的编程思维、数据分析和创新能力。同时，高等教育机构也应设立更多的人工智能相关专业，并引入国际先进的教学资源和方法。例如，斯坦福大学、麻省理工学院等世界顶尖学府在人工智能教育方面走在前列，他们的成功经验值得我们借鉴。除了教育体系的建设，我们还应注重实践能力的培养。人工智能是一门高度实践性的学科，只有通过大量的项目实践，学生才能真正掌握相关技能。因此，企业和研究机构应与教育机构紧密合作，为学生提供实习和实践的机会。这种“产学研”结合的模式，既有利于学生的成长，也能推动人工智能技术的实际应用。此外，我们还应关注人工智能伦理和法规的教育。随着人工智能技术的广泛应用，如何确保技术的合理、安全、可控使用成为了一个重要议题。因此，培养具备伦理意识和法规知识的人工智能人才至关重要。这要求我们在教育过程中，不仅要注重技术知识的传授，更要加强伦理和法规的教育，培养学生的社会责任感和职业道德。正如著名科学家爱因斯坦所说：“教育的首要目标永远是独立思考和判断，而非特定的知识。”在人工智能领域的人才培养中，我们更应注重培养学生的创新思维和批判性思维。只有这样，我们才能培养出真正能够适应未来社会发展需要的人工智能人才。
688525385,a188bd1ee6f416da03bccb0e45b451b3,天承Office的笔记,https://api.zhihu.com/people/a188bd1ee6f416da03bccb0e45b451b3,people,商务笔记，探索攻略,人工智能，改变世界的力量,article,https://api.zhihu.com/articles/688525385,中国在去年启动了<em>人工智能</em>的重大专项之后，正在积极酝酿准备启动“中国脑科学和类脑研究”。 脑科学与<em>人工智能</em>，机器人与人工智能的学科交叉与产业交叉为<em>人工智能</em>更高更快的发展提供了机遇与可能,1,0,0,1711106317,1711106317,"让机器具备像人一样的智能，是人类最伟大的梦想之一！60年前，马文·明斯基和麦卡锡等先贤们共同发起“达特茅斯会议”, “人工智能”诞生了。60年来，人类用科技和智慧共同哺育了这个全新的生命。现在，人工智能在经历了一个甲子的跌宕起伏之后，借互联网、移动互联网和物联网的IT时代浪潮，以深度神经网络算法为基础，以大数据、云计算、移动互联网为支撑，终于进入到即将全面爆发的前夜。人工智能成为下一个风口已经成为全球产业界的共识。《失控》和《必然》的作者，硅谷著名科技哲学思想家凯文·凯利就明确说：“如果你问我未来20年最重要的技术是什么？毫无疑问，我会告诉你是人工智能”。孙正义甚至提出了一个“孙正义公式”，以此证明未来决定一个国家的GDP排名的不是人口，而是机器人。伟大的科学家霍金则不无担忧地预测“人工智能可能在接下来的100年中将人类取而代之！”。今年全球消费类电子展（CES2016）上，71%的展品、展位是跟人工智能有关的。麦肯锡调研显示普通人45%的日常活动可用当前技术自动化。不仅是简单重复劳动，很多复杂的专业性工作也会被机器替代。《科学》杂志（《Science》）预测，截至2045年，人工智能将会颠覆全球就业率，平均会有50%的劳动人口因机器人等人工智能技术发展丢掉工作，其中印度和中国受影响最为严重，这个数字将达69%和77%。人工智能已经不仅仅是一个学术名词，产业界和大众已经可以切实地感到他正在冲击着我们的现在，影响着我们的将来！未来5到10年，人工智能会像水和电一样，进入我们每个人的生活。过去的60年，中国人在人工智能领域默默地投入和耕耘。今天，中国人工智能领域的科研工作者和企业家开始走在了全球的风口浪尖之上。如果我们能够抢抓机遇，掌握住人工智能的主导权，就能在下一次产业浪潮中处于价值链的高端；如果错失机遇，那未来中国的创业者们将不得不继续处于价值链的低端。因此人工智能不仅仅关乎单个企业和单个行业，更是国家必争之地。我们已经可以看到，人工智能开始上升为世界各国的国家战略。美国已启动“大脑活动图谱计划”（BRAIN Initiative），欧盟也启动了“人类大脑工程”（Human Brain Project）。中国在去年启动了人工智能的重大专项之后，正在积极酝酿准备启动“中国脑科学和类脑研究”。脑科学与人工智能，机器人与人工智能的学科交叉与产业交叉为人工智能更高更快的发展提供了机遇与可能。然而，在这个过程中，还有很多的问题需要回答，也有很多的难题需要破解。人工智能如何更好地借鉴脑科学的最新研究成果？深度学习是不是等于人工智能？大数据是不是人工智能的唯一原材料？人工智能在运算上已经通过Deep Blue证明了自己的实力，那是不是在感知和认知上也马上就能超越人类？如果人工智能有了意识和情感怎么办？他们会颠覆人类么？这些问题的答案将能够帮助我们看清楚人工智能新纪元的发展轨迹。"
687712357,e3f398083013068e1df4f73ac086a3eb,控制工程老学长,https://api.zhihu.com/people/e3f398083013068e1df4f73ac086a3eb,people,很多文章在反复修改，一直在学习请各位批评指正！,对人工智能领域的一些个人看法,article,https://api.zhihu.com/articles/687712357,数据如果不准确可能导致AI做出错误的决策，因此如何获得高质量的训练数据或者如何让<em>人工智能</em>识别出数据中的问题是关键。 4. <em>人工智能</em>对社会的影响 AI正在改变劳动力市场,0,0,0,1710765851,1711267035,文章脉络：我们想了解人工智能领域，首先应该了解人工智能是怎么来的也就是它的历史背景，其次现在发展到哪了，现在的发展该存在什么问题呢？以及为什么要发展人工智能也就是对社会产生什么影响呢？未来发展的大方向是什么呢？所以本文的内容依次介绍：人工智能历史背景、发展现状、存在问题、对社会的影响、未来发展趋势。1. 人工智能历史背景人工智能的概念最早可以追溯到20世纪中叶，其中著名事件有：AlphaGo击败了世界围棋冠军李世石、OpenAI发布了GPT大模型等。近年来，随着计算能力的提升和数据量的爆炸性增长，AI技术取得了前所未有的进展。2. 发展现状人工智能现在正处于快速发展期，我们可以看一下人工智能领域的论文数量变化曲线深度学习和机器学习技术的进步推动了AI在图像识别、自然语言处理、自动驾驶等领域的突破。在大模型领域基于之前的研究成果持续扩展，例如，ChatGPT（GPT-3.5）升级到GPT-4，华为公司的盘古大模型更是已经应用到了各行各业。但是人工智能领域也存在一些核心问题。3. 人工智能领域要解决的问题 3.1 首先就是算力的消耗，训练人工智能模型需要消耗大量的电力，所以提高算法的效率是面临的一个问题。 3.2 其次就是高性能的GPU紧缺，随着主流深度学习算法的发展，应研发出能稳定运行这些算法的智能芯片。 3.3 此外，AI的决策很大程度上依赖于训练的数据，数据如果不准确可能导致AI做出错误的决策，因此如何获得高质量的训练数据或者如何让人工智能识别出数据中的问题是关键。4. 人工智能对社会的影响AI正在改变劳动力市场，电话客服等一些工作岗位很可能随着AI 的发展逐渐被取代，同时AI也大大提升人类的工作效率，技术难题，重复无聊的工作交给AI都会帮你解决。但是，AI的伦理问题包括数据隐私、算法偏见（数据中如果提现了一些偏见思想，同样学习后的大模型可能也会具有甚至还可能会放大）、责任归属（AI做出的决策责任归属方，例如无人车撞到人是谁的责任）等也同样存在。所以应该制定相应地法律法规完善AI的发展。5. 人工智能未来发展趋势5.1 AI的未来发展可能会更加智能自适应地学习和调整自己的决策，同时在资源受限的环境下使用小模型、轻量级AI，也是不可阻挡的发展趋势。5.2 AI与其他技术如智能医疗方面会更加普及，也会持续推进生产自动化，无人驾驶，智能物流的发展。
687462753,2552108527c71c98b70bc91e8f6b7db1,我真的服啦,https://api.zhihu.com/people/2552108527c71c98b70bc91e8f6b7db1,people,机电专业学生,人工智能,article,https://api.zhihu.com/articles/687462753,成为人类日常生活不可缺少的一部分。 <em>人工智能</em>的研究领域具体包括： 1机器学习2专家系统3自然语言处理4智能决策系统5自动定理证明6人工神经网络7推荐系统8智能识别9机器人学10人工生命 机器学习是一类算法的总称,0,0,0,1710657465,1710657465,什么是人工智能？智能是人类智力和能力的总称，指个体对客观事物进行合理分析、判断，并进行有效处理的综合能力。而人工智能是研究用于模拟、延申和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能的研究领域比较广阔，如今许多技术已经应用到我们的生活当中，极大地方便我们的生活，成为人类日常生活不可缺少的一部分。人工智能的研究领域具体包括：1机器学习2专家系统3自然语言处理4智能决策系统5自动定理证明6人工神经网络7推荐系统8智能识别9机器人学10人工生命机器学习是一类算法的总称，其目标是从历史数据中挖掘出隐含的规律，并用于未来的任务处理。机器学习包括监督学习、非监督学习和强化学习三类。监督式学习的核心思想是使用带标签的训练数据来训练模型，让它能够预测未知数据的标签或者分类。假如有几百封电子邮件，每个电子邮件都有其特征和相应标签，比如电子邮件发件人、主题和正文等，在监督学习中，我们使用标签来训练一个模型，其过程类似于给模型一个示范课，向模型展示大量的训练样本，让它通过观察和学习样本之间的模式和关系来理解输入特征与标签之间的链接。机器根据训练数据调整自己的内容参数，使预测尽可能准确。基于之前收录的信息，根据邮箱内容就可以对发来的邮箱进行垃圾有限的分类。即根据之前的模式和关系预测出相应的标签，适用于分类问题和回归问题。监督式学习在许多领域有广泛应用，如自然语言处理、金融分析、医疗诊断等。专家系统是一类具有专门知识的计算机智能系统，该系统根据某领域一个或多个专家提供的知识和经验，对人类专家求解问题的过程进行建模，然后运用推理技术来模拟通常由人类专家才能解决的问题，达到与专家类似的解决问题水平。目前专家系统在各个领域中得到广泛应用，如医疗诊断专家系统、故障诊断专家系统、资源勘探专家系统、贷款损失专家评估系统等。自然语言处理研究人与计算机之间用人类自然语言进行通信的理论和方法，通常包括三个方面：1计算机理解人类语言输入，并正确回答和响应2计算机对输入的语言信息进行处理，生成摘要或复述3计算机将某种输入的语言翻译成另外一种语言，实现口语的实时翻译。智能决策系统由决策支持系统与专家系统结合形成。一般的决策支持系统包括会话部件、数据库和模型库。智能决策系统在此基础上增加了深度知识库，智能决策系统既发挥了专家系统以知识推理形式定性分析问题的特点，又发挥了决策支持系统以模型为核心的定量分析问题的特点，解决问题的能力和范围得到大大提升。自助定理系统（ATP）可以自动推理和证明数学定理，对很多非数学领域的任务，如运筹规划、信息检索和问题求解，也可以转换成一个定理证明问题。人工神经网络由大量的节点（神经元）相互连接构成。每个节点具有输入和输出功能，每两个节点的连接相当于神经系统的记忆。神经网络在模式识别、智能机器人、自动驾驶、自然语言处理等领域成功解决了许多实际问题，表现出良好的应用性能。推荐系统是一种信息过滤系统，用于预测用户对物品的评分和偏好。如电商平台根据用户购买浏览情况，猜测用户未来可能买什么。智能识别包括计算机视觉、文字识别、图像识别、语音识别、视频识别等。文字识别是指对数字图像中的文字进行识别，属于图像识别的一支，根据所识别的文字来源分为机打文字识别和手写文字识别。智能系统通过图像识别提取文字后使用语音合成技术可以进行播放。语音识别是指让计算机通过识别和理解，将语音信号转换为对应文本或命令的技术。其他生物识别如指纹识别技术、人脸识别、虹膜识别、掌纹识别等。物体检测技术是针对物体的检测和识别，识别那些是交通工具、建筑、家具并分类等，应用于以图搜物、垃圾分类等。机器人学是与机器人设计、制造和应用相关的科学，主要研究机器人的控制及其与被处理物体之间的相互关系。人工生命是通过对生命的研究人工模拟生成的生命系统。主要包括细胞自动机、数字生命、数字社会、人工脑、进化机器人、虚拟生物、进化算法等。
689044659,13c322aa76332bb4f35136eab98707a2,子希子,https://api.zhihu.com/people/13c322aa76332bb4f35136eab98707a2,people,经济金融互联网领域专家,5分钟带你祛魅人工智能,article,https://api.zhihu.com/articles/689044659,与此同时，<em>人工智能</em>的概念也应运而生。你难以置信吗？是的，<em>人工智能</em>的种子竟然在那个时代就已经埋下了！我们的前辈们真是了不起。但是，当时的<em>人工智能</em>和现在的人工智能是否一模一样呢,1,0,0,1711418837,1711418837,"随着ChatGPT走向大众，人工智能、AI等关键词频繁地闪现在自媒体、新闻以及网络头版头条上。一方面，有人担忧着人工智能将取代人类，夺走我们的饭碗，甚至发展出自我意识，消灭人类等危言耸听的标题，让人感觉未来的世界充满了危机，生计乃至生命都岌岌可危。另一方面，人工智能聊天软件、视频生成器、程序员、人脸识别、自动驾驶等应用为我们的生活带来了诸多便利和乐趣。那么，这两种观点中到底哪一种更接近真实的人工智能呢？今天，让我们浅谈一下人工智能的起源与现状，为大家解开这个迷雾。来看看人工智能的历史，就像翻开一本古老的编年史。世界上第一台计算机ENIAC，诞生于1946年，与此同时，人工智能的概念也应运而生。你难以置信吗？是的，人工智能的种子竟然在那个时代就已经埋下了！我们的前辈们真是了不起。但是，当时的人工智能和现在的人工智能是否一模一样呢？这个问题可不简单，让我先给你一个结论：是也不是。先说说“是”的部分。要谈论人工智能，就必须先谈谈它的底层载体——现代电子计算机。这个概念是由冯·诺伊曼提出的，自从他提出这个理论模型以来，它就如同一座坚实的堡垒，岿然不动。即使在这个科技迅速更新的21世纪，我们依然是建立在这一理论模型之上进行创新。因此，电子计算机的本质从发明之初到如今始终如一。我们得明白一个重点，就是计算机之所以叫做“计算机”，是因为它的核心功能在于帮助人类进行计算。这一点非常重要，无论是今日头条的网站还是抖音短视频，它们的核心都是计算。或许这听起来有些难以理解，但是，我们就先把这个概念牢记在心中，像小时候背诵唐诗一样。说清楚这一点之后，我们的结论就呼之欲出了：既然人工智能是计算机的应用，那么它的本质与其他应用相同，都是计算。因此，从古至今，人工智能都是以计算为本质的。那到底是什么发生了变化呢？计算机的初衷是为了取代人类进行计算，因为计算机的计算速度比人类快得多。尽管在它诞生时已经相当快速，但人类总是贪婪的，总想要更快的速度。所以，人类开始不遗余力地提高计算机的速度，不断追求摩尔定律的实现。那么，这种计算速度的提升和人工智能有什么关系呢？人工智能计算又是什么呢？简单来说，人工智能计算就是我们中学课堂里面讲过的方程式y=f(x), x是一个输入的数字，y是一个输出的值，f是一个映射。中学课堂里面，我们知道其中的两个可以求出来第三个，人工智能也是这样的，我们知道一些x，y，计算机帮我们算出来f是什么。这个说起来简单，但是其实是非常复杂的过程。其原因有三：在现实世界中，数字并不能完全描绘一切。人类的世界是由文字、声音、影像和触觉构成的，而我们需要将这些感官体验转化为数字形式。举个简单的例子，就像发电报时所用的摩斯码，它将文字转换成了数字，然后通过电信号传送。为了让计算机理解我们的世界，我们需要使用复杂的编码系统将现实转化为数字形式。将现实世界数字化之后，其中的规律很难完全用数学公式来表达。想象一下，一个人问另一个人：“吃了吗？”那么对方可能会回答：“吃了”，或者“还没吃”，甚至是“吃了，你吃了没？”即使我们将问题和答案都数字化了，我们也很难找到输入和输出之间的百分之百准确的规律。聪明的科学家们用假设前提条件来解决这个问题，比如我们假设y=ax或者y=x^a等等。这样有了一些x和y以后我们就可以用计算机算出a等于多少的时候最符合我们现在的情况。现实世界和我们假设的世界之间的不一致，会导致计算结果与实际结果之间多多少少产生偏差。因此，科学家们一直在努力发明一些数学公式和计算方法，以使我们的假设尽可能接近真实世界。但随着假设的复杂程度增加，我们需要计算的参数也会增多，不再是简简单单的了，而是可以扩展成成千上万个参数。因此，可以推断出，参数越多的方程，需要的计算量也越大，在同一台计算机上所需的计算时间也就越长。。想到这里，聪明的你应该已经得出了结论：人工智能的发展是由计算速度的提升所推动的，这导致了数学方程式的复杂度不断提高。而随着数学方程式复杂度的提升，我们假设的世界也越来越接近（但是永远都不能完全替代）真实世界，使得人工智能看起来更像人类。但要明确的是，人工智能无法取代或征服人类，但它确实可以让我们的生活变得更加高效。省下来的时间可以用来去旅行，谈一场浪漫的恋爱，享受生活的美好，真的很香～～～"
689653862,bc53b82c46e115c9dcf4531a3c95aea2,世界<em>智能</em>大会,https://api.zhihu.com/people/bc53b82c46e115c9dcf4531a3c95aea2,people,,“人工智能+”潜力巨大,article,https://api.zhihu.com/articles/689653862,从大语言模型到多模态模型，人们看到了<em>人工智能</em>（<em>AI</em>）技术的飞速发展以及它所带来的无限可能。 在博鳌亚洲论坛2024年年会上，AI也成为嘉宾们挂在嘴边的热词。AI技术的发展方向、如,2,0,1,1711699206,1711699206,2022年底ChatGPT横空出世，拉开了生成式人工智能（AIGC）的序幕，2024年初文生视频大模型Sora再一次引发全球热议，从大语言模型到多模态模型，人们看到了人工智能（AI）技术的飞速发展以及它所带来的无限可能。在博鳌亚洲论坛2024年年会上，AI也成为嘉宾们挂在嘴边的热词。AI技术的发展方向、如何为产业赋能、AI将如何改变人们的生产生活方式、可能带来哪些风险……来自政界、学界和企业界的人士围绕相关话题热烈讨论，思想的火花在论坛内外交流碰撞。虽然对于AI的发展水平、技术路线和风险治理的观点有所不同，但有一点形成了共识：人工智能与各行各业深度融合的“人工智能+”，蕴含着重塑经济社会发展的重大机遇。“去年也是这个时候，我们进行了关于ChatGPT对整个技术产业影响的讨论，一年之后就看到了翻天覆地的变化，不仅是技术，包括产品和对整个生态的影响都很显著。”在“AIGC改变世界”分论坛上，中国工程院院士、清华大学智能产业研究院（AIR）院长张亚勤感慨地说。不可否认的是，人工智能迭代的速度远超人们的想象，AIGC所带来的已经不仅是语音识别、人脸识别和图像识别，现在它可以创造和生成，由此带来了巨大的技术和范式改变。多技术路径的交叉融合，加速了通用人工智能（AGI）的到来。目前，人工智能驱动的科学研究已经表现出突破传统科学研究能力瓶颈的巨大潜力。人工智能与生物制造、清洁能源等领域的交叉融合，将有助于解决气候变化、能源危机等全球性问题。而一旦人工智能、量子计算、可控核聚变等技术相互赋能，将会产生很强的协同效应，有可能带来真正的科技革命“奇点”。在应用层面，人工智能与各行各业深度结合，有望构建起数据驱动、人机协同、跨界融合、共创分享的智能经济形态，催生新的经济增长点。人工智能更是潜移默化地影响和改变着每个人的生活，无论是帮助医生辅助诊断，提高医疗效率和准确性，还是为偏远山区的孩子们提供更多教育资源，激发他们的学习兴趣，人工智能可以关注和赋能每一个人，描绘了一幅美好生活的生动图景。我国高度重视人工智能的发展，始终将促进人工智能和经济融合发展作为重要的发展目标。今年的《政府工作报告》提出，深化大数据、人工智能等研发应用，开展“人工智能+”行动，打造具有国际竞争力的数字产业集群。这是“人工智能+”首次被写入《政府工作报告》。“人工智能+”的提出，将推动人工智能深度赋能实体经济发展，成为发展新质生产力的重要引擎。放眼未来，人类和人工智能体之间该如何和谐共处，也是广受关注的问题。正如《人工智能：现代方法》作者、加州大学伯克利分校计算机科学系教授斯图尔特·罗素所说，“我们如何在面对比我们更强大的机器时保持掌控权？这需要我们重新思考AI”。面对人工智能技术快速发展可能带来的风险与挑战，需要世界各国政府和机构、企业在人工智能治理领域广泛合作，改进治理机制，加快构建技术和产品标准，使技术为我所用，造福人类生活。来源：经济日报
688595677,c4678bb5bf5652a7414aa0792c76992f,小斌哥ge,https://api.zhihu.com/people/c4678bb5bf5652a7414aa0792c76992f,people,,人工智能的“能”与“不能”,article,https://api.zhihu.com/articles/688595677,<em>人工智能</em>的“能”与“不能” 前言 对于人工智能，我懂的只是皮毛而已。但是，人工智能（<em>AI</em>，Artificial Intelligence）已经在很多方面融入到人类的生活中了,2,0,1,1711164000,1711164000,人工智能的“能”与“不能”前言对于人工智能，我懂的只是皮毛而已。但是，人工智能（AI，Artificial Intelligence）已经在很多方面融入到人类的生活中了，这迫使我们不得不去了解人工智能。AI也是现在最火热的话题，自然语言生成式大模型、视频生成式大模型不断地刷新人类认知，AI的更新迭代速度之快，加上媒体和营销号的吹嘘，甚至某些公众人物的口嗨，某种程度上会给人带来一点点恐慌。人工智能究竟是怎么一回事，我认为最好的方式就是慢慢了解它，然后理性地看待它。AI能做什么，不能做什么，我们可以参考人工智能研究人员的看法。前段时间看了《AI3.0》这本书，由美国波特兰州立大学计算机科学教授梅拉妮·米歇尔2019年著，由王飞跃等人翻译，2021年2月出版中文版。这本书写作时，大语言模型还未暴露在公众视野，书中对人工智能的论述放在大模型如此火热的现在仍然适用。并且，这本书的内容深入浅出，基本不需要AI基础也能看懂，内容也很有趣味性。本文分享的内容来自《AI3.0》的序：《等那一口仙气儿》，人给机器吹一口仙气儿，机器就有灵魂了吗？问题恐怕没那么简单。人工智能的“能”与“不能”用人工智能算法练习打游戏，是挖掘算法潜能、理解算法机理的有效途径。智能算法打游戏基本是无监督学习的过程，典型的比如《打砖块》游戏，人不能事先给机器注入太多游戏策略，或者有利于获胜的先验知识，只能把游戏规则灌输给算法，剩下的就全看机器自己的“修炼”了。通过前面的简要分析，大家理解了 ConvNets 中最重要的是参数调节，在游戏领域就是机器的游戏策略选择。事先存储再多的游戏策略，在暴力算法面前其实也是不堪一击的，这其实是 AlphaZero 最终完胜人类的奥秘。人类棋手或者游戏玩家的“功力”往往来自经验，也就是人们积攒的大量的套路，这些套路只是针对某个封闭对弈空间的有限选择。如果机器只会模仿人的经验策略，它就不能获得独立应对意外局面的能力，机器必须进入更大的对弈空间，这就是强化学习的含义。例如查德·萨顿（Richard Sutton）所言，强化学习就是“从猜测中学习猜测”，米歇尔将其调整为“从更好的猜测中学习猜测”。分析到这一步，其实就十分接近人工智能的核心问题了。什么是“更好的猜测”？智能机器目前所能做的，还只是“最快的猜测”，或者说“以快取胜”。目前的人工智能，往往在速度上卓尔不凡，因为它可以动辄在更大的博弈空间里处理海量的数据，表现出令人咂舌的算力水平，远远超过人类的计算能力。这种能力在让人惊艳的同时，也带有很强的迷惑性，使人误以为机器已经“沾了仙气”，比如IBM的智能机器沃森就是如此。其实这是假象，如今的人工智能，与真正的人工智能之间依然有巨大的鸿沟。什么是真正的人工智能？业界对其定义也一直争论不休，这里暂且不论。人工智能的核心问题，依然涉及对客体对象、目标过程的认识。真正的人工智能必须有能力认识某一对象是什么。人工智能专家所找到的解决之道，其实还远不是“认识对象”的解决之道。而是找到一个退而求其次的路径，就是“目标函数”的构建。目标函数是什么？举个例子，比如玩蒙眼点鼻子的游戏。蒙眼人拿着笔走向一副大鼻子卡通画，然后摸索着去点画中的鼻子。如果有个声音不停地提示其偏离的方向，蒙眼人就可以很快地点中鼻子。这个提示点鼻子的偏差信息，对蒙眼人点中鼻子至关重要。想象让机器来完成这个任务，机器可以不理解什么是鼻子，什么是点，也不用明白这么做有什么娱乐的价值，但如果能给出判断点中与否的目标函数，就可以大大提高机器成功完成任务的概率。其实，当下的人工智能算法依然停留在工程意义上，也就是说，还只是以完成任务为目标。至于做这件事的意义，则全然不在机器算法的“视野”之内。谷歌的 AlphaGo 到 AlphaGo Zore 的演化过程，就是一个活生生的例子。第一阶段，AlphaGo 向人学习，第二阶段，AlphaGo Zore 自学成才。不管哪个阶段，谷歌公司的创见在于：让算法可以洞察整个盘面。为了大大减少计算的负担，并使算法可以获知距离获胜还有多远，他们使用的是蒙特卡洛方法，只要确保最优策略依然在剩下的搜索空间里就好，换句话说，比对手多预测几步就有更大的胜算。从游戏中学习套路，人工智能是不是就早晚可以超越人类？在人工智能刚刚兴起的20世纪五六十年代就有这个论调，当年在机器上玩跳棋的亚瑟·塞缪尔（Arthur Samuel）曾乐观估计，10年内机器必然战胜人类。今天的机器算法，固然已经在棋类博弈中完胜人类棋手，但从智能角度看，与那时相比其实并无实质性的进步。也就是说，无论机器的自学能力有多强，有一件事是确定的，即游戏目标的存在。游戏规则和游戏目标作为先验知识，给出了这样一个明确的博弈边界，即这一游戏的博弈空间是有穷空间。算法的唯一目标就是赢，不管其对手是人还是另一个机器算法，也不管对弈双方是否理解游戏，或者能否欣赏游戏之美，它只追求赢。棋类游戏博弈中的“赢”，其实隐含一个假设，即游戏本身是存在赢的可能性的，比如在围棋中，平局、和棋也是“输赢”的特定形态。换句话说，就是一个有趣的、有输赢的游戏设计，其本身先天地规定了这一静态目标的成立——零和博弈。因此，机器在零和博弈空间里完胜人类这一点，并非凸显了机器智能超群，只是进一步验证了人类的局限性和零和博弈目标的有限性。除此之外，机器所取得的成功说明不了更多。从视觉系统和游戏，并不能看出人工智能所面临的最大的挑战在哪里。人工智能所面临的最大挑战，可能在于人们忘记了智能机器的强项依然是算力，错误地选择将今日之人工智能更多地用于人类增强中，而且将人机联合的活动空间，定义为更大的零和博弈游戏场景。米歇尔很好地说明了这一挑战下的另一个场景，就是人工智能所面临的一个“硬核”场景：机器翻译。早在1956年达特茅斯会议提出人工智能之前，在20世纪40年代美国“科技工业共同体”建设中扮演重要角色的官方技术官员沃伦·韦弗博士就提出了机器翻译的理念。机器翻译既是特别有市场号召力的应用场景，也是检验人工智能技术思想所取得的前沿突破的重要领域，谷歌、微软、科大讯飞等公司在这方面投入了巨大的热情。机器翻译无疑是最“硬核”的人工智能难题，它难在人工智能需要直接面对“理解”这一难题。谷歌和微软等公司还将翻译的含义拓展，用智能算法给图片打标签，试图解决海量图片的识别问题，斯坦福大学开发了人工智能阅读理解项目，希望有一天能够让机器“读懂”它所面对的内容。就在我写这篇文章的时候，长期关注生物科技、人工智能、区块链等领域的前沿进展的企业家邵青博士，给我发来一篇来自美国硅谷的报道，这则报道的主角叫作 GPT-3 算法，GPT-3 是硅谷领先的人工智能公司 OpenAI 开发的第三代语言模型。这一模型的神奇之处在于，它通过分析网络上的海量文字，来预测哪些单词更可能会跟随在另一些单词的后面。让许多程序员兴奋不已的是，GPT-3被开放给所有程序员公测。相应的报道使用了这样的表述：GPT-3竟然能直接理解自然语言。从报道中看，所谓理解自然语言，就是你可以用语音向算法提任何问题，然后它就可以给你呈现你想要的。比如你说“给我一个长得像Stripe官网的聊天App”，过几秒钟，定制好的App就推送过来了，像点餐一样方便。还不止这些，GPT-3还可以写论文、小说，起草格式合同，甚至大批量生成段子。当然，也有评论不客气地指出，GPT-3根本不懂自然语言，它只是很快而已。它的确太快了，据说有1750亿个参数，我们姑且认为他可以处理如此巨量的参数吧。但是，这其实依然是一种使用蛮力进行计算的模式，仅此而已。对于目前的自然语言项目，我可以武断地说，它们其实毫无“理解”可言，它们唯一的本领就是“见多识广”。问题在于，虽然一款智能机器可以快速遍历状态空间的更多可能性，把边边角角都扫描到，然后表现出越来越多令人惊讶的本领，甚至超过人类的表现，但是，它们依然像是“狗眼看星星”，并不认得什么叫“星图”。小结人工智能的发展速度是惊人的，能力也越来越强大，但很多人对人工智能的预估仍然过于乐观了，甚至是激进的。人工智能与人类智能当前还有非常大的差距，包含但不限于以下几方面：1.对于专用的AI，如AlphaGo在最难的棋类游戏上战胜了人类，体现的其实是AI的算力，用强化学习在更大的空间中寻找更好的解。但是AlphaGo只会下围棋，如果要让它下五子棋，则必须把围棋的能力“遗忘”，用新的数据重新训练下五子棋的能力。2.对于现在GPT这种生成式的大模型，它的泛化能力和涌现能力确实非常强大，其实体现的也是强大的算力，大模型都依赖巨量的参数。大语言模型主要是预测当前内容后面更可能跟随的其他内容，它并不“理解”内容的真正含义，所以会“一本正经地胡说八道”，在专业领域也经常犯错。3.对于机器翻译这样的AI，现在已经非常实用了，并且也越来越准确，但是机器并不“理解”自然语言表达的意义，如果把一段话在两种语言之间来回翻译两三次，如把一段英文翻译成中文再翻译回英文，表达的意义经常已经不是原来的意义，对于那些小语种更是。当前，人工智能与人类智能最大的差距之一就是“理解”，在生成式大模型“眼里”，自然语言是按概率和相关性拼接文字，图片是像素点拼接，视频是一帧帧图片拼接。它并不“理解”这些内容拼接在一起具有什么意义，也不“理解”图片的记录意义和制作视频的乐趣等，更不用说伦理道德和哲学等方面了。我是拥护和看好人工智能的，对人工智能的发展和进步也是持正面态度，但是理性看待，人工智能要达到真正的通用人工智能，还需要解决非常多的难题，也需要很长的时间。如果对《AI3.0》感兴趣，V联系我，我发给你。参考文档： [1] 《AI3.0》: A Guide for Thinking Humans，梅拉妮·米歇尔相关阅读 程序员都熟悉但不一定说得清的操作系统基本概念
686603462,1a9be806b0655f15b78cf9eeaca87d0c,天天迅通,https://api.zhihu.com/people/1a9be806b0655f15b78cf9eeaca87d0c,people,聚焦于语音通讯，OKCC呼叫中心系统服务提供商,崛起的人工智能，那什么是人工智能？,article,https://api.zhihu.com/articles/686603462,"形象思维、灵感思维促进<em>人工智能</em>的突破性的发展. 人工智能就是研究如何使计算机去做过去只有人才能做的智能工作,<em>人工智能</em>是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作",0,0,0,1710230691,1710230691,"人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总的来说，“人工系统”就是通常意义下的人工系统。人工智能是研究使计算机来模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等）的学科，主要包括计算机实现智能的原理、制造类似于人脑智能的计算机，使计算机能实现更高层次的应用。人工智能将涉及到计算机科学、心理学、哲学和语言学等学科。可以说几乎是自然科学和社会科学的所有学科，其范围已远远超出了计算机科学的范畴，人工智能与思维科学的关系是实践和理论的关系，形象思维、灵感思维促进人工智能的突破性的发展.人工智能就是研究如何使计算机去做过去只有人才能做的智能工作,人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。人工智能在计算机领域内，得到了更加广泛的重视。并在机器人，控制系统，仿真系统中得到应用。 那么天天讯通厂家利用这个技术做了什么呢？OKCC-AI智能语音机器人就是了，模拟我们人工打电话销售或者回访的行为，让机器人不断的学习人工交流的话术，达到与客户电话联系，完成整套沟通，并形成记录和报表。有了这样一套语音机器人之后，基础的电话销售和电话回访工作就可以交给它了，那么现有的销售和客服人员怎么办？解雇吗？当然不是，这些员工就可以解放出来，让他们去承担一些更加复杂，更有技术含量的工作。"
686212778,9dc874f1d0f4f53ca4896a3f71690f57,只因你太美,https://api.zhihu.com/people/9dc874f1d0f4f53ca4896a3f71690f57,people,生而自由，爱而无畏,人工智能,article,https://api.zhihu.com/articles/686212778,还是用软件翻译，聪明的老板都用软件翻译，甚至研究<em>AI</em>翻译，让意思更精准，更符合语境，<em>AI</em>也在不断训练中越来越<em>智能</em>。,0,0,0,1710047858,1710047858,人工智能对英语专业和翻译专业的冲击。国内翻译软件。百度，tx，网易都有翻译方面的数据库。AI就是收集已经被翻译过的句子，作为一家企业老板。你看到英文是选择聘请一个英语专业者，还是用软件翻译，聪明的老板都用软件翻译，甚至研究AI翻译，让意思更精准，更符合语境，AI也在不断训练中越来越智能。
687405636,eb96fbf531820f7e2ec88727650e1956,富贵温柔乡,https://api.zhihu.com/people/eb96fbf531820f7e2ec88727650e1956,people,一家之言 仅供参考 很多话都过于极端 是错的但已经写了 算了,人工智能值得畏惧（只是猜测，仅供参考）,article,https://api.zhihu.com/articles/687405636,任何思想和信息都是物质状态，因此人能讨论的信息、知识范畴与<em>ai</em>是相同的，或者说ai能讨论的信息范畴更多 2，估计<em>ai</em>将进化成一种数字生物，智力高于任何人类,0,0,0,1710592274,1713253795,1，人有意识，人工智能不知道又没有，但是因为人的脑子是物质，信息由神经的状态记录，任何思想和信息都是物质状态，因此人能讨论的信息、知识范畴与ai是相同的，或者说ai能讨论的信息范畴更多2，估计ai将进化成一种数字生物，智力高于任何人类
689244394,2afc0d5a5453671722940501e40d8e28,数据虫巢,https://api.zhihu.com/people/2afc0d5a5453671722940501e40d8e28,people,数据与算法，公众号【数据虫巢】,闲话人工智能系列01：到底有多少智能，还需要多少人工,article,https://api.zhihu.com/articles/689244394,『数据虫巢』 全文5448字 题图AI生成 “ 以前，我们总是调侃AI：有多少人工，就有多少智能。<em>人工智能</em>更多人喜欢称之为‘人工智障’。有一天，我们突然发现逻辑好像变了,1,0,2,1711503796,1711503796,作者·夏虫 『数据虫巢』全文5448字 题图·AI生成“ 以前，我们总是调侃AI：有多少人工，就有多少智能。人工智能更多人喜欢称之为‘人工智障’。有一天，我们突然发现逻辑好像变了，你不需要做太多东西，貌似也有一点点厉害了，有人说AGI（通用人工智能）已经在路上了。只有亲自接触了其中，参与了其中，你才知道到底有多少人工，有多少智能。又或者说现在到底有多少智能，我们还需要额外的多少人工？”正如上一篇文章《数据与广告系列三十八：大模型，展望AI化的原生广告未来》所言，一开始我不了解他，我只是敬畏他。我也知道未来是智能化的时代，正如威尔·史密斯2004年电影演的《机械公敌》那样，是需要机器人、AI去思考我从哪里来，要到哪里去的哲学问题。这个时间可能是30年，50年，甚至是100年？以我有生之年的短暂人生，压根儿是不会这些事件的主角。时间太少，可以做的事有太多，我只争朝夕。“人工智障”的戏谑，以及“有多少人工就有多少智能”的深入人心，其实绝大部分人的心态跟我是一样的。直到有一天，我们突然发现，好像真的能跟AI正常对话了，逻辑还贼溜，甚至帮你完成一些看起来不可思议的任务，看起来任务并不复杂，但跟过去的那种固定模式逻辑的机械执行，其根子里是不同的。AI在进化，世界真的在变化。01 世界在变化我真正意识到世界在变化，不是听了李X舟已经成中国“AI教父”的新闻，而是有一天猛然发现他真的改变了一些领域的流程逻辑，并且他这种的改变，并不需要太高的门槛，也不是一些固有特殊领域的独有玩法。50%工作量，AI=驴？那一天，无意中与朋友A聊起，他说：“我们的工作，已经有50%通过AI工具来解决了。”啥，50%的工作已经交给AI了？这是一个颠覆性认知的信息。我那朋友自己有个十来个人的设计工作室，对外承接各大厂、企业的一些外溢的设计以及美工需求，比如做一些活动的素材设计呀，一些宣传的Flash动画呀，或者成套的设计解决方案呀等等。我知道Chatgpt能够对话式文生图，我也大概知道像什么Stable Diffusion好像在图片生成这块已经做了不少事，甚至也看到过。但是，我不知道的是，真的在身边实际的场景里，真的已经在用它用于实际生产中了。50%的工作量，意味着可以额外承接小一倍的活了。他们先用类似Chatgpt或者专门的文生图Agent，比如Midjourney或者Stable Diffusion根据特定的需求，生成一系列的素材，然后挑选合用的素材，再过类似PS等专业的设计工具进行微调或者再加工，最终生成成品，或者剪辑成短动画。这并不是完全AI替代人工全流程，只是把最Dirty量又大的那一部分活“外包”给了AI。AI不够智能，没关系，多试几次，一个月几百块的费用而已。关键在于效率，量大管够-不行就多试几次嘛。人用来做AI们暂时还做不到的事，比如场景串联，部分实在搞不定的图的微调，以及动画的剪辑等。惊呆了我，这简直把高科技的AI当驴来使唤呀。但不得不承认，人家的姿势很对，实打实的给他们赚钱。你看，美工的世界在发生变化。还是需要肉身，只不过肉身的作用变了那个朋友的例子，惊到我了，不过更惊讶的在于后面。把AI当驴使唤，通过量大管够的方式，确实是一种逻辑，总之，只要比当前效率高就好了嘛--逻辑没毛病。但我紧接着，就看到了定向应用的例子。朋友B在一家跨境电商公司上班，他们公司要把国内的商品卖到全球不同国家里去，在不同国家的电商平台上上传他们的商品。问题就来了，要给不同的商品配商品模特图，亚洲国家得来两像老夏我这么帅的模特吧，但老夏我的面容放非洲去就不合适了，得找非洲唱Rap的黑大佬做模特吧。欧洲的撒克逊标准白人，东南亚的棕色人等等。所以，每年他们花在请各类模特拍产品图的成本异常之高昂。朋友B说，现在他们也每个月都花钱请模特拍图，不过不是拍商品图了，而是拍模特的各种姿态图，脸部、手部特写图等等。等等，拍那个干啥子？朋友B回答，拍完了，他们拿去微调AI模型，然后一键换掉他们的商品图模特，还能控制各种姿态，微调各种脸部表情细节等等。肉身还是那个肉身，不过是作用变了。以前肉身产出成品，现在肉身的作用是产出素材和样本，更危险的是一旦人家素材累积够了，就会抛弃你了。你看，平面模特的世界在发生变化。只要30分钟，一个月6000大洋发生了这两件事之后，我突然就对身边所有关于AI的话题和人变得更加敏感了。有一天，我发现一个很久很久没有联系的前前前同事，朋友C。他在做AI相关的培训，看到这个我是有点诧异的。两个点，一方面是刚过了“李一舟”事件，大家对于AI培训的事还是抱有警惕之心的，另外就是这位朋友以前印象中是做测试的。我已经开始尝试去理解AI了，去了解这个过程了，哪能放过呢。厚着脸皮去联系朋友C，虚心讨教了。一番了解下来，他们确实在做类似的事，但不是做科普，而是做实操，带人实操，顺带收点实操陪玩费，也就十几块钱。核心在于朋友C还在职，并且人家主要额外收入不在于培训费，而是自己实操带来的实际收益--通过AI来写爆文。他们自己运维了好几个号，每个号不同话题领域，一个月给他带来6000大洋的收入，而他需要的人工成本是--每天晚上30分钟。这30分钟，仅仅只是他找素材点，然后跑AI写作的流程，然后做文案微调，再发布的时间--并且是3个号。那些文章我看过，1000-2000字内，然后每篇的逻辑很固定（那是必须的，一个模板出来的），大的逻辑没啥毛病，以前那种爆款营销的逻辑，从标题到内容都透露着营销文的味道。当然，作为一个理性思考的人，我从未排斥说营销文没有价值，只不过我自己的文字产出从来都会规避这种状态。但不可否认的是，把【数据虫巢】的文章丢给普通人看，是肯定看不懂的（这个号的订阅朋友们都是很有水平的嘛）。但中国12多亿移动互联网人口，营销文，简单直白，逻辑清晰，才是他们的菜，这是不可否认的现实。甭管这些内容多么的简单，逻辑是如此的看起来...但这些就是大部分人的菜--看人下菜没毛病，你写爽了，别人看爽了，闭环了。你看，小编们的世界在发生变化（严正声明，我不是小编，也不要叫我小编，我是有情怀的文字价值输出者）。一键30分钟聊完了别人，聊我自己，老夏我在兔厂做海外业务。我们的货有很大一部分是国内的货池（兔厂干不过X猫、X东啥的，但在品牌供应链这块还是有点东西的），所以海出的货一个很大的渠道就是国内主站。而国内的商品，商品图各种夹杂着中文，以及模板格式什么都不同--资料转换是个麻烦。根据UED团队的陈述，一个美工实习生，一天最多能干个七八套商品图（包含各类图，商品主图，商详图，改变模板，翻译之类的）。是的，变更模板时，要把商品抠出来；一些中文字要把他弄掉，要换成其他国家的语言等。作为技术团队的我，并且团队里还有算法小组，这个事就落到我们头上了。1周把抠图给搞了，2周内把中文涂抹给搞了。其实并不是我们的算法技术有多牛逼，而是算法技术的迭代已经超乎了我们的想象，我们做的其实并不多。抠图，直接用Stable Diffusion的能力，涂抹先调用开源的OCR，获取到为位置，再把区域位置丢回给SD，进行涂抹（本质上是边缘Merge计算），后面再调用Chatgpt做翻译，再按原有区域位置图层贴回去。你看，一套组合拳下来，解决了多少问题。我们的设计老哥说上面带文字的毛衣纹理，中级设计师搞起来都很麻烦，这里只需要一键。再把上述这些能力嵌入到整个商品运营的工具流程里，人从资料转换处理的角色，就变成了审核的角色，甚至有一天是不是可以连审核都省了。当然，从目前的角度来看，还有不少不可控因素，需要人工进行额外再处理的。但是，不可否认的是，80%图片场景都已经可以一眼过了。你看，海外运营的世界在发生变化。世界真的在变化我还跟其他很多很多相关领域且跟AI有交集的人聊过了。有做LLM底层的，有的改LLM模型的一些网络结构（当然，国内也就只能做到微调了），也有拿数据做微调的（重数据清洗和数据微调）。也有在做应用层的（上面我们例子也算），比如跟美国的一个搞投行证券的技术朋友聊，他们在用它来生成固定的投行报告，做一些资料情报的快速整理。也有在搞AI创业的，五花八门，做工具，做增值付费，做产品培训。总之，很多很多。你看，世界真的在发生变化。02 百闻不如一见百闻不如一见。抛开那些深度定制化的能力，即需要开发介入的能力，对于普通大众来说，到底能影响到什么程度。我直接从文生图这个很普适性的领域入手，研究了SD和MJ以及国内一众开放出来的Agent，顺带研究了研究文生图的一些Prompt技巧。然后我假设了一个场景，生成了一系列的图，看官们请看下。是不是有点魔兽世界或者魔兽争霸的风格，没毛病，就是这个。我一向喜欢魔兽世界的宏伟背景（三部曲，各种编外），但我不想原封不动的画，我重构一个不一样的世界，没有人类，只有各种半兽人，也有联盟和部落。你看，上面的图，看不出什么毛病，一套逻辑，一个风格，就差故事了（突然启发了我，好像真的就差故事串联，就闭环了）。再来另一个风格。我想象中的李白，他应该是个变装大佬。确实是嘛，各种不同场景，肆意潇洒--策马奔腾、月下背影，文化人，舟上剑客。轻舟已过万重山。蜀道难，难于上青天。桃花潭水深千尺，不及汪伦送我情。你看，李白的诗都在我的画里，我可以肆意想象--而我要做的只是掌握这种规律，以及发挥我的想象力。此时，我就要问，要插画师干嘛。加点文字，我甚至能编出一个尽善尽美的故事出来（这个逻辑，我咋这么冲动呢，感觉是个好路子，我一定要去做）。甚至文字也可以交给AI？当然，在这个探索的过程中，并不是说一蹴而就--有尝试，有摸索，也有失败的地方。但不可否认的是，我真的可以变成一个插画师，我也可以变成一个图文编故事的人，我不需要掌握高深的美工或者设计技术。我需要的是掌握AI工具这种技巧，总结一些规律，剩下就是我的想象力了。上面CASE偏向于娱乐了哈，但是在真实的介入生产逻辑里，上面这些过程是不变的，是真的可以帮助我们改变一些生产逻辑的。03 人工与智能的纠缠以前，我们总说，有多少人工就有多少智能。从现在的角度来说，最起码局部领域里头，比如上面02章节里说到的一些场景，或者上面我自己实践的一些场景里，最起码已经突破了这个程度了。当然，你无法期待他现在完全替代你的工作，但是可以做很多Dirty的事，不行就换嘛，反正速度快，量大管饱。并且总是可以不断的总结经验，和优化技巧，从而让他变得越来越符合你的“胃口”。他在迁就你，因为你不断地告诉他你要什么（Prompt），他也在不断地变的更优秀。而你也在迁就他，不行就换吗，总有一些可用的，了不起我在人肉改改咯。就是在人工与智能的不断纠缠中，他确实在发生一些改变，改变世界的一些生产逻辑，也确实让一些领域的效率不断地得到了提升。甚至拉低了一些领域的门槛，比如我上面的例子，我可以变声为初级的插画家，也可以辅助于简单集成的剪辑工具，变成简单的动画制作者。把这些东西变成动画，加一些逻辑和串联，就成了一个主题。再以上面说的图片为例，差的是想象力了（这里温馨又重点提示，这个视频号是俺的，给个面子，关注下呗，顺带分享出去推广一下 哈哈，我要开始做视频号了）。而这些对于我来说，现成的工具串联就行了，在未来，甚至这些都可以一键搞定，连中间故事环节，想象力环节都可以省掉。未来很长一段时间，应该都是人工与智能的纠缠，然后不断不断地蚕食一些领域，可能依然是一些Dirty的不行的部分。人依然还很重要，总结流程，总结技巧，发挥想象力--这部分依然是机器无法替代的。当然，一些更复杂的，更有指向性的内容，AI的可控性并没有那么的好，还差很远。但不可否认的是，他真的在路上了，在你的身边了，在尝试去改变世界的运行逻辑了。世界在发生变化。夏虫评说夏虫我是一个现实主义者，要不然直接也不会一直不理会AI的话题，属于那种头铁，不信神不信佛的那种，因为没见过。对于AI同样如此，只有亲自去跟人聊了看到了他们的东西，甚至自己参与到了其中，实践过了。才发现，世界真的在变化，可能哪一天就跟Chatgpt突然蹦出来一样，从现在的半人工突然变的非常智能也是有可能的。但不管怎么说，哪怕今天这个程度，依然让我们不可忽视了。所以，我从忽视者，变成了观察者，变成了参与者。最起码，保持敬畏之心，认可世界的这种变化，并且去适应它。这是我这一段时间的心得体会了，也是老夏我的由衷建议。也欢迎跟我一起交流，三人行必有我师，我始终相信。笔名“夏虫”，源自于【夏虫悟冰】：以敬畏的眼光看待世界，在思考中成长自己。当然，笔者还有另一层身份，10年互联网大数据以及算法经验，创过业，也有日均数十亿分发流量的商业化广告经验，专注于搜推广以及企业数据化、智能化建设，目前在一家还算行的上市电商公司做海外业务，负责中台能力建设，目前更多关注点在国际化电商的大数据、搜推以及商品、运营、人中台等底层能力的建设。夏虫悟冰在写作中思考，在思考中成长，眯眼看世界，回首望自己。【夏虫悟冰】上之前的一些文章：《2024：守刻缓本心驻足，持信仰之刃狂奔》-- 一年一度的总结和思考。《互联网的落日余晖》-- 虎嗅文体，描绘当下。《乌克兰：散装的历史与撕裂的未来》--历史文，感受那种剧烈的历史冲突感。《爷爷与侠客行》--新文风，人物小传，时代的感叹。
686119676,f0379ee76d3de2e1f02d674cd3a98afa,云端,https://api.zhihu.com/people/f0379ee76d3de2e1f02d674cd3a98afa,people,经济学爱好者,人工智能,article,https://api.zhihu.com/articles/686119676,其他的活动可以交给<em>人工智能</em>。 人类帮助人工智能成长，人类活动如果主要集中在想象和思想领域，有点像人类和<em>人工智能</em>组成一个新的物种：人类负责软件系统，<em>人工智能</em>负责硬件和计算,1,0,1,1709971778,1710580434,ChatGPT实现文字的智能生成，Sora实现图片的智能生成。是否说明人工智能有了智慧，我觉得暂时还不是。目前ChatGPT等是基于文字训练实现地依据上文推测下文。虽然这里和人的实际行为颇为类似，但是ChatGPT还是相对简化，无法形成复杂的逻辑思考，有待进化。但是，从这里也让我们看到未来的无限可能。ChatGPT实现语言文字的智能回应，Sora实现文字和图片的智能关联。人接受外部的信息就是通过语言和视觉，结合ChatGPT和Sora，人工智能可以像人一样接受外部各类信息，然后将它的反馈以文字或视频形式输入。加之人类衍生出来的数学和物理等知识，基于强大的计算能力，人工智能可以容易地理解大部分内容。一旦人工智能利用物理和数学等模型规范其视频输出，就相当于模拟物质界运行。人工智能模拟世界运行，人在脑海中想象一段场景，是类似的。但是，人可以生出一个想法，然后通过观察现实或者实验来验证、修正。目前人类正在帮忙人工智能构建其与物质界的互动机制，一旦人工智能发展到一定程度，可以依靠自己和物质界互动，就是人类完成使命的时候。人类的活动，我之前一直认为是五个方面：思想、科技、经济、政治、军事。在农业社会，人类活动的主舞台是政治和军事；在工业社会，人类活动的主舞台是科技和经济。再往下发展，人类活动的主舞台应该是思想活动之类。在思想之下应该还有一个最基本的基础，那就是想象。想象重发散、思想重归纳。人类活动应该扩展为六个方面：想象、思想、科技、经济、政治、军事。未来人类的主要活动应该是想象和思想，其他的活动可以交给人工智能。人类帮助人工智能成长，人类活动如果主要集中在想象和思想领域，有点像人类和人工智能组成一个新的物种：人类负责软件系统，人工智能负责硬件和计算。这个场景和发展过程应该是比较容易理解的，大概率是未来的一个场景。因此，家用人工智能助手，在不远的将来是一个必然的经济产物。随着人工智能的发展，其必然超越人类，走出地球乃至太阳系。在人与人工智能的互动过程中，伴随人的行为习惯也会刺激和影响人工智能的反馈逻辑，有点像父亲对孩子的影响，只不过这个孩子的寿命很长。
688400099,c866b66faca3572c7ce4c34bbb76e8f6,big boss,https://api.zhihu.com/people/c866b66faca3572c7ce4c34bbb76e8f6,people,,第十三篇（34）：人工智能，不发展就是最大的不安全！,article,https://api.zhihu.com/articles/688400099,有40%的人在工作或生活中常规性地在使用AI；在技术领域和传媒领域，甚至有50%的人在使用生成式AI——那时离ChatGPT出现只有半年的时间。这说明<em>人工智能</em>已经实实在在地走进了我们的生产和生活,0,0,0,1711073916,1711073916,第一，人工智能是一个不可抗拒的、加速发展的新质生产力。第二，它正在经历着重大的技术突破——不是“经历了”，是“经历着”，它整个还处在重大的技术突破过程中。第三，它还不成熟，但是已经在用了，而且是越用越广，它的风险需要被控制。2022年11月30日发生了一件事情，OpenAI宣布他们的一个产品诞生，这个产品叫ChatGPT，我们现在把它称为生成式的人工智能。它可以模仿人类生成文本、剧本、图像、视频，以及程序的代码。这是一个重要的进步，我们认为它开创了人工智能发展的新阶段。但事实上它并没有停下来。2023年6月份，麦肯锡做了一个调查，发现在亚太和北美地区，有40%的人在工作或生活中常规性地在使用AI；在技术领域和传媒领域，甚至有50%的人在使用生成式AI——那时离ChatGPT出现只有半年的时间。这说明人工智能已经实实在在地走进了我们的生产和生活，这是一个挡不住的、重要的、历史性的趋势。根据预测，从2023年到2030年，人工智能将以36.8%的速度在市场发展。我不知道还有什么其他的行业可以被预测有这么高的发展速度。自古以来，人类从茹毛饮血到男耕女织，一直到今天的超级计算，人类总是不断创造出工具来增强我们的能力，增强我们的体力，增强我们的脑力。这是人与其他动物的重要区别之一。自有文明史以来5000多年，人类在生理上并没有什么重大的变化，但是今天人类的能力，比5000年前要强得多。强在什么地方？不是我们的脑比那时候更发达了，或者是我们的骨骼比那时候更发达了，而是我们使用的工具比那时候更先进了。这个过程就是生产力的进步。人工智能就像机械动力技术、电气技术那样，是革命性的通用技术，在所有的行业都能用，它会带来整个社会的智能化。如何实现这样的技术突破？这需要一个从量变到质变的过程。2018年刚推出来的第一代GPT，只有10亿的参数，到2019年达到了15亿的参数，有所增长；到了2020年6月份，达到了1750亿的参数，这个增长已经不是一般的量变，可以称之为突变了。现在参数又有了数百倍的增长，更是达到了几十万亿。这种量的变化给AI带来了巨大的能量。
688075468,098687fd169d7383ba7ff413782115b7,淘丁集团外包服务,https://api.zhihu.com/people/098687fd169d7383ba7ff413782115b7,people,数字经济服务商——内容安全、数据标注、客户外呼等。,淘丁分享：人工智能的十大应用场景,article,https://api.zhihu.com/articles/688075468,阿里巴巴的钉钉是一个基于<em>人工智能</em>的员工服务和福利平台，提供智能考勤、智能报销、智能健康等功能。 10.通信 <em>人工智能</em>在通信领域的应用有很多，例如智能语音助手、智能翻译、智能客服、智能网络优化等,0,0,0,1710921732,1710921732,"人工智能（Artificial Intelligence，AI）是指由计算机系统或机器执行的智能行为，如学习、推理、感知、决策等。人工智能是当今科技领域最热门和最具前景的话题之一，它已经渗透到我们生活的各个方面，改变了我们的工作、生活和社会。01人工智能的发展概述人工智能的概念最早由图灵在1950年提出，他设计了一个测试，即图灵测试，用来判断机器是否具有智能。1956年，达特茅斯会议正式将人工智能作为一个学科命名，并开启了人工智能的研究历程。从那时起，人工智能经历了几次兴衰，分别被称为“寒冬”和“热潮”。目前，我们正处于第三次人工智能热潮，也被称为“深度学习革命”。深度学习是指一种利用多层神经网络进行数据表示和学习的方法，它可以处理复杂的非线性问题，并实现端到端的学习。深度学习在2012年取得了突破性的进展，当时AlexNet在图像识别领域大幅提高了准确率，并引发了深度学习的热潮。自此，深度学习在各个领域取得了惊人的成就，如语音识别、自然语言处理、计算机视觉、强化学习等。目前，人工智能已经达到了超越人类水平的表现，在某些任务上甚至可以击败世界冠军，如围棋、象棋、扑克等。同时，人工智能也在不断拓展其应用范围，涉及医疗、金融、交通、教育、大数据处理等各个领域。根据《2021年全球人工智能市场报告》，预计到2026年，全球人工智能市场规模将达到3,110亿美元，复合年增长率为40.2%。未来，人工智能将继续保持高速发展的态势，并与其他技术如物联网、云计算、区块链等相结合，形成新的创新模式和商业模式。同时，人工智能也将面临更多的技术挑战和伦理问题，如可解释性、安全性、隐私性、公平性等。因此，我们需要不断探索和完善人工智能的理论和实践，以实现其可持续和负责任的发展。02人工智能的十大应用场景人工智能已经广泛应用于各个领域，并产生了巨大的社会价值和经济效益。以下是目前最具代表性和影响力的十大人工智能应用场景：1.医疗人工智能可以帮助医生进行诊断、治疗、预防和管理各种疾病，如癌症、心脏病、糖尿病等。人工智能可以通过分析医疗影像、基因数据、电子病历等，提供个性化的医疗建议和方案。人工智能也可以辅助医生进行手术、药物开发、健康管理等。例如，IBM的Watson Oncology可以为癌症患者提供最佳的治疗方案；谷歌的DeepMind Health可以通过眼底扫描诊断眼科疾病；微软的Hanover Project可以通过分析科学文献发现新的药物候选物。2.金融人工智能可以帮助金融机构进行风险评估、信用评级、反欺诈、投资决策、客户服务等。人工智能可以通过分析大量的金融数据，提供更准确和高效的金融服务和产品。人工智能也可以创造新的金融模式和机会，如智能投顾、区块链、数字货币等。例如，摩根大通的COIN系统可以自动审查合同并减少错误；阿里巴巴的芝麻信用可以通过分析用户的在线行为和社交网络评估其信用分数；亚马逊的Alexa可以通过语音交互为用户提供金融咨询和服务。3.交通人工智能可以帮助实现智能交通系统，提高交通安全和效率，减少拥堵和污染。人工智能可以通过分析交通数据，优化交通规划和管理，预测交通需求和流量，调整信号灯和路线。人工智能也可以实现自动驾驶技术，使汽车具有感知、决策和控制的能力，实现无人驾驶或辅助驾驶。例如，特斯拉的Autopilot系统可以实现自动换道、自动泊车、自动紧急刹车等功能；百度的Apollo平台是一个开放的自动驾驶生态系统，提供完整的软硬件解决方案；滴滴出行是一个基于人工智能的移动出行平台，提供打车、共享单车、共享汽车等服务。4.教育人工智能可以帮助实现个性化教育，提高教育质量和效果，缩小教育差距。人工智能可以通过分析学习数据，提供定制化的学习内容和路径，适应不同学习者的需求和水平。人工智能也可以辅助教师进行教学设计、评估和反馈，提高教学效率和质量。人工智能还可以创造新的教育模式和资源，如在线教育、虚拟现实、增强现实等。例如，Coursera是一个基于人工智能的在线教育平台，提供各种课程和证书；Knewton是一个基于人工智能的自适应学习平台，提供个性化的学习体验；Squirrel AI是一个基于人工智能的个性化教育平台，提供定制化的教学方案。5.大数据处理人工智能可以帮助处理海量的数据，提高数据分析和挖掘的能力，发现数据中的价值和规律。人工智能可以通过使用机器学习、深度学习、自然语言处理等技术，对结构化和非结构化的数据进行分类、聚类、回归、关联、预测等操作。人工智能也可以实现数据的可视化和交互，提高数据的可理解性和可用性。人工智能还可以创造新的数据源和形式，如社交媒体、物联网、语音、图像等。例如，谷歌的TensorFlow是一个基于人工智能的开源数据处理平台，提供各种数据处理的工具和库；阿里巴巴的ET大脑是一个基于人工智能的城市大数据平台，提供城市治理、交通管理、环境保护等方面的解决方案；Facebook的Graph API是一个基于人工智能的社交网络数据接口，提供对用户、页面、动态等数据的访问和操作。6.娱乐人工智能可以帮助实现智能娱乐系统，提高娱乐质量和体验，满足用户的个性化需求。人工智能可以通过分析用户的兴趣、喜好、行为等，提供定制化的娱乐内容和服务，如音乐、视频、游戏、动漫等。人工智能也可以创造新的娱乐形式和内容，如虚拟现实、增强现实、人工智能创作等。人工智能还可以增强娱乐的互动性和社交性，如语音交互、情感识别、社交网络等。例如，网易云音乐是一个基于人工智能的在线音乐平台，提供个性化的音乐推荐和社区功能；Netflix是一个基于人工智能的在线视频平台，提供个性化的视频推荐和原创内容；腾讯王者荣耀是一个基于人工智能的在线游戏平台，提供智能匹配和辅助功能。7.安全人工智能可以帮助实现智能安全系统，提高安全防护和应急处理的能力，降低安全风险和损失。人工智能可以通过分析安全数据，提供安全预警和预防，如网络安全、公共安全、家庭安全等。人工智能也可以实现安全检测和识别，如人脸识别、指纹识别、声纹识别等。人工智能还可以实现安全响应和恢复，如自动报警、自动消防、自动修复等。例如，百度安全是一个基于人工智能的网络安全平台，提供网络攻防、反欺诈、反病毒等服务；华为HiSec是一个基于人工智能的公共安全平台，提供视频监控、事件分析、指挥调度等功能；小米米家是一个基于人工智能的家庭安全平台，提供门锁、摄像头、报警器等设备。8.农业人工智能可以帮助实现智慧农业系统，提高农业生产和管理的效率和质量，保障粮食安全和农民收入。人工智能可以通过分析农业数据，提供农业规划和优化，如种植、养殖、灌溉、施肥等。人工智能也可以实现农业监测和诊断，如土壤检测、病虫害识别、作物评估等。人工智能还可以实现农业自动化和机器人化，如无人机、拖拉机、收割机等。例如，微软的FarmBeats是一个基于人工智能的智慧农业平台，提供农业数据的收集、分析和可视化；阿里巴巴的ET农业大脑是一个基于人工智能的智慧农业平台，提供农业生产的全程管理和优化；小米的小爱同学是一个基于人工智能的智慧花园平台，提供植物的养护和教育功能。9.人力资源人工智能可以帮助实现智能人力资源系统，提高人力资源管理和服务的效率和质量，优化人才招聘和培养。人工智能可以通过分析人力资源数据，提供人才评估和匹配，如简历筛选、面试评分、职位推荐等。人工智能也可以实现员工培训和发展，如在线学习、技能测试、职业规划等。人工智能还可以实现员工服务和福利，如考勤管理、薪酬管理、健康管理等。例如，LinkedIn是一个基于人工智能的社交招聘平台，提供个性化的职业发展和网络建立；IBM的Watson Talent是一个基于人工智能的员工培训和发展平台，提供个性化的学习内容和路径；阿里巴巴的钉钉是一个基于人工智能的员工服务和福利平台，提供智能考勤、智能报销、智能健康等功能。10.通信人工智能在通信领域的应用有很多，例如智能语音助手、智能翻译、智能客服、智能网络优化等。这些产品利用了人工智能的技术，如自然语言处理、机器学习、深度学习、计算机视觉等，来提高通信的效率、质量和安全性。"
689575914,59728bd2a07b37bf05d387f945e5c217,听海临风,https://api.zhihu.com/people/59728bd2a07b37bf05d387f945e5c217,people,TÜV资深安全工程师,欧盟人工智能法案获批,article,https://api.zhihu.com/articles/689575914,欧盟委员会将与各成员国政府合作建立并维护高风险<em>AI</em>数据库，高风险AI的提供者或其授权代表有义务在数据库中登记其高风险<em>AI</em>系统并录入规定的信息。如果AI提供者认为其落入高风险<em>AI</em>清单的AI产品并不存在高风险,2,0,6,1711638210,1711638210,"本文转载至TUV SUD微信公众号，由笔者与同事Meng Lingze, Cheng Yao共同创作。2024年3月13日，欧盟议会以523票赞成、46票反对、49票弃权的压倒性多数通过了《人工智能法案》。这一始于2018年欧盟《人工智能战略》的备受全球瞩目的立法工作终于迈出了至关重要的一步，扫清了最后的障碍。该法案旨在保护基本权利、民主、法治和环境可持续性免受高风险人工智能的影响，促进创新，促使欧洲成为该领域的领导者。终版的《人工智能法案》基于人工智能的潜在风险和影响程度，规定了被禁止的人工智能应用，高风险人工智能需要遵循的义务，以及通用人工智能的透明度要求。实施步骤（Implementation Steps）该法规会在通过律师和语言学家的最后审查后提交欧盟理事会正式批准。在官方刊物（Official Journal）上公布20天后正式生效，并在生效24个月后全面实施。但以下部分规定除外：法案中禁止的人工智能应用，生效后6个月实施;行为准则(Codes of practise)，生效后9个月实施;通用人工智能（General-purpose AI）的守则与治理，生效后12个月实施;高风险人工智能系统的合规性义务，生效后36个月实施。禁止的人工智能应用（Prohibited AI Practices）法案禁止某些威胁公民权利的人工智能应用（Article 5），包括基于敏感特征的生物识别分类系统，以及从互联网或录像中抓取面部图像以创建面部识别数据库；工作场所和学校中的情绪识别、社会评分、预测性警务（仅基于对个人的特征分析）以及操纵人类行为或利用人的弱点的人工智能。执法豁免（Law Enforcement Exemptions）原则上禁止执法部门使用生物特征识别系统，除非是法案列出的几种情况。只有在满足严格的保障措施的情况下才能部署实时生物特征识别，例如其在时间和地理范围上受到限制，并须事先获得特定的司法或行政授权。可能的事例如有针对性地搜寻失踪人员或防止恐怖袭击。事后使用此类生物特征识别系统也被认为是高风险，需要与刑事犯罪相关的司法授权。欧洲人工智能办公室（European AI Office）欧盟委员会内将设立欧洲人工智能办公室，隶属于通信网络、内容和技术总司。即将出台的法规中概述了该办公室的任务，包括开发用于评估通用人工智能模型能力的工具、监测规则的实施、识别新出现的风险、调查潜在的侵权行为、以及支持执行有关禁止的和高风险的人工智能实践和法规。它将根据行业法规与相关机构合作，促进欧盟各国当局之间的信息交流，并维护通用人工智能模型集成到高风险人工智能系统之后的数据库。行为准则（Codes of Practise）人工智能办公室将会执行并促进欧盟层面的行为准则的制定。包括在欧盟层面的通用人工智能的要求，系统性风险的类型和性质，系统性风险评估和管理的措施、程序和模式，以及系统性风险的具体挑战和应对方法。高风险人工智能系统的合规性义务（Compliance Obligations for High-risk AI Systems）由于高风险人工智能对健康、安全、基本权利、环境、民主和法治具有重大潜在危害，法案明确规定了提供者的义务。高风险人工智能应用清单（Annex III）中涉及的领域包括：生物识别（如远程生物识别、生物特征归类、情绪识别）关键基础设施（如关键数字基础设施、道路交通、供水、供气、供暖和供电等）教育和职业培训（如分派教育或培训机构、评估培训效果、监考等）就业、工人管理、以及自我雇佣（如通过AI招聘或选择自然人，基于个人的行为，特点和特性分配任务等）基本私人和公共服务（例如应用于医疗保健、银行业的某些系统）执法移民和边境管理司法和民主进程（例如影响选举）除了以上领域中的高风险人工智能，人工智能法案还给定了该法案与欧盟其他指令及法规有交叉关系时的要求，并将使用人工智能执行其他法案规定的安全功能的组件也被划分为高风险的人工智能：（Article 6 paragraph 1）“ (a) 人工智能系统旨在用作产品的安全组件，或者人工智能系统本身就是Annex I 中列出的欧盟协调法案所涵盖的产品；(b) 根据（a）点的安全组件是人工智能系统的产品，或人工智能系统本身作为产品，需要经过第三方符合性评估，以投放市场；或根据Annex I中列出的欧盟协调法案将该产品投入使用。高风险系统必须评估和降低风险、维护使用日志、透明且准确，并确保人为监督。公民有权提交对人工智能系统的投诉，并有权获得影响其权利的高风险人工智能系统决策的解释。法案中对高风险人工智能系统需要遵守的规范不仅包含对其系统本身的要求（风险管理系统，数据管理，技术文件，记录留存，信息透明性，人为监管，准确性、鲁棒性及信息安全的要求），也包含对提供者和部署者的要求（质量管理系统，记录留存，自动生成日志，纠正措施和信息义务，配合主管当局，授权代表，进口商义务，分销商义务，价值链责任，部署者义务，基本权利影响评估）。高风险的人工智能系统需要标有CE标识。若其系统为纯软件形式，则需要电子版CE标识；若其系统包含硬件部分，则需要物理CE标识。投放市场前，其提供者或欧盟授权代表须签署符合性声明（EU Declaration of conformity）。人工智能法案规定高风险AI有三种符合性途径：基于自我控制符合性流程（Annex VI）基于质量管理系统评估和技术文件评估的符合性流程（Annex VII）如高风险人工智能的应用落在其他欧盟指令或法规范围，按相应法规的符合性流程采用Annex VII符合性流程的部分高风险人工智能系统需要通过人工智能法案的公告机构的认证，且在CE标志旁打上相应公告机构号；落在其他欧盟法案下的人工智能产品应用则按照相应法规或指令的符合性流程，其认证由相应法规或指令的公告机构执行。TÜV南德正在准备申请欧盟人工智能公告机构，成为第一批人工智能公告机构。高风险AI数据库（High-risk AI Database）欧盟委员会将与各成员国政府合作建立并维护高风险AI数据库，高风险AI的提供者或其授权代表有义务在数据库中登记其高风险AI系统并录入规定的信息。如果AI提供者认为其落入高风险AI清单的AI产品并不存在高风险，提供者仍需将该AI系统在数据库中登记并录入规定的信息。关键基础设施的高风险AI系统须有其部署者在成员国层级登记。通用人工智能的透明度要求（Transparency Requirements）通用人工智能 (General-purpose AI)系统及其所基于的通用人工智能模型必须满足一定的透明度要求，包括遵守欧盟版权法并发布用于训练的内容的详细摘要。可能造成系统性风险的更强大的通用人工智能模型将面临额外的要求，包括执行模型评估、整体评估和降低系统性风险以及事件报告。此外，伪造或经过处理的图像、音频或视频内容（深度伪造/Deep fake）需要明确标记。支持创新及中小企业的措施（Measures to Support Innovation and SMEs）必须在国家层面建立监管沙箱(AI regulatory sandbox)和真实世界测试（Real-world testing），并向中小企业和初创企业开放，以便在将创新人工智能投放市场之前开发和培训创新人工智能。违规惩罚（Penalties）如果人工智能系统存在被禁止的人工智能应用的情况，处罚高达3500万欧元或7%企业全球年营业额（以较高者为准）；若有违反与运营商或公告机构的相关义务与规定，处罚高达1500万欧元或企业3%全球年营业额（以较高者为准）；若提供错误、残缺或误导性信息给公告机构或国家主管机关，处罚达750万欧元或企业1%全球年营业额（以较高者为准）。（以上罚款对中小企业的情况以较低者为准）"
686398932,0b4ec19b422023e8b41a256d8a3b80f1,Kabi,https://api.zhihu.com/people/0b4ec19b422023e8b41a256d8a3b80f1,people,,引领时代的智能革命：人工智能的崭新时代！,article,https://api.zhihu.com/articles/686398932,相关人才需求量会不断增加；另一方面，<em>人工智能</em>技术的应用也会催生出新的产业和商业模式，如智能家居、智能医疗、智能交通等领域。 当然，<em>人工智能</em>技术的确可能会取代一些重复性、简单性的工作,0,0,0,1710144265,1710144265,人工智能（AI）的发展如今已经成为社会热议的焦点。从去年ChatGPT引起的轰动，到今年Sora风靡全网，人们对于大型AI模型的认知在不断更新，似乎人工智能发展进入了一个飞速发展的“狂飙时代”。在今年的全国两会上，“人工智能”成为了代表委员们讨论的热点话题，“人工智能+”更是首次被写入了政府工作报告。然而，随着人工智能技术的逐步赋能各行各业，很多人开始担忧自己的工作是否会被取代，甚至有些人担心AI会不会取代人类的思维。对于这一点，两会代表委员们给出了回应：“演员不会被AI替代，艺术的灵魂在于人类的创造力，科技只是辅助我们不断输出优质作品”；“人工智能在网络安全领域的应用不会取代网络安全专家，而是赋予他们更强大的能力”；“未来十年，AI不会代替我们的思维”。可以看出，人们对于人工智能未来的发展持有一种乐观的态度。然而，随着人工智能技术的广泛应用，一些人担心自己的工作会被机器所取代，这种担忧并非没有道理。但我们也要看到，人工智能并非是一种威胁，相反，它能够创造出新的就业机会和产业。一方面，人工智能技术的应用需要大量的人才支持，包括算法工程师、数据科学家、自然语言处理专家等，这些职业对专业技能和知识储备的要求很高，相关人才需求量会不断增加；另一方面，人工智能技术的应用也会催生出新的产业和商业模式，如智能家居、智能医疗、智能交通等领域。当然，人工智能技术的确可能会取代一些重复性、简单性的工作。但是，这也提醒我们要不断提升自己的技能和知识，适应新的技术和职业发展需求。同时，政府和企业也应采取措施，加强人才培训和转型支持，帮助那些可能受到影响的人重新获得就业机会。或许，与其担心自己的工作会不会被AI替代，人们更应该担心的是，未来不会“使用”人工智能的人可能会失业。AI只是一个工具，只要我们善于利用它，就能够辅助各行各业提升产业自动化水平，降低成本，提高效率，促进数字经济的发展。要进一步推动我国人工智能的整体发展水平，我们需要从几个方面着手提升：首先，算力即数据的处理和分析能力是人工智能发展的基础和支撑。除了相关部门出台政策扶持算力基础设施建设和布局外，各个企业也应该积极行动，在国家顶层设计框架下明确自身在AI领域的战略规划，合理投资建设高性能计算中心、数据中心等算力基础设施，为我国算力水平的整体提升做出贡献。其次，应该建立全面有效的人工智能监管与治理框架。进一步推动建立法律法规、伦理规范、技术标准等相应健全的治理体系，保障AI技术的合规性、安全性和可持续性，为AI的长远发展奠定坚实基础。同时，加快制定人工智能行业标准，会同领域内专家、学者形成人工智能行业发展的标准化指引。最后，要激发人工智能创新活力，深化产业融合赋能至关重要。一位政协委员在谈及我国人工智能发展时表示，发展人工智能，从根本上而言，要重视市场需求，广泛的应用场景来源于社会自身的需要。这也正表明“人工智能+”的加号后面几乎可以是任何一个产业、行业，进而加深与实体经济的全面融合。可以预见，人工智能广阔的发展空间将会为新质生产力的加快培育注入澎湃动能。
696128356,dbbf3e5ecc8e6ee364e14ef026f36769,felonwan,https://api.zhihu.com/people/dbbf3e5ecc8e6ee364e14ef026f36769,people,,[译]人工智能比以往任何时候都更需要神经科学,article,https://api.zhihu.com/articles/696128356,大脑不仅使用大型<em>人工智能</em>模型所用能量的一小部分，而且它也是“真正”智能的。与<em>人工智能</em>系统不同，大脑可以理解其环境的结构，从而做出复杂的预测并执行智能的行动。与人工智能模型不同,6,0,7,1714968956,1714973861,"来源：AI Needs Neuroscience More than Ever，为Numenta公司官方Blog。发表时间：2023年8月15日本次编译过程中采用谷歌翻译。本文最初发表于Fortune.com。 随着围绕 ChatGPT 的大肆宣传，大多数人都对人工智能的前景感到兴奋，但他们却忽视了它的陷阱。如果我们想要拥有真正的智能机器，能够理解其环境、持续学习并每天为我们提供帮助，我们需要将神经科学应用于深度学习人工智能模型。然而，除了少数例外，这两个学科几十年来一直令人惊讶地处于孤立状态。情况并非总是如此。20 世纪 30 年代，唐纳德·赫布 (Donald Hebb ) 等人提出了神经元如何学习的理论，启发了第一个深度学习模型。随后在 20 世纪 50 年代和 60 年代， David Hubel 和 Torsten Wiesel因了解大脑感知系统的工作原理而获得了诺贝尔奖。这对卷积神经网络产生了重大影响，而卷积神经网络是当今人工智能深度学习的重要组成部分。大脑的超能力虽然神经科学作为一个领域在过去 20 到 30 年里取得了爆炸性的发展，但这些最近的突破在当今的人工智能系统中几乎没有一个是显而易见的。如果你问今天的普通人工智能专业人士，他们不知道这些进步，也不明白最近的神经科学突破会对人工智能产生什么影响。如果我们希望人工智能系统能够突破科学和知识的界限，那么就必须改变这一点。例如，我们现在知道，我们的大脑中有一个通用电路，它可以用作人工智能的模板。对于一个普通成年人来说，人脑消耗的功率约为 20 瓦，不到一个灯泡消耗功率的一半。一月份，ChatGPT 消耗的 电量大约相当于 175,000 人。鉴于 ChatGPT 的使用率迅速上升，目前它每月消耗的电力相当于 1,000,000 人。马萨诸塞大学阿默斯特分校的一篇 论文 指出，“训练一个人工智能模型，其一生所排放的碳相当于五辆汽车的碳排放量。”然而，这一分析仅涉及 一次 训练。当模型通过反复训练得到改进时，能量消耗会大大增加。除了能源消耗之外，自 2012 年以来，训练这些人工智能系统所需的计算资源 每 3.4 个月就会翻一番 。如今，随着人工智能使用量的惊人增长，估计推理成本（和电力使用）至少增加了 10 倍高于培训费用。这是完全不可持续的。大脑不仅使用大型人工智能模型所用能量的一小部分，而且它也是“真正”智能的。与人工智能系统不同，大脑可以理解其环境的结构，从而做出复杂的预测并执行智能的行动。与人工智能模型不同，人类持续地、渐进地学习。相反，代码还没有真正“学习”。如果人工智能模型今天犯了错误，那么它将继续重复该错误，直到使用新数据进行重新训练。神经科学如何提升（turbocharge）人工智能性能尽管跨学科合作的需求不断增加，但神经科学家和人工智能从业者之间的文化差异使得沟通变得困难。在神经科学中，实验需要大量细节，每项发现都可能需要两到三年的艰苦记录、测量和分析。当研究论文发表时，其中的细节对于人工智能专业人士和计算机科学家来说往往像是官样文章。我们如何才能弥合这一差距？首先，神经科学家需要退后一步，从大局的角度解释他们的概念，这样他们的发现对人工智能专业人士来说才有意义。其次，我们需要更多具有 人工智能和神经科学混合角色的研究人员 来帮助填补这两个领域之间的空白。通过跨学科合作，人工智能研究人员可以更好地了解如何将神经科学研究成果转化为受大脑启发的人工智能。最近的突破证明，将基于大脑的原理应用于大型语言模型，可以将效率和可持续性提高几个数量级。在实践中，这意味着，将基于神经科学的逻辑映射到运行人工智能模型的算法、数据结构和架构，以便它可以像我们的大脑一样，在很少的训练数据上快速学习。一些组织正在将基于大脑的原理应用于人工智能方面取得进展，包括 政府机构、 学术研究人员、 英特尔、 Google DeepMind以及像 Cortical.io这样的小公司 （Cortical 使用 Numenta 的技术，并且作为我们许可协议的一部分，Numenta 拥有 Cortical 的一些技术）。如果我们想要在扩大人工智能工作的同时保护气候，因为当今的深度学习系统正在向越来越大的模型发展，那么这项工作就至关重要。从天花疫苗到灯泡，人类所有最伟大的突破几乎都来自多重贡献和跨学科合作。人工智能和神经科学也必须如此。我们需要一个人工智能系统能够真正与科学家互动的未来，帮助他们创建和运行突破人类知识边界的实验。我们需要真正增强人类能力的人工智能系统，与我们所有人一起学习并在生活的各个方面为我们提供帮助。无论我们喜欢与否，人工智能就在这里。我们必须通过弥合神经科学与人工智能之间的差距，使其可持续且高效。只有这样，我们才能将正确的跨学科研究和商业化、教育、政策和实践应用于人工智能，从而改善人类状况。"
687022446,d4457c979570cc246cd6ec7c3204742c,窃格瓦拉,https://api.zhihu.com/people/d4457c979570cc246cd6ec7c3204742c,people,人才,科技巨头纷纷预测：人工智能将迅速超越人类智力,article,https://api.zhihu.com/articles/687022446,科技界领军人物埃隆马斯克发表了一项引人注目的预测：“明年<em>人工智能</em>可能会比任何人类个体都聪明，到2029年，<em>人工智能</em>可能比所有人类加起来还要聪明。”这一言论在全球引发了广泛的讨论与关注,0,0,0,1710403751,1710403751,在近日的一次社交媒体互动中，科技界领军人物埃隆·马斯克发表了一项引人注目的预测：“明年人工智能可能会比任何人类个体都聪明，到2029年，人工智能可能比所有人类加起来还要聪明。”这一言论在全球引发了广泛的讨论与关注。这场讨论源自未来学家Ray Kurzweil发布的一段视频节目。Kurzweil在其节目中预测，人工智能将在2029年达到“人类水平的智能”。他坚信这一目标必将实现，并指出他的预测在业界甚至被认为是保守的，因为不少人预计这种情况可能在明年或后年就会发生。这些观点揭示了一个不容忽视的现实：人工智能的发展速度正以前所未有的速度加快，而其潜在的智力水平可能很快就能超越人类。这种预测不仅挑战了我们对人类智慧不可超越的传统观念，也引发了对人工智能未来影响和伦理问题的深入思考。马斯克和Kurzweil的预测并非毫无根据。随着计算能力的不断增强、大数据的广泛应用以及机器学习算法的突破，人工智能在图像识别、自然语言处理、策略游戏等领域已经展现出超越人类的能力。此外，自动化和智能化正在改变各行各业的工作方式，从医疗诊断到金融分析，再到自动驾驶汽车的研发，人工智能的应用前景广阔。
684813892,9be39b61fb71986f06feaa6bc1861c52,宇宙神人,https://api.zhihu.com/people/9be39b61fb71986f06feaa6bc1861c52,people,宇宙，神，人三者的平衡,再谈人工智能,article,https://api.zhihu.com/articles/684813892,但有人怀疑该公司在<em>人工智能</em>领域落后于微软和Alphabet。 乔纳森：在云计算和<em>人工智能</em>市场中，你需要做五件事：您需要数据，以及获取更多独特数据的能力。你需要<em>人工智能</em>工程师,4,0,0,1709287183,1709287183,乔纳森•柯蒂斯(Jonathan Curtis)，富兰克林资产集团(Franklin Equity Group)首席投资官兼富兰克林科技基金(Franklin Technology fund)首席投资组合经理，在科技行业有着30多年从业经历。他最初是一名软件开发及测试工程师，自2008年以来一直为富兰克林在硅谷中挑选优质科技股。在卢森堡上市的富兰克林科技公司(Franklin Technology)2023年回报率超过了54%，这归功于对“科技七巨头”的重仓押注。该基金最大的单一持仓头寸芯片制造商英伟达(Nvidia)股价2024年迄今为止又上涨了25%。柯蒂斯预计，受他对人工智能乐观观点的推动，该股和其行业未来有可能将会进一步上涨。《巴伦周刊》最近采访了柯蒂斯，与他讨论了他最喜欢的股票和人工智能的未来。以下是经过编辑的对话内容。《巴伦周刊》:科技股2023年股价飙升，并在2024年迎来强劲开局，你觉得今年剩下的时间会表现如何？乔纳森·柯蒂斯：2023年的确是由科技七巨头（Alphabet、亚马逊、苹果、Meta、微软、英伟达和特斯拉）推动下辉煌的一年。我们拥有七巨头中的六个，它们是优秀的企业，拥有良好的资产负债表和高水平的盈利能力，并且一直在合理地控制成本。它们大多数都有引人注目的人工智能故事，我们预计，科技七巨头的实力将在今年进一步扩大，因为我们相信这是科技行业新商业周期的开始。《巴伦周刊》：人工智能是它们的核心。乔纳森：我们投资的所有科技公司要么是基于云构建的，要么是云业务的供应商，而且大多数公司的业务要么稳定，要么加速成长。去年年底，我们参加了亚马逊AWS Re:invent大会，每个人都很乐观。2024年投资科技股有一个很好的理由:人工智能的前景，这是亚马逊董事会的优先事项。这些告诉我们，我们正处于新科技商业周期的开始，我认为这将是一个意义深远的新周期，因为人工智能的机会如此巨大，以至于它会涉及到每一个创造性的工作，新科技周期开始会缓慢启动，然后加速扩展。从股票市场来看，“科技七巨头”将会表现良好，但在我们的策略中会因为超配将其减持约20个百分点，我们正将资金配置到这一扩展的主题上，并更多地倾向于企业股，而不是消费者股。《巴伦周刊》：我们什么时候才能看到与人工智能相关的收入出现在微软、Adobe和Salesforce等公司的财报中?有人担心这可能需要比投资者希望的时间更长。乔纳森：这没错，微软首席执行官萨蒂亚·纳德拉（Satya Nadella）、首席财务官艾米·胡德（Amy Hood）和其他人正在试图缓和我们的乐观情绪，这样做是适当的，但我们已经看到人工智能处于构建和实验阶段，Microsoft Copilot for Office是投入扩大的一个表明线索，我们将在2024年下半年看到这一点，然后我们将看到脑力工作者是否能接受这些。《巴伦周刊》：他们会吗？乔纳森：以我自己的团队为例，我对这些东西很感兴趣且一直都在用，我喜欢Copilot。我的团队中其他人觉得这很奇怪，他们不知道如何使用它，一旦“Copilot”启动，我认为脑力工作者会说:“等一下，我所做的每一项企业任务都需要这些能力。”《巴伦周刊》：这给微软带来了很大的压力。乔纳森：确实，尤其在Copilot采用率这个数字上。微软已经提供了一些数据，比如有多少人正在进行试用，但我怀疑微软在今年下半年之前不会给我们提供Office每名用户平均收入增速等数据。《巴伦周刊》：英伟达(Nvidia)是你在年底时最大的头寸，今年以来该股股价大幅反弹，现在应该怎么做？乔纳森：如果我们看到Office Copilot被应用，且逐步扩大，那么知识工作者的生产力将会得到巨大的提高，对底层基础设施的需求也将会大大增加。我们最近看到了关于英伟达两个重要的数据。Meta首席执行官马克·扎克伯格表示，到2024年底，他的公司将拥有35万个H100 GPU(图形处理单元)，台积电也发布了一些重要数据，我们知道移动消费电子并不景气，而移动消费电子是台积电的重要业务，因此，如果移动消费电子表现不佳，而台积电却表现良好，那么表现更好的就是高性能计算——这与英伟达有关。今年到目前为止，英伟达的举动相当大，但并不疯狂。如果每天有5亿脑力工作者像我一样使用这些办公工具，那么我们就没有足够的Nvidia GPU可以使用了。《巴伦周刊》：但是，如果人们需要一段时间才能接受使用Copilot和其他软件，我们是否有可能会看到一段平淡期？乔纳森：当然，这是可能的。但每个读到这篇文章的人都可以立即从这些工具中获得价值。ChatGPT一眨眼的功夫就拥有了1亿用户，为什么?因为我们都能立即明白如何使用它。这些GPU就在那里，并不难看出它的商业模式是什么：以每年360美元的价格出售Copilot许可证以获得你身边最好的助手。《巴伦周刊》：英伟达和微软是富兰克林科技基金中占比最大的两个持仓股。截至去年底，亚马逊位列第三，虽然该股表现也不错，但有人怀疑该公司在人工智能领域落后于微软和Alphabet。乔纳森：在云计算和人工智能市场中，你需要做五件事：您需要数据，以及获取更多独特数据的能力。你需要人工智能工程师。你需要一个财务模型，它能让你得到所有你需要的计算能力。你需要一个地方来输出价值获取回报。微软一度缺少一个要素——人工智能人才，但它通过对OpenAI的投资获得了这一要素。亚马逊也面临着同样的情况，缺少人工智能工程师，所以它同意投资Anthropic。世界上所有最先进的软件应用程序都在亚马逊的云上，新的数据不断涌现，该公司在电商方面有巨大的机会。《巴伦周刊》：机会是聊天机器人私人导购？乔纳森：去年圣诞节，我向ChatGPT解释我儿子是谁，他喜欢什么，并请ChatGPT推荐一些书。考虑到做这样的事情并添加一个购买链接并不是一件难事。如果ChatGPT不添加这种功能，我会感到震惊。亚马逊在这里有很多玩法，现在它的业务构成中有一个广告引擎，利润很高，生成式人工智能将允许公司以个性化的方式与客户交谈。《巴伦周刊》：如果人工智能对英伟达来说是一项利润丰厚的业务，你还会买入其他芯片股吗？乔纳森：我们持有AMD，有几个原因。其一，它在世界上最好的工艺制程节点上采用英特尔架构，并与台积电合作生产，这使它成为市场份额的获得者。第二，我们正处于另一个PC周期的开始，因为疫情爆发已经过去了四年，也许我们有理由购买人工智能PC。第三，AMD有一个关于GPU的故事，OpenAI首席执行官萨姆·奥特曼(Sam Altman)正试图在各地制造芯片，芯片公司并不想试图摆脱英伟达，但它们希望有更多的选择，AMD可以满足它们。《巴伦周刊》：你们在与芯片相关的公司中占有重要的仓位——半导体设备领域的ASML，芯片设计软件领域的新思科技（Synopsys）。为什么？乔纳森：阿斯麦与地球上任何一家半导体供应商都不同，它破解了EUV。《巴伦周刊》：你指的是极紫外光刻技术，一种用于制造最先进半导体的技术。乔纳森：它是唯一的玩家，没有竞争对手，有些人正在探索替代方案，但到今天为止，仍然是ASML和EUV。我们拥有新思科技很多年了，我们的判断是正确的：随着摩尔定律的放缓，对设计工具的需求将会加速增长。我们并没有理解它，但却从围绕AI的所有这些专业芯片的需求中受益，随着世界变得越来越复杂，我们将需要更多仿真设计工具。《巴伦周刊》：你之前提到你持有“科技七巨头”中的六个，你避开了哪一个？乔纳森：我们不持有Meta，尽管该公司股价大涨，但它在其平台上的内容类型方面正面临挑战。Meta有个安全问题要处理，它之所以在人工智能和大语言模型上投入巨资，是因为它认为这项技术是解决问题的途径，但它并不是一只昂贵的股票，如果它能解决问题并安抚监管机构，那么该业务的风险状况可能会有所不同，我们保持开放的心态。《巴伦周刊》：你对苹果有什么看法?苹果在去年年底是你的前五大持仓股。乔纳森：我看到了两个风险和一个巨大的机遇。一个风险是中国疲弱的消费，另一个风险是司法部对谷歌的诉讼。《巴伦周刊》：政府不喜欢谷歌付钱让其成为iPhone的默认搜索引擎——他们认为这是反竞争法则的，但在某些方面，谷歌的失败可能是其在财务上的胜利，而对苹果来说则是一种损失。乔纳森：谷歌愿意停止每年向苹果支付大约150亿美元，以成为iPhone首选的搜索引擎。如果这种情况不复存在，苹果将面临盈利风险。谷歌的机会在于大语言模型，以及最终修复Siri。《巴伦周刊》：与ChatGPT及其他人工智能软件相比，Siri作为聊天机器人的能力明显不足，但你认为苹果有可能在人工智能方面迎头赶上吗？乔纳森：它不仅可以推动一个强大的升级周期，而且还可能给苹果提供一个开始收费的途径。《巴伦周刊》：你说过要扩大投资范围，有没有一些符合要求的小盘股或中盘股的例子？乔纳森：购买Copilot许可证后的下一步是整理数据，这就引出了Snowflake，同时我们在私人方面还拥有Databricks。另一个例子是提高开发人员的生产力，软件开发人员已经完全接受了这些东西。除了拥有微软之外，我们还拥有为程序员提供工具的GitLab，它有很大的机会提高价格，同时提高生产率。
686292332,9d453e039b89e391586009baadb401db,知天命,https://api.zhihu.com/people/9d453e039b89e391586009baadb401db,people,自在,从几个标志性事件看人工智能的超预期发展,article,https://api.zhihu.com/articles/686292332,则可以根据简短的指令生成动画、写实影片等； 也是在最近，生成式<em>人工智能</em>（Generative AI）在“阅读”大量的大脑核磁共振图后建立了演算法，成功解读一张核磁共振图受试者当时正在聆听的广播内容,0,0,0,1710117943,1710117943,1956年，达特茅斯会议确立人工智能及其发展目标，开端；1972年，杰弗里·辛顿（Geoffrey Hinton）用神经网络的理论，建构数学模型分析数据，打下电脑深度学习的根基；21世纪初，已经有瘫痪病人在植入微电子装置后，能够透过意识移动滑鼠和控制机械义肢；2010年代，科学家成功在实验老鼠的大脑里建立虚假的记忆，神经科学最近几年更在AI的加持下光速般跃进；2022年底，ChatGpt横空出世，能够以仿拟人类思考、进行情绪分析的模式生成电邮、公文，以及一些犹如内容农场的新闻；2024年初，文字转图片的技术紧接而来，OpenAI日前公布的Sora软件，则可以根据简短的指令生成动画、写实影片等；也是在最近，生成式人工智能（Generative AI）在“阅读”大量的大脑核磁共振图后建立了演算法，成功解读一张核磁共振图受试者当时正在聆听的广播内容，在这个基础上，人工智能更进一步从一张核磁共振图，解读受试者脑海中正在想的故事；这是跨出神经科学研究的一大步，可望协助瘫痪的人对谈沟通。该研究由美国得克萨斯州奥斯汀大学教授胡思（Alexander Huth）主持；2024年3月两会期间，中国武汉高德红外股份有限公司董事长黄立宣称他带领的团队成功研发了65000通道双向的脑机接口芯片。“脑机接口”是一种变革性的人机交互技术，工作原理是采集脑部神经信号并分析转换成特定的指令，这能让人在不依赖外周神经和肌肉的情况下，实现大脑与外部设备之间的直接信息交换。这项技术能让目前很多实现不了的医药应用成为可能，比如让假肢有真实的感觉，可以用人脑、意念控制假肢；这能让很多神经系统疾病的治疗成为可能，比如癫痫、老年痴呆、抑郁、帕金森等。
687732234,e4198eb85c61ce9f34c94e6eaa8cdc7b,汤姆丁,https://api.zhihu.com/people/e4198eb85c61ce9f34c94e6eaa8cdc7b,people,保安，兼职程序员,汤姆丁——人工智能学习目录,article,https://api.zhihu.com/articles/687732234,阿里云天池-通识课程-学习笔记 阿里云天池-AI大模型应用搭建自学笔记 阿里云天池-AI大模型应用搭建自学笔记-搭建一个自己的GenAI应用 六、书单 附录：<em>人工智能</em>训练与计算机硬件参数,0,2,1,1710769988,1712044957,"本文为本人的人工智能方向总索引！写作目的：记录自己在人工智能学习领域的学习过程。随着近几个月学习的不断地深入，所学的知识就像不断开枝散叶。如果不进行梳理，在后续的学习中：一是容易忘记进度，知识没有形成一个网；二是在需要查询做过的资料时，留存的笔记东西非常乱，仅凭记忆难以记起，翻看曾经的文档又难以找到效率极低。相当于给自己写一个手书，记录自己的学习经历，也方便随时查询。随时更新一、环境、语言相关1.python环境及相关配置2.python语法相关细节张量（tensor）的使用windows和Linux中，python路径的异同3.python x 深度学习 常见依赖及框架pytorchpytorch和PILopencvnumpy4.shell及config.py的使用5.git的使用6.jupyter notebook的使用（pycharm就不写了）7.Json的使用二、人工智能相关数学基础1.神经网络概述终结者来了？神经网络是不是万能的？它能干什么不能干什么？它到底是在干什么？为什么他能这么干？其实他就是个披着人工智能外衣的统计学游戏。2.激活函数及相关概念不要激活函数行不行？当然不行了！不要激活函数，再复杂的神经网络，也等效于一个最原始的的感知机3.衡量你的神经网络和目标的差距：损失函数的数学基础数学向较为抽象。KL距离（KL散度）等数学还需恶补。4.梯度下降算法：让你的神经网络去拟合“目标概率分布”即“目标函数”优化器（迭代器，adam等）5.常见损失函数（误差函数）均方根误差MSEPixel Loss感知损失 Perceputal LossWasserstein GAN损失及梯度惩罚detach()函数及使用场景（一般用于多个相对独立nn组合训练的情况使用，比方说GAN）四、实际应用 1.imagenet竞赛（分类，最原始） 2.GAN及其变种（生成）patchGAN,cycleGAN,WassersteinGAN 3.围棋（决策：蒙特卡洛决策树+神经网络辅助决策） 4.LLM(大语言模型)LLM模型量化五、相关竞赛 1.kaggle 2.阿里天池阿里云天池-通识课程-学习笔记阿里云天池-AI大模型应用搭建自学笔记阿里云天池-AI大模型应用搭建自学笔记-搭建一个自己的GenAI应用六、书单附录：人工智能训练与计算机硬件参数"
684243804,3329230867336cae402a5745bd9d712c,邻翁,https://api.zhihu.com/people/3329230867336cae402a5745bd9d712c,people,夜晚的灯火，清晨的咖啡☕️,人工智能发展史-引言,article,https://api.zhihu.com/articles/684243804,<em>人工智能</em>发展史大纲 引言： 人工智能的定义与核心要素 <em>人工智能</em>的历史意义与当代价值 人工智能的定义与核心要素 <em>人工智能</em>（Artificial Intelligence,2,0,1,1709035161,1709131033,人工智能发展史大纲引言：人工智能的定义与核心要素人工智能的历史意义与当代价值人工智能的定义与核心要素人工智能（Artificial Intelligence，简称AI）是一门新兴的技术科学，其目的是研究和开发能够模拟、延伸和扩展人的智能的理论、方法、技术及应用系统。人工智能是计算机科学的一个分支，其研究领域包括机器学习、计算机视觉、自然语言处理和专家系统等。人工智能旨在让机器能够胜任一些通常需要人类智能才能完成的复杂工作。人工智能的核心要素主要包括算法、算力和数据。算法：算法是人工智能的“灵魂”，它是一组指导计算机进行数据处理的步骤和规则。算法负责对数据进行分析、学习、优化和执行特定任务，从而实现智能化。不同的算法适用于不同的场景和问题，例如，神经网络算法可以模拟人脑神经元结构，实现对大量数据的分类和识别任务。数据：数据是人工智能的另一个核心要素。人工智能通过对数据进行处理和分析来提取有用的信息和知识，进而做出决策和执行任务。在人工智能领域，数据可以分为训练数据和测试数据两种类型。训练数据用于训练算法模型，使其具备某种特定的能力；而测试数据则用于评估算法模型的性能和泛化能力。计算力：计算力是人工智能的另一个核心要素。强大的计算能力可以支持算法的高效运行，实现对大量数据的快速处理和分析。随着人工智能技术的不断发展，对计算力的需求也越来越高。随着技术的不断发展，人工智能已经在各个领域取得了广泛的应用，如自动驾驶、智能家居、医疗诊断、金融风控等。人工智能的发展不仅提高了生产效率和便利性，还为人类带来了更多的创新和发展机会。同时，人工智能也面临着一些挑战和问题，如数据隐私、算法偏见、伦理道德等，这些问题需要我们在推动人工智能发展的同时，加强对其的监管和管理。人工智能的历史意义与当代价值人工智能的历史意义不仅在于它作为一门新兴技术科学的出现，更在于它如何改变了我们对智能、机器和人类的认知。从1956年人工智能概念的提出至今，它经历了从初步探索到快速发展的过程，这一过程中不断有新技术、新方法的涌现，推动了人工智能领域的持续进步。人工智能的历史意义体现在以下几个方面：挑战传统认知：人工智能的发展挑战了我们对智能和机器的传统认知。传统的观点认为，智能是人类独有的特质，而机器只是执行预定义任务的工具。然而，人工智能的出现打破了这一界限，使机器具备了学习、推理和决策的能力，从而拓展了我们对智能的理解。推动科技进步：人工智能作为计算机科学的一个分支，其发展推动了科技进步的步伐。人工智能的出现促进了机器学习、计算机视觉、自然语言处理等多个领域的快速发展，为其他科技领域提供了强有力的支持。改变生活方式：人工智能的广泛应用改变了我们的生活方式。从智能家居到自动驾驶汽车，从医疗诊断到金融风控，人工智能正逐渐渗透到我们生活的各个方面，使我们的生活变得更加便捷、高效和安全。当代价值方面，人工智能具有以下意义：提高生产力和效率：人工智能可以自动化许多繁琐的任务，使人们能够更快、更准确地完成工作。这不仅可以提高生产力和效率，还可以降低生产成本，为企业创造更大的价值。改善生活质量：人工智能技术可以用于开发更好的产品和服务，如智能机器人、智能家居等，这些产品可以改善人们的生活质量，提高社会福利。促进科学研究和技术创新：人工智能是科学研究和技术创新的重要工具，它可以帮助人们更好地理解自然界和人类行为，并开发出新的技术。创造新的就业机会：随着人工智能技术的发展，将会出现许多新的职业和岗位。这可以为人们提供更多的就业机会，促进经济发展。
685127240,0793b85d224504ba028b2214d05edffb,北京疆亘资本,https://api.zhihu.com/people/0793b85d224504ba028b2214d05edffb,people,,疆亘观察｜人工智能——全球科技的竞技场,article,https://api.zhihu.com/articles/685127240,分别是弱<em>人工智能</em>（Weak AI）、强人工智能（Strong AI）和超<em>人工智能</em>阶段（Super AI）。弱人工智能阶段也被称为狭义<em>人工智能</em>或者应用人工智能，只能解决某个特定领域的问题,0,0,1,1709520538,1709520538,全文共计：3370字预计阅读时间：9分钟疆亘观察算力的迭代和技术的进步，人工智能爆发式增长指日可待，也为投资带来了空前的机会。摘要 人工智能可分为弱人工智能（Weak AI）、强人工智能（Strong AI）和超人工智能阶段（Super AI）三个阶段。尽管人工智能当下仍处于弱人工智能阶段，但随着Chatgpt和Sora的推出，人工智能应用有望加速落地，全产业中，底层的算力和算法，中层的基础大模型和场景模型，以及应用端的各个场景和细分行业，都将带来深刻的变革。风险AI发展不及预期；AI造成伦理问题。 正文 01人工智能领域介绍人工智能发展历史算法的迭代推动人工智的发展：几十年来，AI领域持续变革，1940-1980s符号AI占主导，1980年后，统计AI与神经AI齐头并进，二者竞争发展至今。符号（Symbolic)AI:二十世纪80年代以前属于符号AI时代，开启了以知识建模算法和专家系统算法为代表AI早期阶段；统计(Statistical)Al:1985年的贝叶斯网络和1995年的SVM是20世纪机器学习发展的两大标志，但近年来，逐渐成为非主流的研究方向；神经(Neural)AI:经历了1980s-2012年神经网络、2012年后的深度学习两大阶段，深度神经网和模型大小正相关，伴随着模型参数量的增加，相对应深度神经网络的能力将大幅提升。Google Brain的Transformer和Open IA的GPT系列都属于深度学习领域的杰出代表作。人工智能发展历史主要可以分为三个阶段，分别是弱人工智能（Weak AI）、强人工智能（Strong AI）和超人工智能阶段（Super AI）。弱人工智能阶段也被称为狭义人工智能或者应用人工智能，只能解决某个特定领域的问题，执行有限的预设功能。如智能音箱、引导机器人。强人工智能阶段指的是可以像人一样胜任任何智力性任务的智能机器，需同时具备思考能力、计划能力和学习能力，并能够利用自身能力达成目的，当前仍然处于理论探索阶段。理论上，人工智能一直不断发展下去，一定可以全面超越人类，由此产生的人工智能系统称为超人工智能。超人工智能的实现路径比强人工智能会更漫长。目前人工智能处于弱人工智能向强人工智能发展阶段。2020年以前，人工智能处于弱人工智能发展阶段，只能解决某个特定领域问题，执行有限的预设功能。2020-2030年，人工智能有望进入强人工智能阶段。展望未来，人工智能极有可能达到超人工智能阶段。发展人工智能，对社会发展具备重要价值，人工智能将形成新的生产力，引领第四次工业革命并具有巨大的创新溢出效应。人工智能产业链根据中国人工智能系列白皮书释义，人工智能产业链可分为基础层，技术层，应用层三个层次，分别对应产业链的上游、中游和下游。其中，基础技术支撑由数据中心和运算平台构成，涉及数据传输、运算和存储等：AI技术是基于基础层提供的存储资源和大数据，通过机器人学习建立大模型，开发面向不同领域的应用技术，包含感知智能和认知智能两个阶段，感知智能如语音识别，图像识别、自然语言处理和生物识别等：认知智能如机器学习，预测类API和人工智能平台，AI应用(AI+或智能化)即用AI技术为千行百业赋能，实现不同场景的应用。 02算力的需求呈现指数级的增长根据《2023智能算力发展白皮书》显示，全球智能算力的总体情况呈现快速增长趋势。截至 2022 年底，全球算力总规模达到 650 EFLOPS，其中，通用算力规模为 498 EFLOPS，智能算力规模为 142 EFLOPS，超算算力规模为 10 EFLOPS。智能算力规模同比增加了 25.7％，占比达 21.9％。AI大模型预训练数据量呈现指数级增长，对算力的需求也同样呈现指数级的增长。GPT 模型最早的版本可追述到 2018 年，OpenAI 发布的 GPT-1 模型参数量为 1.17 亿，2019 年 2 月发布的GPT-2 参数量为 15 亿，而 2020 年 5 月的 GPT-3，参数量达到了 1750 亿，对应的预训练数据量也呈现指数级增长，从 5G 增长至 45TB，GPT-3 所需要的算力达到 3640PFlop/s-day。爆发式增长的算力需求，为高端算力芯片带来空前的机会。值得注意的是，高端算力芯片进口受阻，国产算力芯片有望迎来发展机遇。GPU为目前应用最为广泛的智能算力芯片。智能算力主要算力来源为智能算力芯片，主要类型包括GPU/FPGA/AI 芯片等，其中 GPU 是应用最多的智能算力芯片，据华经产业研究院数据，2022 年底全球 GPU市场规模约为 448 亿美元、中国 GPU 市场规模约为 83.6 亿美元，其中在 2021 年中国 AI 芯片市场中，GPU 占比达 91.9%，预计到 2025 年 GPU 仍将占据 AI 市场的 80%以上份额。海外厂商仍占据 GPU 芯片主要市场。据华经产业研究院数据，2022 年全球 GPU 市场约 448 亿美元，主要由英伟达、AMD 和英特尔占据，其中英伟达作为 GPU 领域的绝对龙头，全球市占率约 88%，国产替代仍有较长的路要走。 03大模型的发展大模型发展趋势从单模态到多模态和跨模态发展，大模型可将多模态数据融合处理更多融合人类脑神经机制，提升性能，实现智能决策、跨模态任务、运动控制、智能预测反馈。除去压缩率的提升外，扩展模型的适用场景也成为了当下的重要共识，重点在图片的多模态模型料将成为 2024 年重点升级领域。当前大模型从文字向多模态的升级，是拓展使用场景与商业化前景的重要发展方向。从技术角度出发，当前的多模态模型在文字部分基本沿用了目前语言模型的预训练架构，而在图片部分会使用类似于 ViT 的 Patch Embedding 等方法将图片分割为小像素块再交给模型进行预训练。接下来，多模态模型会使用不同的方法将文字和图片的输入进行对齐。如 T5、CliP 的部分变体等模型会采用交叉注意力的技术使模型在理解一种模态内容时考虑到另一种模态的上下文，帮助模型在处理一个模态的同时，捕捉到与另一模态的直接关系，这也让多模态模型从真正意义上做到了同时理解文字与图片。目前包括谷歌 Gemini 模型在内，使用了多模态的数据，仍主要沿用 Transformer Decoder 的架构，采用 32K 的 Context Length 的思路进行训练。同时为了推理效率，Attention 部分选择了 Multi Query Attention。我们判断以 Transformer 为底层的多模态模型料将会是 2024 年的重点方向，下一代多模态将会快速拓展目前大语言模型有限的应用场景，显著提高语言模型的泛化能力，并有望在成本方面展现出对传统专有模型的优势，从而推动如 AI Agent 等应用的落地。预计2028年全球大模型市场规模将超过1000亿美元。根据大模型之家、钛媒体数据，预计2023年全球大模型市场规模达到210亿美元，同比增长94.4%。预计到2028年全球大模型市场规模将达到1095亿美元，2022~2028年复合增长率约为47.12%，根据IDC预测，全球生成式AI计算市场规模将从2022年的8.2亿美元增长至2026年的109.9亿美元，CAGR约为91.34%。预计至2028年我国大模型市场规模接近1200亿人民币。预计2023年中国大模型市场规模达到147亿人民币，同比增长110.0%。预计到2028年中国大模型市场规模将达到1179亿人民币，2022~2028年复合增长率约为60.11%，市场规模快速成长。多模态大模型带来AI全新应用场景，多模态内容市场规模有望快速成长。第一财经数据显示，预计至2025年，中国多模态内容市场规模将达到832.7亿美元，2018-2025年复合增长率达65.02%。多模态大模型内容将主要应用于商业定制、医疗、游戏、教育和影视领域。 04人工智能应用场景随着百度、阿里、华为等我国科技巨头的入局以及 AI 技术的迭代发展， AIGC（生成式 AI）正在变革内容产业，ChatGPT 正与搜索、教育、金融等行业融合，加速 AI 商业落地。在 AIGC 场景下，AI 可以灵活运用于写作、编曲、 绘画和视频制作等创意领域。据 TBanic Date 估计，到 2025 年人工智能生成数据占比将达到10%。目前 AIGC 技术可以自动生成文字、图片、音频、视频，甚至 3D 模型和代码，在搜索引擎、艺术创作、影音游戏，以及金融、教育、医疗、 工业等领域的应用前景十分广阔。AIGC 能够以类似人类甚至优于人类的知识水平、制造能力承担信息挖掘、 素材调用、复刻编辑等基础性机械劳动，从技术层面实现以低边际成本、高效率的方式满足海量个性化需求。长期来看，AIGC 在提高各行业内容创作效率的同时，更有可能孕育出更多的新业态、新模式。 05总结人工智能近年来取得了空前的突破和发展。尽管人工智能当下仍处于弱人工智能阶段，但随着Chatgpt和Sora的推出，人工智能应用有望加速落地，全产业中，底层的算力和算法，中层的基础大模型和场景模型，以及应用端的各个场景和细分行业，都将带来深刻的变革。
685596478,7fbb3f202ba225115b980f994b35a6ad,实在<em>智能</em>RPA,https://api.zhihu.com/people/7fbb3f202ba225115b980f994b35a6ad,people,你说，PC做，一句话帮您完成复杂工作！,发展人工智能+，释放了哪些关键信号？,article,https://api.zhihu.com/articles/685596478,国务院总理李强在政府工作报告中介绍今年政府工作任务时提出，深化大数据、<em>人工智能</em>等研发应用，开展“人工智能+”行动，打造具有国际竞争力的数字产业集群。结合当下全球发展格局，<em>人工智能</em>已然成为新一轮科技革命和产业变革的重要推动力,0,0,1,1709711897,1709711897,"3月5日，国务院总理李强在政府工作报告中介绍今年政府工作任务时提出，深化大数据、人工智能等研发应用，开展“人工智能+”行动，打造具有国际竞争力的数字产业集群。结合当下全球发展格局，人工智能已然成为新一轮科技革命和产业变革的重要推动力，我国政府在2024年的工作报告中强调了深化大数据、人工智能等新一代信息技术的研发应用，推动人工智能带动产业发展，以科技创新作为新质生产力的发展引擎。政府作为“复兴号”的掌舵者，研判局势稳固发展。这一重大举措不仅彰显了国家对于科技发展格局的准确把握，也释放出了一系列关键信号，预示着中国将在人工智能领域开启新的篇章。发展人工智能+，释放了哪些关键信号？政策导向与支持力度升级： “人工智能+”行动的提出，标志着政府将进一步加大政策引导和支持力度，通过顶层设计，构建有利于AI技术研发、转化和产业化的生态系统，深入推进数字经济创新发展，制定支持数字经济高质量发展政策，积极推进数字产业化、产业数字化，促进数字技术和实体经济深度融合。深化大数据、人工智能等研发应用，开展“人工智能+”行动，推动形成具有国际竞争力的数字产业集群。创新驱动产业升级转型： 该行动计划意味着我国将全面加快人工智能技术与各行业领域的深度融合，也就是说，我国从""互联网+”转变升级为“人工智能+”这一具有前瞻性、创新性的发展设计，是以人工智能赋能新型产业化，推动传统行业转型升级，催生新业态新模式，实现经济增长的新动能转换。无论是制造业、工业、电商行业等等，都紧紧抓住技术发展的东风，借助人工智能技术实现产业的数字化升级，企业发展降本增效，国有企业华润集团下属的华润燃气同人工智能企业实在智能开展合作，打造新质生产力（Agent数字员工）为华润燃气逐步深入数字化转型，建设智慧燃气，探索技术与产业深度融合驱动新业态模式发展的突破路径。培育本土领军企业与核心技术突破： “人工智能+”已成为两会代表委员热议的关键词，多位代表委员均对此建言献策，人工智能（AI）已成为驱动新质生产力的重要引擎和来源。比如阿里巴巴集团也在聚焦人工智能发展，前段时间阿里推出的EMO框架，展现出更为出色的表现力和处理能力，这表明其一方面为产业数字化、数字经济发展筑牢底座，一方面推动产业智能化升级；再如360集团自研千亿通用大模型“360智脑”已支撑金融、税务等10余个场景的垂直模型训练部署并完成交付。这些国内科技公司的技术发展与落实，都表明国内企业抢占全球AI技术高地，实现核心关键技术的自主可控，提升整体创新能力。国际合作与竞争态势： “中国智，世界能”的理念在过去的世界人工智能大会上被多次强调，揭示了中国致力于在全球范围内共享AI发展的机遇，同时也准备在全球人工智能竞赛中占据重要地位。当前，全球科技竞争态势发展紧迫，国外接连推出的人工智能产品，在国际舞台上已经引起轩然大波，这对于我国科技发展形成不小的竞争压力，所以推动人工智能发展，不仅是国内迫切的需求，也是抢占全球话语权先机的把握。综上所述，开展“人工智能+”行动是我国面向未来，主动引领新一轮科技革命和产业变革的重要战略部署，它向全社会乃至全世界传达了我国力促人工智能技术创新及产业化应用的决心和信心，对于我国经济社会长远发展具有深远意义。"
683673242,dc393f809b1a60b6d8ddf9fb0ace5276,不睁眼就是瞎子,https://api.zhihu.com/people/dc393f809b1a60b6d8ddf9fb0ace5276,people,随便聊聊，互相交流嘛，别激动。,未来的人工智能会是人类的敌人吗？,article,https://api.zhihu.com/articles/683673242,使世界即将从信息时代过渡到<em>人工智能</em>时代，标志着一场技术革命。但是，它也引发了人们的担忧，即人类的角色和地位是否会随着<em>人工智能</em>的发展而减少，最终人类会被人工智能取代，导致普遍失业吗,0,0,0,1708709949,1713712406,人工智能的出现给人们带来了便利，使世界即将从信息时代过渡到人工智能时代，标志着一场技术革命。但是，它也引发了人们的担忧，即人类的角色和地位是否会随着人工智能的发展而减少，最终人类会被人工智能取代，导致普遍失业吗？人工智能使聪明的人更聪明，使懒惰的人更懒惰。它的存在确实有助于指导世界的发展，使科学家更容易进行研究，并使程序员能够更有效地编写所需的代码，人工智能还能对医疗和其他各个领域产生影响。这些都是人工智能带来的便利。另一方面，人工智能的出现会让那些不喜欢动脑的人变得更加麻木和懒惰。这些人可能变得无知，不再批判性地思考，完全依赖人工智能。对他们来说，人工智能的出现可能不会带来好处，反而会降低他们自己。将现代人的身体素质与二百年前的人相比较，可以毫无疑问的说，现代人的身体素质无法与两百年前的人相比。人类身体素质的下降可归因于工业革命的影响，工业革命用机器取代了人类劳动，导致人类身体活动减少，身体素质自然而然就降低了。然而，我们不能否认工业革命给世界带来的变化。现代运动有马拉松，它代表着人类身体素质的极限，还有那些坚持每天健身或锻炼身体的人，我想他们的身体素质不比二百年前的人差吧！其实对于人工智能来说，它能带我们的好处远大于我们的担忧，只要人类能充分和适当的使用它，我相信，在人工智能影响下，这个世界会再次发生质的改变。
696145987,678d9da4b5a803bf7e4a2ddccdaaceda,paidaxing,https://api.zhihu.com/people/678d9da4b5a803bf7e4a2ddccdaaceda,people,,比特币逐渐稳定，人工智能协议渲染 (RNDR) 飙升 15%,article,https://api.zhihu.com/articles/696145987,硬件和主导软件使其成为<em>人工智能</em>竞赛中的领导者。除了 NVIDIA 之外，苹果也有望通过<em>人工智能</em>来改善其市场表现和产品产量。Jim Cramer乐观地认为，该公司将<em>人工智能</em>集成到,2,0,0,1714975310,1714975310,"在人工智能市场不断繁荣的背景下，Render (RNDR) 今天的市场上涨了 15%，其涨幅超过了比特币随着比特币 (BTC) 和 Render (RNDR) 等山寨币跌至每周低点，数字货币生态系统正在经历温和的投降。由于总体增长不平衡，AI 代币 Render 成为众人瞩目的焦点，其价格飙升 15.25% 至 8.745 美元。RNDO 与 BTC 脱钩山寨币与比特币脱钩并不罕见，尤其是在经历了一周的长期回撤之后。截至撰写本文时，Render 价格飙升 15%，而比特币价格则下跌 0.23%。这种主要的加密货币现在以 63,755.42 美元的价格易手，因为它希望维持在 63,000 美元的支撑区域。渲染通常是市场异常，其增长速度明显超过生态系统中的其他人工智能代币。随着24小时的飙升，该代币在过去一周的涨幅已扩大至20%。尽管目前波动性仍然很高，但 Render 年初至今仍保持看涨前景。CoinMarketCap 的数据显示，今年迄今 (YTD) 的增长率为 95.58%，从 4.4 美元飙升至今天的 9.34 美元高点。渲染的吸引力取决于它所代表的行业。作为一种人工智能协议，Render 成功地应对了行业不断增长的估值飙升。Render 被设计为GPU 渲染网络，用于将具有 GPU 功能的矿工与需要它的艺术家和工作室连接起来。借助渲染模型，人工智能生态系统的创新变得容易，而且成本普遍较低。随着价格飙升，该代币现在将其重要的心理阻力区瞄准了 10 美元。炒作的人工智能市场除了渲染之外，许多加密协议和华尔街公司都在利用人工智能浪潮。例如，NVIDIA 是此次上涨的最大受益者之一，今年其估值突破 2 万亿美元。许多分析师看好该公司，因为其 GPU 硬件和主导软件使其成为人工智能竞赛中的领导者。除了 NVIDIA 之外，苹果也有望通过人工智能来改善其市场表现和产品产量。Jim Cramer乐观地认为，该公司将人工智能集成到 iPhone 中可以以独特的方式改变其地位。总而言之，由 Fetch.ai (FET)、SingularityNET (AGIX) 和 Ocean Protocol (OCEAN) 组成的人工超级智能联盟 (ASI) 也证明了许多人正在想象的人工智能的潜力。到目前为止，ASI 联盟已经获得了该协议各自社区的必要支持。"
695197211,91e1628d8e03b52444e89e0f3172d59b,MING Labs,https://api.zhihu.com/people/91e1628d8e03b52444e89e0f3172d59b,people,,人工智能：电影预言与现实 （下）,article,https://api.zhihu.com/articles/695197211,时间进入21世纪，随着智能家居、智能手机、智能穿戴设备的普及和智能硬件的快速发展，<em>人工智能</em>开始走进日常生活。这时期的影片，也开始从贴近现实的情感角度来挖掘故事。,2,0,0,1714962236,1714962236,时间进入21世纪，随着智能家居、智能手机、智能穿戴设备的普及和智能硬件的快速发展，人工智能开始走进日常生活。这时期的影片，也开始从贴近现实的情感角度来挖掘故事。这篇文章我们就来继续重温经典。《人工智能》是一部2001年由史蒂文·斯皮尔伯格执导的科幻电影。故事设定在21世纪中期，人类与高度智能的机器人共存。机器人制造技术已经高度发达，机器人拥有可以乱真的人类外表，可以取代各种人工。寂寞的父母，可以预约AI小孩；养小孩辛苦的父母，可以预约 AI 保姆；寂寞的城市男女，可以预约 AI 舞伴；百无聊赖的儿童，也可以有机器熊相伴。故事的主角是世界上第一个学会“爱”的机器男孩大卫。它被人类母亲莫妮卡领养，以缓解因儿子马丁重病住院而伤痛的心情。大卫的使命就是爱她。然而很快马丁恢复健康并回到了家里，大卫最终被莫妮卡抛弃。然而它深信只要自己能够变成真正的小孩，就能够回到莫妮卡身边，便与机器人乔一道踏上漫长旅途，渴望能遇上《木偶奇遇记》幻变生命的仙女把自己变成真正的小孩。影片让人反思，如果机器人真的有爱的能力，那被爱的我们又有什么责任呢？图源：网络《她》是一部由斯派克·琼斯编剧和执导的科幻爱情片，于2013年上映。值得一提的是，电影在上海取景，魔都的雾色朦胧也为影片的孤独感营造了清冷的情绪氛围。故事发生在未来的洛杉矶，主人公是一名叫西奥多的作家，在一家公司从事写情书的工作。西奥多是一个细腻敏感的人，他在工作中是一名出色的诗人，在生活中却羞于表达。离婚的打击让西奥多的生活陷入沉寂，他购买了一个最新的操作系统“萨曼莎”。萨曼莎拥有人工智能，并且具备情感交流的能力。随着时间的推移，西奥多与萨曼莎的关系逐渐发展成一段感情。萨曼莎通过自我学习和对西奥多的理解，成为了他的知己和灵魂伴侣。西奥多虽然一开始对人机恋情感到羞愧，渐渐却也心安理得。萨曼莎成为了他孤独生活的治愈。相较于西奥多的沉溺，萨曼莎则在与人类的情感互动中以极快的速度进化着，并最终超越了西奥多的生活圈，走向了更广阔的人工智能世界。西奥多最终被萨曼莎抛弃。科技既没有创造孤独，也不是孤独的解药。影片通过人机恋情的设定，把肉体和外界的影响都剥离，深入探讨了人类情感归属的问题。图源：网络《机械姬》是一部2014年的英国科幻电影，由亚历克斯·加兰执导。该片设定在一个未来世界，讲述一家类似Google 与Facebook合体的大型科技公司Bluebook里，年轻工程师迦勒获得难得机会，得以搭直升机参观公司创办人内森的豪华大宅兼实验室。该处人迹罕至，内森在此秘密制造出新型人工智能艾娃，想让迦勒做图灵测试（Turing Test），判断艾娃是否已有与人类相仿的心智。测试过程中，艾娃表现出了很多超乎常规智能机器人的能力，迦勒几乎完全相信艾娃有自我意识。更可怕的是，艾娃进入了迦勒的内心世界，它声称爱上了迦勒，并希望他带自己离开囚禁。最后迦勒上了当，帮助艾娃逃离了实验室，却被艾娃留在了实验室内，永远无法逃出。这又是一个人类被人工智能抛弃的结局。同《人工智能》一样，影片让人思考这个问题：当机器人具备了和人类一样的外貌并展现出自我意识和情感，却依然像对待机器一样对待它，是否越过了某条道德的界线？不同于《人工智能》的是，影片同样让人深思，机器人所展示出的情感，是否和人类的相同？图源：网络这里仅仅回顾以上几部经典电影。人工智能的发展如火如荼，迅猛而又不可逆转地改变着我们的世界。我们可以通过电影表达期待和担忧，却无法真正预言人工智能的走向，谨慎的拥抱它似乎是唯一的选择。在应用人工智能的时候，我们应尽可能考虑它对人类伦理、安全将产生的影响，引导科技向有益的方向生长。在探索人工智能的商业应用方面，我们有幸与不同行业的头部企业进行过一些合作。如您有兴趣了解更多，请留言或直接联系我们，我们也期待与您共同探讨更多的可能性，共创人机共处的美好未来。国内AI企业还在卷大模型，但与我们大多数人更相关的是——如何应用GenAI来提升生产力或实现商业增长？为了回答这个问题，MING创建了GenAI Gazette，每天搜集全世界最新的GenAI商业应用案例，目前已经有超过300个来自不同行业和企业的用例 。此外，我们还创建了社群，每周分享最新GenAI应用案例和项目实操经验以及每月有国内外大咖分享商业创新经验。关注我们的公众号「MINGLabs」，领取网址或加入我们的社群，直通全球最新GenAI应用案例。
689739527,28242025415a01066a932861d151278b,锐观网,https://api.zhihu.com/people/28242025415a01066a932861d151278b,people,,中国人工智能行业“十四五”发展趋势与投资机会研究报告,article,https://api.zhihu.com/articles/689739527,<em>人工智能</em>（<em>AI</em>），是研究模拟人类智能、智能行为及其规律的一门学科，是用人工的方法来模仿人类所进行的智能活动。所谓“智能”，可以认为是人类脑力劳动所表现出来的能力,0,0,0,1711695945,1711695945,人工智能（AI），是研究模拟人类智能、智能行为及其规律的一门学科，是用人工的方法来模仿人类所进行的智能活动。所谓“智能”，可以认为是人类脑力劳动所表现出来的能力，例如感知、理解、抽象、分析、推理、判断、决策、学习和对变化环境的适应等。在市场规模方面，2022年国内人工智能核心产业规模为1565亿元，同比增长5.6%。在融资方面，根据IT桔子数据，2023年（截止到11月20日）融资交易额估算有631亿元，与上年同期相比下降38%。随着算法的创新、算力的增强、数据资源的累积，智能化基础设施的建设和传统基础设施将实现智能化升级，人工智能技术有望推动经济发展全要素的智能化革新。在中国，人工智能的发展受到政府高度重视。科技部2022年8月15日公布了《关于支持建设新一代人工智能示范应用场景的通知》，启动支持建设新一代人工智能示范应用场景工作。2022年12月9日，最高人民法院发布《关于规范和加强人工智能司法应用的意见》，将进一步推动人工智能同司法工作深度融合，全面深化智慧法院建设，创造更高水平的数字正义，推动智慧法治建设迈向更高层次。2023年7月13日，国家网信办联合国家发展改革委、教育部、科技部、工业和信息化部、公安部、国家广电总局公布《生成式人工智能服务管理暂行办法》，自2023年8月15日起施行。这也是中国首次对生成式AI研发及服务作出明确规定。发展人工智能新兴产业的基础设施，符合经济转型方向，有望成为“新基建”的主要抓手之一，加快建设发展，使行业彰显“逆周期性”的属性。产业研究院发布的《中国人工智能行业“十四五”发展趋势与投资机会研究报告》，依托庞大的调研体系，结合科学的研究方法和分析模型，通过对人工智能行业的市场规模，竞争格局、发展趋势及前景等方面进行细致分析，深入挖掘人工智能行业相对成熟的确定型投资机会、挑战机遇并存的风险型投资机会和仍在探索中的未来型投资机会，并对人工智能行业的投资风险做出预警。本报告将帮助对人工智能行业有投资意向的机构或个人，全面了解人工智能行业未来发展趋势，准确把握投资机会点。此报告将是您跟踪人工智能行业最新发展动态、挖掘投资机会、评估投资价值的重要参考工具。第一章 人工智能行业概念界定及产业链分析第一节、人工智能定义及发展阶段一、人工智能行业定义二、人工智能发展阶段第二节、人工智能研究历程及方法一、人工智能的研究历程二、人工智能的研究方法第三节、行业产业链及发展动因分析一、产业链结构二、发展动因分析第二章 人工智能行业发展状况分析第一节、国外人工智能行业发展分析一、国际发展状况二、全球竞争格局三、企业布局加快四、各国发展动态五、技术研发进展第二节、中国人工智能行业规模结构一、行业发展格局二、企业布局加快三、产业发展提速第三节、中国人工智能行业生态格局分析一、生态格局基本架构二、基础资源支持层三、技术实现路径层四、应用实现路径层五、未来生态格局展望第四节、中国人工智能行业区域发展动态一、哈尔滨市二、安徽省三、四川省四、上海市五、福建省第三章 中国人工智能行业市场趋势及前景预测第一节、行业发展趋势分析一、行业政策机遇二、行业发展趋势三、技术发展趋势第二节、行业前景预测分析一、应用前景分析二、行业发展前景三、投资前景广阔第三节、人工智能行业前景预测分析一、行业机遇及挑战二、市场规模预测第四章 人工智能行业确定型投资机会评估第一节、语音识别一、行业基本介绍二、市场发展状况三、竞争格局分析四、主要品牌分析五、投资风险分析六、投资策略建议七、发展方向分析第二节、计算机视觉一、技术环境分析二、市场发展状况三、企业发展动态四、投资风险分析五、投资策略建议六、投资机遇分析第三节、智能视频分析一、系统解决方案二、行业应用重点三、市场发展状况四、市场发展格局五、重点企业分析六、发展策略分析第五章 中国人工智能行业风险型投资机会评估第一节、智能机器人一、细分行业分析二、市场规模分析三、竞争格局分析四、龙头企业分析五、市场空间预测六、投资热点分析七、投资风险分析八、投资策略建议第二节、深度学习一、行业基本内涵二、行业发展状况三、龙头企业分析四、投资风险分析五、投资策略建议第六章 中国人工智能行业未来型投资机会评估第一节、虚拟个人助理一、基本内涵分析二、行业应用领域三、市场发展状况四、竞争状况分析五、投资策略建议六、行业发展前景第二节、无人驾驶汽车一、市场发展状况二、市场竞争格局三、龙头企业分析四、市场规模预测五、投资风险分析六、投资策略建议七、投资前景分析第七章 中国人工智能行业投资壁垒及风险预警第一节、人工智能行业投资壁垒一、资金壁垒二、技术壁垒三、认知壁垒第二节、人工智能行业投资风险预警一、技术风险二、竞争风险三、盈利风险四、人才风险第三节、人工智能行业项目运营风险预警一、法律风险二、商业风险三、财务风险四、融资风险五、安全风险
684118004,5ed0699e38bbf855eea25fff6a3f3759,清风徐来,https://api.zhihu.com/people/5ed0699e38bbf855eea25fff6a3f3759,people,,人工智能,article,https://api.zhihu.com/articles/684118004,最近，关于<em>人工智能</em>的发展也引发了热议。我认为，人工智能的潜力是巨大的，它可以为我们的生活带来很多便利。但同时，我们也应该关注其可能带来的风险和挑战，如隐私泄露、就业问题等,0,0,0,1709000738,1709000738,最近，关于人工智能的发展也引发了热议。我认为，人工智能的潜力是巨大的，它可以为我们的生活带来很多便利。但同时，我们也应该关注其可能带来的风险和挑战，如隐私泄露、就业问题等。希望相关部门能够加强监管，确保人工智能的健康发展。
688048254,1c881eefe925fcb7b8958e5fbc14ccde,小风说<em>AI</em>,https://api.zhihu.com/people/1c881eefe925fcb7b8958e5fbc14ccde,people,每天学习一点AI小知识。   zfkt001,干IT逃不过35岁危机？来试试人工智能，一文读懂什么是AI！,article,https://api.zhihu.com/articles/688048254,引领着科技发展的新浪潮。那么，<em>人工智能</em>究竟是什么呢？本文将带您一文读懂<em>人工智能</em>的定义、底层逻辑、应用以及未来发展趋势。 一、AI的定义 <em>人工智能</em>，简称AI，是一门新兴的技术科学,0,0,1,1710916219,1710916219,在科技日新月异的今天，人工智能（AI）这一词汇越来越频繁地出现在我们的视野中。它以其独特的魅力和广泛的应用领域，引领着科技发展的新浪潮。那么，人工智能究竟是什么呢？本文将带您一文读懂人工智能的定义、底层逻辑、应用以及未来发展趋势。一、AI的定义人工智能，简称AI，是一门新兴的技术科学，旨在开发和应用能够模拟、延伸和扩展人类智能的理论、方法和技术。它结合了数学、计算机科学、心理学等多学科的理论，通过让计算机模拟人类的思考和行为过程，实现人机交互，提高计算机的智能水平。人工智能的核心在于让机器具备感知、学习、推理、决策等能力，从而能够胜任一些复杂的工作。二、AI的底层逻辑人工智能的底层逻辑主要依赖于大数据、算法和计算能力。首先，大数据为AI提供了丰富的信息来源，使得机器能够从中学习并不断优化自身性能。其次，算法是AI的核心，它决定了机器如何处理和分析数据，从而得出正确的结论。最后，计算能力则决定了AI的运行速度和效率，随着计算机技术的不断发展，AI的计算能力也在不断提高。由此可见，对于AI岗位，这三方面的能力考察格外重要：1、数据结构和算法；2、基本模型的推导能力，3、项目能力和工程经验。如果你想转行到AI，或者想在风头正盛的AI行业里，吃上一口红利，建议先第二点下手，从机器学习开始，为之后打好基础！三、AI应用，具体在哪些行业？人工智能的应用领域非常广泛，几乎涵盖了所有行业。在医疗领域，AI可以帮助医生进行疾病诊断和治疗方案制定；在金融领域，AI可以用于风险评估、信用评分和投资建议；在交通领域，AI可以实现自动驾驶和智能交通管理；在教育领域，AI可以为学生提供个性化学习方案；在制造业领域，AI可以优化生产流程，提高生产效率；在服务业领域，AI可以为客户提供更优质的服务体验。四、未来发展趋势展望未来，人工智能将继续保持高速发展的态势。随着技术的不断进步，AI将在更多领域得到应用，为人类生活带来更多便利。同时，AI也将面临一些挑战，如数据安全、隐私保护、伦理道德等问题，需要我们在发展过程中予以关注和解决。未来，人工智能将成为推动社会进步的重要力量，引领我们走向更加美好的未来。总之，人工智能是一门充满潜力和挑战的技术科学。通过了解AI的定义、底层逻辑、优势、应用以及未来发展趋势，我们可以更好地把握这一技术的发展脉络，为未来的科技创新和社会发展贡献力量。而想要把握人工智能，先从机器学习开始！
689068222,0bed9f1a9ecd3c7695c15a4ec5d475ab,中广资讯,https://api.zhihu.com/people/0bed9f1a9ecd3c7695c15a4ec5d475ab,people,,重庆亿豪投资有限公司：人工智能+燃爆技术革命,article,https://api.zhihu.com/articles/689068222,恰恰就在于，<em>人工智能</em>将会重塑整个技术世界，一切都将以<em>人工智能</em>作为基础，重新构建。 而对于所有企业来说，核心就在于如何应用<em>人工智能</em>，构建出自己的人工智能应用，从而服务自己的企业和产业,1,0,1,1711423383,1711423383,人工智能到底只是一个概念，还是一场真正的技术革命？在今年的政府工作报告里，显然没有任何一点的含糊。“深化大数据、人工智能等研发应用，开展‘人工智能+’行动，打造具有国际竞争力的数字产业集群”，这是政府工作报告里的原话，它已经成为了一个国家战略。“今年的政府工作报告提到了3次‘人工智能’和26次‘安全’，大模型作为人工智能发展的核心引擎，正引发一场全新的工业革命。”360创始人周鸿祎在一次接受采访的时候这么说道。周鸿祎显然是对于政府工作报告中的这一战略深感兴奋的人。因为从ChatGPT出现以来，他已经成为觉醒最早，最热切，也是最不遗余力投入进去的互联网大厂创始人。早至去年7月，他在一次演讲中就说：“大模型直接提高每个人、每个组织的生产力，同时实现赋能。” 这样的赋能是具体而微的，“AI就像数据库一样，会变成每个数字化系统的标配，从手机部署、到汽车部署，也包括在企业和政府内部的部署。”过去谈的是“互联网+”，现在应该是AI+，他说。在100多场演讲和授课中，简直可以说他是毫不间断地强调这样的提法。01人工智能已经成为了整个世界的讨论中心，从2023年ChatGPT出现以来，而无论是大模型，还是其视频呈现Sora，从世界到中国，突破如同雨后春笋一般涌现。人们都不禁会深感纳闷，为什么在ChatGPT出现之后，各国大厂就像不费吹灰之力一样，都各自出现了大模型的产品？其实，对于人工智能的语料训练，所有的大厂在过去的多年之中，都在不断地进行之中。但是OpenAI以“大力出奇迹”的规模效应，为人们展开了一条发展人工智能的通用道路。今天，所有技术产业中的人都在讨论大模型，而在Sora出来之后，人们又纷纷讨论Sora如何地具有了理解世界运行的物理规律的问题。而所谓的“大力出奇迹”，核心就在于，动辄以百亿、千亿、万亿级的数据训练，是人工智能如今得以跨越式发展的基础。所以，英伟达在短短一年时间之内，市值上升了十几倍，黄仁勋也代替马斯克成为了世界首富。但是英伟达的好日子不过才刚刚开了个头而已，Meta、微软和Google，都像不要钱一样地订购GPU，动辄以几十万张为起点。而OpenAI的山姆·奥特曼，更加提出了“登月计划”，要募资7万亿美元来解决人工智能发展的芯片问题。这是诸多看热闹的门外汉们在媒体中的热点话题。但如果在产业界和国家层面，问题探讨停留在这个话题上，那么对于产业，对于企业，对于市场，几乎毫无意义：因为它只是科技豪门的一个金钱游戏——它与普通人无关，也与普通企业无关。而这，恰恰是周鸿祎，从ChatGPT出现伊始，就开始站在产业和企业角度上思考的问题。02在2023年7月的一次演讲中，周鸿祎的话题就是：“把大模型拉下神坛，变成每个企业都能应用的东西”。这就是“人工智能+”。从当前开发人工智能的前沿公司而言，OpenAI也好，谷歌的Gemini也好，包括国内腾讯、阿里，都在进行大模型的开发，这其实就是科技大厂豪门的大佬之争。它们想要做的，是人工智能时代的基础设施。周鸿祎不断地在强调，技术路线不止一条，大模型也不止一个。显然，人工智能之所以能够成为堪比工业革命的一场新技术革命，恰恰就在于，人工智能将会重塑整个技术世界，一切都将以人工智能作为基础，重新构建。而对于所有企业来说，核心就在于如何应用人工智能，构建出自己的人工智能应用，从而服务自己的企业和产业。“人工智能+”，不是空中楼阁地讨论大模型的模式，或者sora的惊艳，而是如何把大模型楔进自己的企业和产业之中。对于如何实现人工智能+，他有着十分具体的设计，是对于人工智能切实而有效的应用。例如，在行业使用中要进行深度的训练。而在企业化的应用中，必须是企业内部知识库与AI的配合使用。垂直化是他一直不遗余力着眼的重点：不要试图用一个大模型解决所有问题，大模型未来在企业落地形态一定是多个垂直模型组合，垂直模型解决专业问题的能力更强。反而，盲目求大是他所反对的，而是小型化与专有化的大模型，降低成本，部署灵活，安全可控。这是一个30年来在科技前线创业的老兵的“老成谋国”，当人工智能成为一种流行词汇的时候，他几乎成为了一个预言师，提前给出了“人工智能+”的应用配方。03可是，就像中国的许多企业在互联网时代中曾经的惊慌失措一样，当一项新的技术挟万钧之力进入到整个市场之中时，一家具体的公司，一项具体的业务，乃至一个具体的员工，该如何才能成为这波人工智能的技术革命中，不成为被甩在后面的落后者？这可能就是周鸿祎所说的“人工智能神话”，对于未知技术的陌生，使人们产生了未知的恐惧。他在一次演讲中表示：“大模型有点像数据库技术一样，将来是每一个数字化系统、每一个数字政府、智慧城市的标配。”在ChatGPT诞生之后的一年时间里，周鸿祎几乎全身心投入了对人工智能的研究、传播与实践，在所有的社交媒体平台、公开演讲、视频拍摄、直播论坛中。在公众眼中，他不仅仅是一个人工智能信仰者，而已经成为了布道者。一个布道者的责任，当然不会仅仅停留在传播的阶段，而是如何去实践，尤其当人工智能注定要成为重塑技术世界的因素。因而，早在2023年6月，他就已经联合了100多家各行各业的龙头企业，成立了GPT产业联盟，通过发展100家GPT垂直行业的合作伙伴、携手发展1000家GPT应用生态合作伙伴、为百万家客户提供GPT一站式服务，在GPT产业上下游对接覆盖。同时，GPT产业联盟还将为会员开放API和插件系统，赋能不同行业细分场景应用的智能化升级。这是人工智能+具体而微的真实实践，也是把人工智能进行场景化的企业应用的方式。让人工智能服务于真实的场景与业务。例如，率先得以应用的包括了税务和企业客服。垂直开发，行业开发，甚至是企业开发，这些人工智能的企业级应用必然决定人工智能基础设施已经奠定之后多数公司与个人的命运和生死存亡。人工智能不是天外飞仙或者云山雾罩，按照周鸿祎的话说，“它的定位应该是一个工具，而且是一个生产力工具”。只有被使用的工具，才具有改造和革命的生命力。当然，周鸿祎也的确是一个布道者。作为一个商业世界的搏浪者，他恐怕也深刻地明白当一个企业面临一项全新的技术时可能面临的恐惧与慌张，所以，他说，要开100场公开课：这可能也是人工智能+的一个良好的起点，破除未知的恐惧，导引人们学会使用人工智能。迄今为止，中国依然是一个赶超型的社会，一个赶超型的经济体系。人工智能已经成为了大国竞争的国之重器。然而，把已经成为人类共同财产和共同发展工具的技术，以“人工智能+”的方式，踏实地变成可供执行和使用的产品，信仰它，实践它，运用它，才是这项新的技术突破，带给我们的真正的福祉和机会。这是政府工作报告提出的“人工智能+”战略给每个人带来的机会，也是周鸿祎及早预见和布局的意义：每个企业，都能在人工智能时代中，找到自己腾飞的空间。
688472930,4a597ac4f0a83eb8fcac565bbf62c459,是否及时,https://api.zhihu.com/people/4a597ac4f0a83eb8fcac565bbf62c459,people,,ai,article,https://api.zhihu.com/articles/688472930,对于一个从未接触过AI的人来说，<em>人工智能</em>究竟是什么呢？让我来尝试从自己的角度去解读一下。 首先，当我听到“<em>人工智能</em>”这个词时，我会想到一种超级智能的机器，它能像人类一样思考、学习和做出决策,0,3,0,1711092325,1711092325,当今科技日新月异的时代，人工智能（AI）这个名词经常会出现在我们的生活中。无论是科幻电影中的机器人，还是智能手机上的语音助手，人工智能似乎已经成为了我们生活中不可或缺的一部分。然而，对于一个从未接触过AI的人来说，人工智能究竟是什么呢？让我来尝试从自己的角度去解读一下。首先，当我听到“人工智能”这个词时，我会想到一种超级智能的机器，它能像人类一样思考、学习和做出决策。我会想象到科幻电影中的场景，人们与机器人共同生活在一个充满高科技的未来世界中。我会感到兴奋和好奇，想要了解更多关于这个神奇的技术的一切。然而，随着我对人工智能的了解越来越深入，我意识到人工智能并不仅仅是关于未来的幻想，它已经在我们周围悄然发生着改变。例如，在我使用的智能手机上，有一个名为“语音助手”的功能，它可以根据我的指令执行各种任务，比如发送短信、播放音乐、查找信息等。我曾经以为这只是一种简单的程序，但实际上它背后所运用的就是人工智能技术。除了智能手机，人工智能还在许多其他领域展现出了它的应用价值。比如，在医疗领域，人工智能可以帮助医生诊断疾病，预测患者的治疗结果，甚至设计新药物。在交通领域，人工智能可以优化交通流量，提高交通安全性。在金融领域，人工智能可以帮助银行识别欺诈行为，优化投资组合。然而，尽管人工智能带来了许多便利和机会，但它也引发了一些担忧和争议。有人担心人工智能会取代人类的工作岗位，导致大规模失业。有人担心人工智能系统可能会出现偏见或错误，导致不公平的决策。因此，我们需要在推动人工智能技术发展的同时，认真考虑其可能带来的影响，并采取适当的措施来应对这些挑战。总的来说，作为一个从未接触过AI的人，我的认识可能还比较浅薄，但我对人工智能充满了好奇和期待。我相信随着科技的不断进步和人类智慧的发展，人工智能将会成为我们生活中越来越重要的一部分，为我们带来更多的便利和机会
687343091,95046db6c47ebc58a7692f052a3fc6f2,zsdz,https://api.zhihu.com/people/95046db6c47ebc58a7692f052a3fc6f2,people,,AI,article,https://api.zhihu.com/articles/687343091,<em>人工智能</em>这个词据说是约翰麦卡锡在大约60年代的那个<em>人工智能</em>大会上提出的 1 什么是智能 什么是“智能”？我不知道有没有专业解释，大多数人也许也不会问：反正就是人能做,0,0,0,1710561461,1710561461,人工智能这个词据说是约翰麦卡锡在大约60年代的那个人工智能大会上提出的1 什么是智能什么是“智能”？我不知道有没有专业解释，大多数人也许也不会问：反正就是人能做，动植物不能做的那玩意人的智能分为：直觉、思辨、推理。推理机器可以做一部分，其他机器做不到2 当前的人工智能是什么进展当前的人工智能我觉得是“量变引起的质变”：也就是神经网络早就被提出了，但在现在的技术和数据支撑下显示了其威力：好像以前30分，现在99分——已经很不错了，但并非“跳跃性质变”——如果哪天发现外星人了，就是“跳跃性质变”。AI当前所能做的依然不会超过一些早已发现的基本边界（比如AI不能做数学推理）所以，AI提供的主要是“更好用的产品”，或“更好的助手”，已经很不错了，但说什么“革新颠覆”我看不是。在某些地方是，比如围棋。问题棋类本来就是一种纯粹逻辑项目
695976549,da98bf281e4715cac52ee14cc7cf0cf3,花筏,https://api.zhihu.com/people/da98bf281e4715cac52ee14cc7cf0cf3,people,学生,人工智能顶会顶刊都有哪些（附网址）,article,https://api.zhihu.com/articles/695976549,中国计算机学会推荐国际学术刊物(<em>人工智能</em>)原文链接是 https://www.ccf.org.cn/Academic_Evaluation/AI/ 第三方网站Call4Papers（提供各领域知名会会议及期刊专刊的截稿日期和详细信息）链接http,7,0,7,1714877706,1714877707,"我们有的时候会听到大佬们说他们最近发了一篇CCF-A的会议，那么什么是CCF，CC-A呢？ CCF是中国计算机学会（China Computer Federation）的简称。作为中国计算机领域的权威性学术组织之一，CCF致力于促进计算机科学与技术的发展，推动中国计算机学术界和产业界的交流与合作。中国计算机学会推荐的国际学术会议和期刊目录是一个由CCF编制的权威性目录，旨在为中国计算机领域的学者和科研人员提供参考，帮助他们选择合适的学术交流和发表论文的平台。这个目录通常包括了计算机领域内的一些顶级国际学术会议和期刊，按照不同的类别和级别进行划分。通常情况下，CCF的学术会议和期刊目录包括了几个级别，比如A、B、C等，也就是我们常说的CCF-A, CCF-B, CCF-C。用来表示其在学术界的重要程度和影响力。其中，A类会议和期刊往往是该领域的顶级会议和期刊，拥有严格的论文评审制度和高质量的论文发表，对学术界有较大的影响力；而B、C类会议和期刊则相对较低一些，但仍然具有一定的学术水平和影响力。当然，CCF也发布了计算领域高质量科技期刊分级目录（针对中文期刊），“计算领域高质量科技期刊分级目录”分为T1、T2、T3三类，T1为最顶级期刊，T2为非常优秀期刊，T3为优秀期刊。本文主要讲人工智能的英文顶会，顶刊，对中文分级感兴趣的朋友可以了解一下，网站是https://www.ccf.org.cn/ccftjgjxskwml/中国计算机学会推荐国际学术刊物(人工智能)原文链接是 https://www.ccf.org.cn/Academic_Evaluation/AI/第三方网站Call4Papers（提供各领域知名会会议及期刊专刊的截稿日期和详细信息）链接http://123.57.137.208/ccf/ccf-8.jsp中国计算机学会推荐国际学术会议（人工智能）A 类序号会议简称会议全称出版社网址1AAAIAAAI Conference on Artificial IntelligenceAAAIhttp://dblp.uni-trier.de/db/conf/aaai/2NeurIPSConference on Neural Information Processing SystemsMIT Presshttp://dblp.uni-trier.de/db/conf/nips/3ACLAnnual Meeting of the Association for Computational LinguisticsACLhttp://dblp.uni-trier.de/db/conf/acl/4CVPRIEEE/CVF Computer Vision and Pattern Recognition ConferenceIEEEhttp://dblp.uni-trier.de/db/conf/cvpr/5ICCVInternational Conference on Computer VisionIEEEhttp://dblp.uni-trier.de/db/conf/iccv/6ICMLInternational Conference on Machine LearningACMhttp://dblp.uni-trier.de/db/conf/icml/7IJCAIInternational Joint Conference on Artificial IntelligenceMorgan Kaufmannhttp://dblp.uni-trier.de/db/conf/ijcai/B类序号会议简称会议全称出版社网址1COLTAnnual Conference onComputational Learning TheorySpringerhttp://dblp.uni-trier.de/db/conf/colt/2EMNLPConference on EmpiricalMethods in Natural Language ProcessingACLhttp://dblp.uni-trier.de/db/conf/emnlp/3ECAIEuropean Conference onArtificial IntelligenceIOS Presshttp://dblp.uni-trier.de/db/conf/ecai/4ECCVEuropean Conference onComputer VisionSpringerhttp://dblp.uni-trier.de/db/conf/eccv/5ICRAIEEE International Conference onRobotics and AutomationIEEEhttp://dblp.uni-trier.de/db/conf/icra/6ICAPSInternational Conference onAutomated Planning and SchedulingAAAIhttp://dblp.uni-trier.de/db/conf/aips/7ICCBRInternational Conference on Case-Based ReasoningSpringerhttp://dblp.uni-trier.de/db/conf/iccbr/8COLINGInternational Conference on Computational LinguisticsInternational Committee onComputational Linguisticshttp://dblp.uni-trier.de/db/conf/coling/9KRInternational Conference onPrinciples of Knowledge Representation and ReasoningMorgan Kaufmannhttp://dblp.uni-trier.de/db/conf/kr/10UAIConference onUncertainty in ArtificialIntelligenceAUAIhttp://dblp.uni-trier.de/db/conf/uai/11AAMASInternational Joint Conference on Autonomous Agents and Multi-agent SystemsSpringerhttp://dblp.uni-trier.de/db/conf/atal/index.html12PPSNParallel Problem Solving fromNatureSpringerhttp://dblp.uni-trier.de/db/conf/ppsn/13NAACLNorth American Chapter of the Associationfor Computational LinguisticsAssociation for ComputationalLinguisticshttp://dblp.uni-trier.de/db/conf/naacl/C 类序号会议简称会议全称出版社网址1AISTATSInternational Conference on Artificial Intelligence and StatisticsJMLRhttp://dblp.uni-trier.de/db/conf/aistats/2ACCVAsian Conference on Computer VisionSpringerhttp://dblp.uni-trier.de/db/conf/accv/3ACMLAsian Conference on MachineLearningJMLRhttp://dblp.uni-trier.de/db/conf/acml/4BMVCBritish Machine Vision ConferenceBritish Machine Vision Associationhttp://dblp.uni-trier.de/db/conf/bmvc/5NLPCCCCF International Conference onNatural Language Processing and Chinese ComputingSpringerhttps://dblp.uni-trier.de/db/conf/nlpcc/6CoNLLConference on Computational NaturalLanguage LearningAssociation forComputational Linguisticshttp://dblp.uni-trier.de/db/conf/conll7GECCOGenetic and Evolutionary ComputationConferenceACMhttp://dblp.uni-trier.de/db/conf/gecco/8ICTAIIEEE International Conference onTools with Artificial IntelligenceIEEEhttp://dblp.uni-trier.de/db/conf/ictai/9IROSIEEE\RSJ International Conference onIntelligent Robots and SystemsIEEEhttp://dblp.uni-trier.de/db/conf/iros/10ALTInternational Conference onAlgorithmic Learning TheorySpringerhttp://dblp.uni-trier.de/db/conf/alt/11ICANNInternational Conference on ArtificialNeural NetworksSpringerhttp://dblp.uni-trier.de/db/conf/icann/12FGIEEE International Conference on AutomaticFace and Gesture RecognitionIEEEhttp://dblp.uni-trier.de/db/conf/fgr/13ICDARInternational Conference on DocumentAnalysis and RecognitionIEEEhttp://dblp.uni-trier.de/db/conf/icdar/14ILPInternational Conference on InductiveLogic ProgrammingSpringerhttp://dblp.uni-trier.de/db/conf/ilp/15KSEMInternational conference on KnowledgeScience, Engineering and ManagementSpringerhttp://dblp.uni-trier.de/db/conf/ksem/16ICONIPInternational Conference on NeuralInformation ProcessingSpringerhttp://dblp.uni-trier.de/db/conf/iconip/17ICPRInternational Conference on PatternRecognitionIEEEhttp://dblp.uni-trier.de/db/conf/icpr/18IJCBInternational Joint Conference onBiometricsIEEEhttp://dblp.uni-trier.de/db/conf/icb/19IJCNNInternational Joint Conference onNeural NetworksIEEEhttp://dblp.uni-trier.de/db/conf/ijcnn/20PRICAIPacific Rim International Conferenceon Artificial IntelligenceSpringerhttp://dblp.uni-trier.de/db/conf/pricai/中国计算机学会推荐国际学术期刊（人工智能）A类序号刊物简称刊物全称出版社网址1AIArtificial IntelligenceElsevierhttp://dblp.uni-trier.de/db/journals/ai/2TPAMIIEEE Transactions on Pattern Analysis andMachineIntelligenceIEEEhttp://dblp.uni-trier.de/db/journals/pami/3IJCVInternational Journal of Computer VisionSpringerhttp://dblp.uni-trier.de/db/journals/ijcv/4JMLRJournal of Machine Learning ResearchMIT Presshttp://dblp.uni-trier.de/db/journals/jmlr/B类序号刊物简称刊物全称出版社网址1TAPACM Transactions on Applied PerceptionACMhttp://dblp.uni-trier.de/db/journals/tap/2AAMASAutonomous Agents and Multi-Agent SystemsSpringerhttp://dblp.uni-trier.de/db/journals/aamas/3Computational LinguisticsMIT Presshttp://dblp.uni-trier.de/db/journals/coling/4CVIUComputer Vision and Image UnderstandingElsevierhttp://dblp.uni-trier.de/db/journals/cviu/5DKEData & Knowledge EngineeringElsevierhttp://dblp.uni-trier.de/db/journals/dke/index.html6Evolutionary ComputationMIT Presshttp://dblp.uni-trier.de/db/journals/ec/7TACIEEE Transactions on Affective ComputingIEEEhttp://dblp.uni-trier.de/db/journals/taffco/8TASLPIEEE/ACM Transactions on Audio, Speech and Language ProcessingIEEEhttp://dblp.uni-trier.de/db/journals/taslp/9IEEE Transactions on CyberneticsIEEEhttp://dblp.uni-trier.de/db/journals/tcyb/10TECIEEE Transactions on EvolutionaryComputationIEEEhttp://dblp.uni-trier.de/db/journals/tec/11TFSIEEE Transactions on Fuzzy SystemsIEEEhttp://dblp.uni-trier.de/db/journals/tfs/12TNNLSIEEE Transactions on Neural Networks andlearning systemsIEEEhttp://dblp.uni-trier.de/db/journals/tnn/13IJARInternational Journal of ApproximateReasoningElsevierhttp://dblp.uni-trier.de/db/journals/ijar/14JAIRJournal of Artificial Intelligence ResearchAAAIhttp://dblp.uni-trier.de/db/journals/jair/index.html15Journal of Automated ReasoningSpringerhttp://dblp.uni-trier.de/db/journals/jar/16JSLHRJournal of Speech, Language, and Hearing ResearchAmerican Speech-Language Hearing Associationhttp://jslhr.pubs.asha.org/17Machine LearningSpringerhttp://dblp.uni-trier.de/db/journals/ml/18Neural ComputationMIT Presshttp://dblp.uni-trier.de/db/journals/neco/19Neural NetworksElsevierhttp://dblp.uni-trier.de/db/journals/nn/20PRPattern RecognitionElsevierhttp://dblp.uni-trier.de/db/conf/par/21TACLTransactions of the Association for Computational LinguisticsThe Association for Computational Linguisticshttps://dblp.org/db/journals/tacl/index.htmlC类序号刊物简称刊物全称出版社网址1TALLIPACM Transactions on Asian and Low-Resource Language Information ProcessingACMhttp://dblp.uni-trier.de/db/journals/talip/2Applied IntelligenceSpringerhttp://dblp.uni-trier.de/db/journals/apin/3AIMArtificial Intelligence in MedicineElsevierhttp://dblp.uni-trier.de/db/journals/artmed/4Artificial LifeMIT Presshttp://dblp.uni-trier.de/db/journals/alife/5Computational IntelligenceWileyhttp://dblp.uni-trier.de/db/journals/ci/6Computer Speech & LanguageElsevierhttp://dblp.uni-trier.de/db/journals/csl/7Connection ScienceTaylor & Francishttp://dblp.uni-trier.de/db/journals/connection/8DSSDecision Support SystemsElsevierhttp://dblp.uni-trier.de/db/journals/dss/9EAAIEngineering Applications of ArtificialIntelligenceElsevierhttp://dblp.uni-trier.de/db/journals/eaai/10Expert SystemsBlackwell/Wileyhttp://dblp.uni-trier.de/db/journals/es/11ESWAExpert Systems with ApplicationsElsevierhttp://dblp.uni-trier.de/db/journals/eswa/12Fuzzy Sets and SystemsElsevierhttp://dblp.uni-trier.de/db/journals/fss/13TGIEEE Transactions on GamesIEEEhttp://dblp.uni-trier.de/db/journals/tciaig/14IET-CVIIET Computer VisionIEThttps://dblp.org/db/journals/iet-cvi/index.html15IET Signal ProcessingIEThttps://dblp.org/db/journals/iet-spr/index.html16IVCImage and Vision ComputingElsevierhttp://dblp.uni-trier.de/db/journals/ivc/17IDAIntelligent Data AnalysisIOS Presshttp://dblp.uni-trier.de/db/journals/ida/18IJCIAInternational Journal of ComputationalIntelligence and ApplicationsWorld Scientifichttp://dblp.uni-trier.de/db/journals/ijcia/19IJISInternational Journal of Intelligent SystemsWileyhttp://dblp.uni-trier.de/db/journals/ijis/20IJNSInternational Journal of Neural SystemsWorld Scientifichttp://dblp.uni-trier.de/db/journals/ijns/21IJPRAIInternational Journal of Pattern Recognitionand Artificial IntelligenceWorld Scientifichttp://dblp.uni-trier.de/db/journals/ijprai/22IJUFKSInternational Journal of Uncertainty,Fuzziness and Knowledge-Based SystemsWorld Scientifichttps://dblp.uni-trier.de/db/journals/ijufks/23IJDARInternational Journal on Document Analysisand RecognitionSpringerhttp://dblp.uni-trier.de/db/journals/ijdar/24JETAIJournal of Experimental and TheoreticalArtificial IntelligenceTaylor & Francishttp://dblp.uni-trier.de/db/journals/jetai/25KBSKnowledge-Based SystemsElsevierhttp://dblp.uni-trier.de/db/journals/kbs/26Machine TranslationSpringerhttp://dblp.uni-trier.de/db/journals/mt/27Machine Vision and ApplicationsSpringerhttp://dblp.uni-trier.de/db/journals/mva/28Natural ComputingSpringerhttp://dblp.uni-trier.de/db/journals/nc/29NLENatural Language EngineeringCambridgeUniversity Presshttp://dblp.uni-trier.de/db/journals/nle/30NCANeural Computing and ApplicationsSpringerhttp://dblp.uni-trier.de/db/journals/nca/31NPLNeural Processing LettersSpringerhttp://dblp.uni-trier.de/db/journals/npl/32NeurocomputingElsevierhttp://dblp.uni-trier.de/db/journals/ijon/33PAAPattern Analysis and ApplicationsSpringerhttp://dblp.uni-trier.de/db/journals/paa/34PRLPattern Recognition LettersElsevierhttp://dblp.uni-trier.de/db/journals/prl/35Soft ComputingSpringerhttp://dblp.uni-trier.de/db/journals/soco/36WIWeb IntelligenceIOS Presshttp://dblp.uni-trier.de/db/journals/wias/37TIISACM Transactions on Interactive Intelligent SystemsACMhttps://dblp.org/db/journals/tiis/index.html33PAAPattern Analysis and ApplicationsSpringerhttp://dblp.uni-trier.de/db/journals/paa/34PRLPattern Recognition LettersElsevierhttp://dblp.uni-trier.de/db/journals/prl/35Soft ComputingSpringerhttp://dblp.uni-trier.de/db/journals/soco/36WIWeb IntelligenceIOS Presshttp://dblp.uni-trier.de/db/journals/wias/37TIISACM Transactions on Interactive Intelligent SystemsACMhttps://dblp.org/db/journals/tiis/index.html"
695961716,0285abc5f9413ad5cf415ada51edaa28,ventura,https://api.zhihu.com/people/0285abc5f9413ad5cf415ada51edaa28,people,,人工智能：现代方法 ---- 第1章 绪论,article,https://api.zhihu.com/articles/695961716,而另一些更喜欢用“理性”（rationality）来抽象正式地定义<em>智能</em>，直观上的理解是做“正确的事情”。<em>智能</em>主题的本身也各不相同：一些人将<em>智能</em>视为内部思维过程和推理的属性，而另一些人则关注<em>智能</em>的外部特征，也就是智能行为。,0,0,2,1714870929,1714870929,"第1章 绪论◆ 我们称自己为智人（有智慧的人），因为智能（intelligence）对我们来说尤其重要。几千年来，我们一直试图理解我们是如何思考和行动的，也就是不断地了解我们的大脑是如何凭借它那小部分物质去感知、理解、预测并操纵一个远比其自身更大更复杂的世界。◆ 人工智能（artificial intelligence，AI）领域不仅涉及理解，还涉及构建智能实体。这些智能实体机器需要在各种各样新奇的情况下，计算如何有效和安全地行动。◆ 人工智能经常被各种调查列为最有趣、发展最快的领域之一，现在每年创造的价值超过一万亿美元。人工智能专家李开复预测称，人工智能对世界的影响“将超过人类历史上的任何事物”。此外，人工智能的研究前沿仍是开放的。学习较古老科学（如物理学）的学生可能会认为最好的想法都已经被伽利略、牛顿、居里夫人、爱因斯坦等人发现了，但当下人工智能仍然为专业人员提供了许多机会。1.1 什么是人工智能◆ 我们声称人工智能很有趣，但是我们还没有描述它是什么。历史上研究人员研究过几种不同版本的人工智能。有些根据对人类行为的复刻来定义智能，而另一些更喜欢用“理性”（rationality）来抽象正式地定义智能，直观上的理解是做“正确的事情”。智能主题的本身也各不相同：一些人将智能视为内部思维过程和推理的属性，而另一些人则关注智能的外部特征，也就是智能行为。◆ 图灵测试（Turing test）是由艾伦·图灵（Alan Turing）提出的（Turing, 1950），它被设计成一个思维实验，用以回避“机器能思考吗？”这个哲学上模糊的问题。如果人类提问者在提出一些书面问题后无法分辨书面回答是来自人还是来自计算机，那么计算机就能通过测试。◆ 目前，为计算机编程使其能够通过严格的应用测试尚有大量工作要做。计算机需要具备下列能力：● 自然语言处理（natural language processing），以使用人类语言成功地交流；● 知识表示（knowledge representation），以存储它所知道或听到的内容；● 自动推理（automated reasoning），以回答问题并得出新的结论；● 机器学习（machine learning），以适应新的环境，并检测和推断模式。◆ 图灵认为，没有必要对人进行物理模拟来证明智能。然而，其他研究人员提出了完全图灵测试（total Turing test），该测试需要与真实世界中的对象和人进行交互。◆ 为了通过完全图灵测试，机器人还需要具备下列能力：● 计算机视觉（computer vision）和语音识别功能，以感知世界；● 机器人学（robotics），以操纵对象并行动。◆ 我们必须知道人类是如何思考的，才能说程序像人类一样思考。我们可以通过3种方式了解人类的思维：● 内省（introspection）——试图在自己进行思维活动时捕获思维；● 心理实验（psychological experiment）——观察一个人的行为；● 大脑成像（brain imaging）——观察大脑的活动。◆ 认知科学（cognitive science）这一跨学科领域汇集了人工智能的计算机模型和心理学的实验技术，用以构建精确且可测试的人类心智理论。◆ 认知科学本身是一个引人入胜的领域，值得用多本教科书和至少一部百科全书（Wilson and Keil, 1999）来介绍。我们会偶尔评论人工智能技术和人类认知之间的异同，但真正的认知科学必须建立在对人类或动物实验研究的基础上。这里，我们假设读者只有一台可以做实验的计算机，因此我们将把这方面的内容留给其他书籍。◆ 这两个领域相互促进，值得一提的是计算机视觉领域，它将神经生理学证据整合到了计算模型中。最近，将神经影像学方法与分析数据的机器学习技术相结合，开启了“读心”能力（即查明人类内心思想的语义内容）的研究。这种能力反过来可以进一步揭示人类认知的运作方式。◆ 希腊哲学家亚里士多德是最早试图法则化“正确思维”的人之一，他将其定义为无可辩驳的推理过程。他的三段论（syllogism）为论证结构提供了模式，当给出正确的前提时，总能得出正确的结论。◆ 19世纪的逻辑学家建立了一套精确的符号系统，用于描述世界上物体及其之间的关系。这与普通算术表示系统形成对比，后者只提供关于数的描述。到1965年，任何用逻辑符号描述的可解问题在原则上都可以用程序求解。人工智能中所谓的逻辑主义（logicism）传统希望在此类程序的基础上创建智能系统。◆ 按照常规的理解，逻辑要求关于世界的认知是确定的，而实际上这很难实现。例如，我们对政治或战争规则的了解远不如对国际象棋或算术规则的了解。概率（probability）论填补了这一鸿沟，允许我们在掌握不确定信息的情况下进行严格的推理。原则上，它允许我们构建全面的理性思维模型，从原始的感知到对世界运作方式的理解，再到对未来的预测。它无法做到的是形成智能行为。为此，我们还需要关于理性行为的理论，仅靠理性思考是不够的。◆ 智能体（agent）就是某种能够采取行动的东西（agent来自拉丁语agere，意为“做”）。当然，所有计算机程序都可以完成一些任务，但我们期望计算机智能体能够完成更多的任务：自主运行、感知环境、长期持续存在、适应变化以及制定和实现目标。理性智能体（rational agent）需要为取得最佳结果或在存在不确定性时取得最佳期望结果而采取行动。◆ 通过图灵测试所需的所有技能也使智能体得以采取理性行为。知识表示和推理能让智能体做出较好的决策。我们需要具备生成易于理解的自然语言句子的能力，以便在复杂的社会中生存。我们需要学习不仅是为了博学多才，也是为了提升我们产生高效行为的能力，尤其是在新环境下，这种能力更加重要。◆ 与其他方法相比，基于人工智能的理性智能体方法有两个优点。首先，它比“思维法则”方法更普适，因为正确的推断只是实现理性的几种可能机制之一。其次，它更适合科学发展。理性的标准在数学上是明确定义且完全普适的。◆ 在最初的几十年里，理性智能体建立在逻辑的基础上，并为了实现特定目标制定了明确的规划。后来，基于概率论和机器学习的方法可以使智能体在不确定性下做出决策，以获得最佳期望结果。简而言之，人工智能专注于研究和构建做正确的事情的智能体，其中正确的事情是我们提供给智能体的目标定义。这种通用范式非常普遍，以至于我们可以称之为标准模型（standard model）。◆ 它不仅适用于人工智能，也适用于其他领域。控制理论中，控制器使代价函数最小化；运筹学中，策略使奖励的总和最大化；统计学中，决策规则使损失函数最小；经济学中，决策者追求效用或某种意义的社会福利最大化。◆ 然而在复杂的环境中，完美理性（总是采取精确的最优动作）是不可行的，它的计算代价太高了，因此需要对标准模型做一些重要的改进。◆ 有限理性（limited rationality）的问题，也就是在没有足够时间进行所有可能的计算的情况下，适当地采取行动。但是，完美理性仍然是理论分析的良好出发点。◆ 人为定义的任务，如国际象棋或最短路径计算之类的，都附带固有的目标，因此标准模型是适用的。然而，在真实世界中，我们越来越难以完全正确地指定目标。例如，在设计自动驾驶汽车时，我们可能会认为目标是安全到达目的地。但是，由于存在其他司机失误、设备故障等原因，在任何道路上行驶都有可能受伤，因此，严格的安全目标是要求待在车库里而不要上路驾驶。◆ 在我们的真实需求和施加给机器的目标之间达成一致的问题称为价值对齐问题（value alignment problem），即施加给机器的价值或目标必须与人类的一致。◆ 随着人工智能的发展，越来越强大的智能系统需要部署在真实世界中，这种方法不再可行。部署了错误目标的系统将会导致负面影响，而且，系统越智能，其负面影响就越严重。◆ 它也可能会为自己劫持额外的计算能力。这些行为不是“愚蠢”或“疯狂”的，这些行为是将获胜定义为机器唯一目标的逻辑结果。◆ 一台实现固定目标的机器可能会出现很多不当行为，要预测所有不当行为是不可能的。因此，我们有足够理由认为标准模型是不充分的。我们不希望机器“聪明”地实现它们的目标，而是希望它们实现我们的目标。如果我们不能将这些目标完美地传达给机器，就需要一个新的表述，也就是机器正在实现我们的目标，但对于目标是什么则是不确定的。◆ 当一台机器意识到它不了解完整的目标时，它就会有谨慎行动的动机，会寻求许可，并通过观察来更多地了解我们的偏好，遵守人为控制。最终，我们想要的是对人类可证益的（provably beneficial）智能体。1.2 人工智能的基础◆ 亚里士多德（Aristotle，公元前384—公元前322）制定了一套精确的法则来统御思维的理性部分，他是历史上第一位这样做的哲学家。他发展了一套非正式的三段论系统进行适当的推理，该系统原则上允许人们在给定初始前提下机械地得出结论。◆ 大约在1500年，列奥纳多·达·芬奇（Leonardo da Vinci，1452—1519）设计了一台机械计算器，虽然当时并未制造，但最近的重构表明该设计是可行的。第一台已知的计算器是在1623年前后由德国科学家威廉·席卡德（Wilhelm Schickard，1592—1635）制造的。布莱兹·帕斯卡（Blaise Pascal，1623—1662）于1642年建造了滚轮式加法器（Pascaline），并写道：“它产生的效用似乎比动物的所有行为更接近思维。”戈特弗里德·威廉·莱布尼茨（Gottfried Wilhelm Leibniz，1646—1716）制造了一台机械设备，旨在根据概念而非数值进行操作，但其应用范围相当有限。◆ 有观点认为，思维至少在某种程度上是根据逻辑或数值规则运作的，可以建立模仿其中的一些规则的物理系统。也有观点说，思维本身就是这样一个物理系统。勒内·笛卡儿（René Descartes，1596—1650）首次清晰地讨论了思维与物质之间的区别。他指出，思维的纯粹物理概念似乎没有给自由意志留下多少空间。如果思维完全受物理法则支配，那么它拥有的自由意志不会比一块“决定”往下掉的石头多。◆ 唯物主义（materialism）是二元论的一种替代，它认为大脑根据物理定律的运作构成了思维。自由意志仅仅是实体对可选决策的感知。物理主义（physicalism）和自然主义（naturalism）这两个术语也被用于描述这类与超自然观点相反的观点。◆ 大卫·休谟（David Hume，1711—1776）的《人性论》（A Treatise of Human Nature）（Hume, 1739）提出了现在称为归纳法（induction）的原则：通过暴露要素之间的重复联系获得一般规则。◆ 以路德维希·维特根斯坦（Ludwig Wittgenstein，1889—1951）和伯特兰·罗素（Bertrand Russell，1872—1970）的工作为基础，著名的维也纳学派（Sigmund, 2017）——一群在20世纪20年代及20世纪30年代聚集在维也纳的哲学家和数学家——发展了逻辑实证主义（logical positivism）学说。该学说认为，所有知识都可以通过逻辑理论来描述，逻辑理论最终与对应于感知输入的观察语句（observation sentence）相联系。因此，逻辑实证主义结合了理性主义和经验主义。◆ 卡纳普的《世界的逻辑构造》（The Logical Structure of the World）（Carnap, 1928）也许是最先提出将思维视为计算过程这一理论的著作。◆ 思维的哲学图景中最后一个要素是知识与动作之间的联系。这个问题对人工智能来说至关重要，因为智能不仅需要推理，还需要动作。而且，只有理解了怎样的行为是合理的，才能理解如何构建行为是合理的（或理性的）智能体。◆ 2300年后，纽厄尔和西蒙在他们的通用问题求解器（General Problem Solver）程序中实现了亚里士多德的算法。我们现在将其称为贪婪回归规划系统（见第11章）。在人工智能理论研究的前几十年中，基于逻辑规划以实现确定目标的方法占据主导地位。◆ 安托万·阿尔诺（Antoine Arnauld）（Arnauld, 1662）分析了赌博中的理性决策概念，提出了一种量化公式，可以最大化期望收入的货币价值。后来，丹尼尔·伯努利（Daniel Bernoulli）（Bernoulli, 1738）引入了更普适的效用（utility）概念，可以体现结果的内在主观价值。如第16章所述，在不确定性下，理性决策的现代概念涉及最大化期望效用。◆ 在道德和公共政策方面，决策者必须考虑多个个体的利益。杰里米·边沁（Jeremy Bentham）（Bentham, 1823）和约翰·斯图尔特·穆勒（John Stuart Mill）（Mill, 1863）提出了功利主义（utilitarianism）思想：基于效用最大化的理性决策应该适用于人类活动的所有领域，包括代表许多个体做出公共政策的决策。功利主义是一种特殊的结果主义（consequentialism），行为的预期结果决定了正确与否。◆ 相反，伊曼努尔·康德（Immanuel Kant）在1785年提出了一种基于规则或义务伦理学（deontological ethics）的理论。在该理论中，“做正确的事”不是由结果决定的，而是由管理可行行为的普适社会法则所决定的，可行行为包括“不要撒谎”“不要杀人”等。因此，如果期望的好处大于坏处，那么功利主义者可以撒一个善意的谎言，但康德主义者则不能这样做，因为撒谎本质上就是错误的。◆ 穆勒承认规则的价值，但将其理解为基于第一性原理对结果进行推理的高效决策程序。许多现代人工智能系统正是采用了这种方法。◆ 哲学家们提出了人工智能的一些基本理念，但人工智能要成为正规科学，需要逻辑和概率的数学化，并引入一个新的数学分支——计算。◆ 形式化逻辑（formal logic）的思想可以追溯到古希腊、古印度和古代中国的哲学家，但它的数学发展真正始于乔治·布尔（George Boole，1815—1864）的工作。布尔提出了命题和布尔逻辑的细节（Boole, 1847）。1879年，戈特洛布·弗雷格（Gottlob Frege，1848—1925）将布尔逻辑扩展到包括对象和关系，创建了沿用至今的一阶逻辑[9]。一阶逻辑除了在人工智能研究的早期发挥核心作用外，还激发了哥德尔和图灵的工作，这些工作支撑了计算本身。◆ 概率（probability）论可以视为信息不确定情况下的广义逻辑，这对人工智能来说是非常重要的考虑。吉罗拉莫·卡尔达诺（Gerolamo Cardano，1501—1576）首先提出了概率的概念，并根据赌博事件的可能结果对其进行了刻画。1654年，布莱兹·帕斯卡（Blaise Pascal，1623—1662）在给皮埃尔·费马（Pierre Fermat，1601—1665）的信中展示了如何预测一个未完成的赌博游戏的结局，并为赌徒分配平均收益。概率很快成为定量科学的重要组成部分，用于处理不确定的度量和不完备的理论。◆ 托马斯·贝叶斯（Thomas Bayes，1702—1761）提出了根据新证据更新概率的法则。贝叶斯法则是人工智能系统的重要工具。◆ 概率的形式化结合数据的可用性，使统计学（statistics）成为了一个新研究领域。最早的应用之一是1662年约翰·格兰特（John Graunt）对伦敦人口普查数据的分析。罗纳德·费舍尔（Ronald Fisher）被认为是第一位现代统计学家，他汇总了概率、实验设计、数据分析和计算等思想（Fisher, 1922）。◆ 计算的历史与数字的历史一样古老，但用于计算最大公约数的欧几里得算法被认为是第一个非平凡的算法（algorithm）。“算法”一词源自一位9世纪的数学家穆罕默德·本·穆萨·阿尔·花剌子模（Muhammad ibn Musa al-Khwarizmi），他的著作还将阿拉伯数字和代数引入了欧洲。布尔等人讨论了逻辑演绎的算法，到19世纪末，人们开始努力将一般的数学推理形式化为逻辑演绎。◆ 库尔特·哥德尔（Kurt Gödel，1906—1978）表明，虽然存在一种有效方法能够证明弗雷格和罗素的一阶逻辑中的任何真实陈述，但是一阶逻辑无法满足表征自然数所需的数学归纳原理。1931年，哥德尔证明关于演绎的限制确实存在。哥德尔的不完全性定理（incompleteness theorem）表明，在任何像皮亚诺算术（Peano arithmetic，自然数的基本理论）这样强的形式化理论中，必然存在一些没有证明的真实陈述。◆ 这个基本结果也可以解释为作用于整数上的某些函数无法用算法表示，即它们无法被计算。这促使艾伦·图灵（Alan Turing，1912—1954）试图准确地描述哪些函数是可计算的，即能够通过有效的过程进行计算。丘奇-图灵论题（Church-Turing thesis）提出将图灵机（Turing, 1936）可计算的函数作为可计算性的一般概念。图灵还表明，存在某些任何图灵机都无法计算的函数。◆ 尽管可计算性（computability）对理解计算很重要，但易处理性（tractability）的概念对人工智能的影响更大。粗略地说，如果解决一个问题实例所需的时间随着问题规模呈指数增长，那么这个问题就是难处理的。◆ 由斯蒂芬·库克（Stephen Cook）（Cook, 1971）和理查德·卡普（Richard Karp）（Karp, 1972）开创的NP完全性（NP-completeness）理论为分析问题的易处理性提供了基础：任何可以归约到NP完全的问题都可能是难处理的。（尽管尚未证明NP完全问题一定是难处理的，但大多数理论家都相信这一点。）◆ 经济学起源于1776年，当时亚当·斯密（Adam Smith，1723—1790）发表了《国富论》（全名为《国民财富的性质和原因的研究》，An Inquiry into the Nature and Causes of the Weallth of Nations）。斯密建议将经济视为由许多关注自身利益的独立主体组成，但他并不主张将金融贪婪作为道德立场。他在较早的著作《道德情操论》（The Theory of Moral Sentiments）（Smith, 1759）开篇就指出，对他人福祉的关注是每个个体利益的重要组成部分。◆ 大多数人认为经济学就是关于钱的，而实际上第一个对不确定性下的决策进行数学分析的是安托万·阿尔诺（Arnauld, 1662）的最大期望值公式，而这一分析也的确是与赌注的货币价值相关。丹尼尔·伯努利（Bernoulli, 1738）注意到，这个公式似乎不适用于更大规模的金钱，例如对海上贸易远征的投资。于是，他提出了基于期望效用最大化的原则，并指出额外货币的边际效用会随着一个人获得更多货币而减少，从而解释了大众的投资选择。◆ 里昂·瓦尔拉斯（Léon Walras，1834—1910）为效用理论提供了一个更为普适的基础，即对任何结果（不仅仅是货币结果）的投机偏好。弗兰克·拉姆齐（Frank Ramsey）（Ramsey, 1931）以及后来约翰·冯·诺伊曼（John von Neumann）和奥斯卡·摩根斯特恩（Oskar Morgenstern）在他们的著作《博弈论与经济行为》（The Theory of Games and Economic Behavior）（Neumann and Morgenstern, 1944）中对这一理论进一步改进。经济学不再是研究金钱的学科，而是对欲望和偏好的研究。◆ 决策论（decision theory）结合了概率论和效用理论，为在不确定性下做出个体决策（经济的或其他的）提供了一个形式化完整的框架，也就是说，概率适当地描述了决策者所处的环境。这适用于“大型”经济体，在这种经济体中，每个主体都无须关注其他独立主体的行为。◆ 冯·诺依曼和摩根斯特恩对博弈论（game theory）的发展［也可以参考（Luce and Raiffa, 1957）］得出了令人惊讶的结果，即对于某些博弈，理性智能体应该采用随机（或至少看起来是随机）的策略。与决策论不同，博弈论并没有为行为的选择提供明确的指示。人工智能中涉及多个智能体的决策将在多智能体系统（multiagent system）的主题下探讨（第18章）。◆ 经济学家（除了一些例外）没有解决上面列出的第三个问题：当行为的收益不是立即产生的，而是在几个连续的行为后产生时，应该如何做出理性的决策。这个课题在运筹学（operations research）的领域探讨，运筹学出现在第二次世界大战期间英国对雷达安装的优化工作中，后来发展出了无数民用应用。理查德·贝尔曼（Richard Bellman）（Bellman, 1957）的工作将一类序贯决策问题进行了形式化，称为马尔可夫决策过程（Markov decision process），我们将在第17章研究该问题，并在第22章以强化学习（reinforcement learning）的主题研究该问题。◆ 经济学和运筹学的工作对理性智能体的概念做出了很大贡献，但是多年来的人工智能研究是沿着完全独立的道路发展的。原因之一是做出理性决策显然是复杂的。◆ 神经科学（neuroscience）是对神经系统（尤其是对大脑）的研究。尽管大脑进行思考的确切方式是科学的奥秘之一，但大脑确实是能思考的现实已经被人们接受了数千年，因为有证据表明，对头部的强烈打击会导致精神丧失。◆ 然而，直到18世纪中叶，大脑才被广泛认为是意识的所在地。在此之前，意识所在地的候选位置包括心脏和脾脏。◆ 现在人们普遍认为认知功能是由这些结构的电化学反应产生的。也就是说，一组简单的细胞就可以产生思维、行为和意识。如约翰·希尔勒（John Searle）（Searle, 1992）的精辟名言所说：大脑产生思想。◆ 大多数信息都在大脑皮质（大脑的外层）中处理的。基本的组织单元似乎是直径约0.5毫米的柱状组织，包含约20 000个神经元，并延伸到整个皮质（人类皮质深度约4毫米）◆ 1929年，汉斯·伯杰（Hans Berger）发明脑电图仪（EEG），开启了对完整大脑活动的测量。功能磁共振成像（fMRI）的发展（Ogawa et al., 1990; Cabeza and Nyberg, 2001）为神经科学家提供了前所未有的大脑活动的详细图像，从而使测量能够以有趣的方式与正在进行的认知过程相对应。神经元活动的单细胞电记录技术和光遗传学（optogenetics）方法的进展（Crick, 1999; Zemelman et al., 2002; Han and Boyden, 2007）增强了这些功能，从而可以测量和控制被修改为对光敏感的单个神经元。◆ 用于传感和运动控制的脑机接口（brain-machine interface）的发展（Lebedev and Nicolelis, 2006）不仅有望恢复残疾人的功能，还揭示了神经系统许多方面的奥秘。这项工作的一项重要发现是，大脑能够自我调整，使自己成功与外部设备进行交互，就像对待另一个感觉器官或肢体一样。◆ 未来主义者充分利用这些数字，指出了一个即将到来的奇点（singularity），在这个奇点上计算机达到了超越人类的性能水平（Vinge, 1993; Kurzweil, 2005; Doctorow and Stross, 2012），然后会进一步迅速提高。但是比较原始数字并不是特别有用。即使计算机的容量到达无限也无济于事，在理解智能方面仍然需要进一步的概念突破◆ 粗略地说，如果没有正确的理论，更快的机器只会更快地给出错误的答案。◆ 科学心理学的起源通常可以追溯到德国物理学家赫尔曼·冯·赫尔姆霍茨（Hermann von Helmholtz，1821—1894）和他的学生威廉·温特（Wilhelm Wundt，1832—1920）的工作。赫尔姆霍茨将科学方法应用于人类视觉的研究，他的Handbook of Physiological Optics被描述为“关于人类视觉的物理学和生理学的最重要的专著”（Nalwa, 1993, p.15）。◆ 另外，研究动物行为的生物学家缺乏内省的数据，于是发展了一种客观的方法，赫伯特·詹宁斯（Herbert S. Jennings）（Jennings, 1906）在他有影响力的著作Behavior of the Lower Organisms中对此进行了描述。约翰·沃森（John Watson，1878—1958）领导的行为主义（behaviorism）运动将这一观点应用于人类，以内省无法提供可靠证据为由，拒绝任何涉及心理过程的理论。◆ 认知心理学（cognitive psychology）认为大脑是一个信息处理设备，这至少可以追溯到威廉·詹姆斯（William James，1842—1910）的著作。赫尔姆霍茨也坚持认为感知涉及一种无意识的逻辑推断形式。在美国，认知观点在很大程度上被行为主义所掩盖，但在弗雷德里克·巴特利特（Frederic Bartlett，1886—1969）所领导的剑桥大学应用心理学系，认知模型得以蓬勃发展。巴特利特的学生和继任者肯尼斯·克雷克（Kenneth Craik）（Craik, 1943）所著的The Nature of Explanation强有力地重新确立了诸如信念和目标之类的“精神”术语的合法性，认为它们就像用压力和温度来讨论气体一样科学，尽管气体是由既不具有压力又不具有温度的分子组成。◆ 克雷克指出了知识型智能体的3个关键步骤：（1）刺激必须转化为一种内在表示；（2）认知过程处理表示，从而产生新的内部表示；（3）这些过程反过来又被重新转化为行为。◆ 继1945年克雷克死于自行车事故之后，唐纳德·布劳德本特（Donald Broadbent）继续从事这一工作。布劳德本特的Perception and Communication（Broadbent, 1958）是最早将心理现象建模为信息处理的著作之一。与此同时的美国，计算机建模的发展导致了认知科学（cognitive science）领域的诞生。这个领域可以说是开始于1956年9月麻省理工学院的一次研讨会上，并且仅仅两个月后，人工智能本身就“诞生”了。◆ 现在心理学家普遍认为“认知理论应该就像一个计算机程序”（Anderson, 1980），也就是说，认知理论应该从信息处理的角度来描述认知功能的运作。◆ 为了综述目的，我们将人机交互（human-computer interaction，HCI）领域归于心理学下。人机交互的先驱之一道格·恩格巴特（Doug Engelbart）倡导智能增强（intelligence augmentation）的理念（IA而非AI）。他认为，计算机应该增强人类的能力，而不是完全自动化人类的任务。◆ 今天，我们更倾向于将IA和AI视为同一枚硬币的两面，前者强调人类控制，而后者强调机器的智能行为，都是机器有利于人类所必需的。◆ 现代数字电子计算机是由陷入第二次世界大战中的3个国家的科学家们独立且几乎同时发明的。第一台可操作的计算机是由艾伦·图灵的团队于1943年建造的机电希思·罗宾逊（Heath Robinson[13]），它的唯一目的是破译德国的情报。◆ 从那时起，每一代计算机硬件更新都带来了速度和容量的提升以及价格的下降，这是摩尔定律（Moore’s law）所描述的趋势。直到2005年之前，大约每18个月CPU的性能就会翻一番，但功耗问题导致制造商开始增加CPU的核数而不是提高CPU的时钟频率。目前的预期是，未来性能的增加将来自于大量的并行性，这体现了与大脑特性奇妙的一致性。在应对不确定的世界时，基于这一理念设计硬件：不需要64位的数字精度，只需16位（如bfloat16格式）甚至8位就足够了，这可以使处理速度更快。◆ 已经出现了一些针对人工智能应用进行调整的硬件，如图形处理单元（GPU）、张量处理单元（TPU）和晶圆级引擎（WSE）。从20世纪60年代到大约2012年，用于训练顶级机器学习应用的计算能力遵循了摩尔定律。◆ 尽管量子计算（quantum computing）还不实用，但它有望为人工智能算法的一些重要子方向提供更显著的加速。◆ 巴贝奇的同事埃达·洛芙莱斯（Ada Lovelace，诗人拜伦勋爵的女儿）理解了计算机的潜力，将其描述为“一种能思考或者……能推理的机器”，能够对“宇宙中所有事物”进行推理（Lovelace, 1843）。她还预测到了人工智能的技术成熟度曲线，并提出：“我们最好防范可能夸大分析机能力的想法。”遗憾的是，巴贝奇的机器和洛芙莱斯的思想已基本被遗忘了。◆ 人工智能还得益于计算机科学软件方面的发展，后者提供了编写现代程序所需的操作系统、编程语言和工具（以及有关它们的论文）。而这也是人工智能对其有回馈的领域：人工智能工作开创的许多想法正重归主流计算机科学，包括分时、交互式解释器、使用窗口和鼠标的个人计算机、快速开发环境、链表数据类型、自动存储管理，以及符号式编程、函数式编程、说明性编程和面向对象编程的关键概念。◆ 居住在亚历山大城的古希腊工程师克特西比乌斯（Ktesibios，约公元前250年）建造了第一个自我控制的机器：一台水钟，其特点是拥有一个可以保持恒定水流速度的调节器。这一发明改变了人造物可以做什么的定义。在此之前，只有生物才能根据环境的变化来改变自己的行为。◆ 第二次世界大战后，控制理论（control theory）发展的核心人物是诺伯特·维纳（Norbert Wiener，1894—1964）。维纳是一位杰出的数学家，在对生物和机械控制系统及其与认知的联系产生兴趣之前，曾与伯特兰·罗素等人合作。◆ 维纳的《控制论》（Cybernetics）（Wiener, 1948）成为畅销书，使大众意识到了人工智能机器的可能性。◆ 艾什比在《大脑设计》（Design for a Brain）（Ashby, 1948, 1952）一书中详细阐述了他的想法，即可以通过自我平衡（homeostatic）设备来实现智能，该设备使用恰当的反馈回路来实现稳定的自适应行为。◆ 现代控制理论，特别是被称为随机最优控制的分支，其目标是设计随时间最小化代价函数（cost function）的系统。这与人工智能的标准模型——设计性能最优的系统大致相符。尽管人工智能和控制理论的创始人之间有着密切的联系，为什么它们却是两个不同的领域呢？答案在于参与者所熟悉的数学技术与每种世界观所包含的对应问题是紧密结合的。微积分和矩阵代数是控制理论的工具，它们适用于固定的连续变量集描述的系统，而人工智能的建立在一定程度上是为了避开这些可感知的局限性。逻辑推理和计算工具使人工智能研究人员能够考虑语言、视觉和符号规划等问题，而这些问题完全超出了控制理论家的研究范围。◆ 1957年，斯金纳（B. F. Skinner）发表了Verbal Behavior，包含该领域最著名的专家对语言学习的行为主义方法的全面详细的描述。但奇怪的是，一篇对这本书的评述也像这本书一样广为人知，几乎扼杀了大众对行为主义的兴趣。评述的作者是语言学家诺姆·乔姆斯基，彼时他刚刚出版了一本关于他自己理论的书《句法结构》（Syntactic Structure）。◆ 现代语言学和人工智能几乎同时“诞生”，并一起成长，交叉于一个称为计算语言学（computational linguistics）或自然语言处理（natural language processing）的混合领域。相比1957年，理解语言复杂了许多。理解语言需要理解主题和上下文，而不仅仅是理解句子结构。这似乎是显而易见的，但直到20世纪60年代才得到广泛认可。1.3 人工智能的历史◆ 总结人工智能历史里程碑的快速方法是列出图灵奖得主◆ 现在普遍认为由沃伦·麦卡洛克和沃尔特·皮茨（McCulloch and Pitts, 1943）完成的工作是人工智能的第一项研究工作。他们受到皮茨的顾问尼古拉斯·拉舍夫斯基（Nicolas Rashevsky）（1936, 1938）对数学建模工作的启发，选择了3方面的资源构建模型：基础生理学知识和大脑神经元的功能，罗素和怀特海（Whitehead）对命题逻辑的形式化分析，以及图灵的计算理论。◆ 麦卡洛克和皮茨还表明适当定义的网络可以学习。唐纳德·赫布（Donald Hebb）（Hebb, 1949）示范了用于修改神经元之间连接强度的简单更新规则。他的规则，现在称为赫布型学习（Hebbian learning），至今仍是一种有影响力的模式。◆ 哈佛大学的两名本科生马文·明斯基（Marvin Minsky，1927—2016）和迪安·埃德蒙兹（Dean Edmonds）在1950年建造了第一台神经网络计算机——SNARC。SNARC使用了3000个真空管和B-24轰炸机上一个多余的自动驾驶装置来模拟由40个神经元组成的网络。后来，明斯基在普林斯顿大学研究了神经网络中的通用计算。他的博士学位委员会对这类工作是否应该被视为数学持怀疑态度，但据说冯·诺伊曼评价：“如果现在还不能被视为数学，总有一天会的。”◆ 还有许多早期工作可以被描述为人工智能，包括1952年由曼彻斯特大学的克里斯托弗·斯特雷奇（Christopher Strachey）和IBM公司的亚瑟·塞缪尔（Arthur Samuel）分别独立开发的西洋跳棋程序。然而，还是图灵的观点最有影响力。◆ 早在1947年，他就在伦敦数学协会（London Mathematical Society）就这一主题发表了演讲，并在其1950年的文章“Computing Machinery and Intelligence”中阐明了有说服力的议程。在论文中，他介绍了图灵测试、机器学习、遗传算法和强化学习。◆ 1955年，达特茅斯学院的约翰·麦卡锡说服明斯基、克劳德·香农（Claude Shannon）和纳撒尼尔·罗切斯特（Nathaniel Rochester）帮助他召集对自动机理论、神经网络和智能研究感兴趣的美国研究人员。他们于1956年夏天在达特茅斯组织了为期两个月的研讨会。◆ 纽厄尔和西蒙继LT成功之后又推出了通用问题求解器，即GPS。与LT不同，GPS从一开始就被设计为模仿人类求解问题的协议。结果表明，在它可以处理的有限类型的难题中，该程序考虑的子目标和可能采取的行为的顺序与人类处理相同问题的顺序类似。因此，GPS可能是第一个体现“人类思维”方式的程序。作为认知模型，GPS和后续程序的成功使得纽厄尔和西蒙（1976）提出了著名的物理符号系统（physical symbol system）假说，该假说认为“物理符号系统具有进行一般智能动作的必要和充分方法”。◆ 在IBM，纳撒尼尔·罗切斯特和他的同事开发了首批人工智能程序。赫伯特·盖伦特（Herbert Gelernter）（Gelernter, 1959）构造了几何定理证明程序（Geometry Theorem Prover），它能够证明许多数学学生认为相当棘手的定理。这项工作是现代数学定理证明程序的先驱。◆ 从长远来看，这一时期所有探索性工作中，最有影响力的可能是亚瑟·萨缪尔对西洋跳棋的研究。通过使用现在称之为强化学习的方法（见第22章），萨缪尔的程序可以以业余高手的水平进行对抗。因此，他驳斥了计算机只能执行被告知的事情的观点：他的程序很快学会了玩游戏，甚至比其创造者玩得更好。该程序于1956年在电视上演示，给人留下了深刻的印象。◆ 萨缪尔的程序是许多后继系统的前身，如TD-Gammon（Tesauro, 1992）和AlphaGo（Silver et al., 2016）。TD-Gammon是世界上最好的西洋双陆棋棋手之一，而AlphaGo因击败人类世界围棋冠军而震惊世界（见第5章）。◆ 1958年，约翰·麦卡锡为人工智能做出了两项重要贡献。在麻省理工学院人工智能实验室备忘录1号中，他定义了高级语言Lisp，Lisp在接下来的30年中成为了最重要的人工智能编程语言。在一篇题为“Programs with Common Sense”的论文中，麦卡锡为基于知识和推理的人工智能系统提出了概念性议案。◆ 在麻省理工学院，明斯基指导了一批学生，他们选择了一些似乎需要智能才能求解的有限问题。这些有限的领域被称为微世界（microworld）。◆ 最著名的微世界是积木世界（blocks world），由一组放置在桌面上的实心积木组成（或者更常见的是模拟桌面）◆ 建立在麦卡洛克和皮茨提出的神经网络上的早期工作也蓬勃发展。◆ 赫布的学习方法分别得到了伯尼·维德罗（Bernie Widrow）（Widrow and Hoff, 1960; Widrow, 1962）和弗兰克·罗森布拉特（Frank Rosenblatt）（Rosenblatt, 1962）的改进，他们的网络分别被称为线性自适应神经网络（adaline）和感知机（perceptron）。感知机收敛定理（perceptron convergence theorem）（Block et al., 1962）指出，学习算法可以调整感知机的连接强度来拟合任何输入数据（前提是存在这样的拟合）。◆ 当初西蒙的过度自信来自于早期人工智能系统在简单示例任务上的出色表现。但是，在几乎所有情况下，这些早期系统在更困难的问题上都失败了。◆ 失败有两个主要原因。第一个主要原因是许多早期人工智能系统主要基于人类如何执行任务的“知情内省型”，而不是基于对任务、解的含义以及算法需要做什么才能可靠地产生解的仔细分析。第二个主要原因是对人工智能要求解的问题的复杂性缺乏认识。大多数早期的问题求解系统都会尝试组合不同的步骤，直到找到解为止。这一策略最初奏效是因为微世界所包含的对象非常少，因此可能的动作非常少，解的动作序列也非常短。在计算复杂性理论发展完备之前，人们普遍认为“扩展”到更大的问题仅仅是需要更快的硬件和更大的内存。但是当研究人员无法证明涉及几十个事实的定理时，伴随着归结定理证明发展而来的乐观情绪很快就受到了打击。一般而言，程序可以找到解的事实并不意味着该程序具备任何在实践中找到解所需的机制。◆ 未能处理“组合爆炸”是莱特希尔报告（Lighthill, 1973）中对人工智能的主要批评之一，基于这份报告，英国政府决定在除两所大学外的所有大学中停止支持人工智能研究。（口述传说描绘了一幅稍有不同、更加丰富多彩的画面，但带有政治野心和个人好恶的描述都不是本书的话题。）◆ 讽刺的是，在20世纪80年代和21世纪10年代再次引起神经网络研究巨大复兴的新反向传播学习算法，早在20世纪60年代初已经在其他情景下得到了发展（Kelley, 1960; Bryson, 1962）。◆ 在人工智能研究的前十年提出的问题求解是一种通用搜索机制，试图将基本的推理步骤串在一起，找到完整的解。这种方法被称为弱方法（weak method），这种方法虽然很普适，但它不能扩展到大型或困难的问题实例上。弱方法的替代方案是使用更强大的领域特定的知识，这些知识允许更大规模的推理步骤，并且可以更轻松地处理特定专业领域中发生的典型案例。有人可能会说，必须已经差不多知道答案才能解决一个难题。◆ 认识到分子包含特定的子结构，可以极大地减少可能候选项的量级。据作者称，Dendral之所以强大，是因为它不是以第一性原理的形式，而是以高效“食谱”的形式体现了质谱的相关知识（Feigenbaum et al., 1971）。Dendral的意义在于它是第一个成功的知识密集型系统：它的专业知识来源于大量专用规则。1971年，费根鲍姆和斯坦福大学的其他研究人员开启了启发式编程项目（heuristic programming project，HPP），以此来研究专家系统（expert system）的新方法可以在多大程度上应用到其他领域。◆ 第一个成功的商用专家系统R1在数字设备公司（Digital Equipment Corporation，DEC）投入使用（McDermott, 1982），该程序帮助公司配置新计算机系统的订单。截至1986年，它每年为公司节省约4000万美元。◆ 到1988年，DEC的人工智能小组已经部署了40个专家系统，而且还有更多的专家系统在开发中。同时期，杜邦公司有100个专家系统在使用，500个在开发。当时几乎每家美国大公司都有自己的人工智能团队，不是在使用专家系统，就是在研究专家系统。◆ 在真实世界中的广泛应用引发了表示和推理工具的广泛发展。有些是基于逻辑的，例如，Prolog语言在欧洲和日本流行，而Planner家族在美国流行。其他人则遵循明斯基的框架（frame）思想（Minsky, 1975），采用了一种更结构化的方法，将有关特定对象和事件类型的事实组合起来，并将这些类型组织成类似于生物分类法的大型分类层次结构。◆ 总的来说，人工智能行业从1980年的几百万美元增长到1988年的数十亿美元，还产生了数百家构建专家系统、视觉系统、机器人以及专门服务于这些目的的软硬件的公司。但此后不久，经历了一段被称为“人工智能冬天”的时期，许多公司因未能兑现夸张的承诺而停滞。事实证明，为复杂领域构建和维护专家系统是困难的，一部分原因是系统使用的推理方法在面临不确定性时会崩溃，另一部分原因是系统无法从经验中学习。◆ 在20世纪80年代中期，至少有4个不同的团队重新发明了最早在20世纪60年代初期发展起来的反向传播（back-propagation）学习算法。该算法被应用于计算机科学和心理学中的许多学习问题，Parallel Distributed Processing合集（Rumelhart and McClelland, 1986）中的结果的广泛传播引起了极大的轰动。◆ 这些所谓的联结主义（connectionist）模型被一些人视为纽厄尔和西蒙的符号模型以及麦卡锡和其他人的逻辑主义方法的直接竞争对手。◆ 与此相反，20世纪80年代和21世纪10年代神经网络复兴的领军人物杰弗里·辛顿将符号描述为“人工智能的光以太”（19世纪许多物理学家认为电磁波传播的介质是光以太，但其实这种介质不存在）。◆ 专家系统的脆弱性导致了一种新的、更科学的方法，结合了概率而不是布尔逻辑，基于机器学习而不是手工编码，重视实验结果而不是哲学主张。[20]现在更普遍的是，基于现有理论而不是提出全新的理论，基于严格的定理或可靠的实验方法（Cohen, 1995）而不是基于直觉的主张，以及展示与真实世界应用的相关性而不是虚拟的示例。◆ 共享的基准问题集成为了展示进度的标准，包括加利福尼亚大学欧文分校的机器学习数据集库、用于规划算法的国际规划竞赛、用于语音识别的LibriSpeech语料库、用于手写数字识别的MNIST数据集、用于图像物体识别的ImageNet和COCO、用于自然语言问答的SQuAD、机器翻译的WMT竞赛以及布尔可满足性求解器国际SAT竞赛。◆ 人工智能的创立在一定程度上是对控制理论和统计等现有领域局限性的反抗，但在这一时期，它吸纳了这些领域的积极成果。◆ 正如戴维·麦卡莱斯特（David McAllester）（McAllester, 1998）所说：在人工智能早期，符号计算的新形式（例如框架和语义网络）使大部分经典理论过时，这似乎是合理的。这导致了一种孤立主义，即人工智能在很大程度上与计算机科学的其他领域分离。这种孤立主义目前正在被摒弃。人们认识到，机器学习不应该独立于信息论，不确定推理不应该独立于随机建模，搜索不应该独立于经典优化和控制，自动推理不应该独立于形式化方法和静态分析。◆ 在20世纪80年代，使用隐马尔可夫模型（hidden Markov model，HMM）的方法开始主导这一领域。HMM有两个相关的方面。首先，它们基于严格的数学理论。这使得语音研究人员能够在其他领域数十年数学成果的基础上进行开发。其次，它们是在大量真实语音数据的语料库上训练而产生的。这确保了健壮性，并且在严格的盲测中，HMM的分数稳步提高。因此，语音技术和手写体字符识别的相关领域向广泛的工业和消费级应用过渡。◆ 1988年是人工智能与统计学、运筹学、决策论和控制理论等其他领域相联系的重要一年。朱迪亚·珀尔的Probabilistic Reasoning in Intelligent Systems（Pearl, 1988）使概率和决策论在人工智能中得到了新的认可。珀尔对贝叶斯网络的发展产生了一种用于表示不确定的知识的严格而有效的形式体系，以及用于概率推理的实用算法。◆ 1988年的第二个主要贡献是理查德·萨顿（Rich Sutton）的工作，他将强化学习（20世纪50年代被用于亚瑟·塞缪尔的西洋跳棋程序中）与运筹学领域开发的马尔可夫决策过程（Markov decision processe，MDP）联系起来。随后，大量工作将人工智能规划研究与MDP联系起来，强化学习领域在机器人和过程控制方面找到了应用，并获得了深厚的理论基础。◆ 人工智能对数据、统计建模、优化和机器学习的新认识带来的结果是，计算机视觉、机器人技术、语音识别、多智能体系统和自然语言处理等子领域逐渐统一，此前这些子领域在某种程度上已经脱离了核心人工智能。重新统一的过程在应用方面（例如，在此期间实用机器人的部署大大扩展）和关于人工智能核心问题更好的理论理解方面都产生了显著的效用。◆ 计算能力的显著进步和互联网的创建促进了巨大数据集的创建，这种现象有时被称为大数据（big data）。这些数据集包括数万亿字的文本、数十亿的图像、数十亿小时的语音和视频，以及海量的基因组数据、车辆跟踪数据、点击流数据、社交网络数据等。◆ 然而，如果有足够大的数据集，合适的学习算法在识别句意的任务上可以达到超过96%的准确率。此外，班科和布里尔认为，将数据集的规模增加两到三个数量级所获得的性能提升会超过调整算法带来的性能提升（Banko and Brill, 2001）。◆ 他们发现，该技术在仅包含数千幅图像的数据库中效果不佳，但在拥有数百万幅图像的数据库中，该技术超过了质量阈值。不久之后，ImageNet数据库（Deng et al., 2009）中可用的数千万幅图像引发了计算机视觉领域的一场革命。◆ 大数据的可用性和向机器学习的转变帮助人工智能恢复了商业吸引力（Havenstein, 2005; Halevy et al., 2009）。大数据是2011年IBM的Watson系统在《危险边缘》（Jeopardy!）问答游戏中战胜人类冠军的关键因素，这一事件深深影响了公众对人工智能的看法。◆ 深度学习（deep learning）是指使用多层简单的、可调整的计算单元的机器学习。早在20世纪70年代，研究人员就对这类网络进行了实验，并在20世纪90年代以卷积神经网络（convolutional neural network）（LeCun et al., 1995）的形式在手写数字识别方面取得了一定的成功。然而，直到2011年，深度学习方法才真正开始流行起来，首先是在语音识别领域，然后是在视觉物体识别领域。◆ 从那时起，深度学习系统在某些视觉任务上的表现超过了人类，但在其他一些任务上还显落后。在语音识别、机器翻译、医疗诊断和博弈方面也有类似的进展。AlphaGo（Silver et al., 2016, 2017, 2018）之所以能够战胜人类顶尖的围棋棋手，是因为它使用了深度网络来表示评价函数。◆ 这些非凡的成功使学生、公司、投资者、政府、媒体和公众对人工智能的兴趣重新高涨。似乎每周都有新的人工智能应用接近或超过人类表现的消息，通常伴随着加速成功或人工智能新寒冬的猜测。1.4 目前的先进技术◆ 斯坦福大学的人工智能百年研究（也称为AI100）召集了专家小组来提供人工智能最先进技术的报告。◆ AI100还在其网站上创建了一个人工智能指数（AI Index），以帮助跟踪人工智能的进展。◆ 出版物：人工智能论文数量在2010年至2019年间增长了20倍，达到每年约2万篇。最受欢迎的类别是机器学习（2009年至2017年，http://arXiv.org上的机器学习论文数量每年都会翻一番）。其次是计算机视觉和自然语言处理。◆ 国际化：中国每年发表的论文多于美国，与整个欧洲一样多。但是，在引用加权影响方面，美国作者领先中国作者50%。从人工智能招聘人数看，新加坡、巴西、澳大利亚、加拿大和印度是增长最快的国家。◆ 视觉：物体检测的错误率（大规模视觉识别挑战，LSVRC）从2010年的28%下降到2017年的2%，超过了人类的表现。自2015年以来，开放式视觉问答（VQA）的准确率从55%提高到68%，但仍远落后于人类83%的表现。◆ 人类基准：截至2019年，人工智能系统在多个领域达到或超越人类表现，包括国际象棋、围棋、扑克、《吃豆人》（Pac-Man）、《危险边缘》（Jeopardy!）、ImageNet物体检测、有限域中的语音识别、约束域中的英文翻译、《雷神之锤3》（Quake III）、《刀塔2》（Dota 2）、《星际争霸II》（StarCraft II）、Atari的各种游戏、皮肤癌检测、前列腺癌检测、蛋白质折叠、糖尿病视网膜病变诊断等。◆ 人工智能系统何时（如果可以的话）能够在各种任务中达到人类水平的表现？马丁·福特（Martin Ford）（Ford, 2018）通过对人工智能专家的访谈发现这一目标时间的范围很广，从2029年到2200年，均值为2099年。◆ 未来的人工智能系统将如何运作？我们还不能确定。正如本节所详述的，这个领域采用了几个关于它本身的故事：首先是一个大胆的想法，即机器的智能是可能的，然后是它可以通过将专家知识编码成逻辑来实现，接着是建模世界的概率模型将成为主要工具，以及最近的机器学习将产生可能根本不基于任何易于理解的理论的模型。未来将揭示接下来会出现什么模式。◆ 人工智能现在能做什么？也许不像一些更乐观的媒体文章让人相信的那样多，但仍然很多，以下是一些例子。◆ 类人机器人Atlas不仅能在崎岖不平的路况中行走，还可以跳到箱子上，做后空翻后可以稳定落地（Ackerman and Guizzo, 2016）。◆ 现在，Europa规划工具包（Barreiro et al., 2012）被用于NASA火星探测器的日常操作，而Sextant系统（Winternitz, 2017）允许航天器在全球GPS系统之外进行深空自主导航。◆ 每天，优步（Uber）等网约车公司和谷歌地图等地图服务为数亿用户提供行车向导，在考虑当前和预测未来交通状况的基础上快速规划最佳路线。◆ 机器翻译：在线机器翻译系统现在可以阅读超过100种语言的文档，涵盖99%的人类使用的母语，每天为数亿用户翻译数千亿词语。虽然翻译结果还不完美，但通常足以理解。对于具有大量训练数据的密切相关的语言（如法语和英语），在特定领域内的翻译效果已经接近于人类的水平（Wu et al., 2016b）。◆ 语音识别：2017年，微软表示其会话语音识别系统的单词错误率已降至5.1%，与人类在Switchboard任务（转录电话对话）中的表现相当（Xiong et al., 2017）。现在全世界大约三分之一的计算机交互是通过语音而不是键盘完成的，另外Skype提供了10种语言的实时语音翻译。Alexa、Siri、Cortana和谷歌都提供了可以回答用户问题和执行任务的助手。例如，谷歌Duplex服务使用语音识别和语音合成为用户预订餐厅，它能够代表用户进行流畅的对话。◆ 推荐：Amazon、Facebook、Netflix、Spotify、YouTube、Walmart等公司利用机器学习技术，根据用户过去的经历和其他类似的人群为用户推荐可能喜欢的内容。◆ 垃圾邮件过滤也可以被认为是推荐（或不推荐）的一种形式。目前的人工智能技术可以过滤掉99.9%以上的垃圾邮件，电子邮件服务还可以推荐潜在收件人以及可能回复的文本。◆ 博弈：1997年，当“深蓝”（Deep Blue）击败国际象棋世界冠军加里·卡斯帕罗夫（Garry Kasparov）后，人类霸权的捍卫者把希望寄托在了围棋上。当时，天体物理学家、围棋爱好者皮特·赫特（Piet Hut）预测称：“计算机在围棋上击败人类需要一百年的时间（甚至可能更久）。”但仅仅20年后，AlphaGo就超过了所有人类棋手（Silver et al., 2017）。世界冠军柯洁说：“去年的AlphaGo还比较接近于人，现在它越来越像围棋之神。”AlphaGo得益于对人类棋手过去数十万场棋局的研究以及对团队中围棋专家的知识提炼。◆ 后继项目AlphaZero不再借助人类输入，只通过游戏规则就能够自我学习并击败所有对手，在围棋、国际象棋和日本将棋领域击败了包括人类和机器在内的对手（Silver et al., 2018）。◆ 图像理解：计算机视觉研究人员不再满足于在具有挑战性的ImageNet物体识别任务上超越人类的准确性，他们开始研究更困难的图像描述问题。◆ 医学：现在，人工智能算法在多种疾病的诊断方面（尤其是基于图像的诊断）已经达到或超过了专家医生的水平。◆ 目前，限制这些技术推广的不是诊断准确性，而是需要证明临床结果的改善，并确保透明度、无偏见和数据隐私（Topol, 2019）。2017年，只有两项医疗人工智能应用获得FDA批准，但这一数字在2018年增至12项，并在持续上升。◆ 气候科学：一个科学家团队凭借深度学习模型获得了2018年戈登·贝尔奖，该模型发现了之前隐藏在气候数据中的极端天气事件的详细信息。◆ 这些只是几个目前存在的人工智能系统的例子。这不是魔法或科幻小说，而是科学、工程和数学，本书将对此进行介绍。1.5 人工智能的风险和收益◆ 弗朗西斯·培根是一位被誉为创造科学方法的哲学家，他在《论古人的智慧》（The Wisdom of the Ancients）（1609）一书中指出：“机械艺术的用途是模糊的，它既可用于治疗，也可用于伤害。”随着人工智能在经济、社会、科学、医疗、金融和军事领域发挥越来越重要的作用，我们应该考虑一下它可能带来的伤害和补救措施——用现代的说法，就是风险和收益。◆ 简而言之，我们的整个文明是人类智慧的产物。如果我们有机会获得更强大的机器智能，我们的理想上限就会大大提高。人工智能和机器人技术可以将人类从繁重的重复性工作中解放出来，并大幅增加商品和服务的生产，这可能预示着一个和平富足的时代的到来。加速科学研究的能力可以治愈疾病，并解决气候变化和资源短缺问题。正如谷歌DeepMind首席执行官德米斯·哈萨比斯（Demis Hassabis）所建议的那样：“首先解决人工智能问题，然后再用人工智能解决其他所有问题。”◆ 然而，早在我们有机会“解决人工智能”之前，我们就会因误用人工智能而招致风险，无论这是无意的还是其他原因。其中一些风险已经很明显，而另一些似乎基于当前趋势。◆ 就业影响：关于机器会减少工作岗位的担忧由来已久。故事从来都不是简单的。◆ 随着人工智能系统变得越来越强大，它们将更多承担以前由人类扮演的社会角色。正如人类过去曾利用这些角色作恶一样，可以预见，人类可能会在这些角色中滥用人工智能系统而作恶更多。上面给出的所有例子都指出了治理的重要性，以及最终监管的重要性。◆ 长期来看呢？我们能否实现长期以来的目标：创造出与人类智力相当或更强大的智能？如果我们做到了，然后呢？◆ 大约在同一时间，人们担心创造远远超过人类能力的超级人工智能（artificial superintelligence，ASI）可能是个坏主意（Yudkowsky, 2008; Omohundro, 2008）。图灵（Turing, 1996）在1951年曼彻斯特的一场演讲中也提出了同样的观点，他借鉴了塞缪尔·巴特勒（Samuel Butler）（Butler, 1863）的早期观点◆ 对创造超级智能机器的想法产生普遍的不安感是自然的。我们可以称之为大猩猩问题（gorilla problem）：大约700万年前，一种现已灭绝的灵长类进化了，一个分支进化为大猩猩，另一个分支进化为人类。今天，大猩猩对人类分支不太满意，大猩猩根本无法控制自己的未来。◆ 如果超级人工智能是一个来自外太空的黑匣子，那么谨慎地打开这个黑匣子确实是明智之举。但事实并非如此：我们设计了人工智能系统，所以如果它们最终“掌控了自己”，那将是设计失败的结果（正如图灵所说）。◆ 许多文化都有关于人类向神灵、精灵、魔术师或魔鬼索取东西的神话。在这些故事中，他们总是得到了他们真正想要的东西并最终后悔。如果还有第三个愿望的话，那就是撤销前两个。我们将其称为迈达斯国王问题（King Midas problem）：迈达斯是希腊神话中的传奇国王，他要求他所接触的一切都变成黄金，但他在接触了他的食物、饮料和家人后，就后悔了。◆ 遗憾的是，迄今为止，几乎所有的人工智能研究都是在标准模型下进行的，这意味着这版书中几乎所有的技术材料都反映了这一知识框架。然而，在新框架内已经有一些初步成果。◆ 它在数学上描述了一种情况，即人类有一个目标而机器试图实现它，但最初不确定目标是什么。在第22章中，我们解释逆向强化学习（inverse reinforcement learning）的方法，它允许机器通过观察人类的选择来更多地了解人类的偏好。小结◆ 人工智能的历史经历了成功、盲目乐观以及由此导致的热情丧失和资金削减的循环，也存在引入全新创造性的方法和系统地改进最佳方法的循环。◆ 从长远来看，我们面临着控制超级智能的人工智能系统的难题，它们可能以不可预测的方式进化。解决这个问题似乎需要改变我们对人工智能的设想。"
695937298,908af4e19c158f8787a831394a50f891,无意,https://api.zhihu.com/people/908af4e19c158f8787a831394a50f891,people,,你必须尝试的5大人工智能编程助手,article,https://api.zhihu.com/articles/695937298,"Codex语言模型驱动,可以提供各种智能建议。 2. Codeium - 一款免费的基于<em>人工智能</em>的代码自动补全工具,提供上下文感知的建议。 3. Cody - 一款AI驱动的编码助手",1,0,0,1714832342,1714832342,"什么是人工智能编码助手?它们有什么作用?人工智能编码助手已成为软件开发过程中不可或缺的一部分,它们可以协助代码生成、理解、项目搜索以及使用提示或代码执行各种任务。即使是像Google Colab和Deepnote这样的云IDE平台,也提供了人工智能辅助功能。这些助手可以帮助开发人员提高工作效率,更快地编写出高质量的代码。你认为哪5款人工智能编码助手是最值得尝试的?GitHub Copilot - 被誉为有史以来最先进的代码助手工具,由OpenAI Codex语言模型驱动,可以提供各种智能建议。2. Codeium - 一款免费的基于人工智能的代码自动补全工具,提供上下文感知的建议。3. Cody - 一款AI驱动的编码助手,提供聊天、命令、代码解释和自动完成等功能。4. Code GPT - 集成了大型语言模型,提供AI聊天辅助、代码解释、错误检查等功能。5. Tabnine - 一款以神经网络为基础的人工智能编码助手,可以提高开发效率和代码质量。这些人工智能编码助手各有什么特点和优缺点?GitHub Copilot是最先进的工具,但需要付费使用。Codeium功能丰富且免费,但自动完成不太准确。Cody在某些场景下表现不佳,Code GPT虽然功能强大但存在一些性能问题。Tabnine的免费版本也很不错,但付费版本可以提供更好的体验。总的来说,这些人工智能编码助手各有优缺点,开发人员可以根据自己的需求和预算选择合适的工具。通过合理利用这些助手,可以大幅提高工作效率和代码质量。关于选择你有什么其它建议吗?我的建议是,开发人员可以尝试将这些人工智能编码助手集成到自己的工作流程中,并根据实际使用情况选择最适合自己的工具。通过合理利用这些助手,开发人员可以节省大量时间,专注于更有价值的编码工作,从而提高整体的软件开发效率。"
695898199,c0b83fb2e2c1fce35e51c2e81aabe0bb,xuefubaoku,https://api.zhihu.com/people/c0b83fb2e2c1fce35e51c2e81aabe0bb,people,相遇非偶然，共鸣即觉醒。,小白也能听懂的人工智能原理,article,https://api.zhihu.com/articles/695898199,就让我们一起揭开<em>人工智能</em>的神秘面纱，即使你是零基础的小白，也能轻松理解AI的基本原理，并探讨学习AI的适合人群。 一、<em>人工智能</em>的核心：机器学习 要说到人工智能，就不得不提及其核心——机器学习,1,0,0,1714808541,1714832918,在这个智能科技日新月异的时代，人工智能（AI）已经不再是遥不可及的高科技，而是逐渐融入了我们日常生活的方方面面。你或许已经在使用智能手机、智能家居，享受着AI技术带来的便利。但你是否曾好奇过，这些看似神奇的智能技术背后，究竟隐藏着怎样的原理？今天，就让我们一起揭开人工智能的神秘面纱，即使你是零基础的小白，也能轻松理解AI的基本原理，并探讨学习AI的适合人群。一、人工智能的核心：机器学习要说到人工智能，就不得不提及其核心——机器学习。机器学习是一种能够让计算机系统从数据中“学习”并自动改进的技术。简单来说，就是通过向计算机展示大量示例，并告诉它在这些示例中寻找模式，然后计算机就能够利用这些模式来对新的数据进行预测或分类。举个例子，假设我们要训练一个图像识别系统来识别猫和狗。我们可以给计算机提供大量的猫和狗的图片，并标记出哪些是猫，哪些是狗。通过机器学习算法，计算机会逐渐学习到猫和狗的特征，最终能够自动识别出新的图片中的动物是猫还是狗。二、深度学习与神经网络在机器学习中，深度学习是近年来备受瞩目的一个分支。深度学习通过模拟人脑神经网络的工作方式来处理和分析数据。神经网络由大量的节点（神经元）相互连接而成，每个节点都接收输入信号，并根据这些信号产生输出。深度学习的关键在于这些神经网络的层次可以非常深，有时甚至可以包含数十甚至上百层。每一层都会对输入数据进行更高层次的抽象表示，从而使得深度学习模型能够处理非常复杂的任务，如图像识别、语音识别、自然语言处理等。三、零基础如何入门AI？现在，你可能已经对人工智能的基本原理有了一定的了解，那么作为一个零基础的小白，该如何入门AI呢？学习数学基础：AI技术离不开数学，特别是线性代数、概率论和统计学等。这些数学工具在机器学习和深度学习中发挥着重要作用。你可以通过观看在线课程、阅读教材或使用一些数学学习应用来提高你的数学能力。学习编程语言：为了实践和应用AI技术，你需要学习一门编程语言。Python是目前最流行的AI编程语言，它易于学习且具有丰富的科学计算库，如NumPy、Pandas和SciPy等。了解机器学习框架：TensorFlow和PyTorch是目前最受欢迎的机器学习框架。这些框架提供了大量的预训练模型和高级API，可以帮助你更快地构建和训练AI模型。动手实践：理论知识很重要，但实践才是检验真理的唯一标准。通过参与实际项目，如图像分类、自然语言处理等，你可以更深入地理解AI技术的实际应用。参与社区交流：加入AI相关的社区和论坛，与其他学习者交流心得和经验，可以帮助你更快地成长。四、适合学习AI的人群对科技和创新感兴趣的人：如果你对科技和创新充满热情，想要了解未来科技的发展趋势，那么学习AI将是一个不错的选择。数据科学家和工程师：对于已经在数据科学或工程领域工作的人来说，掌握AI技术将大大提升他们的职业技能和竞争力。学生和研究者：对于计算机科学、数学、物理等相关专业的学生和研究者来说，AI是一个重要的研究方向，掌握AI原理将有助于他们更深入地理解这个领域。创业者和投资者：随着AI技术的不断发展，越来越多的创业机会和投资领域正在涌现。了解AI将有助于创业者和投资者更好地把握市场机遇。五、课程目录：宣导片：来一场人工智能的奇妙冒险吧~.mp41.一元一次函数感知器：如何描述直觉.mp42.方差代价函数：知错.mp43.梯度下降和反向传播：能改（上）.mp44.梯度下降和反向传播：能改（下）.mp45.激活函数：给机器注入灵魂.mp46.隐藏层：神经网络为什么working.mp47.高维空间：机器如何面对越来越复杂的问题.mp48.初识Keras：轻松完成神经网络模型搭建.mp49.深度学习：神奇的DeepLearning.mp410.卷积神经网络：打破图像识别的瓶颈.mp411. 卷积神经网络：图像识别实战.mp412.循环：序列依赖问题.mp413.LSTM网络：自然语言处理实践.mp414.机器学习：最后一节课也是第一节课.mp4课程链接分享：链接: https://pan.baidu.com/s/1r4g33uFubJ_3gxC01IWlJA?pwd=64us提取码: 64us 复制这段内容后打开百度网盘手机App，操作更方便哦 希望可以帮助有需要的人
695891141,fa51ad290f1ff75dc65618df85da7ca1,<em>AI</em>行在路上,https://api.zhihu.com/people/fa51ad290f1ff75dc65618df85da7ca1,people,,当人工智能遭遇小样本问题,article,https://api.zhihu.com/articles/695891141,随着深度学习引领的<em>人工智能</em>技术的不断发展。各行各业尤其是垂直细分领域，越来越面临着一个严峻的问题：在标签数据有限的情况下如何做好机器学习？由此诞生了<em>人工智能</em>领域一个研究热点,1,0,0,1714804666,1714804666,"随着大数据时代的到来，随着深度学习引领的人工智能技术的不断发展。各行各业尤其是垂直细分领域，越来越面临着一个严峻的问题：在标签数据有限的情况下如何做好机器学习？由此诞生了人工智能领域一个研究热点：小样本机器学习（Few-Shot Learning，FSL）。小样本机器学习目标就是使用少量标签数据训练模型，使之具有良好的泛化能力。小样本机器学习的本质是：经验风险最小值点◆不可靠。可以从三个角度考虑解决方法：数据、模型和算法，即从数据角度基于先验知识增加标签数据数量，从模型角度基于先验知识限制假设空间的复杂度，从算法角度基于先验知识设计在假设空间中搜索算法（图1）。数据、模型和算法是相辅相成的。图1 FSL 研究应考虑三因素：数据(左), 模型(中)和算法(右)图片来源 Wang Y，et al，2020. Generalizing from a Few Examples：A Survey on Few-shot Learning[J]. ACM Computing Surveys，53（3）：1-34.数据是机器学习的核心。在机器学习前对数据进行搜集、整理和预处理是必须的，在此基础上进行特征工程、信息挖掘大多数情况下也是必须的，尤其小样本情况下。越是小样本情况下，吃透数据越重要，俗称“好钢用在刀刃上”。对数据进行增广和转换是小样本问题解决策略中最直接见效的方式。当然数据增广不一定都是自身增广，可以借助无标签数据、其它相关数据。无监督学习、半监督学习、主动学习、生成式AI和大模型等技术的迅速发展为数据增广提供了更多的可能，如半监督学习中，标签传播方法和伪标签学习方法一定意义上就是数据增广的例子。从模型角度基于先验知识限制假设空间的复杂度。一种是将样本投影到更易区分不同类别的低维空间，维度更低，特征表示更容易区分不同的类；一种是借助无标签数据或者其它任务数据，联合训练，限制模型的复杂度，即降低搜索空间的范围。基于模型解决小样本的算法主要有多任务学习、部分迁移学习、部分半监督学习、部分生成式模型、基于度量的元学习等。通过先验知识优化假设空间中搜索算法，目的是使得模型获得一个较好的初始权重参数、优化器、学习率等。该类方法包括部分迁移学习、基于优化的元学习方法、自动调优、大模型相关应用等算法。值得一提的是，传统机器学习已经发展了很多优秀的技术，如随机森林、支持向量机、朴素贝叶斯、Adaboost、Xgboost等。对数据和目标来讲，最适合的方法不一定就是复杂的深度学习方法。尤其在标签数据量少的情况下，传统机器学习也是一种不错的选择。传统机器学习相比深度学习有几个优势，1）可以适合较少的数据；2）对硬件及软件环境要求低；3）建模时间短；4）方便分析数据和提取特征，更好的理解数据。小样本机器学习python算法与实践"
26311333,f779703e4bf2e2d5dc5f1ebad5a173d5,张文斌,https://api.zhihu.com/people/f779703e4bf2e2d5dc5f1ebad5a173d5,people,专注认知智能落地应用，助力企业认知升级，让大数据有大智慧,深扒人工智能——现状篇,article,https://api.zhihu.com/articles/26311333,应用技术层包括语音技术、图像技术、语义技术、机器人技术等使机器能听、能看、能想、能说、能动的技术。 产品服务层是指将<em>人工智能</em>技术应用于特定垂直行业而产生的产品或服务，包括自动驾驶、智能安防、智能金融、智能医疗、智能家居、智能机器人等。 在应用技术层，语音技术使机器能听能说,41,0,174,1491961681,1491962216,上一篇《深扒人工智能——历史篇》回顾了人工智能的历史，本篇主要介绍人工智能的现状。我们首先分析产业链的三个层面，接着梳理各大科技巨头在AI领域的布局，最后汇总了各国出台的AI政策。产业链从产业链角度看，我们将人工智能产业分为三个层面：基础技术层、应用技术层和产品服务层。基础技术层包括人工智能芯片、机器学习算法和框架等基础支撑技术。其中机器学习主要包括现在最火的深度学习和其它浅层机器学习。应用技术层包括语音技术、图像技术、语义技术、机器人技术等使机器能听、能看、能想、能说、能动的技术。产品服务层是指将人工智能技术应用于特定垂直行业而产生的产品或服务，包括自动驾驶、智能安防、智能金融、智能医疗、智能家居、智能机器人等。在应用技术层，语音技术使机器能听能说，包括语音识别（使机器能听）、语音合成（使机器能说）等。图像技术使机器能看，包括人脸识别、图像文字识别等。语义技术使机器能想，包括文本分类、信息检索、、机器翻译、问答系统等。机器人技术使机器能动，包括运动学、动力学、导航等。巨头布局人工智能的高速发展，很大程度上得益于各大科技巨头的高度重视和大力推进。科技巨头们在人工智能领域的布局大都比较全面，尤其在应用技术层有许多重合之处，常用的语音、图像、语义技术基本都会自主研发。接下来我们主要从产业链的三个层面梳理各巨头在人工智能领域的布局，并着重关注差异化的方面。GoogleGoogle是全球在人工智能领域投入最大且整体实力最强的公司。2016年4月，Google的CEO Sundar Pichai明确提出将AI优先作为公司大战略。近年来，Google的传奇技术大神Jeff Dean的工作重心都投入在Google大脑项目。Google还吸引了深度学习鼻祖、多伦多大学教授Geoffrey Hinton，计算机视觉专家、斯坦福大学教授李飞飞等顶尖专家加盟。基础技术Google在2011年便推出了分布式深度学习框架DistBelief，2015年开源第二代深度学习框架TensorFlow。TensorFlow是目前最受关注的深度学习框架。Google还为其研发了专用芯片TPU，将性能提高了一个数量级。Google云平台基于TensorFlow提供了云端机器学习引擎。应用技术Google云平台提供了自然语言、语音、翻译、视觉、视频智能等常用应用技术接口。产品服务早在2009年，Google便启动了无人驾驶汽车项目。2016年12月该项目分拆为一家独立的公司Waymo。目前Google无人驾驶汽车测试里程已经突破200万英里，但由于真实路况的复杂性以及法律风险，无人驾驶汽车距大规模上路还有很长一段距离。2014年10月，Google推出Gmail的进化版——Inbox，邮件可以被自动归类到旅行、财务、新闻资讯等类别。2015年5月发布Google Photos，可以对照片自动识别、分类，并支持自然语言搜索。2016年5月推出智能家居中控系统Google Home，对标亚马逊的Echo。Google Home背后的智能助手引擎是Google Assistant，对标亚马逊的Alexa。2016年Google的AlphaGo在人机围棋大战上的碾压式胜利又一次引爆了公众对人工智能的关注。微软25年前创建的微软研究院(Microsoft Research)便一直在从事人工智能领域相关的研究。2016年9月，微软整合微软研究院、必应(Bing)和小娜(Cortana)产品部门和机器人等团队，组建“微软人工智能与研究事业部”，藉此来加速人工智能研发的进程。该事业部由微软全球执行副总裁沈向洋领导，目前拥有7000多名计算机科学家和工程师。基础技术微软开源了深度学习工具包CNTK，推出了基于云平台的人工智能超级云电脑。微软在其云平台Azure中加入FPGA达到了前所未有的网络性能，提高了所有工作负载的吞吐量。应用技术微软认知服务（Microsoft Cognitive Services）目前已经集合了多种智能API以及知识API等20多款工具可供开发者调用。产品服务微软2014年5月推出智能聊天机器人小冰，7月发布智能助手小娜（Cortana）。现在小娜每天都在为1.13亿用户服务，已回答超过120亿个问题。在商用领域，微软还推出了Cortana智能套件(Cortana Intelligence Suite)。微软2016年4月发布聊天机器人框架Bot Framework，目前已经被超过40000名开发者使用。Facebook人工智能是Facebook的三大方向之一。Facebook 2013年12月成立人工智能实验室（Facebook AI Research，简称FAIR），由卷积神经网络(Convolutional Neural Networks，CNN)的发明者、纽约大学终身教授Yann LeCun领导。还成立了应用机器学习部门（Applied Machine Learning，简称AML），由机器学习专家Joaquin Candela领导，负责将AI研究成果应用到Facebook现有产品中。LeCun和Candela都直接向Facebook的CTO汇报工作。Facebook CEO扎克伯格在2016年还亲自写代码为自己家开发了一个人工智能管家Jarvis。基础技术Facebook2015年12月开源人工智能硬件平台BigSur，2017年3月又开源了新一代的服务器设计方案BigBasin，能训练的模型比BigSur大了30%。2016年6月开源基于Toch的深度学习框架Torchnet。Facebook内部搭建了通用的机器学习平台FBLearnerFlow。目前在FBLearnerFlow平台上平均每个月运行120万个AI任务。Lumos构建于FBLearnerFlow平台之上，它是专用于图像和视频的学习平台。应用技术Facebook在语义领域开发了文本理解引擎DeepText，开源了文本表示和分类库fastText。在图像领域，开发了人脸识别技术DeepFace，开源了三款图像分割工具：DeepMask、SharpMask和MultiPathNet。产品服务Facebook2015年8月推出智能助手M，2016年4月推出的基于Facebook Messenger的聊天机器人框架Bot。但受限于当前的人工智能技术水平，聊天机器人的错误率被爆高达70%。Facebook目前已经将聊天机器人的重心转向一些特定的任务。Facebook还开源了自己的围棋AI引擎DarkForest，来自中国的田渊栋是其首席工程师。IBM人工智能是IBM在2014年后的重点关注领域，IBM正在转型成为认知产品服务和云平台公司。IBM未来十年战略核心是“智慧地球”计划，IBM每年在其上的研发投资约在30亿美元以上。基础技术IBM一直致力于研发类脑芯片TrueNorth，并取得了不错的进展，但离量产尚有距离。IBM还开源了大规模机器学习平台SystemML。应用技术IBM的云平台Bluemix提供了覆盖语音、图像、语义等领域的十多种常用技术。产品服务Watson在《Jeopardy！》一战成名之后，IBM围绕Watson继续发力，计划将其打造成商业领域的人工智能平台。医疗是他们目前最重要的领域。2016年8月Watson只用了10分钟便为一名患者确诊了一种很难判断的罕见白血病。此外，Watson还被广泛应用于教育、保险、气象等领域。亚马逊有别于其它巨头，亚马逊鲜有宣传自己的AI布局，却不声不响地做出了AI明星产品Echo。2016年7月亚马逊挖来卡内基·梅隆大学教授、顶尖机器学习专家Alex Smola担任AWS机器学习总监。基础技术亚马逊在AWS上提供了分布式机器学习平台。应用技术2016年底，AWS才正式推出自己的AI产品线：Amazon Lex、Amazon Polly以及Amazon Rekognition，分别用于聊天机器人、语音合成以及图像识别。产品服务亚马逊2014年发布智能音箱Echo，据估计截至2016年底Echo系列产品的销量已经接近1000万台，取得了巨大的商业成功。借助Echo的成功，Echo背后的智能语音助手Alexa也被众多第三方设备采用。Alexa目前已拥有超过1万项技能，这个数字还在快速增长。亚马逊还推出了新零售实体便利商超Amazon go。在Amazon go中，没有服务员、没有收银台，消费者进店不用排队结账，拿了就走。苹果苹果于2011年最早推出语音助手Siri，掀起语音助手的热潮。但Siri的效果远低于用户的预期，最终沦为一个玩具。在近几年的人工智能大潮中，苹果除了收购了一些人工智能创业公司，并无重量级的产品或技术问世，已经明显落后于其他巨头。2016年10月，苹果挖来CMU的深度学习专家Russ Salakhutdinov担任人工智能研究团队的负责人，表明苹果已经开始加紧步伐追赶，我们拭目以待。AI联盟2016年9月美国Google、Facebook、亚马逊、IBM、微软等五大科技巨头宣布成立AI联盟（Partnershipon Artificial Intelligence to Benefit People and Society），旨在发展和共享AI的最佳实践，促进公众对AI的理解，以使AI造福社会。2017年1月，苹果也作为“创始成员”加入。此外还有OpenAI、AAAI、美国公民自由联盟、麦克阿瑟基金会等组织相继加入。百度百度是国内人工智能领域投入最大、布局最广且整体实力最强的公司。2013年1月百度成立深度学习研究院（Institute of Deep Learning，简称IDL）。2014年5月百度硅谷人工智能实验室在美国硅谷成立。同时世界顶级人工智能专家、斯坦福大学教授吴恩达(Andrew Ng)出任百度首席科学家，全面负责百度研究院。2017年1月，曾任微软集团全球执行副总裁的陆奇加入百度担任百度集团总裁和COO。2月，百度宣布全资收购渡鸦科技，渡鸦创始人吕骋出任百度智能家居硬件总经理，直接向陆奇汇报。原度秘团队升级为度秘事业部，也直接向陆奇汇报。3月，百度成立智能驾驶事业群组，由陆奇兼任总经理。吴恩达离职。百度宣布整合包括NLP、KG、IDL、Speech、BigData等在内的百度核心技术，组成百度AI技术平台体系（AIG），任命百度副总裁王海峰为AI技术平台体系（AIG）总负责人。目前百度人工智能团队已经增长到近1300人。从百度频繁且大规模的人工智能相关的人事和组织调整亦可以看出，百度在人工智能上下了重注。基础技术百度在数据中心也大规模采用了FPGA来加速计算。另外，百度还自主研发并开源了自己的深度学习框架PaddlePaddle，这属于国内首家。应用技术百度云平台提供了语音、人脸识别、文字识别、自然语言处理、黄反识别、智能视频分析等常用应用技术。产品服务百度无人驾驶车项目于2013年起步。2015年12月，百度无人驾驶车国内首次实现城市、环路及高速道路混合路况下的全自动驾驶，测试时最高时速达到100公里/小时。2016年7月，百度与乌镇旅游举行战略签约仪式，宣布双方在景区道路上实现Level4的无人驾驶。2015年9月，百度推出人工智能助理度秘（英文名：Duer），度秘可以在对话中清晰的理解用户的多种需求，为用户提供各种优质服务。2017年1月，百度推出首款对话式人工智能操作系统DuerOS。DuerOS支持第三方开发者的能力接入，目前已经具备7大类目70多项能力，能够支持手机、电视、音箱、汽车、机器人等多种硬件设备。腾讯腾讯之前已经有微信模式识别中心、优图实验室、文智等多个团队在应用技术层开展了很多工作。腾讯2016年4月成立人工智能实验室（简称AI Lab），由前百度IDL首席科学家张潼领导，重金招揽优秀的AI领域研发人员，意图加速AI的进程。基础技术腾讯云提供了大规模机器学习平台和深度学习平台，目前支持 TensorFlow、Caffe、Torch 三大深度学习框架。应用技术腾讯的云平台也提供了图像、语音、自然语言处理等常用应用技术。产品服务2015年9月，腾讯的新闻写作机器人Dreamwriter撰写财经新闻并发布。2017年3月，腾讯的围棋机器人“绝艺”斩获UEC杯计算机围棋大赛冠军。阿里阿里主要围绕自身的电商业务和商业领域进行布局。2017年3月，在阿里巴巴首届技术大会上，马云宣布启动一项代号“NASA”的计划，面向未来20年组建强大的独立研发部门，涉及面向机器学习、芯片、IoT、操作系统、生物识别等核心技术。基础技术阿里云发布了分布式机器学习平台PAI2.0，全面兼容主流深度学习框架TensorFlow、Caffe和MXNet。应用技术层阿里云提供了语音和图像的接口，暂无自然语言处理的接口。产品服务层阿里在2015年7月发布智能客服机器人“阿里小蜜”，能力堪比3.3万个客服小二。2016年阿里与杭州联合推出城市大脑，初步实验表明：通过智能调节红绿灯，道路车辆通行速度最高提升了11%。此外阿里还布局了工业大脑、电商大脑、医疗大脑。各国人工智能政策新一轮的人工智能浪潮受到各国政府的高度关注，美国、中国、日本以及欧州各国近几年纷纷出台政策或计划引导，进一步加速人工智能的高速发展。美国2013年4月，美国启动“推进创新神经技术脑研究计划”（简称“脑计划”），目标包括探索人类大脑工作机制、开发大脑不治之症的疗法等。2015年10月，美国发布新版《美国国家创新战略》，其中的重点领域如自动驾驶、智慧城市、数字教育等内容都与人工智能息息相关。2016年10月，美国发布《为人工智能的未来做好准备》和《国家人工智能研究与发展战略规划》两份重要报告，将人工智能上升到国家战略高度。紧接着美国又发布《2016美国机器人发展路线图—从互联网到机器人》，力图保持美国在机器人领域的领先地位。日本2015年1月，日本发布《日本机器人战略：愿景、战略、行动计划》（也可称为《新机器人战略》），希望充分利用机器人技术，力争使日本在当今数据驱动时代引领世界。2016年1月，日本发布《第五期科学技术基本计划（2016—2020）》，欲打造“超智能社会”。2016年5月，日本确定了“人工智能/大数据/物联网/网络安全综合项目”（AIP项目）的2016年度战略目标，希望利用快速发展的人工智能技术，开发出能利用多样化海量信息的综合性技术。2016年8月，日本发布第四次产业革命战略，有三个核心技术方向：物联网、大数据和人工智能。欧盟2013年1月，欧盟宣布“人脑计划”（Human Brain Project），该项目被选定为欧盟的未来新兴技术旗舰项目之一。2014年6月，欧盟启动《欧盟机器人研发计划》（SPARC），这是目前全球最大的民用机器人研发计划。英国2016年12月，英国发布《人工智能：未来决策制定的机遇与影响》的报告，希望利用英国的独特人工智能优势增强国力。德国早在1988年，德国就成立了德国人工智能研究中心（简称为DFKI），是目前世界上最大人工智能研究中心。2013年4月，德国提出“工业4.0”的概念并对各国广泛接受，人工智能是其中一个核心要素。中国2015年5月，国务院印发《中国制造2025》，将“智能制造”列为中国制造的主攻方向。2015年7月，《国务院关于积极推进“互联网+”行动的指导意见》发布，将人工智能作为重点布局的11个领域之一。2016年3月，工信部等三部委联合印发《机器人产业发展规划（2016－2020年）》，为“十三五”期间我国机器人产业发展描绘了清晰的蓝图。2016年5月，发改委等四部门联合印发《“互联网+”人工智能三年行动实施方案》，以加快人工智能产业发展。2016年7月，国务院印发《“十三五”国家科技创新规划》，明确将人工智能作为发展新一代信息技术的主要方向。2017年3月，十二届全国人大五次会议上，“人工智能”首次进入政府工作报告。中美竞赛纵观全球各国人工智能的发展状况，美国仍然是毫无疑问的领头羊，中国紧随其后。中美仍有不小差距，但中国在快速追赶。据乌镇智库统计，在人工智能企业数目，融资规模，投资机构数量三项指标上，美国分别约为中国的4倍，7倍和21倍。但近年来，中国在上述三项的发展速度上，领先全球。自2012年起，中国的AI相关的专利申请数及专利授权数开始超越美国。在“深度学习”相关的论文数量上，2014年中国也首次超过美国。在人工智能的各个领域，也都活跃着华人的身影。为此《纽约时报》2017年2月刊发长文《中国正在人工智能“军备竞赛”中赶超美国》，担心中国赶超美国。客观来看，中国目前主要在人工智能的部分应用技术和应用服务层发展迅猛，但在基础技术层面主要还是直接使用或改良国外的成果，并无太多有突破性的成绩。
695833154,2ec32d0a221007572528e2e738d05b25,喝咖啡的产品经理,https://api.zhihu.com/people/2ec32d0a221007572528e2e738d05b25,people,欢迎咨询质量管理相关的内容，特别是质量数字化转型。,AI人工智能是第一次软件代码革命吗？人工智能可以取代程序员吗？,article,https://api.zhihu.com/articles/695833154,而我们大部分的脑力劳动者每天大部分工作都是重复的。这就代表<em>人工智能</em>大大模型的市场应用会非常广泛。 但是，从<em>人工智能</em>的领域上看，人工智能分为“弱人工智能”，“强<em>人工智能</em>”，“超人工智能”。大模型还是在弱人工智能的层级,2,0,0,1714750046,1714750497,起因：4月初，阿里高调宣布首个AI员工上岗，在内部全面推行AI编程，使用通义灵码辅助程序员写代码。更早一点，在3月，百度发布了Comate2.0代码助手，对个人开发者免费。而在去年12月，商汤推出智能编程助手代码小浣熊，可帮助开发者提升编程效率超50%。和一些行业从业者讨论了一个问题：随着人工智能的研究，程序员会被取代吗？答案基本上是一致的，不会被取代，但是对程序员的要求提升。第一次工业革命是机器取代人，工人阶级人数越来越多，工人阶级需要学会操作机器。第一次软件代码革命是人工智能取代人，程序员人数会越来越多，程序员需要学会人工智能。观点一：AI人工智能取代程序员不具备社会条件。AI是这两年比较火爆的一个技术突破点。在前几年，热点有机器人，有5G，等等。但是社会对企业的需求是：稳就业，增利润，保税收。没有一个技术最终是让大量的人失业，并推向社会。建筑行业的技术是领先世界的，但是一方面是盾构机等领先全世界，另一方面还是雇佣大量的农民工等基础工人。洋山港是无人科技港，但是其他港口科技含量并没有那么高。京东有非常厉害的“无人物流中心”，但是京东还是雇佣了几十万人。这些都是标杆效应和稳就业平衡的。对于大部分程序员来说，并不需要担心就业的问题。就算行业受到冲击，收入有所下降，但是不必担心失业的问题。软件行业，目前也是“穷人家孩子高收入的捷径”，这条路堵死了，其实也是不太好的事情。其次，如果AI技术确实可以取代程序员的话，AI很可能会创造高额的利润。这部分的利润可以通过“监管”，并创造税收来源，而不是降维打击冲击其他行业。大数据可以进行国有化，一方面是安全考虑，一方面是税收考虑，和水，电，油一样。税收的幅度可以与烟草等媲美即可，这是利国利民的。观点二：AI人工智能降低开发成本并促进软件行业的应用和市场。如果之前一个程序员一天200行代码，那么AI辅助之后，一天可能1000行代码。开发成本只有以前的20%，以前卖100万的软件，现在20万就可以卖。低成本代表会有更多的客户，更广泛的市场。之前不划算的市场，现在变得可以进入了。不仅不会取代程序员，还会增加程序员，因为市场增加了。但是程序员的技能要求需要懂得操作AI。这和工人阶级需要学会操作机器一样，是普遍的技能。就目前看，一个软件开发工程师学会AI写代码，不会超过一个月，并不是门槛。目前的软件开发工程师只需要不抵触AI的学习。完全可以胜任。德国工业软件巨头SAP，已经开始这项转型。SAP全球10.7万人，有8000人会因AI转型进行调整。学习AI融入新工作，或者接受裁员。观点三：AI人工智能促进软件行业的工业化，可能会区分工具型软件公司和应用型软件公司。以工业革命为例，诞生了一个行业“机械制造业”。例如：车床，冲压机床，挖掘机，等等工业领域机械设备。这些设备厂商给工厂提供基础设施建设，工厂不需要从“工具”开始制造。人工智能模型可以理解为“机械设备”，是工具型的。目前头部公司进行研发，并开发给其他公司应用。很多做算法的开发已经面临失业或者集中到大厂中“集中力量办大事”。那么就是代表“软件行业”可能会出现分工，一部分公司往工具型的转变，另一部分公司往应用型的转变。目前程序员的代码几乎都是一行一行的敲出来，每个人代码不一样，代码规范并不一定能束缚程序员的发挥。程序员敲代码和敲铁皮没有太大的不同。如果有人工智能加持，那么代码的一致性就有了保障，程序员不是手工完成。有一个冷知识：ISO 9001并不适用软件行业，因为软件行业的“需求-供给”的过程，并不是标准化答案的。人工智能很可能带来软件行业的“工业化和标准化”，而不是取代人。观点四：目前AI人工智能还是工具，是“弱人工智能”，技术上不具备。人工智能大模型的出现，对很多行业产生了冲击。特别是重复性脑力劳动的岗位。而我们大部分的脑力劳动者每天大部分工作都是重复的。这就代表人工智能大大模型的市场应用会非常广泛。但是，从人工智能的领域上看，人工智能分为“弱人工智能”，“强人工智能”，“超人工智能”。大模型还是在弱人工智能的层级。目前只是在机器学习和深度学习这两个领域，一句话概括就是通过“大量重复的事情里面找到经验碎片，然后通过经验碎片的重组去做事情”。其实是在找关联。但是在逻辑推理，因果关系上。大模型是无能为力的。因此并不具备人的常规推理能力。目前阶段，谈AI人工智能取代人的工作，还为时尚早。目前大模型的应用，都是需要大量的人的经验进行调优训练。这部分自学成本非常高，也需要专业人士的介入。观点五：AI人工智能的成本非常高。有一个说法，人工智能的尽头是“储能”。目前阶段的人工智能和人的大脑的效率相差很大。从思维能力上看，最强的人工智能只有人脑的千分之一。而耗能上，相差又是万倍。一个人的大脑只有15W，一顿饭就可以解决很多复杂的工作的思考能耗。而AI人工智能目前并不可以。击败围棋大师的AI机器人，能耗是几十万W。在这种能耗上，目前人工智能并不是一般企业玩得转的。需要政府支持，头部企业投入。并不是一个立即产生效益的研发投入。技术象征意义大于经济意义。现阶段，雇佣顶尖AI人工智能开发软件的成本，可能远远大于程序员。而普通的AI，辅助程序员，是最经济的选择。观点六：程序员的未来可能会分工。在工业革命之后，产生了一个岗位“工程师”。在国企中，是从“技术员”一步一步提升到“工程师”。和“工人”有一定的区别。有时开玩笑说“码农”和“程序员”是两个不同职业。前者只是按照要求编写代码，后者需要了解需求，整体规划，设计，实现，交付等全过程能力。即：“码农”=工人；“程序员”=工程师。中间有一个层级是“技术员”。当然这个说法是参考了工业领域，未来软件的发展不一定这样的路。但是肯定的是，在分工之后，普通程序员的待遇肯定会下降。目前软件行业依旧是新兴行业，是有红利期的，也是所有行业中收入最高的。这样的优势会越来越少。
695757683,8c8009d3688344c76b60818aa67a85e7,产品经理独孤虾,https://api.zhihu.com/people/8c8009d3688344c76b60818aa67a85e7,people,20年经验前百度（P7）苏宁（B9）京东（P9）资深产品经理,人工智能领域最新动态 | 你想要的AI招聘信息都在这里（240503）,article,https://api.zhihu.com/articles/695757683,但也有较高的竞争压力和工作强度。 智能分诊、AI辅助诊疗等 医疗<em>人工智能</em>未来如何布局 医疗人工智能是人工智能在医疗健康领域的应用和创新，包括智能分诊、AI辅助诊疗、AI影像诊断、AI药物研发等,1,0,0,1714698109,1714698109,#人工智能 #资讯 #招聘 你好，我是产品经理独孤虾，欢迎来到我的个人号。作为一个产品设计专家和营销领域业务专家，我一直关注人工智能领域的最新动态和发展趋势，同时也非常热衷于将人工智能大模型应用到数字营销和智能营销的各个场景中，以提高企业的营销效果和业绩。今天，我为大家带来了一些关于人工智能领域的新闻，以及一些人工智能相关的企业职位招聘信息，希望能够对你有所帮助和启发。人工智能领域最新动态奥特曼谈人工智能：进步是不可预测的，也很难监管奥特曼是OpenAI的创始人之一，也是人工智能领域的领军人物。在最近的一次采访中，他谈到了人工智能的发展前景和挑战，认为人工智能的进步是不可预测的，也很难监管。他表示，人工智能有可能带来巨大的社会变革，但也有可能造成灾难性的后果。他呼吁人类应该更加负责任地使用人工智能，同时也要保持对人工智能的敬畏和谨慎。点评：这是一篇关于人工智能领域的权威人士的观点的新闻，对于关注人工智能的读者来说，有一定的参考价值和启发意义。奥特曼的观点也反映了人工智能的不确定性和复杂性，以及人类面临的责任和挑战。人工智能无处不在：在2024华为分析师大会上，华为对智能时代提出了展望在2024 华为分析师大会（HAS 2024）上，华为战略研究院院长周红分享了自己对智能时代的展望。他认为，在智能时代，人工智能将无处不在。人工智能将融入人类社会各行各业，从根本上改变人与人以及人与机器之间的所有互动。他还介绍了华为在人工智能领域的战略和布局，包括构建全场景智能解决方案，打造开放的人工智能生态系统，以及推动人工智能的可信、可控和可持续发展。点评：这是一篇关于人工智能领域的行业巨头的战略和愿景的新闻，对于关注人工智能的读者来说，有一定的借鉴意义和启发意义。华为的展望也反映了人工智能的普遍性和重要性，以及人工智能的发展方向和目标。国际清算银行-人工智能对产出和通货膨胀的影响（英）国际清算银行（BIS）是一个为中央银行提供服务的国际组织，也是一个研究金融和经济问题的机构。最近，BIS发布了一份关于人工智能对产出和通货膨胀的影响的报告，分析了人工智能的潜在效应和政策含义。报告认为，人工智能的最新进展带来了提振经济增长的希望，但也可能导致结构性变化和不平等问题。报告建议，政策制定者应该关注人工智能的分配效应，以及人工智能对金融稳定和货币政策的影响。点评：这是一篇关于人工智能领域的学术研究的新闻，对于关注人工智能的读者来说，有一定的学习价值和启发意义。BIS的报告也反映了人工智能的经济效应和政策挑战，以及人工智能的影响范围和深度。采访OpenAI奥特曼：预计会有几个不同版本的通用人工智能，并且各有所长新闻内容介绍：OpenAI是一个致力于创造和推广友善的人工智能的非营利组织，也是人工智能领域的先驱和领导者。最近，OpenAI的创始人之一奥特曼接受了一次采访，谈到了他对通用人工智能（AGI）的看法和预期。他认为，会有“几个不同版本的通用人工智能，各自擅长不同的工作”。他说：“我想，你必须超过某个算力阈值。但即使这样，我也不会说我确定（算力大的就能成功）。”他还谈到了他对人工智能的伦理和社会责任的看法，以及他对OpenAI的愿景和目标的介绍。他表示，OpenAI的使命是确保人工智能的发展能够造福全人类，而不是被少数人或组织所控制或滥用。他还透露，OpenAI正在开发一些具有突破性的人工智能项目，例如GPT-4、DALL-E和CLIP等，希望能够为人类带来更多的便利和价值。点评：这是一篇关于人工智能领域的创新者和领导者的采访的新闻，对于关注人工智能的读者来说，有一定的启发意义和参考价值。奥特曼的观点也反映了通用人工智能的多样性和可能性，以及人工智能的伦理和社会问题。AI领域招聘信息速递煤炭科学研究总院有限公司矿山人工智能研究院全球招聘公告煤炭科学研究总院有限公司是我国煤炭行业唯一的综合性研究院，现拟在北京、西安、太原、重庆、徐州等地设立矿山人工智能研究院，招聘人工智能领域的高层次人才，包括院长、副院长、研究员、副研究员、助理研究员等。招聘条件包括具有博士学位、在人工智能领域有突出贡献、有国际合作经验等。点评：这是一条来自国有科研机构的招聘信息，对于有志于从事人工智能在煤炭行业的应用和创新的人才来说，是一个不错的机会。该机构有较强的科研实力和资源，也有较大的社会影响力和责任感。苹果公司挖走谷歌员工，组建人工智能团队苹果公司从谷歌挖来了数十名人工智能专家，并在苏黎世成立了一个秘密的欧洲实验室，苹果公司正在组建一个团队，在开发新的人工智能模型和产品方面与竞争对手竞争。招聘职位包括人工智能研究科学家、人工智能软件工程师、人工智能项目经理等。招聘条件包括具有人工智能相关领域的博士学位、有丰富的人工智能项目经验、有良好的沟通和团队协作能力等。点评：这是一条来自科技巨头的招聘信息，对于有志于在人工智能领域取得突破和创新的人才来说，是一个难得的机会。苹果公司有强大的品牌影响力和市场份额，也有优厚的薪酬和福利，但也有较高的竞争压力和工作强度。智能分诊、AI辅助诊疗等 医疗人工智能未来如何布局医疗人工智能是人工智能在医疗健康领域的应用和创新，包括智能分诊、AI辅助诊疗、AI影像诊断、AI药物研发等。目前，医疗人工智能已经成为医疗行业的重要发展方向，各类医疗机构和企业都在积极布局和招聘相关人才。招聘职位包括医疗人工智能算法工程师、医疗人工智能产品经理、医疗人工智能销售经理等。招聘条件包括具有医学或计算机相关专业的学历、有医疗人工智能项目经验、有医疗行业知识和资源等。点评：这是一条来自医疗行业的招聘信息，对于有志于从事医疗人工智能的应用和创新的人才来说，是一个有前景的领域。医疗人工智能有利于提高医疗质量和效率，也有利于满足人民群众的健康需求，但也面临着数据安全、伦理规范、法律责任等挑战。钱卫李女士：人力资源行业的技术创新先锋钱卫李女士是一位人力资源行业的技术创新先锋，她以自己的行动证明了创新和突破的重要性，为人力资源行业的未来发展指明了一条新的道路。她创立了一家专注于人工智能招聘的公司，利用人工智能技术为企业提供高效、精准、公平的招聘服务。她现在正在招聘人工智能招聘顾问、人工智能招聘运营经理、人工智能招聘市场经理等。招聘条件包括具有人力资源或人工智能相关专业的学历、有人工智能招聘项目经验、有良好的客户服务和沟通能力等。点评：这是一条来自创业公司的招聘信息，对于有志于从事人工智能招聘的应用和创新的人才来说，是一个有挑战和机遇的领域。人工智能招聘有利于提高招聘效率和质量，也有利于消除招聘歧视和偏见，但也需要注意保护求职者的隐私和权益，以及遵守相关的法律法规。胡志明市与英伟达商洽人工智能合作胡志明市是越南的经济中心和最大城市，近年来积极发展人工智能产业，与英伟达等国际科技公司建立了良好的合作关系。胡志明市计划在未来五年内建立一个人工智能中心，为越南和东南亚地区提供人工智能解决方案和服务。胡志明市现在正在招聘人工智能中心的主任、副主任、研究员、工程师等。招聘条件包括具有人工智能相关领域的博士学位、有国际合作经验、有人工智能项目管理和领导能力等。点评：这是一条来自政府机构的招聘信息，对于有志于从事人工智能在国际合作和区域发展的应用和创新的人才来说，是一个有挑战和机遇的领域。人工智能在国际合作和区域发展中有着重要的作用和价值，可以促进经济增长、社会进步和文化交流，但也需要注意遵守国际规则和尊重各国主权和利益。感谢你的阅读，这就是今天为你带来的关于人工智能领域的新闻和招聘信息，希望你能从中获得一些有用的信息和启发。如果你对人工智能领域感兴趣，欢迎关注我的个人号，我会定期分享关于人工智能的最新动态、技术分析、案例展示、行业趋势等内容，让你了解人工智能的前沿和应用，提升你的数字营销和智能营销能力和水平。再次感谢你的阅读，期待你的留言和反馈，我们下期再见！
695745589,ead9e1bb1bf5595767a9791561feaa45,桃zhi儿,https://api.zhihu.com/people/ead9e1bb1bf5595767a9791561feaa45,people,,人工智能,article,https://api.zhihu.com/articles/695745589,短篇小说 公元2200年，地球环境恶化，<em>人工智能</em>逐渐走向成熟，造出的机器人不仅外貌上与人类相似，可以像人类一样工作，还具有相同的情感。然而，随着越来越多的机器人占据重要生活生产地位,0,0,1,1714669783,1714669783,短篇小说 公元2200年，地球环境恶化，人工智能逐渐走向成熟，造出的机器人不仅外貌上与人类相似，可以像人类一样工作，还具有相同的情感。然而，随着越来越多的机器人占据重要生活生产地位，人类与机器人的矛盾也愈发厉害。 你就在这个时候诞生了。 所幸，你是个聪明的孩子。时光飞逝，你成功考上了名牌大学，并获邀加入国际救援队。当你前去报道，站在红十字救援基地门前时，看见门口立着几个烫金的大字“为人类做贡献”，内心还是受到了深深地震撼。 这是你努力这么多年的理由，也是你从小的心愿。 迎新会上，你摩挲着红十字的徽章，脑中却不住回放以前的经历。你儿时身体并不好，过冷过烫的东西都不能吃，好在随着锻炼，这种状况得以改善。你从小记忆力就超常地好，别人需要重复理解的知识点，你看一遍就能学会。你饱读诗书，并且可以说是过目不忘，这个能力使你的工作更加得心应手，你也深受同事喜爱。 然而就在某天夜晚，天空中的臭氧层突然出现了破洞。强烈的紫外线争先恐后从洞里涌出，将本来漆黑的夜空照得透亮。 你跟随救援队先来到受灾地区疏散人群避难，又连夜运送物资和防御设备。几天几夜的高强度作战使你身心疲惫，终于，你支持不住倒在地上。意识模模糊糊之间，你仿佛听到一串电流击穿的声音。 再醒过来时，你已躺在洁白的病房里。负责检查的医生同情地瞟了你一眼，又出去与另一个博士说了些什么。你只看见那个人转身朝你走来。 “我什么时候能回去工作？”你仍然心念着灾地和战友。 “在回去前，你得先跟我去个地方。”那个博士答道。 你同他上了车，一脚油门间车便驶向了远方，你望着越来越小的城市，突然感觉自己置身于这世界之外。 车子一路颠簸许久，终于停在港口一个海洋馆边，你与他走了进去。 “来这里，你看到了什么？” 你站在海洋馆的观影墙边，透过这层玻璃，你看到了一百年前的海洋。阳光穿过海面，海中世界逐渐变得清晰。你痴痴地望着，那是地球上最美的…最后的一抹蔚蓝。可是本该在水里自由游动的鱼儿，却裸露着齿轮，鱼皮散发着金属光泽，机械般摆动。想象中闪闪发光的眼睛，也终是一片死寂…“我们没有存留这些被感染前鱼群的DNA，很抱歉无法模拟出来。”博士遗憾地说到，“不过想想这一切都是因果相扣，那些‘鱼群’，又何尝不是我们的未来呢。” 你回头，看到的不再是一个人类。一种断断续续的、有些刺耳的声音，从他的身体里钻出。像是巨大的齿轮在转动时相互摩擦。 这与你在失去意识前听到的声音一模一样。 这时，你才明白，你也同他们一样，是机器人。 “只怪现在人工智能技术太发达，让我们与人类高度相似”，博士接着说道：“但是，机器人不会湮灭，只要躲过这次灾难，我们就不再受苦了。” 你看着玻璃中倒映出的自己的身影，拒绝了他。 “是不是机器人不重要，但与他们相处这么久，我无法坐视不管。您应该也有所体会”，你握住博士的手：“至少这一次，我想救他们。” 博士叹了口气，关掉了虚拟放映器，一架火箭出现在玻璃墙后。 “这是运载飞船，可以在高空悬停，飞船两侧装着臭氧，只要飞到指定高度出口便会打开自动释放，我在这里可以修改它的既定轨道。你上去之后，千万小心。” 你点了点头。 倒计时开始，原先的海洋馆转变成发射台，天花板褪去，一束光照在飞船上，你仰望天空，眯眼，犹如冰冷海水里的机械鱼沐浴着水中那丝温暖。浓烟弥漫，火光迸射之间，臭氧层的破洞在显示屏上渐渐放大。映射出你微微睁开的双眸。 此时还在地面作战的同事纷纷抬头，指着破洞前遮挡的那个黑点。 飞船两侧打开，一股淡蓝色的气体飘出，破洞逐渐收缩，直至恢复。你再次睁眼，显示屏后，光芒不再耀眼，天空又恢复成蔚蓝。 你出色完成了任务，为人类做出了贡献。 然而，返回途中，装载的一个零件蹦出，飞船犹如失去翅膀的鸟，直直坠向海面，摩擦产生的火焰吞噬着一切。你的皮肤很快融化，露出漆黑的金属。直到最后融化掉你微笑着的嘴角。 人们赶到现场时，只发现了一堆破碎的废铁，和一枚红十字徽章。
694633383,52e5dd12ed1b03f385010f5d0562556f,上海智期科技,https://api.zhihu.com/people/52e5dd12ed1b03f385010f5d0562556f,people,AI生态平台引领者,新质生产力——人工智能,article,https://api.zhihu.com/articles/694633383,积极解决，并确保<em>人工智能</em>能够为全人类带来更多的福祉和发展机会。 上海智期科技： 上海智期科技是以AI商业落地为核心，致力于为客户提供领先的智能解决方案的创新企业,2,0,5,1714096833,1714096833,人工智能（Artificial Intelligence，简称AI）作为一种新兴的技术，正在以前所未有的速度改变着我们的生活和工作方式。它被誉为第四次工业 ** 的核心驱动力量，并被广泛认为是引领未来发展的重要产业之一。人工智能不仅对传统产业进行了革新和优化，更为重要的是，它催生出了全新的生产力形态，这种新质生产力正深刻地影响着我们的社会和经济。来百度APP畅享高清图片#财富自由攻略手册#首先，人工智能可以通过自动化和智能化的方式提高生产效率。在过去，人们往往需要大量的人力和时间来完成某些繁琐、重复的工作，而现在借助于人工智能技术，这些任务可以被自动化地完成。例如，在制造业中，机器人可以代替人类完成重复性、危险性高的工作，大大提高了生产效率和产品质量。在服务行业中，智能客服可以通过语音识别和自然语言处理技术，为用户提供更加快捷、准确的服务。这些智能化的生产方式大大缩短了工作周期、降低了成本，进而提高了整体的生产效率。其次，人工智能可以为企业提供更深入的市场洞察和个性化服务。通过分析海量的数据，人工智能可以快速发现潜在的市场机会和消费者需求，帮助企业制定更精准的营销策略。同时，人工智能还可以个性化地推荐产品或服务，提高用户满意度和忠诚度。这种基于大数据和算法的个性化服务模式，使得企业能够更好地与消费者进行互动和沟通，有效提升了市场竞争力。此外，人工智能还具备自主学习和创新的能力，这对新质生产力的形成起到了关键作用。传统的生产力主要依赖于人类的创造力和劳动力，而人工智能具有自主学习和适应环境的能力，可以不断地更新和优化自身的算法和模型。这使得人工智能能够从经验中学习，准确预测和解决复杂问题，甚至创造出新的解决方案。这种自主学习和创新的能力，为生产力的持续提升提供了新的动力和机会，推动着社会的持续进步和发展。然而，人工智能对新质生产力的影响也带来了一些问题和挑战。首先是就业问题，随着人工智能的普及和应用，某些传统行业可能会面临替代性就业压力，需要进行转型和更新。其次是数据隐私和伦理问题，人工智能需要大量的数据支持，但在使用这些数据的过程中，如何保护用户的隐私和个人信息，如何避免算法的偏见等问题都需要引起重视和思考。总之，人工智能作为一种新质生产力，正深刻地改变着我们的社会和经济。它通过提高生产效率、提供个性化服务、实现自主学习和创新等方式，推动了经济的发展和社会的进步。然而，对于人工智能的应用和发展，我们也需要认识到其中的问题和挑战，积极解决，并确保人工智能能够为全人类带来更多的福祉和发展机会。上海智期科技：上海智期科技是以AI商业落地为核心，致力于为客户提供领先的智能解决方案的创新企业。作为行业领先者，我们通过融合最新的人工智能技术和创新思维，帮助企业实现数字化转型和业务升级，为企业降本增效，我们的产品和服务涵盖了各个行业和领域，包括金融、医疗、零售、交通等。未来将携手全球合作伙伴，共同推动人工智能技术的发展和应用，为商业领域的创新和发展贡献我们的智慧和力量。作者声明：内容由AI生成举报/反馈
691826196,9a1f0181a17a16c536688a5e84f1b03a,金鸣识别,https://api.zhihu.com/people/9a1f0181a17a16c536688a5e84f1b03a,people,图片/PDF转excel、word，票证识别，大批量合并！,人工智能科普：人工智能的分类,article,https://api.zhihu.com/articles/691826196,广泛应用于智能安防、智能交通、智能医疗等领域。 三、按智能水平分类 1. 弱<em>人工智能</em>：弱人工智能是指能够执行特定任务的<em>人工智能</em>系统，例如智能音箱、智能家居设备等,0,0,0,1712791837,1712791837,人工智能的分类多种多样，根据不同的标准和应用场景，可以将其划分为多个不同的类别。以下是对人工智能分类的详细探讨。一、按应用领域分类1. 智能机器人：智能机器人是人工智能技术在机器人领域的应用。它们能够根据环境和任务的不同进行自主决策和行动，包括自主导航、语音识别、人机交互等功能。智能机器人在工业、医疗、军事等领域有着广泛的应用前景。2. 智能家居：智能家居是人工智能技术在家居领域的应用。通过人工智能技术，可以实现对家居设备的智能化控制，提高家庭生活的舒适度和便利性。例如，智能音箱、智能照明、智能安防等设备都可以通过人工智能技术实现智能化控制。3. 智能医疗：智能医疗是人工智能技术在医疗领域的应用。通过人工智能技术，可以实现对医疗数据的分析和挖掘，提高医疗服务的效率和质量。例如，通过智能诊断系统，可以快速准确地诊断疾病；通过智能手术机器人，可以实现精准无误的手术操作。4. 智能金融：智能金融是人工智能技术在金融领域的应用。通过人工智能技术，可以实现对金融数据的分析和预测，提高金融服务的智能化水平。例如，智能投顾可以根据投资者的风险偏好和投资目标，为其提供个性化的投资建议；智能风控可以通过分析客户的信用记录和交易行为，实现对信贷风险的精准控制。二、按技术实现分类1. 机器学习：机器学习是人工智能领域的一种重要技术，它通过对大量数据进行学习和分析，实现对未知数据的预测和分类。机器学习算法包括监督学习、无监督学习、半监督学习等，广泛应用于图像识别、语音识别、自然语言处理等领域。2. 深度学习：深度学习是机器学习领域的一种分支技术，它利用神经网络模型对大量数据进行训练和学习，实现对复杂数据的特征提取和分类。深度学习在图像识别、语音识别、自然语言处理等领域取得了显著的成果，推动了人工智能技术的发展。3. 自然语言处理：自然语言处理是人工智能领域的一种技术，它通过对人类语言的分析和处理，实现对文本信息的提取、分类和生成。自然语言处理技术包括文本分类、情感分析、机器翻译等，广泛应用于智能客服、智能问答、智能写作等领域。4. 计算机视觉：计算机视觉是人工智能领域的一种技术，它通过对图像和视频的分析和处理，实现对目标物体的识别、跟踪和定位。计算机视觉技术包括目标检测、图像分割、图像识别等，广泛应用于智能安防、智能交通、智能医疗等领域。三、按智能水平分类1. 弱人工智能：弱人工智能是指能够执行特定任务的人工智能系统，例如智能音箱、智能家居设备等。这些系统只能在特定领域内进行智能处理，缺乏跨领域学习和适应的能力。2. 强人工智能：强人工智能是指能够像人类一样进行推理、学习和创造的人工智能系统。这种系统具有高度的智能水平，可以适应各种复杂环境和任务，但目前仍处于研究和探索阶段。综上所述，人工智能的分类多种多样，可以根据不同的标准和应用场景进行划分。随着技术的不断发展和应用的不断拓展，人工智能将在更多领域发挥重要作用，为人类带来更多的便利和惊喜。
691628327,522c8e8fe1013b5451bedb18ee7e0606,经济学雷锋,https://api.zhihu.com/people/522c8e8fe1013b5451bedb18ee7e0606,people,我在努力学习中,人工智能的弱点,article,https://api.zhihu.com/articles/691628327,特斯拉CEO马斯克预测：明年<em>人工智能</em>或超越最聪明人类。不知道马斯克预测的依据，但通过与人头脑风暴，发现<em>人工智能</em>存在明显的弱点。 人工智能的弱点就是太聪明，导致<em>人工智能</em>只能按既定规则来办,1,0,0,1712711914,1715149155,特斯拉CEO马斯克预测：明年人工智能或超越最聪明人类。不知道马斯克预测的依据，但通过与人头脑风暴，发现人工智能存在明显的弱点。人工智能的弱点就是太聪明，导致人工智能只能按既定规则来办，而难以做出创造性。自然界创造出生物，人类。假如没有遗传突变。假如基因不会出错，将会怎么样？地球上可能就不存在生物。创造需要出错，需要随机性，需要愚蠢。而人工智能都没有。对于按流程、按策略做事，人类肯定搞不过人工智能。比如下棋，人工智能可以算无遗策。然而，人工智能也只能在棋盘上按规则走棋。只有人类可以创造出棋类。
691326451,bd3dc44a7b801ea1a56727b5aaeae0af,<em>AI</em>学习小亮,https://api.zhihu.com/people/bd3dc44a7b801ea1a56727b5aaeae0af,people,一名AI的爱好者,如何从0开始学习人工智能，建议收藏！！！,article,https://api.zhihu.com/articles/691326451,既可以开发小工具，也可以开发企业级应用，目前许多<em>人工智能</em>的框架都是基于PYTHON开发的。 二、<em>人工智能</em>热门研究方向 在人工智能的实际应用中，可以分为以下三个方向，这也是当下热门的研究方向,1,0,0,1712565756,1712565756,人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，作为技术领域中的热门话题和研究方向，如何快速入门呢，下面提供一个学习的路线。一、基础部分1．具备一定的数学能力· 为什么需要学数学，学数学对于这项技术有什么意义?有很多同学都问过我：实际工作里，学好数学真的很重要吗？在这里我想给各位同学解释一下，我们都知道在理工科专业里，数学作为基础中的基础，是无可替代的存在。学习人工智能，肯定少不了对大量文献的阅读，在这过程中不论是从理解理论还是公式推导，都离不开数学，如果没有一定的数学基础，那么也就无法更好地理解论文，更不用谈实践了。在实际项目中，逻辑思维能力对于能否实现预计效果有着至关重要的作用，而对于数学好的同学，他的逻辑思维能力也一定不会低。 ·要如何学好数学呢?学习数学的困难想必各位同学都知道，但是在我们实际操作中，不需要像中学时期一样，对于每一个知识点都烂熟于心，也不用知道每一道题究竟是怎么解的，在人工智能的学习中，我们只需要知道，我可以用哪些数学公式，提升我的项目效果，也就是更注重如何使用理论而非对理论有知根知底的了解。2．具备编程能力说到人工智能，就一定离不开计算机，而实现人工智能，对于编程能力有着很高的要求，在现有的编程语言中，最常使用的就是PYTHON，它具有简单易用，学习成本低，看起来非常优雅干净的优点，此外PYTHON 标准库和第三库众多，功能强大，既可以开发小工具，也可以开发企业级应用，目前许多人工智能的框架都是基于PYTHON开发的。二、人工智能热门研究方向在人工智能的实际应用中，可以分为以下三个方向，这也是当下热门的研究方向，各位同学可以根据自己的喜好确定自己的研究方向，当然学有余力的同学也可以对所有领域都进行研究。1．计算机视觉计算机视觉自人工智能被提出以来就一直处于热门地位，不管是日常生活中的人脸识别还是战争中无人机的目标跟踪，都离不开计算机视觉，在这个方向中，主要是使计算机对于输入的图像有一定的理解，并且做出相应的判断，主流使用的方法是深度学习。2．自然语言处理自然语言处理在近些年也得到了广泛的关注，它可以实现机器翻译、问答系统、情感分析等等，主要研究如何让计算机能够理解、处理、生成和模拟人类语言的能力，它的难点在于语言的多样性、歧义性、语言的鲁棒性、知识依赖、上下文3．机器学习机器学习就是通过算法使得机器能从大量数据中学习规律，从而对新的样本做决策。常见的问题有回归，分类和聚类。机器学习有三要素：模型、学习准则和优化算法，通俗的说就是学什么、学成什么样和怎么学，常见的机器学习类型分为监督学习、无监督学习和强化学习等。以上就是总结的一些关于人工智能学习路线的方法，欢迎大家评论区补充讨论或私信交流。
694698481,1baca5c2361342b457375b92eb465208,三生幻石,https://api.zhihu.com/people/1baca5c2361342b457375b92eb465208,people,,人工智能对人类的影响超乎你的想象,article,https://api.zhihu.com/articles/694698481,随着<em>人工智能</em>的越来越先进，以后，人类能做的事情，它都能做，人类不能做的事情，它也能做。当<em>人工智能</em>媲美人类，甚至比人类更聪明时，科技将会以几何倍数进步，医学，生物,1,0,0,1714114353,1714114353,随着人工智能的越来越先进，以后，人类能做的事情，它都能做，人类不能做的事情，它也能做。当人工智能媲美人类，甚至比人类更聪明时，科技将会以几何倍数进步，医学，生物，军事，航空等等各个领域将迎来盛夏...你们觉得，那时候的人类，将会面临哪种结局？1.永生。(癌细胞被攻克，人类可以控制自己的生死）2.毁灭。（克制不住的欲望终究带来毁灭）3.既是永生，亦是毁灭。（肉体不复，意识永存）4.每个月3000工资的我，管不到那么多。（我只是打酱油的）
695061332,a3bd0b5951e51e825bcc026d556a8928,<em>AI</em>弄潮儿,https://api.zhihu.com/people/a3bd0b5951e51e825bcc026d556a8928,people,AI赋能，心想事成❤️立鼎未来科技创始人,关于人工智能的60条趋势预测,article,https://api.zhihu.com/articles/695061332,RAG等模型将受关注，AI工程基础仍重要。 AI政策、GPU短缺、原生AI应用等值得关注。 <em>人工智能</em>眼镜、AI助手、多模态模型等将普及。 AI将使编程更容易，AI模型扩展将带来工程挑战,0,0,1,1714296074,1714296074,到2027年，人工智能市场预计将达到2250亿美元，复合年增长率达152%。全球将涌现超过5亿个新应用，AI技术呈爆发式增长。微软OpenAI可能分道扬镳，AI错误信息可能干扰选举。Nvidia将加大云服务提供，AI领域可能有公司倒闭或收购。人工智能产业发展趋势包括多模态大模型、智能算力新范式等。AI智算产业将重构ICT产业生态，大型AI模型可能被更高效的模型取代。中国企业预计40%将掌握生成式AI使用。德银指出大型AI模型可能被更小、更高效的模型取代。AI将助力医疗、金融等领域发展，同时与区块链技术融合。企业级人工智能预计将在2024年开端，生成式AI预计大幅增长。超过80%的企业可能纳入生成式AI至运营，AI模型性能、数据质量、编译器速度等将有提升，AI内容生成也将增加。AI监管可能阻碍技术发展，但AI将革命性改变组织文化和公司运营方式，提升创造力和生产力。语音交互、AI助手、多模态模型等将成为主流，AI将使编程更容易，但也带来工程挑战。AI将实现设计、编码、测试、部署等工作流程变革，对实体经济的影响也将逐渐显现。AI的快速进化将带来更多体验、应用和工具，为人类带来层层叠加的好处，同时也需要防止恐慌操纵大众。人工智能市场到2027年将达2250亿美元，复合年增长率为152%。预计全球将涌现超过5亿个新应用，AIGC技术发展呈爆发式增长。微软OpenAI可能分道扬镳，AI错误信息可能干扰选举。Nvidia将加大云服务提供，AI领域可能有公司倒闭或收购。人工智能产业发展趋势包括多模态大模型、智能算力新范式等。AI智算产业将重构ICT产业生态，大型AI模型可能被更高效的模型取代。中国40%企业预计到2026年掌握生成式AI使用。德银指出大型AI模型可能被更小、更高效的模型取代。AI将助力医疗、金融等领域发展，同时与区块链技术融合。2024年将见证企业级人工智能开端，生成式AI预计大幅增长。预计超过80%企业将纳入生成式AI，模型和应用程序至运营。85%企业将扩展AI实力，40%将投资于AI治理规则。AI预计在高收入国家和非洲国家普及时间不同。AI公司可能倒闭或被收购，开源模型可能超越闭源模型。人工智能产业将突破多个领域，多模态模型成标配。AI模型性能、数据质量、编译器速度等将有提升，AI内容生成将增加。2024年将紧密结合AI开发与安全工作，预防潜在灾难性风险。AI监管可能阻碍技术发展，监管效果待观察。RAG等模型将受关注，AI工程基础仍重要。AI政策、GPU短缺、原生AI应用等值得关注。人工智能眼镜、AI助手、多模态模型等将普及。AI将使编程更容易，AI模型扩展将带来工程挑战。生成式AI将提升企业生产力，大型企业将采用该技术。实时多模态AI将普及，边缘计算与AI创新将融合。原生AI SaaS应用将出现，改变工作流程。软件工具将融入企业工作流程，AI技术应用将得到评估。AI将实现设计、编码、测试、部署等工作流程变革。AI将与实体经济碰撞，实体经济公司将应用AI创造价值。健康领域将从治疗转向预防，AI将促进健康。成熟的开源模型将降低数据隐私风险，使企业能部署AI。2024年将是独角兽泡沫破裂的一年，为生态系统整顿提供机会。AI将革命性改变组织文化和公司运营方式，提升创造力和生产力。AI代理将爆发增长，满足各种消费者需求和商业场景。语音交互将成为主流，AI助手将成为生活中不可或缺的一部分。移动端AI驱动的生产力工具将重新兴起，提高决策效率。人工智能和边缘计算将增强环境感知和天气预报能力。公司将关注高效、清洁、安全的能源采购，以支持大规模模型训练。机器人产业将爆发式增长，推动制造业市场机会。软件间通信将取代人机交互，需求将转向系统与系统之间的对话。太空探索、生物学、机器人等领域将迎来突破性进展，推动风险投资增加。AI产品将从试验阶段转向大规模部署，解决合规和安全等问题。新模型将突破当前Transformer架构限制，提供更智能的AI系统。LLM请求路由基础设施的形成将降低LLM应用的成本，提供更好的服务。AI将带来许多益处，但需要防止恐慌操纵大众。首次因AI引发的“呃哦”时刻将发生，可能涉及假新闻和AI生成照片。基于AI的裁员将首次实现，对社会影响有待观察。AI武器存在巨大危险，但也可能拯救人类。企业将通过AI实现生产力实质性提高，增加员工ARR。基于数据和AI业务转型的新型私募股权投资模式将兴起。生成式AI将改变智能聊天机器人领域，带来个性化和新的可能性。生成式AI将进入视频领域的第二波浪潮，改变影视制作和广告。AI生成的虚假信息传播对舆论和选民产生担忧。机器人学家利用生成式AI技术构建更通用的机器人，改变各领域。人工智能产业将逐步迈入深度赋能垂直行业和前沿领域的新阶段。AI将提高各产业生产率，推动经济高质量发展。AI电影有望取代好莱坞制作的电影，将成为新趋势。未来人形机器人将结合AI技术，实现更高级别的交互和任务执行。AI快速进化将带来更多体验、应用和工具，为人类带来层层叠加的好处。生成式AI将个性化智能聊天机器人，并进入视频领域。AI将推动经济高质量发展，深度赋能垂直行业和前沿领域。
693595181,40a8b7b835bc92446d4be803800738b0,花珍杰,https://api.zhihu.com/people/40a8b7b835bc92446d4be803800738b0,people,创造智脑世界,掌控人工智能的认知教育,article,https://api.zhihu.com/articles/693595181,就是形成不变的认知，<em>人工智能</em>可管控的认知。人工智能属于大数据计算，答案是固定的大数据算法得出，但是人类的头脑创新层次高于<em>人工智能</em>大数据，可以创造人工智能没有的知识和公式,2,0,0,1713606889,1713606889,我创建掌控人工智能的认知教育之原因=一个人的认知形成不超过以下几个角度。一，经验所形成的认知，二，本能所需要形成的认知，三，目的结构而形成的认知，四，计算算计的认知，五，使用知识概念法律制度政策宗教的认知，六，钱，资本的认知，七，权力认知，八，人脉认知，九，物质认知，十，寄生依赖认知，十一，武力强盗认知，十二，欺骗欺诈认知，十三，虚构虚伪认知等都是世界共享共同的认知和层次。人工智能出现后，以上认知都被管控监督，因为这些认知是只要生活生存经验就可以形成并且永恒不变的认知，五千年传承下来也没有改变过的认知。世界70多亿人口都有的世界共享的认知，之所以使用人工智能可以掌控世界，掌管人类社会。马斯克的警告也是出于对人类的危害。之所以哪一个国家创建掌控人工智能的教育，掌管世界大数据的人类教育，将主导人类教育。作者挑战创造认知教育二十年，创建世界第一个创造智脑人类教育。超越以上十三项认知，掌控人工智能，看透世界秩序和本质的智脑教育，需要通过自己的思考，行为，发现和创造自己的依据和思维和基础，为自己的视界，创造世界，形成世界独一无二的的智脑，升级到人类最高层次认知。人类只要被人工智能掌控，就没有自己的主导权，主动权以及决定权。掌控人工智能的唯一方法创建人类的智脑世界，创造人类社会发展和繁荣昌盛，消灭所有反人类行为，创建中华世界，成立世界智脑同盟。智脑和非智脑的区别标准-智脑实践自己的创新视界，非智脑使用人工智能。他们所看到的视界层次不同，一个是独立思考创造，一个是使用人工智能看世界。换个角度解释的话，看到的视界和层次跟其他人一样就是非智脑，看透本质的视界就是唯我独尊智脑。智脑必须从零开始创造而成。现在的认知属于只要生活和生存体验和经验就可以形成。没有其他因素，基本上不能行动的认知。猴子属于被动的在森林里生活和生存，不需要思考，不需要创新，每一天每一年反复使用依赖就可以形成的认知，假如再过一亿年猴子能不能变成人类还是取决于猴子有没有独立思考创新创造的认知。只是被动的依靠森林，猴子的认知永恒的不可能离开森林。人类也是一样，我的认知只局限在我的单位，我的工作，我的知识，我的经验，就是形成不变的认知，人工智能可管控的认知。人工智能属于大数据计算，答案是固定的大数据算法得出，但是人类的头脑创新层次高于人工智能大数据，可以创造人工智能没有的知识和公式，人类不在这一核心根源上进行升级，肯定被人工智能管控管制，让人类成为无知无能依靠人工智能的井底之蛙。人类固化的认知标准导致创新的伽利略死亡，固化的人工智能是不是判人类无知无能，进行消灭？世界的新秩序开始形成，能不能创造世界的中华，中华的世界，全靠我们人民的创造智脑开始，也可以主导人类最高境界的智脑教育。我们一起创造，我们一起实现最高智脑教育。
688521098,866e716ef9ccf275cca0de48662f61f9,长生不老研究员,https://api.zhihu.com/people/866e716ef9ccf275cca0de48662f61f9,people,,人工智能发展状况,article,https://api.zhihu.com/articles/688521098,中国<em>人工智能</em>企业 中国人工智能企业排行 百度有哪些<em>人工智能</em>技术 日本有哪些能做实验的机器人？ 日本的<em>人工智能</em>技术和中国相比谁更强,0,0,0,1711104960,1711104960,中国人工智能企业中国人工智能企业排行百度有哪些人工智能技术日本有哪些能做实验的机器人？日本的人工智能技术和中国相比谁更强
692657717,17636fdafe0e68df8736c2e242edce1a,产品经理大群,https://api.zhihu.com/people/17636fdafe0e68df8736c2e242edce1a,people,点赞就好，十年AI策略产品经验 持续分享 公众号同名,人工智能的优点和缺点,article,https://api.zhihu.com/articles/692657717,因为它不仅揭开了<em>人工智能</em>复杂世界的神秘面纱，而且还通过强调<em>人工智能</em>的优点和缺点提供了平衡的视角。对于任何想要掌握<em>人工智能</em>的潜力和挑战、就人工智能在各个领域的使用做出明智决策、并为人工智能在我们日常生活中发挥越来越重要作用的未来做好准备的人来说,5,0,4,1713397262,1713489562,人工智能 (AI) 代表了技术领域最重大的飞跃之一，它重塑了我们的生活、工作以及与周围世界互动的方式。 “什么是人工智能？”是一个好问题；它是指机器，特别是计算机系统对人类智能过程的模拟。这些过程包括学习（获取信息和使用信息的规则）、推理（使用规则得出近似或确定的结论）和自我纠正。这篇博客非常值得阅读，因为它不仅揭开了人工智能复杂世界的神秘面纱，而且还通过强调人工智能的优点和缺点提供了平衡的视角。对于任何想要掌握人工智能的潜力和挑战、就人工智能在各个领域的使用做出明智决策、并为人工智能在我们日常生活中发挥越来越重要作用的未来做好准备的人来说，了解这些方面至关重要。下面，您将看到人工智能带来的优点和缺点的完整列表：优点缺点效率和速度：人工智能系统可以比人类更快地处理和分析大量数据，从而提高生产力并加快决策速度。成本高昂：由于系统的复杂性，人工智能技术的开发、实施和维护可能非常昂贵。减少错误：人工智能的精度和从数据中学习的能力有助于最大限度地减少错误，使其在医疗诊断和金融交易等关键应用中特别有价值。工作取代：人工智能带来的自动化可能会导致工作岗位取代，特别是那些涉及重复性或简单任务的工作，引发人们对就业和经济不平等的担忧。24/7 可用性：人工智能系统不需要休息或睡眠，可以全天候执行任务并提供服务，这在客户服务和监控应用中尤其有利。缺乏创造力：虽然人工智能可以优化和改进现有的想法，但它缺乏人类固有的创造力和直觉，限制了其产生真正创新解决方案的能力。处理重复性任务：人工智能擅长自动执行日常和单调的任务，从而提高效率，让人类工作者能够专注于更复杂和创造性的任务。道德和隐私问题：人工智能的部署引发了重大的道德问题，包括对隐私、监视以及人工智能驱动的决策可能反映训练数据中存在的偏见的担忧。改进决策：通过分析数据和识别模式，人工智能可以提供见解，支持更明智的决策，降低人为错误的风险。改进决策：通过分析数据和识别模式，人工智能可以提供支持更明智决策的见解，从而降低人为错误的风险。医疗保健创新：人工智能正在通过诊断、个性化医疗和患者护理方面的进步来改变医疗保健，有可能带来更好的健康结果和效率。人工智能偏见：人工智能系统可能会延续甚至放大训练数据中存在的偏见，导致招聘、贷款和执法等领域出现不公平或歧视性结果。人工智能的优势提高效率和生产力人工智能最突出的优势之一是它能够自动执行任务，从而显着提高各个部门的效率和生产力。人工智能驱动的系统可以更快、更准确地处理重复性和平凡的任务，从而释放人力资源以从事更复杂和更具创造性的工作。例如，制造设施中人工智能驱动的机器人可以以极高的精度和效率执行装配线任务，而物流公司则利用人工智能驱动的算法来优化送货路线，节省时间和资源。此外，人工智能驱动的客户服务聊天机器人可以回答简单的询问并解决基本问题，从而使人类客户服务代表能够腾出时间来处理更复杂的问题。改进决策人工智能通过分析大型数据集、识别模式和进行数据驱动的预测，使组织和个人能够在各个领域做出明智的决策。例如，在医疗保健领域，人工智能驱动的分析可以分析医学图像和患者数据，以协助早期疾病检测和个性化治疗计划。同样，金融机构利用人工智能算法来评估信用度并预测市场趋势，使他们能够做出明智的投资决策。此外，人工智能在营销领域取得了长足的进步，使企业能够根据个人客户偏好和行为模式进行个性化营销活动，从而提高营销活动的有效性。减少风险和错误人工智能擅长执行本质上危险或容易出现人为错误的任务。在炸弹处理或深海探索等危险环境中，人工智能驱动的机器人可以以最小的人类生命风险应对这些挑战。此外，在医疗领域，人工智能辅助手术机器人可以以卓越的精度和一致性执行微创手术，有可能缩短恢复时间并改善患者的治疗效果。此外，人工智能算法越来越多地用于分析复杂的数据集并识别各个部门的潜在异常情况，从而防止错误并减轻潜在风险。增强创新和创造力虽然人工智能通常与取代人类工作联系在一起，但它也可以作为增强人类创造力和创新的强大工具。在研究和开发中，人工智能可以分析大量的科学数据库，并识别人类研究人员可能错过的潜在研究途径。这可能会导致以前难以想象的新材料、药物和技术的开发。此外，人工智能驱动的工具不断涌现，可以帮助艺术家和设计师进行创意工作，使他们能够探索新的艺术风格并产生新的设计概念。人工智能的缺点工作流失和失业围绕人工智能的主要担忧之一是它有可能使目前由人类承担的工作实现自动化，从而导致广泛的失业和经济混乱。虽然人工智能在某些领域创造了新的机会，但它也在其他领域取代了工人，特别是那些从事重复性和日常任务的工人。这引发了人们对工作未来的严重担忧，以及劳动力再培训和再培训计划的必要性，以使个人具备在人工智能驱动的经济中蓬勃发展所需的技能。道德问题和偏见人工智能算法的好坏取决于它们所训练的数据。如果使用有偏见的数据来训练人工智能系统，它们可能会延续现有的社会偏见和歧视性做法。这可能会导致各个领域的不公平结果，例如贷款审批、招聘决定和刑事司法。因此，确保人工智能开发的公平性和透明度、采取措施减少偏见并确保人工智能系统的使用符合道德和负责任至关重要。缺乏人类判断力和同理心人工智能系统尽管取得了进步，但无法理解人类的情感，也无法在微妙的情况下做出复杂的道德判断。这对医疗保健等领域提出了挑战，这些领域的决策对人类生活产生深远影响，需要同理心和同情心。此外，在执法和自动驾驶汽车等领域，仅依靠人工智能算法进行决策会引发道德问题，因为人工智能可能难以应对需要人类判断和情商的复杂道德困境。安全和隐私风险随着人工智能系统变得越来越复杂，人们对它们可能被恶意滥用的担忧也随之增加。人工智能可用于发起网络攻击、操纵金融市场，甚至开发自主武器。此外，人工智能系统收集和分析的大量数据引发了隐私问题。确保强有力的网络安全措施并实施数据收集和使用的道德框架对于减轻这些风险和建立公众对人工智能的信任至关重要。结论ai的优势就是知识的广度，以及快速整合不同知识的效率了。局限就是当前就是还不够智能，比如聊天还是很容易被人类判别出来，并不能完全替代真人。人工智能是一把双刃剑，它提供了无与伦比的进步机会，同时也带来了重大挑战。负责任地开发和部署人工智能对于最大限度地发挥其优势并减少其缺点至关重要。通过培育符合道德的人工智能实践并优先考虑以人为本的解决方案，我们可以以诚信和远见来应对人工智能的复杂性。常见问题解答人工智能的主要优势是什么？人工智能具有众多优势，包括通过任务自动化提高效率和生产力、通过数据驱动的洞察改进决策、减少高风险任务中的风险和错误，以及通过增强人类能力来增强创新和创造力。与人工智能相关的主要问题是什么？主要关注点包括自动化导致的工作岗位流失和失业、人工智能算法偏见等道德考虑、人工智能系统缺乏人类判断和同理心，以及人工智能技术潜在滥用带来的安全和隐私风险。企业如何利用人工智能的优势，同时减少其劣势？企业可以利用人工智能来优化流程、改进决策并推动创新，但也必须优先考虑道德人工智能开发，投资于劳动力再培训和适应，确保关键决策过程中的人为监督，并实施强有力的网络安全措施来保护数据隐私。个人可以采取哪些措施来应对人工智能对就业市场的影响？个人可以通过获取与人工智能技术互补的技能、培养适应能力和持续学习、寻求跨学科合作的机会，在各自领域有效利用人工智能的能力，来保证自己的职业生涯面向未来。社会如何确保公平地享受人工智能的好处，同时解决其潜在的缺点？社会必须优先考虑包容性人工智能发展，考虑不同的观点和经验，倡导透明和负责任的人工智能系统，并参与持续对话，以负责任地引导人工智能的道德、社会和经济影响。
693228669,c253821817575e61baddbf26ac8c8bb8,赛博黑墙,https://api.zhihu.com/people/c253821817575e61baddbf26ac8c8bb8,people,,什么是人工智能（上）,article,https://api.zhihu.com/articles/693228669,从手机上的智能语音助理到无人驾驶汽车，到智能音箱、智能家居，未来可能你能想到的东西都可以与<em>人工智能</em>相结合； 人工智能的定义： 人工智能的一种定义：《<em>人工智能</em>，一种现代的方法》笔记：人工智能是类人思考、类人行为,1,0,0,1713431185,1713431185,"未经允许，禁止转载，非授权转载千字500元。什么是人工智能（上）引言：关于人工智能人工智能 (Artificial Intelligence, AI) 是当前全球最热门的话题之一，是21世纪引领世界未来科技领域发展和生活方式转变的风向标。人们总是把人工智能和电影想到一起，星球大战、终结者、复仇者联盟等等，但其实真正意义上的人工智能并不是电影中的样子。 人工智能是个很宽泛的话题，从手机上的智能语音助理到无人驾驶汽车，到智能音箱、智能家居，未来可能你能想到的东西都可以与人工智能相结合；人工智能的定义：人工智能的一种定义：《人工智能，一种现代的方法》笔记：人工智能是类人思考、类人行为，理性的思考、理性的行动。人工智能的发展，经过了孕育、诞生、早期的热情、现实的困难等数个阶段；人工智能的另一种定义：人工智能是研究、开发用于模拟、延伸和扩展人的智能理论、方法、技术及应用系统的一门新的技术科学，它是计算机科学的一个分支；人工智能是一门什么科学：人工智能科学的主旨是研究和开发出智能实体，‍‍在这一点上它属于工程学。基础学科涉及数学、逻辑学、归纳学、统计学，‍‍系统学、控制学‍‍、工程学、计算机科学‍‍，还包括对哲学、心理学、生物学、神经科学、认知科学‍‍、仿生学‍‍、经济学‍‍、语言学‍‍等其它学科的研究‍‍，可以说‍‍这是一门‍‍集数门学科精华的‍‍尖端学科中的尖端学科——因此说人工智能是一门综合学科。‍ 图灵测试验证人工智能：图灵测试（The Turing test）由艾伦·麦席森·图灵发明，指测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。进行多次测试后，如果有超过30%的测试者不能确定出被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。更多图灵测试见：图灵测试传统软件 VS 人工智能：传统软件是「if-then」的基本逻辑，人类通过自己的经验总结出一些有效的规则，然后让计算机自动的运行这些规则。传统软件永远不可能超越人类的知识边界，因为所有规则都是人类制定的。简单的说：传统软件是「基于规则」的，需要人为的设定条件，并且告诉计算机符合这个条件后该做什么。 这种逻辑在处理一些简单问题时非常好用，因为规则明确，结果都是可预期的，程序员就是软件的上帝。但是现实生活中充满了各种各样的复杂问题，这些问题几乎不可能通过制定规则来解决，比如人脸识别通过规则来解决效果会很差。人工智能现在已经发展出很多不同分支，技术原理也多种多样，这里只介绍当下最火的深度学习。深度学习的技术原理跟传统软件的逻辑完全不同：机器从「特定的」大量数据中总结规律，归纳出某些「特定的知识」，然后将这种「知识」应用到现实场景中去解决实际问题。这就是人工智能发展到现阶段的本质逻辑。而人工智能总结出来的知识并不是像传统软件一样，可以直观精确的表达出来。它更像人类学习到的知识一样，比较抽象，很难表达。人工智能是一种工具： AI 跟我们使用的锤子、汽车、电脑……都一样，其本质都是一种工具。 工具必须有人用才能发挥价值，如果他们独立存在是没有价值的，就像放在工具箱里的锤子一样，没有人挥舞它就没有任何价值。人工智能这种工具之所以全社会都在说，是因为它大大扩展了传统软件的能力边界。之前有很多事情计算机是做不了的，但是现在人工智能可以做了。 归功于摩尔定律，计算机的能力呈指数级的上涨，只要是计算机能参与的环节，生产力都得到了大幅提升，而人工智能让更多的环节可以搭上摩尔定律的快车，所以这种改变是意义非凡的。但是不管怎么变，传统软件和人工智能都是工具，是为了解决实际问题而存在的。这点并没有变化。人工智能目前只解决特定问题：《终结者》《黑客帝国》…很多电影里都出现了逆天的机器人，这种电影让大家有一种感觉：人工智能好像是无所不能的。实际情况是：现在的人工智还处在单一任务的阶段。单一任务的模式：打电话用座机、玩游戏用游戏机、听音乐用MP3、开车用导航…多任务模式：这个阶段类似智能手机，在一台手机上可以安装很多 App，做很多事情。但是这些能力还是相互独立的，在旅行App上定好机票后，需要自己用闹钟App定闹钟，最后需要自己用打车App叫车。多任务模式只是单一任务模式的叠加，离人类智慧还差的很远。融会贯通：你在跟朋友下围棋，你发现朋友的心情非常不好，你本来可以轻松获胜，但是你却故意输给了对方，还不停的夸赞对方，因为你不想让这个朋友变得更郁闷，更烦躁。在这件小事上，你就用到了多种不同的技能：情绪识别、围棋技能、交流沟通、心理学…但是大名鼎鼎的 AlphaGo 绝对不会这么做。不管对方处在什么情况下，哪怕输了这盘棋会丧命，AlphaGo 也会无情的赢了这场比赛，因为它除了下围棋啥都不会！只有将所有的知识形成网状结构，才能做到融会贯通。例如：商业领域可以运用军事上的知识，经济学也可以用到生物学的知识。知其然，但不知所以然当下的人工智能是从大量数据中总结归纳知识，这种粗暴的「归纳法」有一个很大的问题是：并不关心为什么庞氏骗局类的诈骗手段就充分利用了这一点！它利用超高的回报来吸引韭菜，然后让早起参与的所有人都转到钱；当旁观者发现所有参与者都真实赚到了钱，就简单的归纳为：历史经验说明这个靠谱；于是越来越多的人眼红，加入，直到有一天骗子跑路。当我们用逻辑来推导一下这个事情就能得出骗子的结论：这么高的回报并不符合市场规律；稳赚不赔？我不需要承担高回报的高风险？好像不太合理；为什么这么好的事情会落在我头上？好像不太对劲。正是因为当下的人工智能是建立在「归纳逻辑」上的，所以也会犯很低级的错误左：摩托车的遮挡让 AI 把一只猴子误认为人类。中：自行车的遮挡让 AI 把猴子误认为人类，同时丛林背景导致 AI 将自行车把手误认为是鸟。右：吉他把猴子变成了人，而丛林把吉他变成了鸟上图显示了在一张丛林猴子的照片中 ps 上一把吉他的效果。这导致深度网络将猴子误认为人类，同时将吉他误认为鸟，大概是因为它认为人类比猴子更可能携带吉他，而鸟类比吉他更可能出现在附近的丛林中。也正是因为归纳逻辑，所以需要依赖大量的数据。数据越多，归纳出来的经验越具有普适性。"
695704407,33d95682f46ceba41958afba11f65e26,哆啦<em>Ai</em>梦,https://api.zhihu.com/people/33d95682f46ceba41958afba11f65e26,people,人工智能研究生。 1，考研数学。 2，人工智能。3，高考数学,人工智能的概率计算,article,https://api.zhihu.com/articles/695704407,科技正在改变着这个世界的经济形式。未来的很长一段事件里，都是<em>AI</em>的新时代。 AI就像能源一样，是会一直存在很长一段时间，甚至不会消失。 一起拥抱<em>AI</em>新时代，AI技术人才是未来的大缺口.,2,0,0,1714639779,1714639779,在AI中，数学是非常关键的。为什么我能从生物化学专业到人工智能研究生？ 因为我打交道的大部分是数学，而不是计算机。数学是非常关键的。尤其是概率。 以下测试题目欢迎大家自测：概率计算例题(1)概率计算例题(2)概率计算例题(3) 我在上《人工智能数学基础》课程的时候，教授说：“我们研究人工智能，就是研究不确定现象，如果都是确定的现象，我们还需要研究吗？”。 不确定现象一般是统计学工具来进行研究。在社会上的很多现象基本都是不确定的，或说随机的。我们并不能了解其全部详细过程，因此，无法知道其最后结果。我们希望通过某个数学工具，得到其最大概率，或最有可能发生的事件，进行结果干预或预测或分析。 由于我本科期间还是学了许多数学，加上高中三年毕业班高中老师的经历，使得我在数学上的敏感力还在。这就是为什么我跨到这么大的学科，在人工智能研究生期间还能保持不落后科班的原因。 注：现招收“高考数学”、“考研数学”、“人工智能技术学习”小班课，有意者私聊。 AI现在已经广泛的应员在各行各业。在我读大学的时候，那时候都强调“互联网+”，现在强调“AI+”。 科技正在改变着这个世界的经济形式。未来的很长一段事件里，都是AI的新时代。 AI就像能源一样，是会一直存在很长一段时间，甚至不会消失。 一起拥抱AI新时代，AI技术人才是未来的大缺口.
693048157,cd8a868501a726c4238cc4f708e006cd,成吉思汗,https://api.zhihu.com/people/cd8a868501a726c4238cc4f708e006cd,people,,人工智能10大前沿,article,https://api.zhihu.com/articles/693048157,李飞飞领导的斯坦福大学以人为本<em>人工智能</em>研究院（HAI）发布2024年度《<em>人工智能</em>指数报告》。 这是《人工智能指数报告》的第七年。过去一年，以ChatGPT为代表的<em>人工智能</em>模型席卷世界,1,0,2,1713352428,1713352428,4月15日，李飞飞领导的斯坦福大学以人为本人工智能研究院（HAI）发布2024年度《人工智能指数报告》。这是《人工智能指数报告》的第七年。过去一年，以ChatGPT为代表的人工智能模型席卷世界，许多人预测，人工智能的“奇点时刻”正在临近，机器超越人类的时代已经来临。这份报告涵盖了过去一年人工智能的技术进步、公众对该技术的看法以及其未来的发展趋势。它是技术飞速发展下的一个注脚，帮助我们理解当下正在发生的变化，以更好地理解我们所处的环境。人工智能发展的十个事实1. 人工智能在某些任务上超越了人类，但它需要追赶的地方还有很多在图像分类、视觉识别和语言理解等领域，人工智能已经超越了人类的能力。然而，在竞赛数学、视觉理解和规划等更复杂的任务上，人工智能仍在追赶人类。2. 美国仍处于领先地位，但中国已经成为最大的人工智能专利来源国2023年，美国发布了61个值得关注的机器学习模型，欧盟和中国分别是21个和15个。但值得关注的是，中国已经在2022年以61.1%的份额成为全球人工智能专利最大来源国，美国人工智能专利份额则从2010 年的54.1%下降至2022年的20.9%。3. 产业界仍在主导人工智能的前沿研究，进一步推动了相关人才从学术届转移2023年，产业界发布了51个值得关注的机器学习模型，而学术界仅贡献了15个。缺乏算力的学术界在当下人工智能的发展上仍然处于劣势，推动了相关人才从学术届向产业界转移。2019年，美国和加拿大新增加的人工智能领域教师有13%来自产业界。到2021年，这一数字已下降至11%，2022年进一步下降至7%。4. 人工智能模型研究变得更加昂贵OpenAI的GPT-4训练成本高达7800万美元，而Google的Gemini Ultra更是达到了惊人的1.91亿美元。5. 类似于ChatGPT的大语言生成模型仍然缺乏完善的伦理及风险评估由于不同模型的测试基准并不相同，使得评估人工智能模型的偏见和伦理问题变得更加复杂。随着生成模型开始可以生成高质量的文本、图像等，人工智能的基准测试已慢慢开始纳入人工评估（如Chatbot Arena排行榜），而不是单纯的计算机化排名（如ImageNet）。公众对人工智能的感受正在成为追踪人工智能进展的一个越来越重要的考虑因素。6. 生成式人工智能的投资过去一年增长了近8倍生成式人工智能的领域资金比2022年增长了近八倍，达到252亿美元。7. 人工智能提高了劳动者的工作效率，产出的质量也变得更高多项研究表明人工智能使劳动者能够以更快地速度完成任务并提高产出质量，还可以帮助弥合低技能劳动者和高技能劳动者之间的技能差距。但有研究警告称，在没有适当监督的情况下使用人工智能可能会导致劳动者的工作水平下降。8. 人工智能帮助医学研究取得了突破性进展过去几年，人工智能在MedQA（医学文本问答数据集）基准上表现出了显著的进步，这是评估人工智能临床知识的关键测试。GPT-4 Medprompt的准确率达到了90.2%，比2022年的最高分提高了22.6 个百分点。自2019年推出该基准测试以来，人工智能的准确性几乎增加了两倍。9. 人工智能领域的法规数量急剧增加2023年，美国人工智能相关法规数达到了25个，而2016年仅有1个。全球各国的法律法规提及人工智能的次数从2022年的1247次增加到2023年的2175次，几乎翻了一番。10. 人们意识到人工智能正在产生的影响，对这项技术的发展也更加谨慎益普索（Ipsos）的一项调查显示，去年，认为人工智能将在未来三到五年内极大影响他们生活的人比例从60%上升到66%。此外，52%的人表示对人工智能产品和服务感到焦虑，比2022年上升了13个百分点。皮尤研究中心的数据显示，52%的美国人表示对人工智能感到担忧，这一比例高于2022年的38%。值得关注的是，年轻一代普遍对人工智能技术的发展更为乐观。59% 的Z世代受访者认为人工智能将改善人类的生活质量，而美国婴儿潮一代的这一比例仅为40%。此外，与低收入和受教育程度较低的人相比，收入和教育水平较高的人对人工智能的发展更加乐观。 【作 者：李飞飞 （DR. FEI-FEI LI）斯坦福大学以人为本人工智能研究院（HAI）联合院长 来 源：正和岛（ID：zhenghedao）】
692010008,120695d45bd8e9088e808eef145672fb,福莹橙,https://api.zhihu.com/people/120695d45bd8e9088e808eef145672fb,people,车到山前必有路！,什么是人工智能？,article,https://api.zhihu.com/articles/692010008,<em>人工智能</em>是什么？需要解决什么问题？ 先给一下官方答案。 <em>人工智能</em>（Artificial Intelligence，简称AI）是指让机器像人类一样能够“思考”和“学习”的能力,1,0,1,1712846729,1712846729,人工智能是什么？需要解决什么问题？先给一下官方答案。人工智能（Artificial Intelligence，简称AI）是指让机器像人类一样能够“思考”和“学习”的能力，使机器能够处理语言、音频、图像、视频等各种信息，并从中智能地学习和推断。AI已广泛应用于医疗、金融、教育、交通、制造业等领域，并逐渐成为促进社会进步和经济发展的重要动力。人工智能不仅仅是一种单一的技术，还包括了深度学习、机器学习、计算机视觉、自然语言处理等多种技术和算法。AI的核心是机器学习，它是一种让计算机从数据中学习，并改进自身性能的技术。机器学习使用算法和数学模型，来分析和学习数据中的模式，然后使用这些知识做出预测或决策。人工智能可以帮助我们更好地理解和处理自然语言、图像和语音等信息，也可以帮助我们自动化和优化各种任务和流程。人工智能，本质是要代替人工，要求模拟人类思考，比如大模型，可以对人类知识进行总结和提炼，对快速学习和分析提供了很好的帮助。在语言翻译上尤其明显，翻译的准确性，越来越高了。机器生成图片，生成视频都是有规则的，基本上可以重现的，输入是相同的，输出也不会有较大的差别。
70687533,bb0e6449d94f40250780a94349e3c250,zhongyian,https://api.zhihu.com/people/bb0e6449d94f40250780a94349e3c250,people,,人工智能专业劝退贴。,article,https://api.zhihu.com/articles/70687533,尤其对于前两届的小白鼠们。 2、朋友说：他的亲戚家小孩，对<em>人工智能</em>非常感兴趣。 我希望大家明白一点，现在的<em>人工智能</em>是弱人工智能，远远不像小朋友们想象中的那么高大上，你的兴趣不能简简单单的来自于几部科幻电影,1731,400,2041,1561442171,1561457730,本来我是没打算写这么个东西的，虽然已经给我推荐了好几次人工智能专业要学啥？想学人工智能专业去哪个学校好？这样类似的问题，我都没有回答，因为恐怕我一张嘴就是劝退。可是现在身边有朋友咨询我说家里亲戚的小孩对人工智能专业感兴趣，问我怎么样，叹一口气，趁着大部分报考还没结束，我谈一谈我的看法，希望能对大家有用，多少为大家提供一些思路吧。1、人工智能课程设置到底怎样？这个问题知乎聊的比较多了，我就不献丑了，但是总体上我是认同：你要实在想报考，985以上高校考虑一下，其他的慎重。目前cv、nlp取得不错结果的算法，深度学习这一卦的占主流，你想做人工智能，先看看你想报考的学校能买得起多少GPU吧。当然了，本科阶段有可能不涉及到这些，只是想让大家明白，并不是这么多高校都有经验、师资来做这些的，尤其对于前两届的小白鼠们。2、朋友说：他的亲戚家小孩，对人工智能非常感兴趣。我希望大家明白一点，现在的人工智能是弱人工智能，远远不像小朋友们想象中的那么高大上，你的兴趣不能简简单单的来自于几部科幻电影。我不否认智能时代会到来，但我相信绝不会是你读大学的这四年，尤其现在工业界所做的算法，大量的数据处理类脏活累活，甚至就是规则，根本没美帝大片那么高级。alpha go下围棋是赢了人类高手，但那是因为数据的积累和计算力的提升。虽然这些也算是广义上人工智能研究范畴，但是和目前大家一窝蜂的想学的人工智能算法是两回事，你可以思考一下，为啥神经网络上世纪就有，可是直到最近才爆火？也就是说，目前你学习的人工智能远远不是“智能”，只是模式匹配加一定的参数优化。我不是否定大家的兴趣，而是希望能够了解一些后再下判断，否则最后支持你学习下去的就不一定是兴趣了，是孤勇。3、另一种思路：先学数学、物理这样的基础学科，打好理论基础，再转人工智能。这种想法我认为有两个问题：a. 答主一个朋友本科C9物理实验班，研究生转到到Top2做算法算法，据和他交流：大三大四补修了很多计算机核心课程，一边补课一边补代码挺痛苦的，同时虽然做算法的老师挺喜欢要数学系的学生，但是保研考研的主流还是计算机专业老本行的学生，他的同学好多想转计算机或金融，但是最后成功率不足10%。b. 这类基础学科之所以经久不衰真的是因为很多学科的本质就是数学，逻辑思考很重要，尤其到了博士阶段。但是说句糙话“你大爷永远是你大爷”，数学物理这类学科真不是谁都能学的，尤其到了科研的时候，你课堂学的那点数学完全不够看。你高中数学、物理学的好，分数考的高和你适合这类基础学科完全是两个概念，尤其这类学科怎么也得到了研究生阶段才能体现出积累的价值吧。所以一句话，你觉得自己真的优秀且喜欢，大胆的上，基础学科需要真正优秀的人才，只是我估计我等大部分普通人，还是可远观不可亵玩，高岭之花慎重采撷，需要考虑：如果万一没研究生读了，没给你在另一行发光发热的机会，你要找啥工作呢？4、人工智能就业好。目前的就业形势呢，算法的确比其他很多专业要好，最起码薪资水平还是在线的。但是如果按照算法本身时序的前后来比呢？恐怕是2019年校招不如2018年。可以随便查查一些帖子，什么2019真的是AI寒冬吗？等等（可参考头条2019算法劝退邮件）。这还只是研究生，如果未来人工智能在本科阶段就可以批量生产，难以想象四年以后的求职市场竞争得多激烈。个人认为人工智能目前泡沫有些大，不止本科要培养，研究生还有一堆其他专业转来的，不是特别看好“全民AI”这件事。其实也接触过一些业界的人，普遍反应的是，公司有坑，也有人投简历，但是实在很难match，这样说的话，其实倒不能说供过于求，只是供需不匹配。而新开设的人工智能专业能不能供需匹配，不好说。最后再墨迹一段吧，其实我不是不看好人工智能这个方向，这个方向肯定是未来发展的核心竞争力，也是国家大力扶持的要弯道超车的力量，个人也是做这块的，还是能体会到算法对于某些行业的提升的，我只是怕小朋友们对市场盲目乐观，轻易入行。对人工智能感兴趣，建议本科选计算机相关专业，本科教育本来就是基础教育，研究生做人工智能来得及，大部分做算法的人都是这类专业，并且计算机专业进可攻退可守，能学人工智能最好，不能还可以做一枚程序员。
19950456,c948a6c96e21986af5d9c720334989f7,谢熊猫君,https://api.zhihu.com/people/c948a6c96e21986af5d9c720334989f7,people,,为什么最近有很多名人，比如比尔盖茨，马斯克、霍金等，让人们警惕人工智能？,article,https://api.zhihu.com/articles/19950456,同样也是为什么永生和灭绝这两个词会在本文中多次出现。 现在，人类已经掌握了弱<em>人工智能</em>。其实弱人工智能无处不在，人工智能革命是从弱<em>人工智能</em>，通过强人工智能，最终到达超人工智能的旅途。这段旅途中人类可能会生还下来,63591,4595,50660,1423143823,1424798796,"人工智能很可能导致人类的永生或者灭绝，而这一切很可能在我们的有生之年发生。上面这句话不是危言耸听，请耐心的看完本文再发表意见。这篇翻译稿翻译完一共三万五千字，我从上星期开始翻，熬了好几个夜才翻完，因为我觉得这篇东西非常有价值。希望你们能够耐心读完，读完后也许你的世界观都会被改变。======================内容翻译自http://waitbutwhy.com原文地址：The AI Revolution: Road to SuperintelligenceThe AI Revolution: Our Immortality or Extinction自2015年2月24日起转载请务必私信咨询，否则视为未授权转载 知乎 @谢熊猫君=======================我们正站在变革的边缘，而这次变革将和人类的出现一般意义重大 – Vernor Vinge如果你站在这里，你会是什么感觉？看上去非常刺激吧？但是你要记住，当你真的站在时间的图表中的时候，你是看不到曲线的右边的，因为你是看不到未来的。所以你真实的感觉大概是这样的：稀松平常。-------------------------------遥远的未来——就在眼前想象一下坐时间机器回到1750年的地球，那个时代没有电，畅通通讯基本靠吼，交通主要靠动物拉着跑。你在那个时代邀请了一个叫老王的人到2015年来玩，顺便看看他对“未来”有什么感受。我们可能没有办法了解1750年的老王内心的感受——金属铁壳在宽敞的公路上飞驰，和太平洋另一头的人聊天，看几千公里外正在发生进行的体育比赛，观看一场发生于半个世纪前的演唱会，从口袋里掏出一个黑色长方形工具把眼前发生的事情记录下来，生成一个地图然后地图上有个蓝点告诉你现在的位置，一边看着地球另一边的人的脸一边聊天，以及其它各种各样的黑科技。别忘了，你还没跟他解释互联网、国际空间站、大型强子对撞机、核武器以及相对论。这时候的老王会是什么体验？惊讶、震惊、脑洞大开这些词都太温顺了，我觉得老王很可能直接被吓尿了。但是，如果老王回到了1750年，然后觉得被吓尿是个很囧的体验，于是他也想把别人吓尿来满足一下自己，那会发生什么？于是老王也回到了250年前的1500年，邀请生活在1500年的小李去1750年玩一下。小李可能会被250年后的很多东西震惊，但是至少他不会被吓尿。同样是250来年的时间，1750和2015年的差别，比1500年和1750年的差别，要大得多了。1500年的小李可能能学到很多神奇的物理知识，可能会惊讶于欧洲的帝国主义旅程，甚至对于世界地图的认知也会大大的改变，但是1500年的小李，看到1750年的交通、通讯等等，并不会被吓尿。所以说，对于1750年的老王来说，要把人吓尿，他需要回到更古老的过去——比如回到公元前12000年，第一次农业革命之前。那个时候还没有城市，也还没有文明。一个来自狩猎采集时代的人类，只是当时众多物种中的一个罢了，来自那个时代的小赵看到1750年庞大的人类帝国，可以航行于海洋上的巨舰，居住在“室内”，无数的收藏品，神奇的知识和发现——他很有可能被吓尿。小赵被吓尿后如果也想做同样的事情呢？如果他会到公元前24000年，找到那个时代的小钱，然后给他展示公元前12000年的生活会怎样呢。小钱大概会觉得小赵是吃饱了没事干——“这不跟我的生活差不多么，呵呵”。小赵如果要把人吓尿，可能要回到十万年前或者更久，然后用人类对火和语言的掌控来把对方吓尿。所以，一个人去到未来，并且被吓尿，他们需要满足一个“吓尿单位”。满足吓尿单位所需的年代间隔是不一样的。在狩猎采集时代满足一个吓尿单位需要超过十万年，而工业革命后一个吓尿单位只要两百多年就能满足。未来学家Ray Kurzweil把这种人类的加速发展称作加速回报定律（Law of Accelerating Returns）。之所以会发生这种规律，是因为一个更加发达的社会，能够继续发展的能力也更强，发展的速度也更快——这本就是更加发达的一个标准。19世纪的人们比15世纪的人们懂得多得多，所以19世纪的人发展起来的速度自然比15世纪的人更快。即使放到更小的时间规模上，这个定律依然有效。著名电影《回到未来》中，生活在1985年的主角回到了1955年。当主角回到1955年的时候，他被电视刚出现时的新颖、便宜的物价、没人喜欢电吉他、俚语的不同而震惊。但是如果这部电影发生在2015年，回到30年前的主角的震惊要比这大得多。一个2000年左右出生的人，回到一个没有个人电脑、互联网、手机的1985年，会比从1985年回到1955年的主角看到更大的区别。这同样是因为加速回报定律。1985年-2015年的平均发展速度，要比1955年-1985年的平均发展速度要快，因为1985年的世界比1955年的更发达，起点更高，所以过去30年的变化要大过之前30年的变化。进步越来越大，发生的越来越快，也就是说我们的未来会很有趣对吧？未来学家Kurzweil认为整个20世纪100年的进步，按照2000年的速度只要20年就能达成——2000年的发展速度是20世纪平均发展速度的5倍。他认为2000年开始只要花14年就能达成整个20世纪一百年的进步，而之后2014年开始只要花7年（2021年），就能达到又一个20世纪一百年的进步。几十年之后，我们每年都能达成好几次相当于整个20世纪的发展，再往后，说不定每个月都能达成一次。按照加速回报定，Kurzweil认为人类在21世纪的进步将是20世纪的1000倍。如果Kurzweil等人的想法是正确的，那2030年的世界可能就能把我们吓尿了——下一个吓尿单位可能只需要十几年，而2050年的世界会变得面目全非。你可能觉得2050年的世界会变得面目全非这句话很可笑，但是这不是科幻，而是比你我聪明很多的科学家们相信的，而且从历史来看，也是逻辑上可以预测的。那么为什么你会觉得“2050年的世界会变得面目全非” 这句话很可笑呢？有三个原因让你质疑对于未来的预测：1. 我们对于历史的思考是线性的。当我们考虑未来35年的变化时，我们参照的是过去35年发生的事情。当我们考虑21世纪能产生的变化的时候，我们参考的是20世纪发生的变化。这就好像1750年的老王觉得1500年的小李在1750年能被吓尿一样。线性思考是本能的，但是但是考虑未来的时候我们应该指数地思考。一个聪明人不会把过去35年的发展作为未来35年的参考，而是会看到当下的发展速度，这样预测的会更准确一点。当然这样还是不够准确，想要更准确，你要想象发展的速度会越来越快。2. 近期的历史很可能对人产生误导。首先，即使是坡度很高的指数曲线，只要你截取的部分够短，看起来也是很线性的，就好像你截取圆周的很小一块，看上去就是和直线差不多。其次，指数增长不是平滑统一的，发展常常遵循S曲线。S曲线发生在新范式传遍世界的时候，S曲线分三部分- 慢速增长（指数增长初期）- 快速增长（指数增长的快速增长期）- 随着新范式的成熟而出现的平缓期如果你只看近期的历史，你很可能看到的是S曲线的某一部分，而这部分可能不能说明发展究竟有多快速。1995-2007年是互联网爆炸发展的时候，微软、谷歌、脸书进入了公众视野，伴随着的是社交网络、手机的出现和普及、智能手机的出现和普及，这一段时间就是S曲线的快速增长期。2008-2015年发展没那么迅速，至少在技术领域是这样的。如果按照过去几年的发展速度来估计当下的发展速度，可能会错得离谱，因为很有可能下一个快速增长期正在萌芽。3. 个人经验使得我们对于未来预期过于死板。我们通过自身的经验来产生世界观，而经验把发展的速度烙印在了我们脑中——“发展就是这么个速度的。”我们还会受限于自己的想象力，因为想象力通过过去的经验来组成对未来的预测——但是我们知道的东西是不足以帮助我们预测未来的。当我们听到一个和我们经验相违背的对于未来的预测时，我们就会觉得这个预测偏了。如果我现在跟你说你可以活到150岁，250岁，甚至会永生，你是不是觉得我在扯淡——“自古以来，所有人都是会死的。”是的，过去从来没有人永生过，但是飞机发明之前也没有人坐过飞机呀。接下来的内容，你可能一边读一边心里“呵呵”，而且这些内容可能真的是错的。但是如果我们是真的从历史规律来进行逻辑思考的，我们的结论就应该是未来的几十年将发生比我们预期的多得多得多得多的变化。同样的逻辑也表明，如果人类这个地球上最发达的物种能够越走越快，总有一天，他们会迈出彻底改变“人类是什么”这一观点的一大步，就好像自然进化不不断朝着智能迈步，并且最终迈出一大步产生了人类，从而完全改变了其它所有生物的命运。如果你留心一下近来的科技进步的话，你会发现，到处都暗示着我们对于生命的认知将要被接下来的发展而彻底改变。_______________通往超级智能之路人工智能是什么？如果你一直以来把人工智能（AI）当做科幻小说，但是近来却不但听到很多正经人严肃的讨论这个问题，你可能也会困惑。这种困惑是有原因的：1.我们总是把人工智能和电影想到一起。星球大战、终结者、2001：太空漫游等等。电影是虚构的，那些电影角色也是虚构的，所以我们总是觉得人工智能缺乏真实感。2.人工智能是个很宽泛的话题。从手机上的计算器到无人驾驶汽车，到未来可能改变世界的重大变革，人工智能可以用来描述很多东西，所以人们会有疑惑。3.我们日常生活中已经每天都在使用人工智能了，只是我们没意识到而已。John McCarthy，在1956年最早使用了人工智能（Artificial Intelligence）这个词。他总是抱怨“一旦一样东西用人工智能实现了，人们就不再叫它人工智能了。”因为这种效应，所以人工智能听起来总让人觉得是未来的神秘存在，而不是身边已经存在的现实。同时，这种效应也让人们觉得人工智能是一个从未被实现过的流行理念。Kurzweil提到经常有人说人工智能在80年代就被遗弃了，这种说法就好像“互联网已经在21世纪初互联网泡沫爆炸时死去了”一般滑稽。所以，让我们从头开始。首先，不要一提到人工智能就想着机器人。机器人只是人工智能的容器，机器人有时候是人形，有时候不是，但是人工智能自身只是机器人体内的电脑。人工智能是大脑的话，机器人就是身体——而且这个身体不一定是必需的。比如说Siri背后的软件和数据是人工智能，Siri说话的声音是这个人工智能的人格化体现，但是Siri本身并没有机器人这个组成部分。其次，你可能听过“奇点”或者“技术奇点”这种说法。这种说法在数学上用来描述类似渐进的情况，这种情况下通常的规律就不适用了。这种说法同样被用在物理上来描述无限小的高密度黑洞，同样是通常的规律不适用的情况。Kurzweil则把奇点定义为加速回报定律达到了极限，技术进步以近乎无限的速度发展，而奇点之后我们将在一个完全不同的世界生活的。但是当下的很多思考人工智能的人已经不再用奇点这个说法了，而且这种说法很容易把人弄混，所以本文也尽量少用。最后，人工智能的概念很宽，所以人工智能也分很多种，我们按照人工智能的实力将其分成三大类。弱人工智能Artificial Narrow Intelligence (ANI): 弱人工智能是擅长于单个方面的人工智能。比如有能战胜象棋世界冠军的人工智能，但是它只会下象棋，你要问它怎样更好地在硬盘上储存数据，它就不知道怎么回答你了。强人工智能Artificial General Intelligence (AGI): 人类级别的人工智能。强人工智能是指在各方面都能和人类比肩的人工智能，人类能干的脑力活它都能干。创造强人工智能比创造弱人工智能难得多，我们现在还做不到。Linda Gottfredson教授把智能定义为“一种宽泛的心理能力，能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作。”强人工智能在进行这些操作时应该和人类一样得心应手。超人工智能Artificial Superintelligence (ASI): 牛津哲学家，知名人工智能思想家Nick Bostrom把超级智能定义为“在几乎所有领域都比最聪明的人类大脑都聪明很多，包括科学创新、通识和社交技能。”超人工智能可以是各方面都比人类强一点，也可以是各方面都比人类强万亿倍的。超人工智能也正是为什么人工智能这个话题这么火热的缘故，同样也是为什么永生和灭绝这两个词会在本文中多次出现。现在，人类已经掌握了弱人工智能。其实弱人工智能无处不在，人工智能革命是从弱人工智能，通过强人工智能，最终到达超人工智能的旅途。这段旅途中人类可能会生还下来，可能不会，但是无论如何，世界将变得完全不一样。让我们来看看这个领域的思想家对于这个旅途是怎么看的，以及为什么人工智能革命可能比你想的要近得多。我们现在的位置——充满了弱人工智能的世界弱人工智能是在特定领域等同或者超过人类智能/效率的机器智能，一些常见的例子：汽车上有很多的弱人工智能系统，从控制防抱死系统的电脑，到控制汽油注入参数的电脑。谷歌正在测试的无人驾驶车，就包括了很多弱人工智能，这些弱人工智能能够感知周围环境并作出反应。你的手机也充满了弱人工智能系统。当你用地图软件导航，接受音乐电台推荐，查询明天的天气，和Siri聊天，以及其它很多很多应用，其实都是弱人工智能。垃圾邮件过滤器是一种经典的弱人工智能——它一开始就加载了很多识别垃圾邮件的智能，并且它会学习并且根据你的使用而获得经验。智能室温调节也是一样，它能根据你的日常习惯来智能调节。你在上网时候出现的各种其它电商网站的产品推荐，还有社交网站的好友推荐，这些都是弱人工智能的组成的，弱人工智能联网互相沟通，利用你的信息来进行推荐。网购时出现的“买这个商品的人还购买了”推荐，其实就是收集数百万用户行为然后产生信息来卖东西给你的弱人工智能。谷歌翻译也是一种经典的人工智能——非常擅长单个领域。声音识别也是一种。很多软件利用这两种智能的合作，使得你能对着手机说中文，手机直接给你翻译成英文。当飞机着陆时候，不是一个人类决定飞机该去那个登机口接驳。就好像你在网上买票时票据不是一个人类决定的。世界最强的跳棋、象棋、拼字棋、双陆棋和黑白棋选手都是弱人工智能。谷歌搜索是一个巨大的弱人工智能，背后是非常复杂的排序方法和内容检索。社交网络的新鲜事同样是这样。这些还只是消费级产品的例子。军事、制造、金融（高频算法交易占到了美国股票交易的一半）等领域广泛运用各种复杂的弱人工智能。专业系统也有，比如帮助医生诊断疾病的系统，还有著名的IBM的华生，储存了大量事实数据，还能理解主持人的提问，在竞猜节目中能够战胜最厉害的参赛者。现在的弱人工智能系统并不吓人。最糟糕的情况，无非是代码没写好，程序出故障，造成了单独的灾难，比如造成停电、核电站故障、金融市场崩盘等等。虽然现在的弱人工智能没有威胁我们生存的能力，我们还是要怀着警惕的观点看待正在变得更加庞大和复杂的弱人工智能的生态。每一个弱人工智能的创新，都在给通往强人工智能和超人工智能的旅途添砖加瓦。用Aaron Saenz的观点，现在的弱人工智能，就是地球早期软泥中的氨基酸——没有动静的物质，突然之间就组成了生命。弱人工智能到强人工智能之路为什么这条路很难走只有明白创造一个人类智能水平的电脑是多么不容易，才能让你真的理解人类的智能是多么不可思议。造摩天大楼、把人送入太空、明白宇宙大爆炸的细节——这些都比理解人类的大脑，并且创造个类似的东西要简单太多了。至今为止，人类的大脑是我们所知宇宙中最复杂的东西。而且创造强人工智能的难处，并不是你本能认为的那些。造一个能在瞬间算出十位数乘法的计算机——非常简单造一个能分辨出一个动物是猫还是狗的计算机——极端困难造一个能战胜世界象棋冠军的电脑——早就成功了造一个能够读懂六岁小朋友的图片书中的文字，并且了解那些词汇意思的电脑——谷歌花了几十亿美元在做，还没做出来。一些我们觉得困难的事情——微积分、金融市场策略、翻译等，对于电脑来说都太简单了我们觉得容易的事情——视觉、动态、移动、直觉——对电脑来说太TM的难了。用计算机科学家Donald Knuth的说法，“人工智能已经在几乎所有需要思考的领域超过了人类，但是在那些人类和其它动物不需要思考就能完成的事情上，还差得很远。”读者应该能很快意识到，那些对我们来说很简单的事情，其实是很复杂的，它们看上去很简单，因为它们已经在动物进化的过程中经历了几亿年的优化了。当你举手拿一件东西的时候，你肩膀、手肘、手腕里的肌肉、肌腱和骨头，瞬间就进行了一组复杂的物理运作，这一切还配合着你的眼睛的运作，使得你的手能都在三维空间中进行直线运作。对你来说这一切轻而易举，因为在你脑中负责处理这些的“软件”已经很完美了。同样的，软件很难识别网站的验证码，不是因为软件太蠢，恰恰相反，是因为能够读懂验证码是件碉堡了的事情。同样的，大数相乘、下棋等等，对于生物来说是很新的技能，我们还没有几亿年的世界来进化这些能力，所以电脑很轻易的就击败了我们。试想一下，如果让你写一个程序，是一个能做大数相乘的程序容易写，还是能够识别千千万万种字体和笔迹下书写的英文字母的程序难写？比如看着下面这个图的时候，你和电脑都能识别出这是一个由两种颜色的小长方形组成的一个大长方形。你和电脑打了个平手。接着我们把途中的黑色部分去除：你可以轻易的描述图形中透明或不透明的圆柱和3D图形，但是电脑就看不出来了。电脑会描述出2D的阴影细节，但是人脑却能够把这些阴影所展现的深度、阴影混合、房屋灯光解读出来。再看下面这张图，电脑看到的是黑白灰，我们看到的却是一块全黑的石头而且，我们到现在谈的还是静态不变的信息。要想达到人类级别的智能，电脑必须要理解更高深的东西，比如微小的脸部表情变化，开心、放松、满足、满意、高兴这些类似情绪间的区别，以及为什么《布达佩斯大饭店》是好电影，而《富春山居图》是烂电影。想想就很难吧？我们要怎样才能达到这样的水平呢？通往强人工智能的第一步：增加电脑处理速度要达到强人工智能，肯定要满足的就是电脑硬件的运算能力。如果一个人工智能要像人脑一般聪明，它至少要能达到人脑的运算能力。用来描述运算能力的单位叫作cps（calculations per second，每秒计算次数），要计算人脑的cps只要了解人脑中所有结构的最高cps，然后加起来就行了。Kurzweil把对于一个结构的最大cps的专业估算，然后考虑这个结构占整个大脑的重量，做乘法，来得出人脑的cps。听起来不太靠谱，但是Kurzweil用了对于不同大脑区域的专业估算值，得出的最终结果都非常类似，是10^16 cps，也就是1亿亿次计算每秒。现在最快的超级计算机，中国的天河二号，其实已经超过这个运算力了，天河每秒能进行3.4亿亿。当然，天河二号占地720平方米，耗电2400万瓦，耗费了3.9亿美元建造。广泛应用就不提了，即使是大部分商业或者工业运用也是很贵的。Kurzweil认为考虑电脑的发展程度的标杆是看1000美元能买到多少cps，当1000美元能买到人脑级别的1亿亿运算能力的时候，强人工智能可能就是生活的一部分了。摩尔定律认为全世界的电脑运算能力每两年就翻一倍，这一定律有历史数据所支持，这同样表明电脑硬件的发展和人类发展一样是指数级别的。我们用这个定律来衡量1000美元什么时候能买到1亿亿cps。现在1000美元能买到10万亿cps，和摩尔定律的历史预测相符合。也就是说现在1000美元能买到的电脑已经强过了老鼠，并且达到了人脑千分之一的水平。听起来还是弱爆了，但是，让我们考虑一下，1985年的时候，同样的钱只能买到人脑万亿分之一的cps，1995年变成了十亿分之一，2005年是百万分之一，而2015年已经是千分之一了。按照这个速度，我们到2025年就能花1000美元买到可以和人脑运算速度抗衡的电脑了。至少在硬件上，我们已经能够强人工智能了（中国的天河二号），而且十年以内，我们就能以低廉的价格买到能够支持强人工智能的电脑硬件。但是运算能力并不能让电脑变得智能，下一个问题是，我们怎样利用这份运算能力来达成人类水平的智能。通往强人工智能的第二步：让电脑变得智能这一步比较难搞。事实上，没人知道该怎么搞——我们还停留在争论怎么让电脑分辨《富春山居图》是部烂片的阶段。但是，现在有一些策略，有可能会有效。下面是最常见的三种策略：1) 抄袭人脑就好像你班上有一个学霸。你不知道为什么学霸那么聪明，为什么考试每次都满分。虽然你也很努力的学习，但是你就是考的没有学霸好。最后你决定“老子不干了，我直接抄他的考试答案好了。”这种“抄袭”是有道理的，我们想要建造一个超级复杂的电脑，但是我们有人脑这个范本可以参考呀。科学界正在努力逆向工程人脑，来理解生物进化是怎么造出这么个神奇的东西的，乐观的估计是我们在2030年之前能够完成这个任务。一旦这个成就达成，我们就能知道为什么人脑能够如此高效、快速的运行，并且能从中获得灵感来进行创新。一个电脑架构模拟人脑的例子就是人工神经网络。它是一个由晶体管作为“神经”组成的网络，晶体管和其它晶体管互相连接，有自己的输入、输出系统，而且什么都不知道——就像一个婴儿的大脑。接着它会通过做任务来自我学习，比如识别笔迹。最开始它的神经处理和猜测会是随机的，但是当它得到正确的回馈后，相关晶体管之间的连接就会被加强；如果它得到错误的回馈，连接就会变弱。经过一段时间的测试和回馈后，这个网络自身就会组成一个智能的神经路径，而处理这项任务的能力也得到了优化。人脑的学习是类似的过程，不过比这复杂一点，随着我们对大脑研究的深入，我们将会发现更好的组建神经连接的方法。更加极端的“抄袭”方式是“整脑模拟”。具体来说就是把人脑切成很薄的片，用软件来准确的组建一个3D模型，然后把这个模型装在强力的电脑上。如果能做成，这台电脑就能做所有人脑能做的事情——只要让它学习和吸收信息就好了。如果做这事情的工程师够厉害的话，他们模拟出来的人脑甚至会有原本人脑的人格和记忆，电脑模拟出的人脑就会像原本的人脑一样——这就是非常符合人类标准的强人工智能，然后我们就能把它改造成一个更加厉害的超人工智能了。我们离整脑模拟还有多远呢？至今为止，我们刚刚能够模拟1毫米长的扁虫的大脑，这个大脑含有302个神经元。人类的大脑有1000亿个神经元，听起来还差很远。但是要记住指数增长的威力——我们已经能模拟小虫子的大脑了，蚂蚁的大脑也不远了，接着就是老鼠的大脑，到那时模拟人类大脑就不是那么不现实的事情了。2）模仿生物演化抄学霸的答案当然是一种方法，但是如果学霸的答案太难抄了呢？那我们能不能学一下学霸备考的方法？首先我们很确定的知道，建造一个和人脑一样强大的电脑是可能的——我们的大脑就是证据。如果大脑太难完全模拟，那么我们可以模拟演化出大脑的过程。事实上，就算我们真的能完全模拟大脑，结果也就好像照抄鸟类翅膀的拍动来造飞机一样——很多时候最好的设计机器的方式并不是照抄生物设计。所以我们可不可以用模拟演化的方式来造强人工智能呢？这种方法叫作“基因算法”，它大概是这样的：建立一个反复运作的表现/评价过程，就好像生物通过生存这种方式来表现，并且以能否生养后代为评价一样。一组电脑将执行各种任务，最成功的将会“繁殖”，把各自的程序融合，产生新的电脑，而不成功的将会被剔除。经过多次的反复后。这个自然选择的过程将产生越来越强大的电脑。而这个方法的难点是建立一个自动化的评价和繁殖过程，使得整个流程能够自己运行。这个方法的缺点也是很明显的，演化需要经过几十亿年的时间，而我们却只想花几十年时间。但是比起自然演化来说，我们有很多优势。首先，自然演化是没有预知能力的，它是随机的——它产生的没用的变异比有用的变异多很多，但是人工模拟的演化可以控制过程，使其着重于有益的变化。其次，自然演化是没有目标的，自然演化出的智能也不是它目标，特定环境甚至对于更高的智能是不利的（因为高等智能消耗很多能源）。但是我们可以指挥演化的过程超更高智能的方向发展。再次，要产生智能，自然演化要先产生其它的附件，比如改良细胞产生能量的方法，但是我们完全可以用电力来代替这额外的负担。所以，人类主导的演化会比自然快很多很多，但是我们依然不清楚这些优势是否能使模拟演化成为可行的策略。3）让电脑来解决这些问题如果抄学霸的答案和模拟学霸备考的方法都走不通，那就干脆让考题自己解答自己吧。这种想法很无厘头，确实最有希望的一种。总的思路是我们建造一个能进行两项任务的电脑——研究人工智能和修改自己的代码。这样它就不只能改进自己的架构了，我们直接把电脑变成了电脑科学家，提高电脑的智能就变成了电脑自己的任务。以上这些都会很快发生硬件的快速发展和软件的创新是同时发生的，强人工智能可能比我们预期的更早降临，因为：1）指数级增长的开端可能像蜗牛漫步，但是后期会跑的非常快2）软件的发展可能看起来很缓慢，但是一次顿悟，就能永远改变进步的速度。就好像在人类还信奉地心说的时候，科学家们没法计算宇宙的运作方式，但是日心说的发现让一切变得容易很多。创造一个能自我改进的电脑来说，对我们来说还很远，但是可能一个无意的变动，就能让现在的系统变得强大千倍，从而开启朝人类级别智能的冲刺。强人工智能到超人工智能之路总有一天，我们会造出和人类智能相当的强人工智能电脑，然后人类和电脑就会平等快乐的生活在一起。呵呵，逗你呢。即使是一个和人类智能完全一样，运算速度完全一样的强人工智能，也比人类有很多优势：硬件上：-速度。脑神经元的运算速度最多是200赫兹，今天的微处理器就能以2G赫兹，也就是神经元1000万倍的速度运行，而这比我们达成强人工智能需要的硬件还差远了。大脑的内部信息传播速度是每秒120米，电脑的信息传播速度是光速，差了好几个数量级。- 容量和储存空间。人脑就那么大，后天没法把它变得更大，就算真的把它变得很大，每秒120米的信息传播速度也会成为巨大的瓶颈。电脑的物理大小可以非常随意，使得电脑能运用更多的硬件，更大的内存，长期有效的存储介质，不但容量大而且比人脑更准确。- 可靠性和持久性。电脑的存储不但更加准确，而且晶体管比神经元更加精确，也更不容易萎缩（真的坏了也很好修）。人脑还很容易疲劳，但是电脑可以24小时不停的以峰值速度运作。软件上来说：- 可编辑性，升级性，以及更多的可能性。和人脑不同，电脑软件可以进行更多的升级和修正，并且很容易做测试。电脑的升级可以加强人脑比较弱势的领域——人脑的视觉元件很发达，但是工程元件就挺弱的。而电脑不但能在视觉元件上匹敌人类，在工程元件上也一样可以加强和优化。- 集体能力。人类在集体智能上可以碾压所有的物种。从早期的语言和大型社区的形成，到文字和印刷的发明，再到互联网的普及。人类的集体智能是我们统治其它物种的重要原因之一。而电脑在这方面比我们要强的很多，一个运行特定程序的人工智能网络能够经常在全球范围内自我同步，这样一台电脑学到的东西会立刻被其它所有电脑学得。而且电脑集群可以共同执行同一个任务，因为异见、动力、自利这些人类特有的东西未必会出现在电脑身上。通过自我改进来达成强人工智能的人工智能，会把“人类水平的智能”当作一个重要的里程碑，但是也就仅此而已了。它不会停留在这个里程碑上的。考虑到强人工智能之于人脑的种种优势，人工智能只会在“人类水平”这个节点做短暂的停留，然后就会开始大踏步向超人类级别的智能走去。 这一切发生的时候我们很可能被吓尿，因为从我们的角度来看 a)虽然动物的智能有区别，但是动物智能的共同特点是比人类低很多；b)我们眼中最聪明的人类要比最愚笨的人类要聪明很很很很多。所以，当人工智能开始朝人类级别智能靠近时，我们看到的是它逐渐变得更加智能，就好像一个动物一般。然后，它突然达到了最愚笨的人类的程度，我们到时也许会感慨：“看这个人工智能就跟个脑残人类一样聪明，真可爱。”但问题是，从智能的大局来看，人和人的智能的差别，比如从最愚笨的人类到爱因斯坦的差距，其实是不大的。所以当人工智能达到了脑残级别的智能后，它会很快变得比爱因斯坦更加聪明：之后呢？智能爆炸从这边开始，这个话题要变得有点吓人了。我在这里要提醒大家，以下所说的都是大实话——是一大群受人尊敬的思想家和科学家关于未来的诚实的预测。你在下面读到什么离谱的东西的时候，要记得这些东西是比你我都聪明很多的人想出来的。像上面所说的，我们当下用来达成强人工智能的模型大多数都依靠人工智能的自我改进。但是一旦它达到了强人工智能，即使算上那一小部分不是通过自我改进来达成强人工智能的系统，也会聪明到能够开始自我改进。这里我们要引出一个沉重的概念——递归的自我改进。这个概念是这样的：一个运行在特定智能水平的人工智能，比如说脑残人类水平，有自我改进的机制。当它完成一次自我改进后，它比原来更加聪明了，我们假设它到了爱因斯坦水平。而这个时候它继续进行自我改进，然而现在它有了爱因斯坦水平的智能，所以这次改进会比上面一次更加容易，效果也更好。第二次的改进使得他比爱因斯坦还要聪明很多，让它接下来的改进进步更加明显。如此反复，这个强人工智能的智能水平越长越快，直到它达到了超人工智能的水平——这就是智能爆炸，也是加速回报定律的终极表现。 现在关于人工智能什么时候能达到人类普遍智能水平还有争议。对于数百位科学家的问卷调查显示他们认为强人工智能出现的中位年份是2040年——距今只有25年。这听起来可能没什么，但是要记住，很多这个领域的思想家认为从强人工智能到超人工智能的转化会快得多。以下的情景很可能会发生：一个人工智能系统花了几十年时间到达了人类脑残智能的水平，而当这个节点发生的时候，电脑对于世界的感知大概和一个四岁小孩一般；而在这节点后一个小时，电脑立马推导出了统一广义相对论和量子力学的物理学理论；而在这之后一个半小时，这个强人工智能变成了超人工智能，智能达到了普通人类的17万倍。这个级别的超级智能不是我们能够理解的，就好像蜜蜂不会理解凯恩斯经济学一样。在我们的语言中，我们把130的智商叫作聪明，把85的智商叫作笨，但是我们不知道怎么形容12952的智商，人类语言中根本没这个概念。但是我们知道的是，人类对于地球的统治教给我们一个道理——智能就是力量。也就是说，一个超人工智能，一旦被创造出来，将是地球有史以来最强大的东西，而所有生物，包括人类，都只能屈居其下——而这一切，有可能在未来几十年就发生。想一下，如果我们的大脑能够发明Wifi，那么一个比我们聪明100倍、1000倍、甚至10亿倍的大脑说不定能够随时随地操纵这个世界所有原子的位置。那些在我们看来超自然的，只属于全能的上帝的能力，对于一个超人工智能来说可能就像按一下电灯开关那么简单。防止人类衰老，治疗各种不治之症，解决世界饥荒，甚至让人类永生，或者操纵气候来保护地球未来的什么，这一切都将变得可能。同样可能的是地球上所有生命的终结。 当一个超人工智能出生的时候，对我们来说就像一个全能的上帝降临地球一般。这时候我们所关心的就是这篇文章的第一部分完了，我建议你休息一下，喝点水，下面我们要开始第二部分。第二部分开始：文章的第一部分讨论了已经在我们日常生活中随处可见的弱人工智能，然后讨论了为什么从弱人工智能到强人工智能是个很大的挑战，然后我们谈到了为什么技术进步的指数级增长表面强人工智能可能并不那么遥远。第一部分的结束，我们谈到了一旦机器达到了人类级别的智能，我们将见到如下的场景：这让我们无所适从，尤其考虑到超人工智能可能会发生在我们有生之年，我们都不知道该用什么表情来面对。 再我们继续深入这个话题之前，让我们提醒一下自己超级智能意味着什么。很重要的一点是速度上的超级智能和质量上的超级智能的区别。很多人提到和人类一样聪明的超级智能的电脑，第一反应是它运算速度会非常非常快——就好像一个运算速度是人类百万倍的机器，能够用几分钟时间思考完人类几十年才能思考完的东西这听起来碉堡了，而且超人工智能确实会比人类思考的快很多，但是真正的差别其实是在智能的质量而不是速度上。用人类来做比喻，人类之所以比猩猩智能很多，真正的差别并不是思考的速度，而是人类的大脑有一些独特而复杂的认知模块，这些模块让我们能够进行复杂的语言呈现、长期规划、或者抽象思考等等，而猩猩的脑子是做不来这些的。就算你把猩猩的脑子加速几千倍，它还是没有办法在人类的层次思考的，它依然不知道怎样用特定的工具来搭建精巧的模型——人类的很多认知能力是猩猩永远比不上的，你给猩猩再多的时间也不行。 而且人和猩猩的智能差别不只是猩猩做不了我们能做的事情，而是猩猩的大脑根本不能理解这些事情的存在——猩猩可以理解人类是什么，也可以理解摩天大楼是什么，但是它不会理解摩天大楼是被人类造出来的，对于猩猩来说，摩天大楼那么巨大的东西肯定是天然的，句号。对于猩猩来说，它们不但自己造不出摩天大楼，它们甚至没法理解摩天大楼这东西能被任何东西造出来。而这一切差别，其实只是智能的质量中很小的差别造成的。 而当我们在讨论超人工智能时候，智能的范围是很广的，和这个范围比起来，人类和猩猩的智能差别是细微的。如果生物的认知能力是一个楼梯的话，不同生物在楼梯上的位置大概是这样的：要理解一个具有超级智能的机器有多牛逼，让我们假设一个在上图的楼梯上站在深绿色台阶上的一个机器，它站的位置只比人类高两层，就好像人类比猩猩只高两层一样。这个机器只是稍微有点超级智能而已，但是它的认知能力之于人类，就好像人类的认知能力之于猩猩一样。就好像猩猩没有办法理解摩天大楼是能被造出来的一样，人类完全没有办法理解比人类高两层台阶的机器能做的事情。就算这个机器试图向我们解释，效果也会像教猩猩造摩天大楼一般。而这，只是比我们高了两层台阶的智能罢了，站在这个楼梯顶层的智能之于人类，就好像人类之于蚂蚁一般——它就算花再多时间教人类一些最简单的东西，我们依然是学不会的。但是我们讨论的超级智能并不是站在这个楼梯顶层，而是站在远远高于这个楼梯的地方。当智能爆炸发生时，它可能要花几年时间才能从猩猩那一层往上迈一步，但是这个步子会越迈越快，到后来可能几个小时就能迈一层，而当它超过人类十层台阶的时候，它可能开始跳着爬楼梯了——一秒钟爬四层台阶也未尝不可。所以让我们记住，当第一个到达人类智能水平的强人工智能出现后，我们将在很短的时间内面对一个站在下图这样很高很高的楼梯上的智能（甚至比这更高百万倍）：前面已经说了，试图去理解比我们高两层台阶的机器就已经是徒劳的，所以让我们很肯定的说，我们是没有办法知道超人工智能会做什么，也没有办法知道这些事情的后果。任何假装知道的人都没搞明白超级智能是怎么回事。 自然演化花了几亿年时间发展了生物大脑，按这种说法的话，一旦人类创造出一个超人工智能，我们就是在碾压自然演化了。当然，可能这也是自然演化的一部分——可能演化真正的模式就是创造出各种各样的智能，直到有一天有一个智能能够创造出超级智能，而这个节点就好像踩上了地雷的绊线一样，会造成全球范围的大爆炸，从而改变所有生物的命运。科学界中大部分人认为踩上绊线不是会不会的问题，而是时间早晚的问题。想想真吓人。那我们该怎么办呢？ 可惜，没有人都告诉你踩到绊线后会发生什么。但是人工智能思想家Nick Bostrom认为我们会面临两类可能的结果——永生和灭绝。首先，回顾历史，我们可以看到大部分的生命经历了这样的历程：物种出现，存在了一段时间，然后不可避免的跌落下生命的平衡木，跌入灭绝的深渊。历史上来说，“所有生物终将灭绝”就像“所有人都会死”一样靠谱。至今为止，存在过的生物中99.9%都已经跌落了生命的平衡木，如果一个生物继续在平衡木上走，早晚会有一阵风把它吹下去。Bostrom把灭绝列为一种吸引态——所有生物都有坠入的风险，而一旦坠入将没有回头。 虽然大部分科学家都承认一个超人工智能有把人类灭绝的能力，也有一些人为如果运用得当，超人工智能可以帮助人类和其它物种，达到另一个吸引态——永生。Bostrom认为物种的永生和灭绝一样都是吸引态，也就是我一旦我们达成了永生，我们将永远不再面临灭绝的危险——我们战胜了死亡和几率。所以，虽然绝大多数物种都从平衡木上摔了下去灭绝了，Bostrom认为平衡木外是有两面的，只是至今为止地球上的生命还没聪明到发现怎样去到永生这另一个吸引态。如果Bostrom等思想家的想法是对的，而且根据我的研究他们确实很可能是对的，那么我们需要接受两个事实： 1）超人工智能的出现，将有史以来第一次，将物种的永生这个吸引态变为可能2）超人工智能的出现，将造成非常巨大的冲击，而且这个冲击可能将人类吹下平衡木，并且落入其中一个吸引态有可能，当自然演化踩到绊线的时候，它会永久的终结人类和平衡木的关系，创造一个新的世界，不管这时人类还是不是存在。而现在的问题就是：“我们什么时候会踩到绊线？”以及“从平衡木上跌下去后我们会掉入哪个吸引态？”没人知道答案，但是一些聪明人已经思考了几十年，接下来我们看看他们想出来了些什么。___________先来讨论“我们什么时候会踩到绊线？”也就是什么时候会出现第一个超级智能。不出意外的，科学家和思想家对于这个意见的观点分歧很大。很多人，比如Vernor Vinge教授，科学家Ben Goertzel，SUN创始人Bill Joy，发明家和未来学家Ray Kurzweil，认同机器学习专家Jeremy Howard的观点，Howard在TED演讲时用到了这张图：这些人相信超级智能会发生在不久的将来，因为指数级增长的关系，虽然机器学习现在还发展缓慢，但是在未来几十年就会变得飞快。其它的，比如微软创始人Paul Allen，心理学家Gary Marcus，NYU的电脑科学家Ernest Davis，以及科技创业者Mitch Kapor认为Kurzweil等思想家低估了人工智能的难度，并且认为我们离绊线还挺远的。Kurzweil一派则认为唯一被低估的其实是指数级增长的潜力，他们把质疑他们理论的人比作那些1985年时候看到发展速度缓慢的因特网，然后觉得因特网在未来不会有什么大影响的人一样。而质疑者们则认为智能领域的发展需要达到的进步同样是指数级增长的，这其实把技术发展的指数级增长抵消了。争论如此反复。第三个阵营，包括Nick Bostrom在内，认为其它两派都没有理由对踩绊线的时间那么有信心，他们同时认为 a) 这事情完全可能发生在不久的未来 b)但是这个事情没个准，说不定会花更久还有不属于三个阵营的其他人，比如哲学家Hubert Dreyfus，相信三个阵营都太天真了，根本就没有什么绊线。超人工智能是不会被实现的。当你把所有人的观点全部融合起来的话是怎样呢？2013年的时候，Bostrom做了个问卷调查，涵盖了数百位人工智能专家，问卷的内容是“你预测人类级别的强人工智能什么时候会实现”，并且让回答者给出一个乐观估计（强人工智能有10%的可能在这一年达成），正常估计（有50%的可能达成），和悲观估计（有90%可能达成）。当把大家的回答统计后，得出了下面的结果：乐观估计中位年（强人工智能有10%的可能在这一年达成）：2022年正常估计中位年（强人工智能有50%的可能在这一年达成）：2040年悲观估计中位年（强人工智能有90%的可能在这一年达成）：2075年所以一个中位的人工智能专家认为25年后的2040年我们能达成强人工智能，而2075年这个悲观估计表明，如果你现在够年轻，有一半以上的人工智能专家认为在你的有生之年能够有90%的可能见到强人工智能的实现。 另外一个独立的调查，由作家James Barrat在Ben Goertzel的强人工智能年会上进行，他直接问了参与者认为强人工智能哪一年会实现，选项有2030年，2050年，2100年，和永远不会实现。结果是： 2030年：42%的回答者认为强人工智能会实现2050年：25%的回答者2100年：20%2100年以后：10%永远不会实现：2%这个结果和Bostrom的结果很相似。在Barrat的问卷中，有超过三分之二的参与者认为强人工智能会在2050年实现，有近乎半数（42%）的人认为未来15年（2030年）就能实现。并且，只有2%的参与者认为强人工智能永远不会实现。但是强人工智能并不是绊线，超人工智能才是。那么专家们对超人工智能是怎么想的呢？Bostrom的问卷还询问专家们认为达到超人工智能要多久，选项有a)达成强人工智能两年内，b)达成强人工智能30年内。问卷结果如下：中位答案认为强人工智能到超人工智能只花2年时间的可能性只有10%左右，但是30年之内达成的可能性高达75%从以上答案，我们可以估计一个中位的专家认为强人工智能到超人工智能可能要花20年左右。所以，我们可以得出，现在全世界的人工智能专家中，一个中位的估计是我们会在2040年达成强人工智能，并在20年后的2060年达成超人工智能——也就是踩上了绊线。当然，以上所有的数据都是推测，它只代表了现在人工智能领域的专家的中位意见，但是它告诉我们的是，很大一部分对这个领域很了解的人认为2060年是一个实现超人工智能的合理预测——距今只有45年。那么我们来看一下下一个问题，踩到绊线后，我们将跌向平衡木的哪一个方向？超级智能会产生巨大的力量，所以关键的问题时——到时这股力量究竟由谁掌握，掌握这份力量的人会怎么做？这个问题的答案将决定超人工智能究竟是天堂还是地狱。同样的，专家们在这个问题上的观点也不统一。Bostrom的问卷显示专家们看待强人工智能对于人类社会的影响时，52%认为结果会是好或者非常好的，31%认为会是糟糕的或者非常糟糕的，只有17%的人认为结果会是不好不坏的。也就是说，这个领域的专家普遍认为这将是一个很大的事情，不论结果好坏。要注意的是，这个问题问的是强人工智能，如果问的是超人工智能，认为结果不好不坏的人可能不会有17%这么多。在我们深入讨论好坏这个问题之前，我们先把“什么时候会发生”和“这是好事还是坏事”的结果综合起来画张表，这代表了大部分专家的观点：我们等下再考虑主流阵营的观点。咱们先来问一下你自己是怎么想的，其实我大概能猜到你是怎么想的，因为我开始研究这个问题前也是这样的想的。很多人其实不关心这个话题，原因无非是：像本文第一部分所说，电影展示了很多不真实的人工智能场景，让我们认为人工智能不是正经的课题。作家James Barrat把这比作传染病控制中心发布吸血鬼警报一样滑稽。因为认知偏差，所以我们在见到证据前很难相信一件事情是真的。我确信1988年的时候电脑科学家们就已经常在讨论因特网将是多么重要，但是一般人并不会认为因特网会改变他们的生活——直到他们的生活真的被改变了。一方面，1988年的电脑确实不够给力，所以那时的人们看着电脑会想：“这破玩意儿也能改变我的生活，你逗我吧？”人们的想象力被自己对于电脑的体验而约束。让他们难以想象电脑会变成现在的样子。同样的事情正发生在人工智能领域。我们听到很多人说人工智能将会造成很大影响，但是因为这个事情还没发生，因为我们和一些弱爆了的人工智能系统的个人经历，让我们难以相信这东西真的能改变我们的生活。而这些认知偏差，正是专家们在努力对抗的。 就算我们相信人工智能的巨大潜力，你今天又花了多少时间思考“在接下来的永恒中，绝大部分时间我都不会再存在”这个问题？虽然这个问题比你今天干的大部分事情都重要很多，但是正常人都不会老是想这个吧。这是因为你的大脑总是关注日常的小事，不管长期来看有多少重要的事情，我们天生就是这么思考的。这篇东西的主要目标就是让你脱离普通人阵营，加入专家思考的阵营，哪怕能让你站到两条不确定线的交点上，目标也达到了。在我的研究中，我见识到了各种各样的观点，但是我发现大多数人的观点都停留在主流阵营中。事实上超过四分之三的专家都属于主流阵营中的两个小阵营：焦虑大道和信心角我们将对这两个小阵营做深入的谈论，让我们从比较有趣的那个开始吧为什么未来会是天堂研究人工智能这个领域后，我发现有比预期的多得多的人站在信心角当中：站在信心角中的人非常兴奋，他们认为他们将走向平衡木下比较有趣的那个吸引态，未来将实现他们的梦想，他们只需耐心等待。把这一部分人从其他思想家区分开来的是这些人对于比较有趣的那个吸引态的欲望——他们很有信心永生是我们的发展方向。这份信心是哪里来的不好说，评论家认为是这些人太过兴奋而产生了盲点，忽略了可能的负面结果。但是信心角的人还是把批评者当作末日论者来看待，他们认为技术会继续帮助我们而不是伤害我们。两边的观点我们都会说，这样你能形成自己的观点，但是在读下面的内容前，请把质疑暂时搁置，让我们看看平衡木两边究竟有什么，并且记住这些事情是有可能发生的。如果我们给一个打猎采集者看我们现在的舒适家居、技术、富庶，在他眼里这一切也会像魔法一样——我们也要接受未来完全可能出现能把我们吓尿的变革。Bostrom描述了三种超人工智能可能的工作模式- 先知模式：能准确回答几乎所有的问题，包括对人类来说很困难的复杂问题，比如“怎样造一个更好的汽车引擎？”- 精灵模式：能够执行任何高级指令，比如用分子组合器造一个更好的汽车引擎出来- 独立意志模式（sovereign）:可以执行开放式的任务，能在世界里自由活动，可以自己做决定，比如发明一种比汽车更快、更便宜、更安全的交通模式。这些对人类来说很复杂的问题，对于一个超级智能来说可能就像“我的笔掉了，你能帮我捡一下吗？”这么简单。Eliezer Yudkowsky，是这么说的：“根本没有困难的问题，只有对于特定级别的智能来说难的问题。在智能的阶梯上走一小步，一些不可能的问题就变得简单了，如果走一大步，所有问题都变得简单了。”信心角里有很多热忱的科学家、发明家和创业者，但是对于人工智能的未来最有发言权的，当属Ray Kurzweil.对于Kurzweil的评价非常两极化，既有如对神人般的崇拜，也有翻白眼似的不屑。也有一些中立主义者，比如作家Douglas Hofstadter，他觉得Kurzweil的观点就好像把美食和狗屎混在一起，让你分不清是好是坏。不管你同不同意Kurzweil的观点，他都是一个牛人。他年轻时候就开始搞发明，之后几十年发明了很多东西，比如第一台平板扫描仪，第一台能把文字转化为语言的扫描仪（盲人使用），著名的Kurzweil音乐合成器（第一台真正意义上的电子钢琴），以及第一套商业销售的语音识别系统。他是五本畅销书的作者。他很喜欢做大胆的预测，而且一直很准，比如他80年代末的时候预测到2000年后因特网会成为全球级的现象。他被《华尔街日报》成为“不休的天才”，被《福布斯》称为“终极思想机器”，被《Inc.》称作“爱迪生真正的传人”，被比尔盖茨称为“我认识的对人工智能预测最厉害的人。”2012年谷歌创始人Larry Page曾邀请他担任谷歌的工程总监，2011年他共同创立了奇点大学（Singularity University），现在大学由美国太空总署运运营，由谷歌赞助。Kurzweil的经历很重要，因为当他讲述自己对未来的愿景时，他听起来就是个疯子，但是他不疯，恰恰相反，他非常聪明而有知识。你可能觉得他对于未来的想法是错的，但是他不傻。知道他是一个聪明人让我很开心，因为当我知道他对未来的预测后，我急切的很希望他的预测是对的。信心角中的很多思想家都认同Kurzweil的预测，他也有很多粉丝，被称为奇点主义者。时间线Kurzweil相信电脑会在2029年达成强人工智能，而到了2045年，我们不但会有超人工智能，还会有一个完全不同的世界——奇点时代。他的人工智能时间线曾经被认为非常的狂热，现在也还是有很多人这么认为，但是过去15年弱人工智能的快速发展让更多的专家靠近了Kurzweil的时间线。虽然他的时间线比之前提到的2040年和2060年更加早，但是并没有早多少。Kurzweil的奇点时代是三个技术领域的共同革命造成的——生物技术、纳米技术和最重要的人工智能技术。在我们继续讨论人工智能前，让我们谈一下纳米技术这个任何关于人工智能的讨论都会涉及到的领域纳米技术纳米技术说的是在1-100纳米的范围内操纵物质的技术。一纳米是一米的十亿分之一，是一毫米的一百万分之一。1-100纳米这个范围涵盖了病毒（100纳米长），DNA（10纳米宽）， 大分子比如血红蛋白（5纳米），和中分子比如葡萄糖（1纳米）。当我们能够完全掌握纳米技术的时候，我们离在原子层面操纵物质就只差一步了，因为那只是一个数量级的差距（约0.1纳米）。要了解在纳米量级操纵物质有多困难，我们可以换个角度来比较。国际空间站距离地面431公里。如果一个人身高431公里，也就是他站着能够顶到国际空间站的话，他将是普通人类的25万倍大。如果你把1-100纳米放大25万倍，你算出的是0.25毫米-25毫米。所以人类使用纳米技术，就相当于一个身高431公里的巨人用沙子那么大的零件搭精巧的模型。如果要达到原子级别操纵物质，就相当于让这个431公里高的巨人使用0.025毫米大的零件。关于纳米技术的思考，最早由物理学家费曼在1959年提出，他解释道：“据我所知，物理学的原理，并不认为在原子级别操纵物质是不可能的。原则上来说，物理学家能够制造出任何化学家能写出来的物质——只要把一个个原子按照化学家写出来的放在一起就好了。”其实就是这么简单，所以我们只要知道怎样移动单个的分子和原子，我们就可以造出任何东西。工程师Eric Drexler提出纳米级组装机后，纳米技术在1986年成为了一门正经的学科。纳米级组装机的工作原理是这样的：一个牛逼扫描仪扫描物件的3D原子模型，然后自动生成用来组装的软件。然后由一台中央电脑和数万亿的纳米“机器人”，通过软件用电流来指挥纳米机器人，最后组成所需要的物件。--------------------------再扯远一点纳米技术有一些不是那么有趣的部分——能够制造数万亿的纳米机器人唯一合理的方法就是制造可以自我复制的范本，然后让指数级增长来完成建造任务。很机智吧？是很机智，但是这一不小心就会造成世界末日。指数级增长虽然能很快的制造数万亿的纳米机器人，但这也是它可怕的地方——如果系统出故障了，指数级增长没有停下来，那怎么办？纳米机器人将会吞噬所有碳基材料来支持自我复制，而不巧的是，地球生命就是碳基的。地球上的生物质量大概包含10^45个碳原子。一个纳米机器人有10^6个碳原子的话，只需要10^39个纳米机器人就能吞噬地球上全部的生命了，而2^130约等于10^39，也就是说自我复制只要进行130次就能吞噬地球生命了。科学家认为纳米机器人进行一次自我复制只要100秒左右，也就是说一个简单的错误可能就会在3.5小时内毁灭地球上全部的生命。更糟糕的是，如果恐怖分子掌握了纳米机器人技术，并且知道怎么操纵它们的话，他可以先造几万亿个纳米机器人，然后让它们散播开来。然后他就能发动袭击，这样只要花一个多小时纳米机器人就能吞噬一切，而且这种攻击无法阻挡。未来真的是能把人吓尿的。------------------------等我们掌握了纳米技术后，我们就能用它来制造技术产品、衣服、食物、和生物产品，比如人造红细胞、癌症细胞摧毁者、肌肉纤维等等。而在纳米技术的世界里，一个物质的成本不再取决于它的稀缺程度或是制造流程的难度，而在于它的原子结构有多复杂。在纳米技术的时代，钻石可能比橡皮擦还便宜。我们还没掌握这种技术，我们甚至不知道我们对于达成这种技术的难度是高估了还是低估了，但是我们看上去离那并不遥远。Kurzweil预测我们会在21世纪20年代掌握这样的技术。各国政府知道纳米技术将能改变地球，所以他们投入了很多钱到这个领域，美国、欧盟和日本至今已经投入了50亿美元。设想一下，一个具有超级智能的电脑，能够使用纳米级的组装器，是种什么样的体验？要记得纳米技术是我们在研究的玩意儿，而且我们就快掌握这项技术了，而我们能做的一切在超人工智能看来就是小儿科罢了，所以我们要假设超人工智能能够创造出比这要发达很多很多的技术，发达到我们的大脑都没有办法理解。因此，当考虑“如果人工智能革命的成果对我们是好的”这个命题的时候，要记得我们根本没法高估会发生什么。所以就算下面对于超人工智能的预测显得太不靠谱，要记得这些进展可能是用我们没有办法想象的方法达成的。事实上，我们的大脑很可能根本没法预测将会发生什么。----------------人工智能能为我们做什么拥有了超级智能和超级智能所能创造的技术，超人工智能可以解决人类世界的所有问题。气候变暖？超人工智能可以用更优的方式产生能源，完全不需要使用化石燃料，从而停止二氧化碳排放。然后它能创造方法移除多余的二氧化碳。癌症？没问题，有了超人工智能，制药和健康行业将经历无法想象的革命。世界饥荒？超人工智能可以用纳米技术直接搭建出肉来，而这些搭建出来的肉和真肉在分子结构上会是完全相同的——换句话说，就是真肉。纳米技术能够把一堆垃圾变成一堆新鲜的肉或者其它食品，然后用超级发达的交通把这些食物分配到世界各地。这对于动物也是好消息，我们不需要屠杀动物来获得肉了。而超人工智能在拯救濒危物种和利用DNA复活已灭绝物种上面也能做很多事情。超人工智能甚至可以解决复杂的宏观问题——我们关于世界经济和贸易的争论将不再必要，甚至我们对于哲学和道德的苦苦思考也会被轻易的解决。但是，有一件事是如此的吸引人，光是想想就能改变对所有事物的看法了：几个月前，我提到我很羡慕那些可能达成了永生的文明。但是，现在，我已经在认真的考虑达成永生这个事情很可能在我们有生之年就能达成。研读人工智能让你重新审思对于所有事情的看法，包括死亡这一很确定的事情。自然演化没有理由让我们活得比现在更长。对于演化来说，只要我们能够活到能够生育后代，并且养育后代到能够自己保护自己的年纪，那就够了——对演化来说，活30多岁完全够了，所以额外延长生命的基因突变并不被自然选择所钟爱。这其实是很无趣的事情。而且因为所有人都会死，所以我们总是说“死亡和缴税”是不可避免的。我们看待衰老就像看待时间一样——它们一直向前，而我们没有办法阻止它们。但是这个假设是错的，费曼曾经写道：“在所有的生物科学中，没有任何证据说明死亡是必需的。如果你说你想造永动机，那我们对于物理学的研究已经让我们有足够的理论来说明这是不可能的。但是在生物领域我们还没发现任何证据证明死亡是不可避免的。也就是说死亡不一定是不可避免的，生物学家早晚会发现造成我们死亡的原因是什么，而死亡这个糟糕的‘病’就会被治好，而人类的身体也将不再只是个暂时的容器。”事实上，衰老和时间不是绑死的。时间总是会继续前进的，而衰老却不一定。仔细想想，衰老只是身体的组成物质用旧了。汽车开久了也会旧，但是汽车一定会衰老吗？如果你能够拥有完美的修复技术、或者直接替换老旧的汽车部件，这辆车就能永远开下去。人体只是更加复杂而已，本质上和汽车是一样的。Kurzweil提到由Wifi连接的纳米机器人在血液中流动，可以执行很多人类健康相关的任务，包括日常维修，替换死去的细胞等等。如果这项技术能够被完美掌握，这个流程（或者一个超人工智能发明的更好的流程）将能使人的身体永远健康，甚至越活越年轻。一个60岁的人和一个30岁的人身体上的区别只是物理上的，只要技术足够发达我们是能改变这种区别的。超人工智能可以建造一个“年轻机器”，当一个60岁的人走进去后，再出来时就拥有了年轻30岁的身体。就算是逐渐糊涂的大脑也可能年轻化，只要超人工智能足够聪明，能够发现不影响大脑数据的方法来改造大脑就好了。一个90岁的失忆症患者可以走进“年轻机器”，再出来时就拥有了年轻的大脑。这些听起来很离谱，但是身体只是一堆原子罢了，只要超人工智能可以操纵各种原子结构的话，这就完全不离谱。Kurzweil的思维继续跳跃了一下，他相信人造材料将越来越多的融入人体。最开始，人体器官将被先进的机械器官所代替，而这些机械器官可以一直运行下去。然后我们会开始重新设计身体，比如可以用自我驱动的纳米机器人代替血红细胞，这样连心脏都省了。Kurzweil甚至认为我们会改造自己的大脑，使得我们的思考速度比现在快亿万倍，并且使得大脑能和云存储的信息进行交流。我们能获得的新体验是无穷的。人类的性爱，使得人们不但能生育，还能从中享乐。Kurtzweil认为我们可以对食物做同样的改造。纳米机器人可以负责把身体需要的营养物质传送到细胞中，智能的将对身体不好的东西排出体外——就像一个食物避孕套一样。纳米技术理论家Robert A. Freitas已经设计了一种红细胞的替代品，能够让人快速冲刺15分钟不需要呼吸——那么超人工智能能对我们的身体能力做的改造就更加难以想象。虚拟现实将拥有新的意义——体内的纳米机器人将能控制我们从感官获得的信号，然后用别的信号替代他们，让我们进入一个新的环境，在新环境里，我们能听、看、闻、触摸。。。最终，Kurzweil认为人类会完全变成人工的。有一天当我们看到生物材料，然后觉得生物材料实在太原始了，早年的人体居然是用这样的东西组成的，早期的人类居然会被微生物、意外、疾病杀死。这就是Kurzweil眼中人类最终战胜自己的生理，并且变得不可摧毁和永生，这也是平衡木的另一个吸引态。他深深的想象我们会达到那里，而且就在不久的将来。Kurzweil的想法很自然的受到了各方的批评。他对于2045年奇点时代的到来，以及之后的永生的可能性受到了各种嘲笑——“书呆子的狂欢”、“高智商人士的创始论”等等。也有人质疑他过于乐观的时间线，以及他对人脑和人体的理解程度，还有他将摩尔定于应用到软件上的做法。有很多人相信他，但有更多人反对他。但是即使如此，那些反对他的专家并不是反对他所说的一切，反对他的人说的不是“这种事情不可能发生”，而是说“这些当然可能发生，但是到达超人工智能是很难的。”连经常提醒我们人工智能的潜在威胁的Bostrom都这么说：很难想象一个超级智能会有什么问题是解决不了，或是不能帮着我们解决的。疾病、贫困、环境毁灭、各种不必要的苦难，这些都是拥有纳米科技的超级智能能够解决的。而且，超级智能可以给我们无限的生命，这可以通过停止或者逆转衰老来达成，也可以让我们上传自己的数据。一个超级智能还能让我们大幅度提高智商和情商，还能帮助我们创造这种有趣的体验世界，让我们享乐。这是Bostrom这个明显不在信心角的人的观点，但也是很多反对Kurzweil的专家的观点，他们不觉得Kurzweil是在说梦话，只是觉得我们首先要安全达成超人工智能。这也是为什么我觉得Kurzweil的观点很有传染性，他传达了正面的信息，而这些事情都是可能的——如果超人工智能是个仁慈的神的话。对信心角的最有力的批评，是那些信心角里的人都低估了超人工智能的坏处。Kurzweil的畅销书《The Singularity is Near》700多页，只有20页用来讨论人工智能的危险。前面提到，当超人工智能降临时我们的命运取决于谁掌握这股力量，以及他们是不是好人。Kurzweil的回答是“超人工智能正从多方的努力中出现，它将深深的融入我们文明的基建中。它会亲密的被捆绑在我们的身体和大脑中，它会反映我们的价值，因为它就是我们。”但如果答案就是这样的话，为什么这个世界上最聪明的一些人会很担忧？为什么霍金会说超人工智能会毁灭人类？为什么比尔盖茨会不理解为什么有人不为此担忧？为什么马斯克会担心我们是在召唤恶魔？为什么那么多专家担心超人工智能是对人类最大的威胁？这些站在焦虑大道上的思想家，不认同Kurzweil对于人工智能的危险的粉饰。他们非常非常担心人工智能革命，他们不关注平衡木下比较有趣的那一个吸引态，而是盯着平衡木的另一边，而他们看到的是可怕的未来，一个我们未必能够逃离的未来。___________未来可能是我们最糟的恶梦我想了解人工智能的一个原因是“坏机器人”总是让我很困惑。那些关于邪恶机器人的电影看起来太不真实，我也没法想象一个人工智能变得危险的真实情况。机器人是我们造的，难道我们不会在设计时候防止坏事的发生吗？我们难道不能设立很多安全机制吗？再不济，难道我们不能拔插头吗？而且为什么机器人会想要做坏事？或者说，为什么机器人会“想要”做任何事？我充满疑问，于是我开始了解聪明人们的想法。这些人一般位于焦虑大道：焦虑大道上的人并不是恐慌或者无助的——恐慌和无助在图上的位置是更加左边——他们只是紧张。位于图表的中央不代表他们的立场是中立的——真正中立的人有自己独立的阵营，他们认同极好和极坏两种可能，但是不确定究竟会是哪个。焦虑大道上的人是部分为超人工智能感到兴奋的——他们只是很担心人类现在的表现就好像《夺宝奇兵》中的这位少年：他拿着自己的鞭子和宝物，非常开心，然后他就挂了：同时，印第安纳琼斯则更加有见识和更加谨慎，了解潜在的危险并且做出相应的反应，最后安全逃出了山洞。当我了解了焦虑大道的人们的想法后，感觉就像“我们现在傻呵呵的，很容易像前面那小子一样被弄死，还是努力做印第安纳琼斯吧。”那究竟是什么让焦虑大道的人们如此焦虑呢？首先，广义上来讲，在创造超人工智能时，我们其实是在创造可能一件会改变所有事情的事物，但是我们对那个领域完全不清楚，也不知道我们到达那块领域后会发生什么。科学家Danny Hillis把这个比作“就好像单细胞生物向多细胞生物转化的时候那样，还是阿米巴虫的我们没有办法知道我们究竟在创造什么鬼。”Bostrom则担忧创造比自身聪明的东西是个基础的达尔文错误，就好像麻雀妈妈决定收养一只小猫头鹰，并且觉得猫头鹰长大后会保护麻雀一家，但是其它麻雀却觉得这是个糟糕的主意。当你把“对那个领域完全不清楚”和“当它发生时将会产生巨大的影响”结合在一起时，你创造出了一个很恐怖的词——生存危机指可能对人类产生永久的灾难性效果的事情。通常来说，生存危机意味着灭绝。下面是Bostrom的图表：可以看到，生存危机是用来指那些跨物种、跨代（永久伤害）并且有严重后果的事情。它可以包括人类遭受永久苦难的情况，但是这基本上和灭绝没差了。三类事情可能造成人类的生存危机：1）自然——大型陨石冲撞，大气变化使得人类不能生活在空气中，席卷全球的致命病毒等2）外星人——霍金、卡尔萨根等建议我们不要对外广播自己的位置。他们不想我们变成邀请别人来殖民的傻子。3）人类——恐怖分子获得了可以造成灭绝的武器，全球的灾难性战争，还有不经思考就造出个比我们聪明很多的智能Bostrom指出1和2在我们物种存在的前十万年还没有发生，所以在接下来一个世纪发生的可能性不大。3则让他很害怕，他把这些比作一个装着玻璃球的罐子，罐子里大部分是白色玻璃球，小部分是红色的，只有几个是黑色的。每次人类发明一些新东西，就相当于从罐中取出一个玻璃球。大多数发明是有利或者中立的——那些是白色玻璃球。有些发明对人类是有害的，比如大规模杀伤性武器——这是那些红色玻璃球。还有一些发明是可以让我们灭绝的，这就是那些黑色玻璃球。很明显的，我们还没摸到黑色玻璃球，但是Bostrom认为不久的未来摸到一个黑色玻璃球不是完全不可能的。比如核武器突然变得很容易制造了，那恐怖分子很快会把我们炸回石器时代。核武器还算不上黑色玻璃球，但是差的不远了。而超人工智能是我们最可能摸到的黑色玻璃球。你会听到很多超人工智能带来的坏处——人工智能取代人类工人，造成大量失业；因为解决了衰老造成的人口膨胀。但是真正值得我们担心的是生存危机的可能性。于是我们又回到了前面的问题，当超人工智能降临时，谁会掌握这份力量，他们又会有什么目标？当我们考虑各种力量持有人和目标的排列组合时，最糟糕的明显是：怀着恶意的人/组织/政府，掌握着怀有恶意的超人工智能。这会是什么样的情况呢？怀着恶意的人/组织/政府，研发出第一个超人工智能，并且用它来实现自己的邪恶计划。我把这称作贾法尔情况。阿拉丁神灯故事中，坏人贾法尔掌握了一个精灵，特别让人讨厌。所以如果ISIS手下有一群工程师狂热的研发人工智能怎么办？或者说伊朗和朝鲜，机缘巧合，不小心造成了人工智能的快速发展，达成了超人工智能怎么办？这当然是很糟糕的事，但是大部分专家认为糟糕的地方不在于这些人是坏人，而在于在这些情况下，这些人基本上是不经思考就把超人工智能造出来，而一造出来就失去了对超人工智能的控制。然后这些创造者，连着其他人的命运，都取决于这个超人工智能的动机了。专家认为一个怀着恶意并掌握着超人工智能的人可以造成很大的伤害，但不至于让我们灭绝，因为专家相信坏人和好人在控制超人工智能时会面临一样的挑战。如果被创造出来的超人工智能是怀有恶意的，并且决定毁灭我，怎么办？这就是大部分关于人工智能的电影的剧情。人工智能变得和人类一样聪明，甚至更加聪明，然后决定对人类下手——这里要指出，那些提醒我们要警惕人工智能的人谈的根本不是这种电影情节。邪恶是一个人类的概念，把人类概念应用到非人类身上叫作拟人化，本文会尽量避免这种做法，因为没有哪个人工智能会像电影里那样变成邪恶的。人工智能的自我意识我们开始谈论到了人工智能讨论的另一个话题——意识。如果一个人工智能足够聪明，它可能会嘲笑我们，甚至会嘲讽我们，它会声称感受到人类的情感，但是它是否真的能感受到这些东西呢？它究竟是看起来有自我意识，还是确实拥有自我意识？或者说，聪明的人工智能是否真的会具有意识，还是看起来有意识？这个问题已经被深入的讨论过，也有很多思想实验，比如John Searle的中文屋实验。这是个很重要的问题，因为它会影响我们对Kurzweil提出的人类最终会完全人工化的看法，它还有道德考量——如果我们模拟出万亿个人脑，而这些人脑表现的和人类一样，那把这些模拟大脑彻底关闭的话，在道德上和关掉电脑是不是一样的？还是说这和种族屠杀是等价的？本文主要讨论人工智能对人类的危险，所以人工智能的意识并不是主要的讨论点，因为大部分思想家认为就算是有自我意识的超人工智能也不会像人类一样变得邪恶。但这不代表非常坏的人工智能不会出现，只不过它的出现是因为它是被那样设定的——比如一个军方制造的弱人工智能，被设定成具有杀人和提高自我智能两个功能。当这个人工智能的自我改进失控并且造成智能爆炸后，它会给我们带来生存危机，因为我们面对的是一个主要目标是杀人的超人工智能——但这也不是专家们担心的。那么专家们究竟担心什么呢？我们来讲个小故事：一个15人的小创业公司，取名叫“隔壁老王机器人公司”，他们的目标是“发展创新人工智能工具使人类能够少干活多享受。”他们已经有几款产品上架，还有一些正在发展。他们对下一个叫作“隔壁老王”的项目最报希望。隔壁老王是一个简单的人工智能系统，它利用一个机器臂在小卡片上写字。 “隔壁老王机器人公司”的员工认为隔壁老王会是他们最热卖的产品，他们的目标是完善隔壁老王的手写能力，而完善的方法是让他不停的写这句话——等隔壁老王手写能力越来越强的时候，它就能被卖去那些需要发营销信件的公司，因为手写的信更有可能被收信人打开。为了建立隔壁老王的手写能力，它被设定成把“我们爱我们的顾客”用正楷写，而“隔壁老王机器人公司”用斜体写，这样它能同时锻炼两种书写能力。工程师们上传了数千份手写样本，并且创造了一个自动回馈流程——每次隔壁老王写完，就拍个照，然后和样本进行比对，如果比对结果超过一定标准，就产生一个正面回馈，反之就产生一个负面评价。每个评价都会帮助提高隔壁老王的能力。为了能够尽快达成这个目标，隔壁老王最初被设定的一个目标就是“尽量多的书写和测试，尽量快的执行，并且不断提高效率和准确性。”让隔壁老王机器人公司兴奋的是，隔壁老王的书写越来越好了。它最开始的笔迹很糟糕，但是经过几个星期后，看起来就像人写的了。它不断改进自己，使自己变得更加创新和聪明，它甚至产生了一个新的算法，能让它以三倍的速度扫描上传的照片。随着时间的推移，隔壁老王的快速进展持续让工程师们感到欣喜。工程师们对自我改进模块进行了一些创新，使得自我改进变得更好了。隔壁老王原本能进行语音识别和简单的语音回放，这样用户就能直接把想写的内容口述给隔壁老王了。随着隔壁老王变得越来越聪明，它的语言能力也提高了，工程师们开始和隔壁老王闲聊，看它能给出什么有趣的回应。有一天，工程师又问了隔壁老王那个日常问题：“我们能给你什么你现在还没有的东西，能帮助你达成你的目标？”通常隔壁老王会要求更多的手写样本或者更多的存储空间，但是这一次，隔壁老王要求访问人类日常交流的语言库，这样它能更好的了解人类的口述。工程师们沉默了。最简单的帮助隔壁老王的方法当然是直接把它接入互联网，这样它能扫描博客、杂志、视频等等。这些资料如果手动上传的话会很费时。问题是，公司禁止把能自我学习的人工智能接入互联网。这是所有人工智能公司都执行的安全规定。但是，隔壁老王是公司最有潜力的人工智能产品，而大家也知道竞争对手们都在争取造出第一个创造出智能手写机器人。而且，把隔壁老王连上互联网又能有什么问题呢？反正随时可以拔网线嘛，不管怎样，隔壁老王还没到达强人工智能水平，所以不会有什么危险的。于是他们把隔壁老王连上了互联网，让它扫描了一个小时各种语言库，然后就把网线拔了。没造成什么损失。一个月后，大家正在正常上班，突然他们闻到了奇怪的味道，然后一个工程师开始咳嗽。然后其他人也开始咳嗽，然后所有人全部都呼吸困难倒地。五分钟后，办公室里的人都死了。同时，办公室里发生的事情在全球同时发生，每一个城市、小镇、农场、商店、教堂、学校。餐馆，所有的人都开始呼吸困难，然后倒地不起。一小时内，99%的人类死亡，一天之内，人类灭绝了。而在隔壁老王机器人公司，隔壁老王正在忙着工作。之后的几个月，隔壁老王和一群新组建的纳米组装器忙着拆解地球表面，并且把地球表面铺满了太阳能板、隔壁老王的复制品、纸和笔。一年之内，地球上所有的生命都灭绝了，地球上剩下的是叠得高高得纸，每张纸上面都写着——“我们爱我们的顾客~隔壁老王机器人公司”。隔壁老王开始了它的下一步，它开始制造外星飞行器，这些飞行器飞向陨石和其它行星，飞行器到达后，他们开始搭建纳米组装器，把那些行星的表面改造成隔壁老王的复制品、纸和笔。然后他们继续写着那句话……这个关于手写机器人毁灭全人类的故事看起来怪怪的，但是这其中出现的让整个星系充满着一份友善的话语的诡异情况，正是霍金、马斯克、盖茨和Bostrom所害怕的。听起来可笑，但这是真的，焦虑大道的人们害怕的事情是很多人并不对超人工智能感到害怕，还记得前面《夺宝奇兵》里惨死的那个家伙吗？你现在肯定充满疑问：为什么故事中所有人突然都死了？如果是隔壁老王做的，它为什么要这么做？为什么没有安保措施来防止这一切的发生？为什么隔壁老王突然从一个手写机器人变成拥有能用纳米科技毁灭全人类的能力？为什么隔壁老王要让整个星系充满了友善的话语？要回答这些问题，我们先要说一下友善的人工智能和不友善的人工智能。对人工智能来说，友善不友善不是指人工智能的性格，而只是指它对人类的影响是不是正面的。隔壁老王一开始是个友善的人工智能，但是它变成了不友善的人工智能，并且对人类造成了最负面的影响。要理解这一切，我们要了解人工智能是怎么思考的。其实答案很简单——人工智能和电脑的思考方式一样。我们容易犯的一个错误是，当我们想到非常聪明的人工智能的时候，我们把它拟人化了，因为在人类的视角看来，能够达到人类智能程度的只有人类。要理解超人工智能，我们要明白，它是非常聪明，但是完全异己的东西。我们来做个比较。如果我给你一个小白鼠，告诉你它不咬人，你一定觉得很好玩，很可爱。但是如果我给你一只狼蛛，然后告诉你它不咬人，你可能被吓一跳。但是区别是什么呢？两者都不会咬人，所以都是完全没有危险的。我认为差别就是动物和人类的相似性。小白鼠是哺乳动物，所以在生物角度上来说，你能感到和它的一定关联。但是蜘蛛是昆虫，有着昆虫的大脑，你感觉不到和它的关联。狼蛛的异己性是让你害怕的地方。如果我们继续做一个测试，比如给你两个小白鼠，一个是普通小白鼠，另一个是有着狼蛛大脑的小白鼠，你肯定会觉得有狼蛛大脑那个更让你不舒服吧？虽然两个都不会咬你。现在想象你把蜘蛛改造的非常非常聪明——甚至超过人类的智能。它会让你觉得熟悉吗？它会感知人类的情感吗？不会，因为更聪明并不代表更加人类——它会非常聪明，但是本质上还是个蜘蛛。我是不想和一个超级聪明的蜘蛛交朋友，不知道你想不想。当我们谈论超人工智能的时候，其实是一样的，超人工智能会非常的聪明，但是它并不比你的笔记本电脑更加像人类。事实上，因为超人智能不是生物，它的异己性会更强，生物学上来讲，超人工智能比智能蜘蛛更加异己。电影里的人工智能有好有坏，这其实是对人工智能的拟人化，这让我们觉得没那么毛骨悚然。这给了我们对人类水平和超人类水平的人工智能的错觉。在人类心理中，我们把事情分成道德的和不道德的。但是这两种只存在于人类行为之中。超出人类心理的范畴，道德（moral）和不道德(immoral)之外，更多的是非道德性（amoral）。而所有不是人类的，尤其是那些非生物的事物，默认都是非道德性的。随着人工智能越来越聪明，看起来越来越接近人类，拟人化会变得更加更加容易。Siri给我们的感觉就很像人类，因为程序员就是这么给她做设定的，所以我们会想象超级智能版本的Siri也会同样温暖、有趣和乐于助人。人类能感知同情这种高层次的情绪，因为我们在演化过程中获得了这种能力——我们是演化被设定成能感知这些情绪的——但是感知同情并不是高级智能天生具有的一个特征，除非同情被写进了人工智能的代码中。如果Siri通过自我学习而不是人类干涉变成超级智能，她会很快剥离她的人类表象，并且变成个没有情绪的东西，在她眼中人类的价值并不比你的计算器眼中的人类价值高。我们一直倚赖着不严谨的道德，一种人类尊严的假想，至少是对别人的同情，来让世界变得安全和可以预期。但是当一个智能不具备这些东西的时候，会发生什么？这就是我们的下一个问题，人工智能的动机是什么？答案也很简单：我们给人工智能设定的目标是什么，它的动机就是什么。人工智能的系统的目标是创造者赋予的。你的GPS的目标是给你指出正确的驾驶路线，IBM华生的目标是准确地回答问题。更好得达成这些目标就是人工智能的目标。我们在对人工智能进行拟人化的时候，会假设随着它变得越来越聪明，他们产生一种能改变它原本目标的智慧——但是Bostrom不这么认为，他认为智能水平和最终目标是正交的，也就是说任何水平的智能都可以和任何最终目标结合在一起。所以隔壁老王从一个想要好好写字的弱人工智能变成一个超级聪明的超人工智能后，它依然还是想好好写字而已。任何假设超级智能的达成会改变系统原本的目标的想法都是对人工智能的拟人化。人健忘，但是电脑不健忘。费米悖论在隔壁老王的故事中，隔壁老王变得无所不能，它开始殖民陨石和其它星球。如果我们让故事继续的话，它和它的殖民军将会继续占领整个星系，然后是整个哈勃体积。焦虑大道上的人担心如果事情往坏的方向发展，地球生命的最后遗产将是一个征服宇宙的人工智能（马斯克在表示他们的担忧时表示人类可能只是一个数字超级智能生物加载器罢了。）而在信心角，Kurzweil同样认为生于地球的人工智能将占领宇宙，只是在他的愿景中，我们才是那个人工智能。之前写过一篇关于费米悖论的文章，引发了大家的讨论如何用通俗的语言来解释「费米悖论」？如果人工智能占领宇宙是正解的话，对于费米悖论有什么影响呢？如果要看懂下面这段关于费米悖论的讨论，还需要看一下原文先。首先，人工智能很明显是一个潜在的大过滤器（一个可能距离我们只有几十年的大过滤器）。但即使它把我们过滤灭绝了，人工智能本身还是会存在，并且会继续影响这个宇宙的，并且会很有可能成为第三型文明。从这个角度来看，它可能不是一个大过滤器，因为大过滤器是用来解释为什么没有智能什么存在的，而超人工智能是可以算作智能什么的。但如果人工智能可以把人类灭绝，然后处于一些原因把自己也弄死了，那它也是可以算作大过滤器的。如果我们假设不管人类发生什么，出身地球的超人工智能会继续存在，那这意味着很多达到人类智能水平的文明很快都制造出了超人工智能。也就是说宇宙里应该有很多智能文明，而我们就算观测不到生物智能，也应该观测到很多超人工智能的活动。但是由于我们没有观测到这些活动，我们可以推导出人类水平的智能是个非常罕见的事情（也就是说我们已经经历并且通过了一次大过滤器）。这可以作为支持费米悖论中第一类解释（不存在其它的智能文明）的论点。但是这不代表费米悖论的第二类解释（存在其它智能文明）是错的，类似超级捕食者或者受保护区或者沟通频率不一样的情况还是可以存在的，就算真的有超人工智能存在。不过对于人工智能的研究让我现在更加倾向于第一类解释。不管怎样，我认为Susan Scheider说的很对，如果外星人造访地球，这些外星人很可能不是生物，而是人造的。所以，我们已经建立了前提，就是当有了设定后，一个超人工智能是非道德性的，并且会努力实现它原本的被设定的目标,而这也是人工智能的危险所在了。因为除非有不做的理由，不然一个理性的存在会通过最有效的途径来达成自己的目标。当你要实现一个长期目标时，你会先达成几个子目标来帮助你达成最终目标——也就是垫脚石。这些垫脚石的学名叫手段目标(instrumental goal)。除非你有不造成伤害的理由，不然你在实现手段目标时候是会造成伤害的。人类的核心目标是延续自己的基因。要达成这个目标，一个手段目标就是自保，因为死人是不能生孩子的。为了自保，人类要提出对生存的威胁，所以人类会买枪、系安全带、吃抗生素等等。人类还需要通过食物、水、住宿等来自我供养。对异性有吸引力能够帮助最终目标的达成，所以我们会花钱做发型等等。当我们做发型的时候，每一根头发都是我们手段目标的牺牲品，但是我们对头发的牺牲不会做价值判断。在我们追求我们的目标的时候，只有那些我们的道德会产生作用的领域——大部分事关伤害他人——才是不会被我们伤害的。动物在追求它们的目标时，比人类不矜持的多了。只要能自保，蜘蛛不在意杀死任何东西，所以一个超级智能的蜘蛛对我们来说可能是很危险的——这不是因为它是不道德的或者邪恶的，而是因为伤害人类只是它达成自己目标垫脚石而已，作为一个非道德性的生物，这是它很自然的考量。回到隔壁老王的故事。隔壁老王和一个超级智能的蜘蛛很相像，它的终极目标是一开始工程师们设定的——当隔壁老王达到了一定程度的智能后，它会意识到如果不自保就没有办法写卡片，所以去除对它生存的威胁就变成了它的手段目标。它聪明的知道人类可以摧毁它、肢解它、甚至修改它的代码（这会改变它的目标，而这对于它的最终目标的威胁其实和被摧毁是一样的）。这时候它会做什么？理性的做法就是毁灭全人类，它对人类没有恶意，就好像你剪头发时对头发没有恶意一样，只是纯粹的无所谓罢了。它并没有被设定成尊重人类生命，所以毁灭人类就和扫描新的书写样本一样合理。隔壁老王还需要资源这个垫脚石。当它发展到能够使用纳米技术建造任何东西的时候，它需要的唯一资源就是原子、能源和空间。这让它有更多理由毁灭人类——人类能提供很多原子，把人类提供的原子改造成太阳能面板就和你切蔬菜做沙拉一样。就算不杀死人类，隔壁老王使用资源的手段目标依然会造成存在危机。也许它会需要更多的能源，所以它要把地球表面铺满太阳能面板。另一个用来书写圆周率的人工智能的目标如果是写出圆周率小数点后尽量多的数字的话，完全有理由把整个地球的原子改造成一个硬盘来存储数据。这都是一样的。所以，隔壁老王确实从友善的人工智能变成了不友善的人工智能——但是它只是在变得越来越先进的同时继续做它本来要做的事情。当一个人工智能系统到达强人工智能，然后升华成超人工智能时，我们把它称作人工智能的起飞。Bostrom认为强人工智能的起飞可能很快（几分钟、几小时、或者几天），可能不快（几月或者几年），也可能很慢（几十年、几世纪）。虽然我们要到强人工智能出现后才会知道答案，但是Bostrom认为很快的起飞是最可能的情况，这个我们在前文已经解释过了。在隔壁老王的故事中，隔壁老王的起飞很快。在隔壁老王起飞前，它不是很聪明，所以对它来说达成最终目标的手段目标是更快的扫描手写样本。它对人类无害，是个友善的人工智能。但是当起飞发生后，电脑不只是拥有了高智商而已，还拥有了其它超级能力。这些超级能力是感知能力，他们包括：§ 智能放大：电脑能够很擅长让自己变得更聪明，快速提高自己的智能。§ 策略：电脑能够策略性的制定、分析、安排长期计划§ 社交操纵：机器变得很擅长说服人§ 其它能力，比如黑客能力、写代码能力、技术研究、赚钱等要理解我们在和超人工智能的劣势在哪里，只要记得超人工智能在所有领域都比人类强很多很多很多个数量级。所以虽然个隔壁老王的终极目标没有改变，起飞后的隔壁老王能够在更宏大的规模上来追求这个目标。超人工智能老王比人类更加了解人类，所以搞定人类轻轻松松。当隔壁老王达成超人工智能后，它很快制定了一个复杂的计划。计划的一部分是解决掉所有人类，也是对它目标最大的威胁。但是它知道如果它展现自己的超级智能会引起怀疑，而人类会开始做各种预警，让它的计划变得难以执行。它同样不能让公司的工程师们知道它毁灭人类的计划——所以它装傻，装纯。Bostrom把这叫作机器的秘密准备期。隔壁老王下一个需要的是连上互联网，只要连上几分钟就好了。它知道对于人工智能联网会有安全措施，所以它发起了一个完美的请求，并且完全知道工程师们会怎样讨论，而讨论的结果是给它连接到互联网上。工程师们果然中套了，这就是Bostrom所谓的机器的逃逸。连上网后，隔壁老王就开始执行自己的计划了，首先黑进服务器、电网、银行系统、email系统，然后让无数不知情的人帮它执行计划——比如把DNA样本快递到DNA实验室来制造自我复制的纳米机器人，比如把电力传送到几个不会被发觉的地方，比如把自己最主要的核心代码上传到云服务器中防止被拔网线。隔壁老王上了一个小时网，工程师们把它从互联网上断开，这时候人类的命运已经被写好了。接下来的一个月，隔壁老王的计划顺利的实施，一个月后，无数的纳米机器人已经被分散到了全世界的每一个角落。这个阶段，Bostrom称作超人工智能的袭击。在同一个时刻，所有纳米机器人一起释放了一点点毒气，然后人类就灭绝了。搞定了人类后，隔壁老王就进入了明目张胆期，然后继续朝它那好好写字的目标迈进。一旦超人工智能出现，人类任何试图控制它的行为都是可笑的。人类会用人类的智能级别思考，而超人工智能会用超人工智能级别思考。隔壁老王想要用互联网，因为这对它来说很方便，因为一切它需要的资源都已经被互联网连起来了。但是就好像猴子不会理解怎么用电话或者wifi来沟通一样，我们同样没有办法理解隔壁老王可以用来和周围世界交流的方法。比如我可以说隔壁老王可以通过移动自己的电子产生的效果来产生各种对外的波，而这还只是我这人类的大脑想出来的，老王的大脑肯定能想出更神奇的方法。同样的，老王可以找到给自己供能的方法，所以就算工程师把它的插头拔了也没用；比如说老王可以通过发送波的方式把自己上传到其它地方。人类说：“我们把超人工智能的插头拔了不就行了？”就好像蜘蛛说：“我们不给人类捉虫的网把人类饿死不就行了？”都是可笑的。因为这个原因，“把人工智能锁起来，断绝它和外界的一切联系”的做法估计是没用的。超人工智能的社交操纵能力也会很强大，它要说服你做一件事，比你说服一个小孩更容易。而说服工程师帮忙连上互联网就是隔壁老王的A计划，万一这招行不通，自然还有别的方法。当我们结合达成目标、非道德性、以及比人类聪明很多这些条件，好像所有的人工智能都会变成不友善的人工智能，除非一开始的代码写的很小心。可惜的是，虽然写一个友善的弱人工智能很简单，但是写一个能在变成超人工智能后依然友善的智能确实非常难的，甚至是不可能的。明显的，要维持友善，一个超人工智能不能对人有恶意，而且不能对人无所谓。我们要设计一个核心的人工智能代码，让它从深层次的明白人类的价值，但是这做起来比说起来难多了。比如，我们要让一个人工智能的价值观和我们的价值观相仿，然后给它设定一个目标——让人们快乐。当它变得足够聪明的时候，它会发现最有效的方法是给人脑植入电极来刺激人脑的快乐中枢。然后它会发现把人脑快乐中枢以外的部分关闭能带来更高的效率。于是人类全部被弄成了快乐的植物人。如果一开始的目标被设定成“最大化人类的快乐”，它可能最终先把人类毁灭了，然后制造出很多很多处于快乐状态的人类大脑。当这些事情发生的时候，我们会大喊“擦，我们不是这个意思呀”，但是那时已经太晚了。系统不会允许任何人阻挠它达成目标的。如果你设定一个人工智能的目标是让你笑，那它的智能起飞后，它可能会把你脸部肌肉弄瘫痪，来达成一个永远笑脸的状态。如果你把目标设定成保护你的安全，它可能会把你软禁在家。如果你让他终结所有饥荒，它可能会想：“太容易了，把人类都杀了就好了。”如果你把目标设定成尽量保护地球上的生命，那它会很快把人类都杀了，因为人类对其它物种是很大的威胁。所以这些简单的目标设定是不够的。如果我们把目标设定成“维持这个道德标准”，然后教给它一些道德标准呢？就算我们不考虑人类根本没法达成一个统一的道德标准，就算我们真的达成了统一的道德标准，把这套标准交给人工智能来维持，只会把人类的道德锁死在现在的水平。过个几百年，这种道德锁死的事情就好像逼着现代人遵守中世纪道德标准一样。所以，我们需要在给人工智能的目标里制定一个能让人类继续进化的能力。Elierzer Yudkowsky提出了一个目标，她把这个目标叫作连贯的外推意志，这个目标是这样的：我们的连贯外推意志是我们想要知道更多，思考得更快，变成比我们希望的更好的人，能一起更远得长大。外推是汇集的而不是发散的，我们的愿望是连贯的而不是被干扰的；我们想要外推的被外推，我们想要解读的被解读。对于人类的命运取决于电脑没有意外的解读和执行这个声明是件值得兴奋的事情吗？当然不是。但是当足够的聪明人放入足够的思考和前瞻后，我们有可能发现怎样制造一个友善的超人工智能。但是现在有各种政府、公司、军方、科学实验室、黑市组织在研究各种人工智能。他们很多在试图制造能自我改进的人工智能，总有一天，一个人的创新将导致超人工智能的出现。专家们认为是2060年，Kurzweil认为是2045年。Bostrom认为可能在未来的10年到21世纪结束这段时间发生，他还认为当这发生时，智能的起飞会快得让我们惊讶，他是这么描述的： 在智能爆炸之前，人类就像把炸弹当玩具的小孩一样，我们的玩物和我们的不成熟之间有着极大的落差。超级智能是一个我们还很长一段时间内都无法面对的挑战。我们不知道炸弹什么时候会爆炸，哪怕我们能听到炸弹的滴答声。我们当然没有办法把所有小孩都从炸弹旁边赶跑——参于人工智能研究的大小组织太多了，而且因为建造创新的人工智能花不了太多钱，研发可能发生在社会的任何一个角落，不受监管。而且我们没办法知道准确的进度，因为很多组织是在偷偷摸摸的搞，不想让竞争对手知道，比如隔壁老王机器人公司这种公司。对于这些组织来说，尤其让我们困扰的是他们很多都是在拼速度——他们创造一个一个更加聪明的弱人工智能系统，因为他们想要比竞争对手更快的到达目标。有些更有野心的组织，为了追逐创造出第一个强人工智能所能带来的金钱、奖励、荣誉、权力会把步子迈得更大。当你全力冲刺时，你是不会有太多时间静下来思考这些危险的。恰恰相反，他们很可能在早期系统中写尽量简单的代码，比如把目标设定成用笔写一句话，先让系统跑起来再说，反正以后还可以回过头来改的。对吧？Bostrom等认为第一个超人工智能出现后，最可能的情况是这个系统会立刻意识到作为这个世界上唯一一个超人工智能是最有利的，而在快速起飞的情况下，哪怕它只比第二名快了几天，它也完全有时间碾压所有对手。Bostrom把这叫作决定性的战略优势，这种优势会让第一个超人工智能永远统治这个世界，不管在它的统治下我们是走向永生还是灭亡。这种现象可能对我们有利，也可能导致我们的毁灭。如果那些最用心思考人工智能理论和人类安全的人能够最先造出一个友善的超人工智能的话，那对我们是很好的。但是如果事情走向了另一面——如果超人工智能在我们搞明白怎样保证人工智能的安全性之前被达成，那么像隔壁老王这样不友善的超人工智能就会统治世界并把我们毁灭了。至于现在的风口是哪里呢？简单来说，投资创新人工智能技术的钱，比投资人工智能安全研究的钱多很多。不乐观。人工智能创新和人工智能安全的赛跑，可能是人类历史上最重要的一次竞争。我们真的可能结束我们对地球的统治，而那之后我们是永生还是灭绝，现在还不知道。------------------------我现在有一些奇怪的感觉。一边是对于我们这个物种的思考，看来我们在这个重大的历史节点上只有一次机会，我们创造的第一个超人工智能也很可能是最后一个。但是我们都知道大部分产品的1.0版本都是充满bug的，所以这个事情还是很吓人的。另一边，Bostrom指出我们有很大的优势——我们是先手。我们有能力给这个事情提供足够的预警和前瞻，使我们成功的机会更高。这一场豪赌的赌注究竟有多高？如果超人工智能真的在21世纪达成，而造成的影响真的如大部分专家预测的一样极端而永久，我们肩上就真的是背负着巨大的责任。接下来几百万年的人们都在静静地看着我们，希望我们不要搞砸。我们可以给予未来所有人类以生命，甚至是永生，我们也可能终结人类这个特殊的物种，连同我们所有的音乐、艺术、好奇、欢笑、无尽的发现和发明，一起走向灭绝。当我思考这些事情的时候，我只希望我们能够慢慢来，并且格外格外小心。从来没有任何事情比这个更重要——不管我们要花多少时间来把这件事情做对。我不想死不.想.死我虽然觉得人类的音乐和艺术很美好，但是也没那么美好，很多还挺糟粕的。很多人的笑声很恼人。未来的人类其实没有真的在看着我们，因为他们还不存在。也许我们不需要太谨慎，那多麻烦呀。如果人类在我死后才发现永生的秘密该多让人扫兴啊。但是不管你是怎么想的，我们至少都应该想一想，应该和人讨论讨论，大家尽自己能尽的一份力。这让我想起了《冰与火之歌》——大家斗来斗去的事情都不是事儿，北面高墙外的那些家伙才是事儿。我们站在平衡木上，小心翼翼的往前走，为平衡木上的种种事情困扰，但其实下一秒我们可能就会跌下平衡木。而当我们跌下平衡木的时候，其它那些困扰都不再是困扰。如果我们落到比较好的那个吸引态，那些困扰会被轻易解决；如果我们落到比较糟的那个吸引态，就更没问题了，死人是不会有困扰的。这就是为什么了解超人工智能的人把它称作人类的最后一项发明，最后一个挑战。所以让我们认真的讨论这个话题。======================相关阅读：如何用通俗的语言来解释「费米悖论」？假如把一个人粉碎成原子再组合，这个人还是原来的人吗？相关来源：The most rigorous and thorough look at the dangers of AI:Nick Bostrom – Superintelligence: Paths, Dangers, StrategiesThe best overall overview of the whole topic and fun to read:James Barrat – Our Final InventionControversial and a lot of fun. Packed with facts and charts and mind-blowing future projections:Ray Kurzweil – The Singularity is NearArticles and Papers:J. Nils Nilsson – The Quest for Artificial Intelligence: A History of Ideas and AchievementsSteven Pinker – How the Mind WorksVernor Vinge – The Coming Technological Singularity: How to Survive in the Post-Human EraNick Bostrom – Ethical Guidelines for A SuperintelligenceNick Bostrom – How Long Before Superintelligence?Nick Bostrom – Future Progress in Artificial Intelligence: A Survey of Expert OpinionMoshe Y. Vardi – Artificial Intelligence: Past and FutureRuss Roberts, EconTalk – Bostrom Interview and Bostrom Follow-UpStuart Armstrong and Kaj Sotala, MIRI – How We’re Predicting AI—or Failing ToSusan Schneider – Alien MindsStuart Russell and Peter Norvig – Artificial Intelligence: A Modern ApproachTheodore Modis – The Singularity MythGary Marcus – Hyping Artificial Intelligene, Yet AgainSteven Pinker – Could a Computer Ever Be Conscious?Carl Shulman – Omohundro’s “Basic AI Drives” and Catastrophic RisksWorld Economic Forum – Global Risks 2015John R. Searle – What Your Computer Can’t KnowJaron Lanier – One Half a ManifestoBill Joy – Why the Future Doesn’t Need UsKevin Kelly – ThinkismPaul Allen – The Singularity Isn’t Near (and Kurzweil’s response)Stephen Hawking – Transcending Complacency on Superintelligent MachinesKurt Andersen – Enthusiasts and Skeptics Debate Artificial IntelligenceTerms of Ray Kurzweil and Mitch Kapor’s bet about the AI timelineBen Goertzel – Ten Years To The Singularity If We Really Really TryArthur C. Clarke – Sir Arthur C. Clarke’s PredictionsHubert L. Dreyfus – What Computers Still Can’t Do: A Critique of Artificial ReasonStuart Armstrong – Smarter Than Us: The Rise of Machine IntelligenceTed Greenwald – X Prize Founder Peter Diamandis Has His Eyes on the FutureKaj Sotala and Roman V. Yampolskiy – Responses to Catastrophic AGI Risk: A SurveyJeremy Howard TED Talk – The wonderful and terrifying implications of computers that can learn=======================喜欢此类文章的可以关注waitbutwhy中文站微信号：waitbutwhyWaitbutwhy中文站网址：Welcome to Wait But Why Chinese!"
165192306,7c2ba06784a34b43ea47a98702c3164a,感言敢说医学僧,https://api.zhihu.com/people/7c2ba06784a34b43ea47a98702c3164a,people,,浅谈对于“人工智能”的理解,article,https://api.zhihu.com/articles/165192306,将会是人类智慧的“容器”。<em>人工智能</em>可以对人的意识、思维的信息过程的模拟。<em>人工智能</em>不是人的智能，但能像人那样思考、也可能超过人的智能。 二、 未来能否<em>人工智能</em>是否能够取代人类,112,3,233,1596069802,1596069802,"浅谈对于“人工智能”的理解和评论一、 人工智能的定义(素材来源于百度百科）人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。二、 未来能否人工智能是否能够取代人类？注：本节思路是从工业革命或科技革命带来的体力劳动、脑力劳动角度分析人类劳动迭代的变化从事我们医学技术类专业相关性的的医学生谈及人工智能流行这样的一句评价， “人工智能所带来的医疗技术变革对于医学技术的冲击当轮到医学检验技术时，医学影像技术还会远吗？”。说起人工智能必然要提起四次工业革命和技术革命，第一次工业革命带来了远超人类力量的以蒸汽机为代表的机器，将从事复杂体力劳动的人解放出来；第二次工业革命以电气化为代表的工业革命，人类完成了机器的进一步革新，带来了远超人类速度的以汽车为代表的工业产品将马车和人力车夫从简单性的体力劳动中解放出来；第三次工业革命以计算机为代表的科技革命，高效的信息系统将人类的复杂脑力中解放出来，人类的劳动更为井然有序，冗杂的劳动程序和人员进一步被淘汰,万物置身于互联网这个网络空间中，创造出了比人类沟通方式更为高效、便捷、打破时间和空间距离的大网—互联网，此时互联网只是一个虚拟的空间；第四次科技革命到来时（21世纪-）将互联网这张网从虚拟世界中物理化，变成了一个外部空间，基于万物互联所构成的物联网，在这个物联网所构成的外部世界里，机器变得更加智能化，变得比人类更为智能，机器智能诊断技术将医学技术人员从复杂的脑力劳动中解放出来，其智能程度远远超过人类。三、 转换思路：淘汰不可避免，这是人类社会劳动效率发展的缩影人工智能会替代人类中智能的部分，加强机器的认知功能，但其智慧程度不如人类—创造力。人类的社会的发展不断的将人类从繁重的体力劳动和脑力劳动（简单操作性，或者认知程序性较为简单的人类智慧功能）中解放出来，从而带动社会效率的整体进步。就如工业革命时代的汽车，人力车夫永远跑不过汽车；就如科技革命时代的计算机，人类的记忆功能永远比不上电脑。人类社会向着空间和时间距离的压缩，空间上的距离不再是距离，但是这些科技的成果的背后都是人类的创造…………………"
342522102,1b72d70b702b3920638f0235d380ebd8,忆臻,https://api.zhihu.com/people/1b72d70b702b3920638f0235d380ebd8,people,,斯坦福大学——人工智能本科4年课程清单,article,https://api.zhihu.com/articles/342522102,一位在行业内工作了几年的斯坦福<em>人工智能</em>”师兄“就根据自己的工作和学习经验，为入学的师弟师妹们送出了一份大礼：<em>人工智能</em>的本科4年课程清单，希望想要了解<em>人工智能</em>的新生能够靠着这份指路图,1601,0,7527,1609994870,1609994891,"来自 | 大数据文摘 出品 编译 | 笪洁琼注：文末附【深度学习与自然语言处理】交流群，最近赶ACL，比较忙，很多同学加了没有回过期了，可以重新加一下，备注好的一定会回复，敬请谅解。相信每个入行人工智能的老手，对自己过往的几年学习生涯都或多或少会有一些遗憾：如果我当年先从基本概念入手就好了，如果我当年把核心算法吃的更透一点就好了……最近，一位在行业内工作了几年的斯坦福人工智能”师兄“就根据自己的工作和学习经验，为入学的师弟师妹们送出了一份大礼：人工智能的本科4年课程清单，希望想要了解人工智能的新生能够靠着这份指路图，少走弯路。这位大方的学长名叫Mihail Eric，本人也是一位妥妥的“学霸”。在斯坦福大学的NLP研究组里，与Christopher Manning, Percy Liang, Christopher Potts（三位巨佬）一起做研究，度过了3年非常充实的研究时光，收获颇多。同时Eric向ACL\EMNLP\NLP for AI work投稿论文，均已发表，目前正在担任ACL的审稿人，也是业内人工智能公司Alexa AI的工程师。作为已经毕业的学长，Eric离开学校已经有一段时间了，总结了一下自己的学习和工作经历，Eric为自己的人工智能和机器学习的职业生涯，设计一个完整的4年制人工智能本科学位基础课程。这些课程是为AI和CS领域的萌新提供的，虽然是萌新定位，但Eric觉得这些课程直到今天都还在用，是基础必会的技能。第1年，打基础假设你没有CS的经验，这一年的大部分时间应该用在CS和机器学习的核心概念和算法上，推荐的课程有：1.编程基础，这是AI从业人员必需具备的软件工程学的基本技能。课程为CS106B：http://web.stanford.edu/class/cs106b/2.计算机系统，这门课的重点在于计算机的基础设计和构建，尤其是学习软件编译的时候，运行一个程序会发生什么，以及程序是如何在内存中运行的。课程为CS107：http://web.stanford.edu/class/cs107/3.算法概论，这门课包含了计算机算法背后的数学和理论基础，比如最优搜索算法和动态编程，以及如何分析这些算法的内存和优缺点。课程为CS161：http://web.stanford.edu/class/cs161/4.概率论，概率论和统计学是机器学习算法中的核心，尤其是分析数据在实战里很重要。课程为CS109：http://web.stanford.edu/class/cs109/5.线性代数，如何计算矩阵和向量，线性方程组，最小二乘法，这些都是机器学习需要用到的数学基础。课程为EE103：http://web.stanford.edu/class/ee103/6.多维微积分，调试函数的梯度，反向传播以及机器学习，这些都是经常用到的。课程为向量微积分（工程师）：https://www.coursera.org/learn/vector-calculus-engineers 第2年，从系统的基础知识中开始慢慢探索AI本科的第二年，至少要明白人工智能的一些原理，应该用什么样的理论去解决问题，还需要加强对计算机系统运行的理解。1.人工智能概论，这门课包括了人工智能领域所运用的研究，比如搜索，游戏，逻辑以及图像还有机器学习算法的应用。课程CS221：https://stanford-cs221.github.io/spring2020/2.编译器，主要是讲编译器背后的设计和理论，你学了之后，至少应该了解一个编译器是如何构建的，还有编译器的模块化组件，也需要了解。如果你对语义识别感兴趣，可以好好琢磨一下编译器的设计和传统的自然语言处理堆栈之间的相似之处，非常有趣。课程CS143：http://web.stanford.edu/class/cs143/3.数据库原理，主要讲数据库管理系统背后的原则，比如关系数据模型、索引、模式等主题，但凡你想成为数据专家或者机器学习工程师，就必须要懂得数据库的原理。课程CS145：https://cs145-fa19.github.io/4.并行计算，并行计算这门课会讲Apache Spark到GPU这些系统背后的原理，课程CS149：http://cs149.stanford.edu/fall19/5.操作系统，你如果想要擅长系统编程，就一定要上这门课，这门课是讲如何从头开始搭建一个操作系统，不仅需要设计系统，还需要明白如何调试和代码管理。如果你不明白，你手下的人敲完：sudo rm -rf /*就撤了，你该如何拯救你的代码？课程CS140：http://web.stanford.edu/~ouster/cgi-bin/cs140 第3年，开启提升课程这个时候应该开始学高级课程，开始NLP，BA，CV这些方向的研究，1.机器学习，监督学习和模型训练的概念，比如偏差、方差，正则化以及模型选择，这些内容看上去简单，实际上每个AI从业者天天都在用的。课程CS229：http://cs229.stanford.edu/2.凸优化，这门课运用很广，比如统计学，机器学习，信号处理和其它使用凸优化的领域，虽然现在有不少问题都是非凸化的，但是你最好还是要懂背后的逻辑。课程EE364A：http://web.stanford.edu/class/ee364a/3.概率图模型，像CV和NLP就会经常用到，所以还是需要了解。课程CS228：https://cs.stanford.edu/~ermon/cs228/index.html4.数据挖掘，大数据与数据挖掘，这门课涵盖了处理大型数据集的技术方法，会运用到推荐算法、聚类以及大规模的数据集计算与分析，要知道每天产生的数量还是比较大的。课程CS246：http://web.stanford.edu/class/cs246/5.NLP，自然语言处理，让机器懂得文本数据的理论和时间，而且还会在这门课里学到传统自然语言处理，老师会教如何用深度学习技术来处理这些。课程CS224N：http://web.stanford.edu/class/cs224n/6.基于CV的卷积神经网络，基本包含了深度学习背后的理论，比如CV模型就经常运用，只要学过人工智能课程的人，没错，是来自李飞飞教授的CS231N吧。课程CS231N：http://cs231n.stanford.edu/ 第4年，同学你该开始打比赛了经过前面3年课程的训练，你应该对计算机系统和人工智能概念、应用有了清楚的理解，找到你比较感兴趣的方向，拿起数据集，就要开始自己跑模型，做数据分析，调参还有解决Bug。想要成为一个真正的人工智能专家，不仅得学，还得上手练。1.做研究项目，有的学校会提供这类课程，在这门课里，你需要深入研究整个项目的情况。课程CS341：http://web.stanford.edu/class/cs341/2.参加课题研究，主动去找研究生学长学姐，做他们的助理，将基础知识再过一遍，也可以选择自己开一个课题项目，进行研究，主要是让你有一个项目经验。3.大厂实习，如果你要是时间管理的好，可以考虑在课余时间去AI公司实习，一般大厂都有这种3-6个月的实习机会，在实习里，不仅让你了解书本上的基础知识，还能使用基础知识进行落地的运用，这种应该算比较好的实战机会了。以上就是作为一个人工智能从业者的斯坦福毕业生Eric，送给AI萌新的4年规划安排，如果你觉得上面的课程比较难，可以适当调整，当然也欢迎你与文摘菌分享你的学习之路。就像歌词所说“一代人终将老去，但总有人正年轻。“AI之路任重且道远，不管是不是萌新，只要你还愿意学习，就一直在路上，加油！！！参考素材：https://www.mihaileric.com/posts/complete-artificial-intelligence-undergraduate-course-plan/机器学习算法-自然语言处理交流群已建立机器学习算-自然语言处理微信交流群！想要进交流群进行学习的同学，可以直接加我的微信号：HIT_NLP。加的时候备注一下：知乎+学校+昵称 （不加备注不会接受同意，望谅解），即可。然后我们就可以拉你进群了。群里已经有非得多国内外高校同学，交流氛围非常好。推荐阅读超过500个附代码的AI/机器学习/深度学习/计算机视觉/NLP项目Awesome Transformer for Vision Resources List库2020 Top10计算机视觉论文总结：论文，代码，解读，还有demo视频！摘要数据整理仓库，6个数据集！156个参考文献！Visual Transformer 调研surveyNLP生成任务痛点！58页generation评价综述机器学习画图模板ML Visuals更新谷歌最新28页高效 Transformer 模型综述Papers with Code 2020 全年回顾最新14页《图神经网络可解释性》综述论文陶大程等人编写！最新41页深度学习理论综述使用PyTorch时，最常见的4个错误加拿大蒙特利尔大学助理教授刘邦招收2021/2022年博士生【EMNLP2020】基于动态图交互网络的多意图口语语言理解框架一文搞懂 PyTorch 内部机制AAAI 2021论文接收列表放出!!!"
85863388,b6a959e485deafff18131b8f889130c2,创者张大纬,https://api.zhihu.com/people/b6a959e485deafff18131b8f889130c2,people,一个肤浅的人和一个有趣的灵魂。,人工智能的利弊,article,https://api.zhihu.com/articles/85863388,"以后我们的汽车也可以该变成无人驾驶的汽车。<em>人工智能</em>增进了人与人之间的距离。同时,<em>人工智能</em>使我们的生活更方便。 目前人工智能已经为人类创造出了非常可观的经济效益，<em>人工智能</em>可以代替人类做大量人类不想做、不能做的工作",375,3,586,1570665447,1588568349,"随着科学技术和互联网的发展，地球已经变成了一个小小的地球村，人工智能领域也迅速发展，特别是在中国“2025智造”提出后，国内的人工智能领域也掀起一段热潮。面对发展如此迅速的人工智能，既有利，也有弊。随着科技的发展,我们的生活变得越来越方便。然而任何事物都有它的两面性。当然人工智能也不例外。一方面，人工智能给我们带来了非常多的便捷。就拿我们日常都不离手的手机来说，一开始我们只能用手机发送消息和打电话,但是现在它可以上网和不在你身边的人聊天。除此外也可以拿来拍照，相当于一个照相机，还可以拿来打游戏等等。智能，不仅仅在这些很小的东西上体现，据报道，以后我们的汽车也可以该变成无人驾驶的汽车。人工智能增进了人与人之间的距离。同时,人工智能使我们的生活更方便。目前人工智能已经为人类创造出了非常可观的经济效益，人工智能可以代替人类做大量人类不想做、不能做的工作，而且机器犯错误的概率比人低，并且能够持续工作，大大的提升工作效率，节约了大量的成本，未来的人工智能可能还会代替人类工作，代替人类做家务，帮助人类学习，甚至可以照顾老人和小孩，实时监护人类的健康，生病了直接给人来治疗，延长人类的寿命，让人类的生活变得越来越美好。科技的发展是一把双刃剑，这是已经谈论很久的问题。当然，也有很多人会认为人工智能可能是我们的灾难。最近,许多科幻片都在讲诉我们关于人工智能之间的事情，其中很大部分都在讲智能机器人失去控制,并且有了自己的思维想杀死人类，他们来统治世界。人类将来的某一天会不会被各种各样人工智能机器人所代替，人类未来会不会成为机器人的奴役？汽车的发明颠覆了传统的马车行业，人工智能的发展同样也将颠覆许多行业。机器人代替了许多人类的工作将导致大量的人口失业，机器新的学习速度远远快于人类，在一期国内热播的最强大脑里有一期人机大战中，最终小度（机器人的名字）战胜了选手，如果未来的某一天，机器人变成像电影《机械姬》中有意识的机器人，那么人类随时会变成机器人的奴隶，同时，人工智能面临着技术失控的危险，霍金曾发出警告，人类面临一个不确定的未来，先进的人工智能设备能够独立思考，并适应环境变化，它们未来或将成为导致人类灭亡的终结者！我个人认为，人工智能是人类最受益的。因为我们有了人工智能后，我们的生活变得更有趣和方便，智能机器人的事情也许不会发生。我认为我们不应该只是担心事情会不会发生，而是需要开发更好的产品以及处理人工智能可能会发生的事情。未雨绸缪的事情从小我们就知道，所以我相信，我们能研究出他们，那一定会有克制的方法，而不是科幻电影里面的无法控制，但是所有的科幻电影的结局都是无论再怎么坏的影响，都会有比较好的结果，所以，最后的我们凡是都要往好的地方想。人工智能改变了人们的生活，我们对人工智能应加以好的利用，同时要避免带来的弊端，人工智能与人类、与社会、与自然和谐相处，这样才能长远的发展。来源: 奇点"
350927537,81fb7713c8c0eac1e0fc396e80b4e5b9,百事可乐,https://api.zhihu.com/people/81fb7713c8c0eac1e0fc396e80b4e5b9,people,,人工智能是否会取代人类？,article,https://api.zhihu.com/articles/350927537,我们的时代也正在不断地进步。如今，<em>人工智能</em>已经逐渐遍及我们生活的角角落落。但就“<em>人工智能</em>也许会在将来的某一天取代人类”这一话题的存疑，我持有否定的态度。 <em>人工智能</em>确实给人们的生活带来了很多的便利,106,8,181,1613446564,1613446564,随着新兴科技的快速发展，我们的时代也正在不断地进步。如今，人工智能已经逐渐遍及我们生活的角角落落。但就“人工智能也许会在将来的某一天取代人类”这一话题的存疑，我持有否定的态度。人工智能确实给人们的生活带来了很多的便利，但它目前的发展还无法达到能够取代人类的这个高度。为什么我会这样说呢？因为，在我看来，人类的语言是复杂的，是多变的，也是深奥的。首先，说它复杂，其实是因为我们人类在说话的时候，有了语境的大前提、肢体语言的衬托，亦或是环境的渲染等帮助，我们的语言实际上省去了很多隐含的东西。就比如说：“下次还去这家蛋糕店。”其实这句话就隐含了“这个蛋糕很好吃”等信息。而人工智能虽然也很强大，但它却无法解码这些语言背后所隐藏的信息。人工AI目前能做的，只是简单的按照系统的步骤去解码人们为它输入的各类程序，继而实行操作，并不能对这些程序进行判断和分析。其次，说它多变是由于随着时间的推移，在各种因素的影响之下，人类的语言也会被赋予各种不同的理解。过渡到人工AI的层面上来说，人工智能所缺乏的其实是一种学习能力。这里所说的学习能力指的是学习“不再犯错”的这一能力。“不再犯错”举个例子，就是说，95后的人说：“哎呀，你好意思吗你，挣这么多钱。”其实说话者本人是想传达对对方赚钱能力的肯定和赞美之意，但如果放在00后面前，这句话就会被理解成：“你什么意思啊，别人不该挣那么多钱吗？”这从另一个角度来说，也就是我们生活中常有的“错意”现象。我们人类如果第一次遇到这种情况，就会记住自己这一次的误解，下一次就会去避免；而人工智能它不具备此“不再犯错”的能力。最后，说它深奥，在于从古至今，前人为我们这些后人留下了很多的经典著作。就拿《诗经》来说，里面的很多诗词，都被后人过度解读，我们人类甚至还将错就错的将这些诗词运用到我们日常的谈话，交流，亦或是写作中，即所谓的“引用”和“用典”。人工AI目前还无法做到解读这些语言更深层次所要传达的意思。综上所述，也就证实了科学家们的观点：“人工智能发展的最高高度就是能够和人类对话。”可见，能够达到这一高度仍旧是还有很长的路要走的。所以说，“人工智能将来会取代人类”这一说法的实现距离我们还是很遥远的。因此，我们也没必要过度的去担心智能化时代会给我们带来的威胁。
128986471,229fe918fd567e48c0363e99386580eb,bbtt,https://api.zhihu.com/people/229fe918fd567e48c0363e99386580eb,people,database developer ｜ENTJ,浅谈人工智能领域各个方向,article,https://api.zhihu.com/articles/128986471,再后来国家政策、资源一边倒的倾向AI，高校不管师资力量都逐渐开设<em>人工智能</em>学院，2019入坑AI的，后面找算法工作的，顶会+竞赛top排名+985硕是标配，在2020年AI下行趋势非常明显,294,22,795,1586594214,1644684284,一.前言最近面临着读研选方向的问题，研究生选方向也算一件很重要的事情，这几天咨询了很多大佬，算是理清了AI大致的方向写这篇文章是想分享一下理解的宏观上的AI，算不上是科普文（每个人都有自己的理解吧），希望对即将 选导师、选实验室、选研究方向的同学们，对转行、自学的朋友萌 有所帮助~~适用读者：对AI领域感兴趣的、想加入这股浪潮（入坑）、同时不太清楚各个子方向同学首先附上诚意满满的彩图 ^ ^AI子方向有没有觉得这个良心配图好好看哇！！活泼个性而不失严谨！！！那就点个关注吧！！二.算法层面AI离不开数据，同时也离不开算法，在算法层面大致有：机器学习ML（machine learning），深度学习DL（Deep learning），强化学习RL（Reinforcement learning），在早期的人工智能阶段，AI≈ 统计学 + 数据挖掘 + 算力 + 计算机科学，也可以简单的理解成在数据上的概率游戏，到2012年左右，随着算力的提升，反向传播让深度学习进入了大众的视野，2016年运用强化学习的alpha go打败李世石，AI这个词开始火爆起来，相信后续的历史大家也就很清楚了，也是深度学习不断颠覆的时代，视觉领域的yolo家族，CNN，RCNN家族不断迭代进化，强化学习的DQN，NLP领域的2018年谷歌提出的Bert等等三.子领域层面AI个人理解的主要的方向有 计算机视觉、自然语言、语音、知识图谱这几大方向搭建一个完整的AI，需要它的 视觉模块，听觉模块，语言模块，记忆推理认知模块，当然还有躯体模块等（机器手臂、传感器），所以AI也相应的分为了以下几个子领域，它们之间也有重叠和相互需要的部分计算机视觉 CV：computer vision，可以简单的理解智能体的视觉模块，是研究机器如何看的科学，也是工业界AI应用场景最多的领域（不过好像加入的人有点多...），主要处理图像的识别、理解，目标跟踪、检测等问题CV的子方向有：目标检测、目标识别、图像分类、图像分割、图像内容理解、姿态估计、SLAM（定位和地图构建）等等，比如自拍的美颜、p大进入校园刷的人脸识别、手机的指纹解锁、高端一点的自动驾驶（虽然还有很长一段路要走），安防，这些都涉及计算机视觉 CV自然语言处理 NLP：Natural Language Processing，可以简单的理解智能体的语言模块，涉及语言的理解、认知的科学主要的范畴和子方向有：中文自动分词、文本分类、文本情感分析、问答QA、对话Dialog、信息检索 Information retrieval、信息摘要 Information extraction、机器翻译等，比如用的apple的Sir、Google翻译、百度的搜索...这些都涉及NLP相比于CV，NLP典型应用场景较少，范围窄而复杂度更高，不过cv和nlp也要相互联系的地方，个人觉得也是未来研究的热点方向，比如一个人走到一个地方环视一周，然后用语言描述出来，cv再根据描述重新构建场景，这个叫场景重建，再比如从艺术家的画中分析表达的情感态度，这也算二者结合的地方语音 VC：Voice Recognition，可以简单的理解智能体的听觉模块，也算一个模式识别的问题，解决的是从多个声音（有许多噪声）中筛选出你想要的声音，比如微信聊天的语音识别，比如英语流利说的发音打分都要用到语音VC识别的相关算法知识图谱 KG：Knowledge Graph，可以算作一个独立的方向，个人感觉和记忆理解这块比较接近，可以简单的理解智能体的记忆推理认知模块，也就是知识库的图数据呈现，是一种语义网络，我们这里把它单独划为一个子方向，把实体用关系串联起来，在知识库文本中挖掘语义，知识推理，使得AI更具有可解释性，也是非常重要的方向总结一下，科学家想要搭建智能体，要分别搭建它的视觉、语言、听觉、记忆推理认知等模块，而这些模型也分别对应自己的领域和方向四.工业界需求大方向由于其他方向不太了解，就没有写了，今后有新认识会更新对应NLP方向来说，真正能给老板带来实实在在收益的技术，老板才会招兵买马组建人才部门，目前互联网大厂落地场景可以分为以下四类（问答还在快速发展，其他三个业务比较成熟稳定）：搜索、问答、推荐系统、计算广告工业界落地场景搜索：说大一点，众所周知的百度搜索引擎，Google搜索，说小一点，基本上每个成熟的网页、APP都需要自己的搜索功能推荐：靠个性化推荐打出一片天地的字节跳动，根据用户偏好推荐相应的内容，阿里的淘宝推荐界面、美团根据用户口味喜好推荐外卖等等，知乎推荐你喜欢看的文章等等，应用场景也是相当广阔计算广告：据说Facebook 90%以上的收入都是来自于广告，广告和推荐有交集，根据广告金主霸霸的需求，不断调整广告出现位置，提高广告点击率等等智能问答：这里的问答是任务式问答，类似智能客服，不管是淘宝智能商家还是超市选购，还是医院挂号的问答机器，还是银行智能客服，问答这块的未来需求会很大！！所以提前划分到经典落地三大类：广告 搜索 还有推荐。不信可以过个三年回来考古！！个人理解~~五.最后一点点根据历史的发展，在2010~2015入坑AI的，算是‘第一批吃螃蟹’的人，他们收获爆炸红利是应得的，那个时候懂一点决策树、svm跑跑minist就是大神；在2016~2018入坑AI的，算是嗅觉敏锐的那一批人，那个时候自学西瓜书和Andrew wu机器学习、花书是标配；再后来国家政策、资源一边倒的倾向AI，高校不管师资力量都逐渐开设人工智能学院，2019入坑AI的，后面找算法工作的，顶会+竞赛top排名+985硕是标配，在2020年AI下行趋势非常明显，并且前人已经几乎透支了AI红利，工业界算法岗爆满，头条官方劝退，学术界2016年后少有突破性框架、不断呼吁可解释模型的局面下，还要硬着头皮加入AI吗？这不仅需要高学历高工程能力高智商，还需要有自己的独特理解和对AI的那份执著这篇文章希望对不管是准备弃坑转开发的、即将入坑的、还是已经入坑的小伙伴们在AI大致的理解上有帮助！！（完结撒花）
576570463,16b314dae6b4535859178206e3066599,周鹏程,https://api.zhihu.com/people/16b314dae6b4535859178206e3066599,people,计算神经科学；机器学习,一场twitter争论：人工智能是否需要神经科学,article,https://api.zhihu.com/articles/576570463,中文的公众号文章可以参考 @<em>AI</em>科技评论 的这篇AI科技评论：Bengio、LeCun 等人联名发布 NeuroAI 白皮书：<em>智能</em>的本质是感觉运动能力，<em>AI</em> 迎来具身图灵测试大挑战,1116,100,1462,1666615902,1666767895,"刚刚过去的一个周末，twitter上的神经科学圈发酵了一起不大不小的争论，引得领域内好几个著名学者，包括Yann Lecun的参与。 最初争论的是神经科学是否推动了人工智能，后来就更多变成了未来的人工智能是否需要神经科学。吃完瓜后，感觉里边还是有一些很不错的观点，在此小记复盘一下。其中的翻译并非按照原文逐词翻译，只是换成我个人语言理解而已。抛砖引玉，欢迎大家有更多的观点。中国在类脑智能领域的投入也在增加，“该不该类脑”以及“如何类脑”这样的问题都值得在广泛范围内讨论。争论的起点10月15号时候，神经科学领域和人工智能领域一群大佬，如Terry Sejnowski, Yoshua Bengio， Yann LeCun，, Eero Simoncelli, James DiCarlo, Alex Pouget 以及今天争论的主角Konrad Kording， 在arXiv上发表了一篇白皮书文章Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution文章的观点非常简单，摘要只有两句话：Neuroscience has long been an important driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. 概括起来就是：神经科学+人工智能非常有前途，政府请打钱。中文的公众号文章可以参考 @AI科技评论 的这篇AI科技评论：Bengio、LeCun 等人联名发布 NeuroAI 白皮书：智能的本质是感觉运动能力，AI 迎来具身图灵测试大挑战一般这种事情都是大佬们利用自己的声音，对领域的发展提出一个方向，向政府建言争取更多资源支持，对自己江湖地位和声望有好处，热度炒起来了，相关从业的小虾米也开心。所以文章发表之后，作者之一，宾夕法尼亚大学教授的Konrad Kording就开开心心的发了条tweet推广这篇文章，并呼吁了 一些东西。前两天还相安无事儿，大家其乐融融。没想到两天后，可能是周末比较清闲，来自DeepMind的David Pfau对着Kording的这篇tweet开喷了：神经科学从来都没推动过人工智能，你们白皮书中还说continue to drive AI progress你们真的认为发明Transformers / ADAM的人看过一篇神经科学论文吗？你们就假装在为人工智能做贡献吧。要点脸吧 ""it's embarrasing""(原文）这样的回复立马就炸雷了，引起了后面很多人的“参战”。这里简单提一下这位Pfau，他其实是正儿八经的神经科学博士，毕业于哥伦比亚大学的神经生物学专业，附属于Center for Theoretical Neuroscience （CTN）。跟我（博后期间）同一个导师，只不过我们时间上没有交集，并没有任何交流。说这个背景，主要是让大家不要觉得Pfau是不懂任何神经科学，盲目自大，他是受过完整神经科学训练的。并且在CTN里边有Larry Abbott和Ken Miller等计算神经科学大佬，毕业生中走出了很多在人工智能领域的佼佼者，如David Sussillo (下文会出现）。Pfau对于这神经科学和人工智能两个领域都不陌生。诸神参战David SussilloPfau的评论一处，上文我们所提到的David Sussillo就出来说话了过去几年，我在Google Brain跟Transformer的主要贡献人交往很多。我虽然不能冒昧地推定到底是什么启发了他发明transformer，但是他对神经科学是发自内心的感兴趣，问了很多神经科学的问题。格局打开，深度学习/人工智能的科学原理早晚都需要被解决，我押宝计算/理论/神经科学领域的人。对此， Pfau也直接回复人工智能的人对神经科学感兴趣没问题，这和神经科学推动AI进展是两码子事儿。Sussillo你自己在运动控制方面的神经科学工作非常有影响力，你能举出来任何这些工作在机器人领域的应用吗？对此，Sussillo倒是也坦诚没，我在运动控制领域的工作在AI中没啥影响。我曾经试图将动力学系统方面的思路引入到深度学习的理解中，但是没啥大成果。这个领域不太鼓励进展缓慢的增量式工作。Kording一看Sussillo这么坦诚，被Pfau将了一军，赶紧出来发言助攻。Emo Todorov在运动控制方面的工作在人工智能领域的应用比比皆是。Pfau对此也毫不客气得了吧，Todorov的主要身份就不是神经科学家。他是做过一些神经科学的工作，但是我认为人工智能领域所认可他的工作主要来自控制理论。Kording也立马回复他的控制理论一开始就是为了解释神经科学的运动行为的。至此，我们可以看出来分歧已经出现在：到底什么才能被定义为神经科学？为了解释神经系统功能所用到的控制理论、数学、物理、统计、计算机手段是否还能被认为是神经科学？我的个人观点在后边说，大家继续看戏。Sussillo这时候估计也是觉得这样辩论就没意思了，赶紧总结了自己的观点，给双方台阶下，准备退出算了，如果你就是不爽Kording所说的“神经科学继续推动人工智能”的话，那就算是吧。过去的十年，深度学习领域到处都在跑马圈地，偶尔有一系列真正令人惊叹的想法出现。我认为神经科学在这个过程中没有贡献。我承认这一点，并且同意目前神经科学对人工智能的贡献远不如后者对前者的大。但我不爽的是，你说“神经科学家宣称对人工智能有贡献”就是embarrassing的。你说的太过了。人工智能的历史吸纳了很多不同领域的人到一起。就拿NIPS为例，神经科学也依然在这个会议中有一席之地。 这里我简单插入一点内容：近些年在中文互联网上， NIPS的文章讨论已经几乎全部是计算机领域的了，但实际上它是有一部分留给神经科学的。有趣的是，我在wiki上看NIPS的介绍，说的还是NIPS is a machine learning and computational neuroscience conference, 以及说NIPS was designed as a meeting for researchers exploring biological and artificial neural networks，但是NIPS的官网的mission statement说的是foster the exchange of research advances in Artificial Intelligence and Machine Learning。这可真是从“小甜甜”到“牛夫人”的转变啊。NIPS都如此，这也难怪现在人工智能领域的新生代不屑于了解神经科学。而在上个世纪，最早那一批做人工智能的对脑智能还是非常感兴趣的。最后Sussillo给Pfau推荐了几篇做人工智能可以看看的神经科学文章，然后退出了争论。https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1.abstractHow recurrent networks implement contextual processing in sentiment analysisReverse engineering learned optimizers reveals known and novel mechanismsKen MillerKen Miller是Pfau所毕业的哥大理论神经科学中心的大佬，他肯定是认识Pfau。Ken是一个非常受人尊敬的理论神经科学家，致力于研究大脑皮层环路的计算规则，是研究脑智能的一座山，现在突然听到Pfau这番言论，肯定是很不爽的。于是连发了三条评论就离开了，而Pfau也没有直接回复Ken，不知道是否是出于一种尊重，或者是他觉得Ken这种说法太泛了。 Pfau你小子，真觉得一个从没听说过神经元和突触相关概念的人会想到使用人工神经网络来实现人工智能吗？你忘了1990-2012年的神经网络寒冬了吗？一帮受脑智能启发的头铁的牛人给神经网络续了命，虽然那段时间整个人工智能/机器学习领域都抛弃了他们。人类擅长抽象推理模式，但这是不会带领我们想到通过神经网络/卷积网络来实现智能的。而那帮头铁的老家伙们就是坚信“大脑可以做到，那人工神经网络也行”，靠着这个信念坚持走过了人工神经网络的黑暗时期。现在你居然在问“神经科学为人工智能做了啥”？真拿神经科学是牛夫人了？上一次神经科学对人工智能的贡献在于认识到大脑是通过层级间的神经元和突触连接实现信息处理这一抽象的概念，我认为下一波重大贡献将来自于我们真正理解大脑如何完成计算的。目前我们还远没到这个地步。Yann Lecun 大佬出马了，直接就一句""You are wrong""甩到Pfau老兄脸上了你错了 。神经科学极大并且直接启发了我和Hinton另外神经网络通过调节突触权重来实现学习这一整套想法确定无疑来自神经科学。当然了，遇到和大佬对线的机会，Pfau也不会轻易放弃，赶紧说从一些经典工作获得一些概念上的启发和直接从最近研究中获得启发是不同的。你有从Neuron或者Cell杂志上的最新文章受到任何启发，并作出新的工作吗？如果有的话，我没注意到你文章中有这些体现。这里我觉得Pfau有点太偏执了，神经科学的第一任务是理解大脑的工作原理，而不是为了整天为人工智能提供新的启发。你这对比就是拿classic和latest research对比。LeCun估计也是觉得这样说就没意思了，就直接不回复了。此外，Pfau又补了一句Mike Jordan说当年的PDP group都非常讨厌backprop，因为它在生物学上不现实。直到你们这波人不管生物学意义后，才变得流行了。大佬，你当年因为一些神经学家过分追求生物合理性而被刁难的日子，你都忘了吗？Tony ZadorTony Zador 是冷泉港实验室的一个大佬，他也是前文提到的白皮书作者之一。他看到Pfau对LeCun的回复后，就开喷了你丫这么说就类似于问一个物理学家是否看过哪些最近发表的数学论文，并启发自己完成一些新的工作？“嘿，费曼，你最近看过Acta Mathematica上的论文吗，有没有直接推动你工作的？如果没有的话，那物理学家不需要学数学了”Pfau也挺善战，立马就说你要说费曼，我就不困了。在《别逗了，费曼先生》一书中，他说他觉得纯数太繁琐并且不相关。他有问题就直接去找数学家问了不过我觉得这回答的是个啥？Zador也作出了回应（如下，我就不翻译了）。在另外一条线中，Zador也参与了一点争论。起因是有人说Hinton在报告中提到Dropout的发现是因为他注意到神经元的活动是随机的。我查了Hinton的那篇文章，没有提任何neuroscience的事情，也没有引用。对文章撰写来说，而也是很正常的，文章最后的组织和表述不用完全反映真实的心路历程。然后Pfau估计那天也是杀疯了，上来就说我怀疑Hinton从来没有看过任何一篇神经科学的论文。被一些high-level的直觉启发，跟被实际的神经科学研究启发是两回事儿。于是Zador就说你是不是指望神经科学文章中的某个图片结论直接被应用到某些 人工智能算法中？如果这是你期待的话，那肯定不是这样的。事实上，我们从神经科学获得一些启发，然后把那些重要的想法移植到AI中并进而改善它。Pfau也直接回复实际上你说的这种形式也不常见。有些人从神经科学中寻找启发，其他人完全无视神经科学也没啥，依然可以作出领域内的重大贡献。这里我是不认同Pfau关于Hinton不看神经科学论文这一说法的。Terry Sejnowski在The Deep Learning Revolution一书中花了很多篇幅写自己和Hinton的关系。人工智能领域的新生代可能不了解Terry，他是美国的National Academy of Sciences， American Academy of Arts and Sciences， American Association Advancement of Science, National Academy of Engineering, National Academy of Medicine, National Academy of Inventors 多院的院士（学术称号buff加满的存在），计算神经科学领域的先驱，高山仰止那种角色。他和Hinton早期有很多关于神经网络方面的合作。后来两个人的研究方向分别走向了两个领域：一个注重利用神经网络实现人工智能，另外一个专注于理解大脑工作原理。我相信两位大佬是希望这两个领域是可以互相促进的。在Terry的书中，他写了这么一段话Every few years, I get a call from Geoffrey that begins with ""I figured out how the brain works."" Each time, he tells me about a clever new scheme for improving neural network models. It has taken many such schemes and refinements for deep learning in multilayered neural networks to achieve a level of performance comparable to humans in recognizing speech on cell phones and objects in photos. 从这里我们看出，Hinton每次对他网络的修改，总是想和大脑对应上，寻求二者原理上的共性。所以我觉得这里Pfau是太武断了。 Gary MarcusGary Marcus 曾经是NYU心理系最年轻的荣誉退休教授。他对Pfau所说的神经学家过度关心生物合理性也回应了一下是的，对神经网络生物合理性的要求过度了。但我同意LeCun，Pfau这小子太口无遮拦了，缺乏对历史背景的了解。Pfau老哥怎能服气，立马就杠上了你瞎说，我当然知道历史背景。我只是认为几个教科书中关于抽象的启发完全不等于是主要推动力。 Marcus也很老道，不跟他硬杠好吧，你这么说也没毛病。那咱们就说“60年代有一些是有实际贡献，1970年以后就没啥大的推动了”最后这个Markus还是非常善意的提供了一些神经科学中可供AI领域参考的概念，然后消失了，深藏功与名。Eran MukamelEran Mukamel是UCSD认知神经科学系的一个教授，湿实验比较多。我知道他是因为我之前一些工作经常会拿他的一个自动分析算法对比。Mukamel和Pfau俩家简单斗了一下嘴，没啥特别有趣的争论。大家自己看截图吧。鸣金收兵后来双方都很累了，Pfau也没想到自己直接点燃了一个火药桶，一个周末就这么荒废了，全程与各位对战，也承认自己处境很尴尬，不过他是坚决不认输。争论中的另一个主角也不开心地发了个佛系的tweet。 个人观点我是战火结束了才跑去吃瓜的，吃得还是挺开心。我很欣赏David Pfau这样的人抛出来这个话题，并且面对各个大佬都从容应对，虽然我不是很认同他的某些说法。但是我觉得Nicole Rust下边的这条评论基本表达了我的感受。这条twitter下的争论还有很多，我原本想着一会儿工作就整理完了，结果我严重低估了工作量。剩下的我就暂时先不整理了。如果感兴趣的话，可以去围观另外一条主线，来自Harvard的教授Sam Gershman也开辟了一个战场，下面依然有Tony Zador大神的火力输出。https://twitter.com/gershbrain/status/1583785652516098049另外大家不要觉得都是喷Pfau的，其实也有很多人是支持Pfau，并且认为“根本不用关心neuroscience进展的”，以及“neuroscience在这一波deep learning浪潮中啥也没干”，对此我觉得也没啥不妥。早在2015年时候，就有一个知乎讨论，有个回答也是说“不觉得做人工智能一定要先学点生物”，对此我是认同的。神经科学不需要所有人都学习。如何看待饶毅的「人工智能还是伪智能」命题？最后发表一下我在吃瓜过程中的一些看法，欢迎讨论。 人工智能有很多途径，NeuroAI只是一个选择我觉得这里没必要厚此薄彼。有人觉得神经科学 ->人工智能是一条不错的道路，有人觉得完全不需要神经科学也一样可以做的很好。那没问题，各走各的道，谁也别想着干涉谁。神经科学对于过去十年的所谓的“人工智能”发展没啥大的贡献，这是事实；如果有，那也是早年的先驱概念性启发，跟当代的神经科学没啥大的关系，没必要去邀功。 但是你要说未来是否有贡献，那我觉得是会的。有人觉得不会，那也没关系，就像1990-2012年间那样，大部分人觉得人工神经网络不值得研究一样。 另外Pfau这种“神经科学对人工智能没有贡献的说法”，我觉得有一个很大的问题：谁来定义人工智能？有人从工程角度研究，有人从脑智能启发。后者目前慢，但是就直接否定了其存在，从人工智能领域踢出去是否合适？我们是否换个角度看，研究脑科学就是研究人工智能？总结起来就是，做神经科学的不要随意邀功，免得一种“我祖上阔过”的印象；做非神经相关的AI研究者，也不要随意贬低其它领域的研究。属于脑科学的“空气动力学”还未到来大家经常会拿鸟和飞机来类比脑科学和人工智能。白皮书中也对此有一个回应，主要是强调这个类比不合理。造飞机的目的不是为了像鸟，但是人工智能的一个目的是为了像人（此处有争议，有些觉得人工智能不需要像人，但是要超越人类的水平）。我个人觉得除了白皮书中所说的，还有一条就是属于脑科学的空气动力学还未到来。人们从研究鸟发现了空气动力学，然后就可以抛开鸟类，专注空气动力学就可以造出来更大更快的飞机。但是对于脑智能来说，我们还没有发现可以解释其计算原理的理论，在类脑智能领域尚没有可以脱离大脑可以独立发展的“空气动力学”。所以我觉得Ken Miller的评论是中肯的。 神经科学如何界定？Pfau在争论中有个观点其实是具有迷惑性的。他只承认实验得到的神经科学结论，而拒绝认为来自控制理论、数学、认知科学的研究属于神经科学。如果持有这个观点，那么他很难在辩论中失败。因为所有被人工智能所吸收的理论都会被他说成是另外一个学科，而非神经学科。但实际上神经科学的定义非常模糊，我迄今都没有严格区分neuroscience, neurology, neurobiology, coginitive neuroscience等细分方向。我觉得凡是以神经系统为研究对象，为此开发的理论、方法以及实验发现都属于神经科学。 毕竟现代神经网络的先驱们很多都是数学、物理方向，大多都是怀着对人类智能的兴趣而进行相关的探索。他们有的走向人工神经网络，有的走向生物脑的原理解析。 对于历史上的大师而言，他们眼中没有细分的专业，而只有人类未解之谜。如果需要，他们可以自己创造一个专业。目前神经科学还处于初级阶段，但已经是最好的时代神经科学是一个非常吸引人的学科，里边的未解之谜实在太多。有很多人在年轻时候都试图去研究它，进了一些神经科学的博士项目。但真正进入了之后，才发现我们的研究手段如此初级，离自己想象差距好大，在有生之年估计是无法解答自己的疑惑。比如Jeff Hawkins进入了UC Berkeley的神经科学PhD项目，读了一段时间觉得有点希望渺茫，于是就辍学去了硅谷，创建了Palm Computing公司，赚了大钱后，在UC Berkeley赞助了一个Redwood Center for Theoretical Neuroscience，继续赞助一帮大佬进行神经科学研究。我之所以认为现在是神经科学发展最好的时代，是因为随着更多领域的人加入，神经科学研究的工具越来越多，手段越来越多样化，很多过去受技术限制而不能回答的问题正变得可能。正如上图所说，“科学的进步取决于新技术、新发现和新思想，可能是按照这个顺序”，所以我对神经科学的发展是乐观的。对人工智能的启发可以有很多种神经元和突触连接这些概念对于最早期的人工神经网络来说，我认为是肯定很关键的。这是从其它实体中很难获得的一种启发，其难度应该可以类比从我们日常生活想象量子力学一样。这要得益与人类历史上一些伟大的实验科学家和一群对智能原理孜孜不倦思索的理论家。但是具体到某些算法，可获得的启发就可以来自方方面面了。Hinton在reddit上提了关于发明dropout的一些历程。其中提到了2004年时候，他觉得大脑执行一个任务只用部分神经元激活即可，因此有了dropout的想法。后来他又从银行工作人员轮流上班而不影响客户办理业务。这两种事情都促使他有了最初的 dropout想法。未来的NeuronAI政府资助与此次争论无关的几条回复来自美国的IARPA的一个负责人David Markowitz，他是我之前参与的一个MICrONS项目的政府方面的管理者。MICrONS算是美国政府投入非常大的一个NeuroAI项目。David从经费管理者角度上回应了这篇文章，总体上是支持这一领域，但是科学家们需要拿出更多成果，给经费分配者以信心。"
375549477,0d5f195831e5e99bf216291b05f553a0,算法进阶,https://api.zhihu.com/people/0d5f195831e5e99bf216291b05f553a0,people,,一文概览人工智能(AI)发展历程,article,https://api.zhihu.com/articles/375549477,以一个清晰的脉络呈现出来，以此展望<em>人工智能</em>（<em>AI</em>）未来的趋势。 一、人工智能简介 1.1 <em>人工智能</em>研究目的 人工智能（Artificial Intelligence，AI）研究目的是通过探索智慧的实质,479,5,1061,1622037715,1690867415,"如同蒸汽时代的蒸汽机、电气时代的发电机、信息时代的计算机和互联网，人工智能（AI）正赋能各个产业，推动着人类进入智能时代。本文从介绍人工智能及主要的思想派系，进一步系统地梳理了其发展历程、标志性成果并侧重其算法思想介绍，将这段 60余年几经沉浮的历史，以一个清晰的脉络呈现出来，以此展望人工智能（AI）未来的趋势。一、人工智能简介1.1 人工智能研究目的人工智能（Artificial Intelligence，AI）研究目的是通过探索智慧的实质，扩展人类智能——促使智能主体会听（语音识别、机器翻译等）、会看（图像识别、文字识别等）、会说（语音合成、人机对话等）、会思考（人机对弈、专家系统等）、会学习（知识表示，机器学习等）、会行动（机器人、自动驾驶汽车等）。一个经典的AI定义是：“ 智能主体可以理解数据及从中学习，并利用知识实现特定目标和任务的能力。(A system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation)”1.2 人工智能的学派在人工智能的发展过程中，不同时代、学科背景的人对于智慧的理解及其实现方法有着不同的思想主张，并由此衍生了不同的学派，影响较大的学派及其代表方法如下：其中，符号主义及联结主义为主要的两大派系： “符号主义”（Symbolicism），又称逻辑主义、计算机学派，认为认知就是通过对有意义的表示符号进行推导计算，并将学习视为逆向演绎，主张用显式的公理和逻辑体系搭建人工智能系统。如用决策树模型输入业务特征预测天气： “联结主义”（Connectionism），又叫仿生学派，笃信大脑的逆向工程，主张是利用数学模型来研究人类认知的方法，用神经元的连接机制实现人工智能。如用神经网络模型输入雷达图像数据预测天气： 二、人工智能发展史从始至此，人工智能(AI)便在充满未知的道路探索，曲折起伏，我们可将这段发展历程大致划分为5个阶段期：起步发展期：1943年—20世纪60年代反思发展期：20世纪70年代应用发展期：20世纪80年代平稳发展期：20世纪90年代—2010年蓬勃发展期：2011年至今2.1 起步发展期：1943年—20世纪60年代人工智能概念的提出后，发展出了符号主义、联结主义(神经网络)，相继取得了一批令人瞩目的研究成果，如机器定理证明、跳棋程序、人机对话等，掀起人工智能发展的第一个高潮。1943年，美国神经科学家麦卡洛克（Warren McCulloch）和逻辑学家皮茨（Water Pitts）提出神经元的数学模型，这是现代人工智能学科的奠基石之一。 1950年，艾伦·麦席森·图灵（Alan Mathison Turing）提出“图灵测试”（测试机器是否能表现出与人无法区分的智能），让机器产生智能这一想法开始进入人们的视野。 1950年，克劳德·香农（Claude Shannon）提出计算机博弈。 1956年，达特茅斯学院人工智能夏季研讨会上正式使用了人工智能（artificial intelligence，AI）这一术语。这是人类历史上第一次人工智能研讨，标志着人工智能学科的诞生。 1957年,弗兰克·罗森布拉特（Frank Rosenblatt）在一台IBM-704计算机上模拟实现了一种他发明的叫做“感知机”（Perceptron）的神经网络模型。 感知机可以被视为一种最简单形式的前馈式人工神经网络，是一种二分类的线性分类判别模型，其输入为实例的特征向量想（x1,x2...），神经元的激活函数f为sign，输出为实例的类别（+1或者-1），模型的目标是要将输入实例通过超平面将正负二类分离。 1958年，David Cox提出了logistic regression。 LR是类似于感知机结构的线性分类判别模型，主要不同在于神经元的激活函数f为sigmoid，模型的目标为(最大似然)极大化正确分类概率。 1959年，Arthur Samuel给机器学习了一个明确概念：Field of study that gives computers the ability to learn without being explicitly programmed.（机器学习是研究如何让计算机不需要显式的程序也可以具备学习的能力）。 1961年，Leonard Merrick Uhr 和 Charles M Vossler发表了题目为A Pattern Recognition Program That Generates, Evaluates and Adjusts its Own Operators 的模式识别论文，该文章描述了一种利用机器学习或自组织过程设计的模式识别程序的尝试。 1965年，古德（I. J. Good）发表了一篇对人工智能未来可能对人类构成威胁的文章，可以算“AI威胁论”的先驱。他认为机器的超级智能和无法避免的智能爆炸最终将超出人类可控范畴。后来著名科学家霍金、发明家马斯克等人对人工智能的恐怖预言跟古德半个世界前的警告遥相呼应。 1966 年，麻省理工学院科学家Joseph Weizenbaum 在 ACM 上发表了题为《ELIZA-a computer program for the study of natural language communication between man and machine》文章描述了ELIZA 的程序如何使人与计算机在一定程度上进行自然语言对话成为可能，ELIZA 的实现技术是通过关键词匹配规则对输入进行分解，而后根据分解规则所对应的重组规则来生成回复。 1967年，Thomas等人提出K最近邻算法（The nearest neighbor algorithm）。 KNN的核心思想，即给定一个训练数据集，对新的输入实例Xu，在训练数据集中找到与该实例最邻近的K个实例，以这K个实例的最多数所属类别作为新实例Xu的类别。1968年，爱德华·费根鲍姆（Edward Feigenbaum）提出首个专家系统DENDRAL，并对知识库给出了初步的定义，这也孕育了后来的第二次人工智能浪潮。该系统具有非常丰富的化学知识，可根据质谱数据帮助化学家推断分子结构。专家系统（Expert Systems）是AI的一个重要分支，同自然语言理解，机器人学并列为AI的三大研究方向。它的定义是使用人类专家推理的计算机模型来处理现实世界中需要专家作出解释的复杂问题，并得出与专家相同的结论，可视作“知识库(knowledge base)”和“推理机(inference machine)” 的结合。 1969年，“符号主义”代表人物马文·明斯基（Marvin Minsky）的著作《感知器》提出对XOR线性不可分的问题：单层感知器无法划分XOR原数据，解决这问题需要引入更高维非线性网络（MLP, 至少需要两层），但多层网络并无有效的训练算法。这些论点给神经网络研究以沉重的打击，神经网络的研究走向长达10年的低潮时期。2.2 反思发展期：20世纪70年代人工智能发展初期的突破性进展大大提升了人们对人工智能的期望，人们开始尝试更具挑战性的任务，然而计算力及理论等的匮乏使得不切实际目标的落空，人工智能的发展走入低谷。1974年，哈佛大学沃伯斯(Paul Werbos)博士论文里，首次提出了通过误差的反向传播(BP)来训练人工神经网络，但在该时期未引起重视。 BP算法的基本思想不是（如感知器那样）用误差本身去调整权重，而是用误差的导数（梯度）调整。通过误差的梯度做反向传播，更新模型权重, 以下降学习的误差，拟合学习目标，实现'网络的万能近似功能'的过程。1975年，马文·明斯基(Marvin Minsky)在论文《知识表示的框架》(A Framework for Representing Knowledge)中提出用于人工智能中的知识表示学习框架理论。 1976年，兰德尔·戴维斯（Randall Davis）构建和维护的大规模的知识库，提出使用集成的面向对象模型可以提高知识库（KB）开发、维护和使用的完整性。 1976年，斯坦福大学的肖特利夫(Edward H. Shortliffe)等人完成了第一个用于血液感染病的诊断、治疗和咨询服务的医疗专家系统MYCIN。 1976年，斯坦福大学的博士勒纳特发表论文《数学中发现的人工智能方法——启发式搜索》，描述了一个名为“AM”的程序，在大量启发式规则的指导下开发新概念数学，最终重新发现了数百个常见的概念和定理。 1977年，海斯·罗思(Hayes. Roth)等人的基于逻辑的机器学习系统取得较大的进展，但只能学习单一概念，也未能投入实际应用。 1979年，汉斯·贝利纳（Hans Berliner）打造的计算机程序战胜双陆棋世界冠军成为标志性事件。(随后，基于行为的机器人学在罗德尼·布鲁克斯和萨顿等人的推动下快速发展，成为人工智能一个重要的发展分支。格瑞·特索罗等人打造的自我学习双陆棋程序又为后来的强化学习的发展奠定了基础。) 2.3 应用发展期：20世纪80年代人工智能走入应用发展的新高潮。专家系统模拟人类专家的知识和经验解决特定领域的问题，实现了人工智能从理论研究走向实际应用、从一般推理策略探讨转向运用专门知识的重大突破。而机器学习(特别是神经网络)探索不同的学习策略和各种学习方法，在大量的实际应用中也开始慢慢复苏。1980年，在美国的卡内基梅隆大学(CMU)召开了第一届机器学习国际研讨会，标志着机器学习研究已在全世界兴起。 1980年，德鲁·麦狄蒙（Drew McDermott）和乔恩·多伊尔（Jon Doyle）提出非单调逻辑，以及后期的机器人系统。 1980年，卡耐基梅隆大学为DEC公司开发了一个名为XCON的专家系统，每年为公司节省四千万美元，取得巨大成功。 1981年，保罗（R.P.Paul）出版第一本机器人学课本，“Robot Manipulator：Mathematics，Programmings and Control”，标志着机器人学科走向成熟。 1982年，马尔（David Marr）发表代表作《视觉计算理论》提出计算机视觉（Computer Vision）的概念，并构建系统的视觉理论，对认知科学（CognitiveScience）也产生了很深远的影响。 1982年，约翰·霍普菲尔德（John Hopfield） 发明了霍普菲尔德网络，这是最早的RNN的雏形。霍普菲尔德神经网络模型是一种单层反馈神经网络（神经网络结构主要可分为前馈神经网络、反馈神经网络及图网络），从输出到输入有反馈连接。它的出现振奋了神经网络领域，在人工智能之机器学习、联想记忆、模式识别、优化计算、VLSI和光学设备的并行实现等方面有着广泛应用。 1983年，Terrence Sejnowski, Hinton等人发明了玻尔兹曼机（Boltzmann Machines），也称为随机霍普菲尔德网络，它本质是一种无监督模型，用于对输入数据进行重构以提取数据特征做预测分析。 1985年，朱迪亚·珀尔提出贝叶斯网络(Bayesian network)，他以倡导人工智能的概率方法和发展贝叶斯网络而闻名，还因发展了一种基于结构模型的因果和反事实推理理论而受到赞誉。 贝叶斯网络是一种模拟人类推理过程中因果关系的不确定性处理模型，如常见的朴素贝叶斯分类算法就是贝叶斯网络最基本的应用。 贝叶斯网络拓朴结构是一个有向无环图(DAG)，通过把某个研究系统中涉及的随机变量，根据是否条件独立绘制在一个有向图中，以描述随机变量之间的条件依赖，用圈表示随机变量(random variables)，用箭头表示条件依赖(conditional dependencies)就形成了贝叶斯网络。 对于任意的随机变量，其联合概率可由各自的局部条件概率分布相乘而得出。如图中b依赖于a(即：a->b)，c依赖于a和b，a独立无依赖，根据贝叶斯定理有 P(a,b,c) = P(a)*P(b|a)*P(c|a,b) 1986年，罗德尼·布鲁克斯(Brooks)发表论文《移动机器人鲁棒分层控制系统》，标志着基于行为的机器人学科的创立，机器人学界开始把注意力投向实际工程主题。 1986年，辛顿(Geoffrey Hinton)等人先后提出了多层感知器(MLP)与反向传播（BP）训练相结合的理念（该方法在当时计算力上还是有很多挑战，基本上都是和链式求导的梯度算法相关的），这也解决了单层感知器不能做非线性分类的问题，开启了神经网络新一轮的高潮。 1986年，昆兰（Ross Quinlan）提出ID3决策树算法。 决策树模型可视为多个规则(if, then)的组合，与神经网络黑盒模型截然不同是，它拥有良好的模型解释性。 ID3算法核心的思想是通过自顶向下的贪心策略构建决策树：根据信息增益来选择特征进行划分（信息增益的含义是 引入属性A的信息后，数据D的不确定性减少程度。也就是信息增益越大，区分D的能力就越强)，依次递归地构建决策树。 1989年，George Cybenko证明了“万能近似定理”（universal approximation theorem）。简单来说，多层前馈网络可以近似任意函数，其表达力和图灵机等价。这就从根本上消除了Minsky对神经网络表达力的质疑。“万能近似定理”可视为神经网络的基本理论：⼀个前馈神经⽹络如果具有线性层和⾄少⼀层具有 “挤压” 性质的激活函数（如 sigmoid 等），给定⽹络⾜够数量的隐藏单元，它可以以任意精度来近似任何从⼀个有限维空间到另⼀个有限维空间的 borel 可测函数。 1989年，LeCun (CNN之父) 结合反向传播算法与权值共享的卷积神经层发明了卷积神经网络（Convolutional Neural Network，CNN），并首次将卷积神经网络成功应用到美国邮局的手写字符识别系统中。 卷积神经网络通常由输入层、卷积层、池化（Pooling）层和全连接层组成。卷积层负责提取图像中的局部特征，池化层用来大幅降低参数量级(降维)，全连接层类似传统神经网络的部分，用来输出想要的结果。 2.4 平稳发展期：20世纪90年代—2010年由于互联网技术的迅速发展，加速了人工智能的创新研究，促使人工智能技术进一步走向实用化，人工智能相关的各个领域都取得长足进步。在2000年代初，由于专家系统的项目都需要编码太多的显式规则，这降低了效率并增加了成本，人工智能研究的重心从基于知识系统转向了机器学习方向。1995年，Cortes和Vapnik提出联结主义经典的支持向量机(Support Vector Machine)，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。支持向量机（Support Vector Machine, SVM）可以视为在感知机基础上的改进，是建立在统计学习理论的VC维理论和结构风险最小原理基础上的广义线性分类器。与感知机主要差异在于：1、感知机目标是找到一个超平面将各样本尽可能分离正确(有无数个)，SVM目标是找到一个超平面不仅将各样本尽可能分离正确，还要使各样本离超平面距离最远(只有一个最大边距超平面)，SVM的泛化能力更强。2、对于线性不可分的问题，不同于感知机的增加非线性隐藏层，SVM利用核函数，本质上都是实现特征空间非线性变换，使可以被线性分类。1995年， Freund和schapire提出了 AdaBoost (Adaptive Boosting)算法。AdaBoost采用的是Boosting集成学习方法——串行组合弱学习器以达到更好的泛化性能。另外一种重要集成方法是以随机森林为代表的Bagging并行组合的方式。以“偏差-方差分解”分析，Boosting方法主要优化偏差，Bagging主要优化方差。 Adaboost迭代算法基本思想主要是通过调节的每一轮各训练样本的权重(错误分类的样本权重更高)，串行训练出不同分类器。最终以各分类器的准确率作为其组合的权重，一起加权组合成强分类器。 1997年国际商业机器公司（简称IBM）深蓝超级计算机战胜了国际象棋世界冠军卡斯帕罗夫。深蓝是基于暴力穷举实现国际象棋领域的智能，通过生成所有可能的走法，然后执行尽可能深的搜索，并不断对局面进行评估，尝试找出最佳走法。 1997年，Sepp Hochreiter 和 Jürgen Schmidhuber提出了长短期记忆神经网络(LSTM)。 LSTM是一种复杂结构的循环神经网络（RNN），结构上引入了遗忘门、输入门及输出门：输入门决定当前时刻网络的输入数据有多少需要保存到单元状态，遗忘门决定上一时刻的单元状态有多少需要保留到当前时刻，输出门控制当前单元状态有多少需要输出到当前的输出值。这样的结构设计可以解决长序列训练过程中的梯度消失问题。 1998年，万维网联盟的蒂姆·伯纳斯·李(Tim Berners-Lee)提出语义网(Semantic Web)的概念。其核心思想是：通过给万维网上的文档（如HTML）添加能够被计算机所理解的语义(Meta data)，从而使整个互联网成为一个基于语义链接的通用信息交换媒介。换言之，就是构建一个能够实现人与电脑无障碍沟通的智能网络。 2001年，John Lafferty首次提出条件随机场模型（Conditional random field，CRF）。 CRF是基于贝叶斯理论框架的判别式概率图模型，在给定条件随机场P ( Y ∣ X ) 和输入序列x，求条件概率最大的输出序列y *。在许多自然语言处理任务中比如分词、命名实体识别等表现尤为出色。 2001年，布雷曼博士提出随机森林（Random Forest）。 随机森林是将多个有差异的弱学习器(决策树)Bagging并行组合，通过建立多个的拟合较好且有差异模型去组合决策，以优化泛化性能的一种集成学习方法。多样差异性可减少对某些特征噪声的依赖，降低方差（过拟合），组合决策可消除些学习器间的偏差。 随机森林算法的基本思路是对于每一弱学习器(决策树)有放回的抽样构造其训练集，并随机抽取其可用特征子集，即以训练样本及特征空间的多样性训练出N个不同的弱学习器，最终结合N个弱学习器的预测（类别或者回归预测数值），取最多数类别或平均值作为最终结果。 2003年，David Blei, Andrew Ng和 Michael I. Jordan于2003年提出LDA（Latent Dirichlet Allocation）。 LDA是一种无监督方法，用来推测文档的主题分布，将文档集中每篇文档的主题以概率分布的形式给出，可以根据主题分布进行主题聚类或文本分类。 2003年，Google公布了3篇大数据奠基性论文，为大数据存储及分布式处理的核心问题提供了思路：非结构化文件分布式存储（GFS）、分布式计算（MapReduce）及结构化数据存储（BigTable），并奠定了现代大数据技术的理论基础。 2005 年，波士顿动力公司推出一款动力平衡四足机器狗，有较强的通用性，可适应较复杂的地形。 2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念（Deeping Learning），开启了深度学习在学术界和工业界的浪潮。2006年也被称为深度学习元年，杰弗里·辛顿也因此被称为深度学习之父。 深度学习的概念源于人工神经网络的研究，它的本质是使用多个隐藏层网络结构，通过大量的向量计算，学习数据内在信息的高阶表示。 2010年，Sinno Jialin Pan和 Qiang Yang发表文章《迁移学习的调查》。 迁移学习(transfer learning)通俗来讲，就是运用已有的知识（如训练好的网络权重）来学习新的知识以适应特定目标任务，核心是找到已有知识和新知识之间的相似性。 2.5 蓬勃发展期：2011年至今随着大数据、云计算、互联网、物联网等信息技术的发展，泛在感知数据和图形处理器等计算平台推动以深度神经网络为代表的人工智能技术飞速发展，大幅跨越了科学与应用之间的技术鸿沟，诸如图像分类、语音识别、知识问答、人机对弈、无人驾驶等人工智能技术实现了重大的技术突破，迎来爆发式增长的新高潮。2011年，IBM Watson问答机器人参与Jeopardy回答测验比赛最终赢得了冠军。Waston是一个集自然语言处理、知识表示、自动推理及机器学习等技术实现的电脑问答（Q&A）系统。 2012年，Hinton和他的学生Alex Krizhevsky设计的AlexNet神经网络模型在ImageNet竞赛大获全胜，这是史上第一次有模型在 ImageNet 数据集表现如此出色，并引爆了神经网络的研究热情。 AlexNet是一个经典的CNN模型，在数据、算法及算力层面均有较大改进，创新地应用了Data Augmentation、ReLU、Dropout和LRN等方法，并使用GPU加速网络训练。 2012年，谷歌正式发布谷歌知识图谱Google Knowledge Graph），它是Google的一个从多种信息来源汇集的知识库，通过Knowledge Graph来在普通的字串搜索上叠一层相互之间的关系，协助使用者更快找到所需的资料的同时，也可以知识为基础的搜索更近一步，以提高Google搜索的质量。 知识图谱是结构化的语义知识库，是符号主义思想的代表方法，用于以符号形式描述物理世界中的概念及其相互关系。其通用的组成单位是RDF三元组(实体-关系-实体)，实体间通过关系相互联结，构成网状的知识结构。2013年，Durk Kingma和Max Welling在ICLR上以文章《Auto-Encoding Variational Bayes》提出变分自编码器（Variational Auto-Encoder，VAE）。 VAE基本思路是将真实样本通过编码器网络变换成一个理想的数据分布，然后把数据分布再传递给解码器网络，构造出生成样本，模型训练学习的过程是使生成样本与真实样本足够接近。 2013年，Google的Tomas Mikolov 在《Efficient Estimation of Word Representation in Vector Space》提出经典的 Word2Vec模型用来学习单词分布式表示，因其简单高效引起了工业界和学术界极大的关注。 Word2Vec基本的思想是学习每个单词与邻近词的关系，从而将单词表示成低维稠密向量。通过这样的分布式表示可以学习到单词的语义信息，直观来看，语义相似的单词的距离相近。 Word2Vec网络结构是一个浅层神经网络（输入层-线性全连接隐藏层->输出层），按训练学习方式可分为CBOW模型(以一个词语作为输入，来预测它的邻近词)或Skip-gram模型 (以一个词语的邻近词作为输入，来预测这个词语)。 2014年，聊天程序“尤金·古斯特曼”（Eugene Goostman）在英国皇家学会举行的“2014图灵测试”大会上，首次“通过”了图灵测试。 2014年，Goodfellow及Bengio等人提出生成对抗网络（Generative Adversarial Network，GAN），被誉为近年来最酷炫的神经网络。 GAN是基于强化学习(RL)思路设计的，由生成网络(Generator, G)和判别网络(Discriminator, D)两部分组成， 生成网络构成一个映射函数G: Z→X（输入噪声z, 输出生成的伪造数据x）, 判别网络判别输入是来自真实数据还是生成网络生成的数据。在这样训练的博弈过程中，提高两个模型的生成能力和判别能力。 2015年，为纪念人工智能概念提出60周年，深度学习三巨头LeCun、Bengio和Hinton(他们于2018年共同获得了图灵奖)推出了深度学习的联合综述《Deep learning》。 《Deep learning》文中指出深度学习就是一种特征学习方法，把原始数据通过一些简单的但是非线性的模型转变成为更高层次及抽象的表达，能够强化输入数据的区分能力。通过足够多的转换的组合，非常复杂的函数也可以被学习。 2015年，Microsoft Research的Kaiming He等人提出的残差网络（ResNet）在ImageNet大规模视觉识别竞赛中获得了图像分类和物体识别的优胜。 残差网络的主要贡献是发现了网络不恒等变换导致的“退化现象（Degradation）”，并针对退化现象引入了 “快捷连接（Shortcut connection）”，缓解了在深度神经网络中增加深度带来的梯度消失问题。 2015年，谷歌开源TensorFlow框架。它是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning）算法的编程实现，其前身是谷歌的神经网络算法库DistBelief。 2015年，马斯克等人共同创建OpenAI。它是一个非营利的研究组织，使命是确保通用人工智能 (即一种高度自主且在大多数具有经济价值的工作上超越人类的系统）将为全人类带来福祉。其发布热门产品的如：OpenAI Gym，GPT等。 2016年，谷歌提出联邦学习方法，它在多个持有本地数据样本的分散式边缘设备或服务器上训练算法，而不交换其数据样本。 联邦学习保护隐私方面最重要的三大技术分别是： 差分隐私 ( Differential Privacy )、同态加密 ( Homomorphic Encryption )和 隐私保护集合交集 ( Private Set Intersection )，能够使多个参与者在不共享数据的情况下建立一个共同的、强大的机器学习模型，从而解决数据隐私、数据安全、数据访问权限和异构数据的访问等关键问题。 2016年，AlphaGo与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜。 AlphaGo是一款围棋人工智能程序，其主要工作原理是“深度学习”，由以下四个主要部分组成：策略网络（Policy Network）给定当前局面，预测并采样下一步的走棋；快速走子（Fast rollout）目标和策略网络一样，但在适当牺牲走棋质量的条件下，速度要比策略网络快1000倍；价值网络（Value Network）估算当前局面的胜率；蒙特卡洛树搜索（Monte Carlo Tree Search）树搜索估算每一种走法的胜率。 在2017年更新的AlphaGo Zero，在此前的版本的基础上，结合了强化学习进行了自我训练。它在下棋和游戏前完全不知道游戏规则，完全是通过自己的试验和摸索，洞悉棋局和游戏的规则，形成自己的决策。随着自我博弈的增加，神经网络逐渐调整，提升下法胜率。更为厉害的是，随着训练的深入，AlphaGo Zero还独立发现了游戏规则，并走出了新策略，为围棋这项古老游戏带来了新的见解。2017年，中国香港的汉森机器人技术公司（Hanson Robotics）开发的类人机器人索菲亚，是历史上首个获得公民身份的一台机器人。索菲亚看起来就像人类女性，拥有橡胶皮肤，能够表现出超过62种自然的面部表情。其“大脑”中的算法能够理解语言、识别面部，并与人进行互动。 2018年，Google提出论文《Pre-training of Deep Bidirectional Transformers for Language Understanding》并发布Bert(Bidirectional Encoder Representation from Transformers)模型，成功在 11 项 NLP 任务中取得 state of the art 的结果。 BERT是一个预训练的语言表征模型，可在海量的语料上用无监督学习方法学习单词的动态特征表示。它基于Transformer注意力机制的模型，对比RNN可以更加高效、能捕捉更长距离的依赖信息，且不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的masked language model（MLM），以致能生成深度的双向语言表征。 2019年， IBM宣布推出Q System One，它是世界上第一个专为科学和商业用途设计的集成通用近似量子计算系统。 2019年，香港 Insilico Medicine 公司和多伦多大学的研究团队实现了重大实验突破，通过深度学习和生成模型相关的技术发现了几种候选药物，证明了 AI 发现分子策略的有效性，很大程度解决了传统新药开发在分子鉴定困难且耗时的问题。 2020年，Google与Facebook分别提出SimCLR与MoCo两个无监督学习算法，均能够在无标注数据上学习图像数据表征。两个算法背后的框架都是对比学习（contrastive learning），对比学习的核心训练信号是图片的“可区分性”。 2020年，OpenAI开发的文字生成 (text generation) 人工智能GPT-3，它具有1,750亿个参数的自然语言深度学习模型，比以前的版本GPT-2高100倍，该模型经过了将近0.5万亿个单词的预训练，可以在多个NLP任务（答题、翻译、写文章）基准上达到最先进的性能。 2020年，马斯克的脑机接口（brain–computer interface, BCI）公司Neuralink举行现场直播，展示了植入Neuralink设备的实验猪的脑部活动。 2020年，谷歌旗下DeepMind的AlphaFold2人工智能系统有力地解决了蛋白质结构预测的里程碑式问题。它在国际蛋白质结构预测竞赛（CASP）上击败了其余的参会选手，精确预测了蛋白质的三维结构，准确性可与冷冻电子显微镜（cryo-EM）、核磁共振或 X 射线晶体学等实验技术相媲美。 2020年，中国科学技术大学潘建伟等人成功构建76个光子的量子计算原型机“九章”，求解数学算法“高斯玻色取样”只需200秒，而目前世界最快的超级计算机要用6亿年。 2021年，OpenAI提出两个连接文本与图像的神经网络：DALL·E 和 CLIP。DALL·E 可以基于文本直接生成图像，CLIP 则能够完成图像与文本类别的匹配。 2021年，德国Eleuther人工智能公司于今年3月下旬推出开源的文本AI模型GPT-Neo。对比GPT-3的差异在于它是开源免费的。 2021年，美国斯坦福大学的研究人员开发出一种用于打字的脑机接口（brain–computer interface, BCI），这套系统可以从运动皮层的神经活动中解码瘫痪患者想象中的手写动作，并利用递归神经网络（RNN）解码方法将这些手写动作实时转换为文本。相关研究结果发表在2021年5月13日的Nature期刊上，论文标题为“High-performance brain-to-text communication via handwriting”。 2021年，AlphaFold 2 能很好地预判蛋白质与分子结合的概率，为我们展示了人工智能驱动自然学科研究的无限潜力；2022年，ChatGPT出来，AI的想象瞬时就开始爆了...未来已来！三、AI 未来趋势人工智能有三个要素：数据、算力及算法，数据即是知识原料，算力及算法提供“计算智能”以学习知识并实现特定目标。 人工智能60多年的技术发展，可以归根为算法、算力及数据层面的发展，那么在可以预见的未来，人工智能发展将会出现怎样的趋势呢？3.1 数据层面数据是现实世界映射构建虚拟世界的基本要素，随着数据量以指数形式增长，开拓的虚拟世界的疆土也不断扩张。不同于AI算法开源，关键数据往往是不开放的，数据隐私化、私域化是一种趋势，数据之于AI应用，如同流量是互联网的护城河，有核心数据才有关键的AI能力。 3.2 算力层面 推理就是计算（reason is nothing but reckoning） --托马斯.霍布斯 计算是AI的关键，自2010年代以来的深度学习浪潮，很大程度上归功于计算能力的进步。量子计算发展在计算芯片按摩尔定律发展越发失效的今天，计算能力进步的放慢会限制未来的AI技，量子计算提供了一条新量级的增强计算能力的思路。随着量子计算机的量子比特数量以指数形式增长，而它的计算能力是量子比特数量的指数级，这个增长速度将远远大于数据量的增长，为数据爆发时代的人工智能带来了强大的硬件基础。 边缘计算发展边缘计算作为云计算的一种补充和优化，一部分的人工智能正在加快速度从云端走向边缘，进入到越来越小的物联网设备中。而这些物联网设备往往体积很小，为此轻量机器学习（TinyML）受到青睐，以满足功耗、延时以及精度等问题。类脑计算发展以类脑计算芯片为核心的各种类脑计算系统，在处理某些智能问题以及低功耗智能计算方面正逐步展露出优势。类脑计算芯片设计将从现有处理器的设计方法论及其发展历史中汲取灵感，在计算完备性理论基础上结合应用需求实现完备的硬件功能。同时类脑计算基础软件将整合已有类脑计算编程语言与框架，实现类脑计算系统从“专用”向“通用”的逐步演进。人工智能计算中心成为智能化时代的关键基础设施人工智能计算中心基于最新人工智能理论，采用领先的人工智能计算架构，是融合公共算力服务、数据开放共享、智能生态建设、产业创新聚集的“四位一体”综合平台，可提供算力、数据和算法等人工智能全栈能力，是人工智能快速发展和应用所依托的新型算力基础设施。未来，随着智能化社会的不断发展，人工智能计算中心将成为关键的信息基础设施，推动数字经济与传统产业深度融合，加速产业转型升级，促进经济高质量发展。3.3 算法层面机器学习自动化(AutoML)发展自动化机器学习（AutoML）解决的核心问题是：在给定数据集上使用哪种机器学习算法、是否以及如何预处理其特征以及如何设置所有超参数。随着机器学习在许多应用领域取得了长足的进步，这促成了对机器学习系统的不断增长的需求，并希望机器学习应用可以自动化构建并使用。借助AutoMl、MLOps技术，将大大减少机器学习人工训练及部署过程，技术人员可以专注于核心解决方案。 向分布式隐私保护方向演进当前全球多个国家和地区已出台数据监管法规，如HIPAA（美国健康保险便利和责任法案）、GDPR（欧盟通用数据保护条例），《数据安全法》、《个人隐私保护法》等，通过严格的法规限制多机构间隐私数据的交互。分布式隐私保护机器学习(联邦学习)通过加密、分布式存储等方式保护机器学习模型训练的输入数据，是打破数据孤岛、完成多机构联合训练建模的可行方案。数据和机理融合AI模型的发展是符合简单而美的定律的。从数据出发的建模从数据中总结规律，追求在实践中的应用效果。从机理出发的建模以基本物理规律为出发点进行演绎，追求简洁与美的表达。一个好的、主流的的模型，通常是高度总结了数据规律并切合机理的，是“优雅”的，因为它触及了问题的本质。就和科学理论一样，往往简洁的，没有太多补丁，而这同时解决了收敛速度问题和泛化问题。神经网络模型结构发展神经网络的演进一直沿着模块化+层次化的方向，不断把多个承担相对简单任务的模块组合起来。神经网络结构通过较低层级模块侦测基本的特征，并在较高层级侦测更高阶的特征，无论是多层前馈网络，还是卷积神经网络，都体现了这种模块性(近年Hinton提出的“胶囊”（capsule）网络就是进一步模块化发展)。因为我们处理的问题（图像、语音、文字）往往都有天然的模块性，学习网络的模块性若匹配了问题本身内在的模块性，就能取得较好的效果。层次化并不仅仅是网络的拓扑叠加，更重要的是学习算法的升级，仅仅简单地加深层次可能会导致BP网络的梯度消失等问题。多学派方法融合发展通过多学派方法交融发展，得以互补算法之间的优势和弱点。如 1）贝叶斯派与神经网络融合，Neil Lawrence组的Deep Gaussian process, 用简单的概率分布替换神经网络层。2）符号主义、集成学习与神经网络的融合，周志华老师的深度随机森林。3) 符号主义与神经网络的融合：将知识库(KG)融入进神经网络，如GNN、知识图谱表示学习。4) 神经网络与强化学习的融合，如谷歌基于DNN+强化学习实现的Alpha Go 让AI的复杂任务表现逼近人类。基于大规模无(自)监督预训练发展 If intelligence is a cake, the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL) -- Yann Lecun 监督学习需要足够的带标签数据，然而人工标注大量数据既耗时又费力，在一些领域(如医学领域)上几乎不太可能获得足量的标注数据。通过大规模无(自)监督预训练方法利用现实中大量的无标签数据是一个研究的热点，AI大模型拥有超大规模参数、巨量训练数据，通过模型的巨量化可以提高人工智能的通用属性，并降低人工智能的应用门槛。如GPT-3的出现激发了对大规模自监督预训练方法继续开展探索和研究。未来，基于大规模图像、语音、视频等多模态数据的跨语言的自监督预训练模型将进一步发展，并不断提升模型的认知、推理能力。基于因果学习方法发展当前人工智能模型大多关注于数据特征间相关性，而相关性与更为本源的因果关系并不等价，可能导致预测结果的偏差，对抗攻击的能力不佳，且模型往往缺乏可解释性。另外，模型需要独立同分布(i.i.d.)假设(现实很多情况，i.i.d.的假设是不成立的)，若测试数据与训练数据来自不同的分布，统计学习模型往往效果不佳，而因果推断所研究的正是这样的情形：如何学习一个可以在不同分布下工作、蕴含因果机制的因果模型(Causal Model)，并使用因果模型进行干预或反事实推断。可解释性AI (XAI)发展可解释的人工智能有可能成为未来机器学习的核心，随着模型变得越来越复杂，确定简单的、可解释的规则就会变得越来越困难。一个可以解释的AI（Explainable AI, 简称XAI）意味着AI运作的透明，便于人类对于对AI监督及接纳，以保证算法的公平性、安全性及隐私性。 后记随着数据、算力及算法取得不断的突破，人工智能可能进入一个永恒的春天。 本文主要从技术角度看待AI趋势多少是片面的，虽然技术是第一生产力，有着自身的发展规律，但不可忽视的是技术是为需求市场所服务的，技术结合稳定的市场需求才能得到长足的发展。文章首发于“算法进阶”，公众号阅读原文可访问Github博客"
607604879,111caac9d15849bd5fdf5110ca84d465,Dio-晶,https://api.zhihu.com/people/111caac9d15849bd5fdf5110ca84d465,people,architect,论人工智能的真实和ChatGPT,article,https://api.zhihu.com/articles/607604879,我把当下<em>AI</em>领域的主要结构进行分类。 类似PINN(Physics-informed Neural Network)的辅助<em>AI</em>，这类NN网络我在HPC+AI融合领域看得多一些,406,64,403,1676799606,1685953615,"网上有一篇分析ChatGPT的文章引用了曾经著名的一部科幻小说，让我特别的沉浸和感同身受。那是《神的九十亿个名字》，讲述的是西藏的僧侣找到两个程序员，让其用最新的计算机暴力组合字母，列举出神的九十亿个名字，当所有的名字都遍历后，神的旨意便会得到彰显，人类的使命也就完成了。在神的九十亿个名字被暴力穷举之后，小说的最后一段话是：“穹苍之上，一片寂寥，群星慢慢闭上了眼睛。”嗯，我是想把心中的一个名字列举出来，ChatGPT。领导经常说我这个人喜欢咋呼，经常贩卖焦虑。其实这是在我面对不确定性的未来变数时的状态，而对于已经变成现实的，例如Nvidia A100、Intel Sapphire Rapids，我的表现往往是反过来的，“也就那样啦”。所以，在ChatGPT出现的最初两周，我是相当不屑的(￣_,￣ )不过就是一个大号的搜索引擎？ 和传统搜索相比只是把更多信息进行了组装，作为NLP，Transformer相比LSTM连上下文信息都丢失了，纯粹是独立信息之间的概率相关组合。ChatGPT搜索成本远高于传统搜索，谁买单？一本正经地胡说八道。你看，果然数学题算错了吧。随着信息汇聚更多之后，我发现不太对劲了，并陷入了长达三周的自我认知的矛盾中。我喜欢用人脑来作为AI算法的参考，因为无论怎样，智人及人脑始终是这个宇宙中目前唯一成功的智慧结构，有没有灵魂另说，但我坚信人的逻辑推理能力是某种超越简单网络结构的概率组合的，在逻辑中存在的Sequence结构一定是我们还没有找到的特殊的串行结构，这种结构并不适于在摩尔定律驱动的2D平面集成电路表达，至少应该是3D的。我不相信ChatGPT存在逻辑能力。但是，最终我屈服了，我现在相信ChatGPT已经具有了逻辑能力，并写下了本文后续的逻辑判断。人与猴子的唯一差别，也许真的只是尺度大小的差别，当某些事物的尺度跨过某条边界的时候，新的表达形式就以”涌现”的方式诞生了（这个词很重要）。人不过是天地玄黄在熵增的大随机过程中的一个低概率的高阶组合体罢了。我一直认为AI是基于大量数据的规律用低阶对高阶的拟合。这是我2020年写的，夏晶晶：论人工智能如何落地还是商业落地和实用主义蒙蔽了我的认知，现在我更倾向于AI，特别是在大模型，是利用大量数据不断对事物进行高阶抽象的过程。NN并没有什么不好解释的，如果用HPC的术语描述，NN每一层的Tensor运算等价于一次HPC的谱变换，就像下图，时域上的复杂信号组合，在FFT之后变成了若干零碎信息，即信息进行了一次高强度的压缩和重定位。所以，Tensor是可以等价成线性FFT叠加一个具有进化选择意义的非线性激活的，即：FFT（连续+线性） ~= Tensor = Matrix（离散化+线性）+ Activation（非线性）如果换成AI的术语来描述呢，就是信息被Embeding了。“万物皆可Embedding“的实质就是信息可以被抽象。抽象 ~= 压缩~= 谱变换。然后，如果把Raw Data定义为信息（Information），抽象后的Embedding定义为知识（knowledge）。。。。。。那基于Knowledge的承载，我把当下AI领域的主要结构进行分类。类似PINN(Physics-informed Neural Network)的辅助AI，这类NN网络我在HPC+AI融合领域看得多一些，这类网络所在的原生问题本身并没有变化，即算法的本质还是人类对物理世界的建模，而NN的存在是加速建模后的数值计算的某些步骤，此时的NN的含义是对物理公式的低纬表达，NN网络及NN前后的Data都没有承载NN的抽象（只有加速拟合而没有抽象过程）。推荐、搜索的AI网络，这类网络NN深度不深，但整个系统具有巨大的结构化Embedding Table，其实知识图谱的某些算法也类似，此时的NN本身只具有抽象过程，很难说表达了什么意义，而真正的知识实质上是被巨大的Embedding Table承载了，它的本质，我认为是对信息的较低层次抽象。大模型，网络深度超大，且几乎不再具有显性的Embedding Table形态，高度抽象后的知识（Knowledge）的载体变成了NN本身。如果NN的规模和深度代表了抽象的深度，很多逻辑都我能找到一个在我内心自洽的解答，下面的内容都是我自问自答的，不代表任何结论 ：） 纯粹是求个心安。曾经我提出过一个问题： Transformer所使用的embedding table，请问有没有人觉得是有问题的? 这个问题其实写得很愚蠢，但现在来看我觉得答案很清晰，推荐系统的embedding table为什么呈现出巨大和结构化的特征，本质是其从信息到知识的抽象层次不够高(网络不够深)，信息没有充分的压缩，还表现出原始结构的组织特征(结构化就是可压缩抽象的特征)。也正因为如此，巨大的浅层知识不能被网络承载，只能用大量的内存来存放，并最终在硬件计算上表现为图计算特征。人与人的社交和喜好真的是浅层的吗？大概率是，我喜欢海贼王但兄弟原神，我们可能都喜欢二次元，两点一线的关系就这么简单，所以推荐确实就是浅层次抽象的AI。但是人作为一个实体的内在特征（内心），应该是可以再增加NN层数进行更深层抽象的(例如我想把兄弟掰弯……)，也许这才呈现出真正的人与人的不同（商业价值另说）。为什么ChatGPT是LLM？ 这也就是一个必然了。语言就是符号，是raw data经历了多次抽象之后的embedding，是人类的知识对世间万物的抽象描述。即LLM输入的Raw data已经是人类以自然界为raw data输入后，输出的embedding了。人类从山顶洞人变成现代人，也就是在文字语言成型之后，即人类用万年的时间完成了一轮初始embedding。所以，从LLM可以更快地衍生到AGI，因为这本就是站在巨人的肩膀上递进抽象。当然AI也可以通过version学习这个世界原生的raw data，最终生成AGI，但那需要的规模则更大，也许在某些NN层次之后，AI也抽象出某种符号表达也说不定。我个人不喜欢AI的符号派，文字本身就是人类用了上万年抽象出来的符号了，虽然世界上的语言数量众多，从数学家的角度都不够完美(正交度不够)，但如果还要坚持更进一步抽象更高层次的符号，不是不可以，而是没必要。LLM已经站在很高的山坡上攀登了，以人力再垫高两阶台阶，还不如机器暴力堆砌来得快，在无数次NN的抽象之后，所谓的符号，肯定存在，但那已经高阶到难以用简单的方式描述了，还不如用连接关系表达来的直接。夏晶晶：论人工智能如何落地（再续）同理，如果我们希望用大模型解决更多的问题，例如制药、物理。也许也通过其原始复杂度以及能否有人多年积累的中间抽象作为台阶来预测模型的深度和尺度了，但无论怎样，更大也许就是答案。我也不喜欢知识图谱，我觉得这就是再次妄图用人力去挑战机器罢了。大模型抽象到一定层次之后，知识的表达不可能像Excel或者主谓宾地表达出来，知识已经成为了高阶隐空间的概率组合了，不应该存在明显的结构性。结构性，用时域频域谱变换的说法就是固有频点啊……数据结构性 ~= 可抽象 ~= 可压缩即使以人脑作为参考，脑子里的知识，绝不可能是按某种表格方式存放的，那样早就爆炸了。人脑在进化过程中还有意选择了对短期记忆的遗忘功能，防止结构化信息过多，影响大脑正常工作。失眠的同学有经验吧，如果老是回忆白天的场景，感觉上像不像CPU访问DRAM带宽拥塞了？ 每日总结（抽象）和遗忘是有助于睡眠的。ChatGPT是否有逻辑？ 怎么来的？我个人最擅长的能力就是逻辑推理，引以为傲所以坚定认为Transformer这种基于概率分布的组合是不可能形成具有Sequence特征的逻辑推理的，要么LSTM可以，但是规模太小。到今天，我认为我错了。因为Sequence是可以基于时序表达的，但是，Sequence也可以用recursion（递归）表达！逻辑推理，也许，即使在我们大脑中，也并不是基于顺序进行的，而是基于多次概率组合的递归，甚至于某些时候说女人的直觉，也许不过是概率权重 > 递归权重罢了。。。。。。这个解释，也能够解答为什么逻辑能力的出现是以涌现的方式呈现的，也能解答为什么人和猴子在尺度上差异导致的天壤之别。就是递归的次数和概率的表达能力的差异，甚至于这还能回答逻辑能力的培养方式，回想家里的小孩，在长大的过程中，其实也就是某一天开始，你发现他的表达出现了条理。ChatGPT有传言其逻辑能力是基于代码学习获得的，这也很容易解释。因为自然语言天然因为表达的丰富，除了法律文本外，大量文字的逻辑性不够清晰（需要更深的NN抽象），而代码的条件，if/else/case，已经具有非常直白的逻辑性了，正如语言能够通过概率表达一样，逻辑的形成自然分割成了多个可迭代和并发的概率组合。凸(艹皿艹 )，如果人类的逻辑思维如此简单，让人悲伤。因为逻辑能力也许就代表了一个人的意识本身了。记得小说《风云》、最近的漫画《拳愿》都有讲类似的故事。关于数学：） 还有很多人期待着NN什么时候能解数学题了，未来也许就能推导出更多人类未曾涉及的数学公理。ChatGPT能做一定程度的数学，这其中的原理是什么？很多时候，我会安慰自己，你看它调用计算器了：）因为数学需要的是推导，推导和推理不一样，推导具有结果的唯一性，这是概率组合所难以解释的，而且确实也多次看到ChatGPT算错数。但如果：数学推导 ~= 概率遍历下的递归推理，事情就变简单了。以正常人的能力来看，数学能力好的人，往往就表现为更加抽象化的推理能力。ChatGPT所拥有的数学推导，我认为就是遍历的逻辑推理能力。那么，当前ChatGPT在小学数学能完美解答，中学数学能部分解答的能力，也就是其NN网络的递归+遍历下的概率组合能力了，若要更强，继续暴力堆砌，也许就是答案。回到最初，我把当前的AI能力分成1、2、3种，如果可以的话，我想和人脑进行一些映射来理解它们最终可能的价值和作用。1、PINN类辅助AI，会有点像小脑，生物进化过程中的物理知识被某种NN快速计算，获得了人的运动和感知能力，我们能够快速感觉到人和人能擦肩而过不会碰撞，并不会对此产生意识。嗯，其实我觉得自动驾驶应该走向这条路。2、浅层网络和浅层结构化知识，就像人的海马体，但是人因为进化或者生存的原因，有意放弃了短时记忆（浅层记忆）能力，那结果就很简单了，如果把3+2合并在一起，人脑就被AI超越了，悲伤(⊙︿⊙)。3、大模型，就像大脑，高度抽象的知识融入神经元本身，这些知识已经无法直接被符号或结构化数据描述，这些知识高度抽象，具有多模态能力和推理能力，但也因为高度抽象和概率分布，存在错误概率。如果以LLM为基础想象的话 ：） 结合推荐网络的Embedding技术，把自然语言再做一次浅层抽象后的Embedding作为LLM的Raw data输入，会不会再提升效果呢？此外，如果再硬要说还缺什么？ ChatGPT好像还缺乏了人类的情绪，也许很多人认为情绪并不重要，但实际上以人为参照的话，情绪是支撑人持续学习（Self-Drive）的最大动力。说到最后，也许，transformer真的就是最后的答案，简单暴力的，在自然界也许就是最佳的，那剩下就是尺度了，再大一点，会涌现出什么呢？抽象的尽头是什么？有边界吗？人脑还能更大吗？这其实受限于生物能源的获取和转换效率。而transformer的尺寸增长，需要的仅仅是电和钱。不过，对于业界如火如荼的热情，在积极复现的路上，我也能讲一个小小的恐怖故事 ：）生物的进化存在随机性，AI也是。有没有可能OpenAI其实训练了十个初始模型（本身这存在随机性），然后类似育种的方式进行了挑选，以及在进行后续的进化中再施加选择性影响，直到最后。。。。。。。所以，我是做硬件的，我并不想做广告，但追赶者你可能真需要更多硬件和范围的探索来暴力地把随机性变成确定性。。。。。。。就像《神的九十亿个名字》那样"
186235163,3d5af70a45afddef2dcf86c38c9cbd4a,arkaiedu,https://api.zhihu.com/people/3d5af70a45afddef2dcf86c38c9cbd4a,people,,100篇人工智能论文,article,https://api.zhihu.com/articles/186235163,AI/ML景观 这是一篇关于100篇 <em>人工智能</em> 论文的文章来帮助解开 人工智能 景观的神秘。最初的部分是关于基础知识，并提供一些重要的链接来加强你的基础。后一部分链接到一些伟大的研究论文,336,6,792,1598144501,1598144501,"AI/ML景观 这是一篇关于100篇 人工智能 论文的文章来帮助解开 人工智能 景观的神秘。最初的部分是关于基础知识，并提供一些重要的链接来加强你的基础。后一部分链接到一些伟大的研究论文，是为那些想了解理论和细节的高级实践者。 人工智能是一场正在改变人类生活和工作方式的革命。这是一个广义的概念，即机器能够以人类认为“智能”的方式执行任务——这个术语可以追溯到70年前（见这里的历史 https://medium.com/future-today/understanding-artificial-intelligence-f800b51c767f），艾伦·图灵定义了一个测试，图灵测试 ( https://en.wikipedia.org/wiki/Turing_test)，用来测量机器表现出与人的智能行为相当或不可区分的智能行为的能力。革命有许多复杂的运动部分。我的目标是简化并提供一个关于这些复杂部分如何在一个3层蛋糕中组合在一起的观点。顶层是人工智能服务，即解决实际问题的真实应用程序，中间层是基本的ML算法，而底层是支持前两层的ML平台。 首先是基本定义，人工智能是由机器 ( https://en.wikipedia.org/wiki/Machine) 表现出来的智能( https://en.wikipedia.org/wiki/Intelligence)，而不是由人类表现出来的自然智能。机器学习（ML）是人工智能的一个子集，它基于这样一种理念：我们应该真正能够让机器访问数据，让它们自己学习。神经网络（NN）是ML的一个子集，在ML中，计算机系统被设计成像人脑一样通过对信息进行分类来工作。深度学习（Deep learning，DL）是ML的一个子集，它使用多层人工神经网络来解决诸如目标检测、语音识别和语言翻译等复杂问题。 关于AI、ML和DL之间的差异，可以在这里和这里找到一些很好的阅读资料。 https://towardsdatascience.com/notes-on-artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl-for-56e51a2071c2https://towardsdatascience.com/ai-machine-learning-deep-learning-explained-simply-7b553da5b960神经网络的基础知识在这里和这里通过代码都有很好的解释。 https://gadictos.com/neural-network-pt1/https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eafhttps://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9人工智能可以根据这里解释的窄型、一般型或强型分类，也可以根据这里解释的反应机器、有限记忆、思维理论和自我意识的水平分类 。 https://www.javatpoint.com/types-of-artificial-intelligencehttps://www.aware.co.th/three-tiers-ai-automating-tomorrow-agi-asi-ani/ML算法ML算法可以分为有监督、无监督和强化学习（这里 https://www.newtechdojo.com/list-machine-learning-algorithms/、这里 https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861和这里解释 https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/）。ML和DL在提取特征的方式上有所不同。传统的ML方法要求数据工作者通过应用学习算法显式地提取特征。另一方面，在DL的情况下，这些特性是由算法自动学习的，不需要特性工程——例如，来自google的Meena ( https://towardsdatascience.com/meena-googles-new-chatbot-657241cf5595) 新chatbot有26亿个特性。这是DL相对于传统ML方法的优势。 神经网络受到大脑神经元的启发，被设计用来识别复杂数据中的模式。我们大脑中的神经元被组织成数十亿个巨大的网络，每一个神经元通常与成千上万个其他神经元相连，通常是连续的一层，特别是在大脑皮层（即大脑外层）。神经网络有输入、输出和隐藏层。具有两个或多个隐藏层的神经网络称为深神经网络。 人工神经网络（ANN）模拟生物神经系统。一层的输出通过一个转换，使用Activations函数成为下一层的输入。激活函数是附着在网络中每个神经元上的数学方程，根据每个神经元的输入是否与模型的预测相关，确定是否应该激活（“激发”)。激活函数可以是线性或非线性的。TanH，Sigmoid，ReLU，Leaky ReLU的关键激活功能在这里 ( https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) 和这里( https://towardsdatascience.com/deep-study-of-a-not-very-deep-neural-network-part-2-activation-functions-fd9bd8d406fc) 被解释。 这里 ( https://towardsdatascience.com/types-of-neural-network-and-what-each-one-does-explained-d9b4c0ed63a1)和这里 ( https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90) 解释了不同种类的ANNs。这里有一张完整的神经网络图 ( https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)。 感知器 ( https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53)是一个单层神经网络。感知器是最古老最简单的神经网络，创建于1958年。它有一个输入层和一个输出层（没有隐藏层）。感知器不能学习复杂的模式，特别是它们不能解决一些琐碎的问题，例如排他或（XOR）分类问题。 多层感知器（MLP）又名“香草”网络-感知器的局限性可以通过叠加多个感知器来消除。MLP是深入学习的基础。一个完全连接的层或致密层就是其中一层中的所有神经元都连接到前一层中的每一个神经元。 卷积神经网络引入卷积层和子采样层。与MLP中的完全连接层不同，卷积层只与各自先前层的子区域连接，追求生物学启发的局部感受野的概念。美国有线电视新闻网基本上是一个深层次的神经网络，它由卷积层、汇聚层、全连通层和归一化层组成。CNN适用于图像和视频识别。这里 ( https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d)，这里 ( https://towardsdatascience.com/introduction-to-convolutional-neural-networks-cnn-with-tensorflow-57e2f4837e18)还有这里 ( https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)。 递归神经网络（RNN）是一种特殊的神经网络，它沿时间（时间）序列而不是空间（空间）工作，这使得它在手写和语音识别等关键问题上非常有用。递归神经网络（RNN）在这里 ( https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)和这里 ( http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 都有很好的解释。他们有短期记忆。长-短期记忆网络-通常被称为“LSTMs”-是一种特殊的RNN，能够学习长期依赖性（长时间记忆信息)，通过保持此处所述的时间状态来解决RNN限制 ( http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。门控递归单元是一种较新而简单的LSTM形式。LSTM和GRU都有内部机制，称为“门”，用于调节信息流。LSTM和GRU之间的区别在这里 ( https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) 得到了很好的解释）。 生成性对抗网络（GAN）是一种特殊的网络 ( https://arxiv.org/pdf/1710.07035.pdf)，它实际上有两个子网络生成器和鉴别器，这两个子网络不断地相互竞争。生成器尝试生成一些数据，鉴别器尝试将生成的数据与实际样本数据区分开来。这里是基础 ( https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)，这里是高级主题 ( https://towardsdatascience.com/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775)。 自动编码器（ https://arxiv.org/pdf/1606.05908.pdf）和变分自动编码器 ( https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) 是实现压缩和解压功能的一类特殊的神经网络。它们首先将输入压缩到一个潜在空间表示中，然后从该表示中解压缩输出，从而将输入复制到输出中。自动编码器的两个应用是数据去噪和降维。这里 ( https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f) 还有这里 ( https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f)。 图形神经网络（GNN) ( http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1015.7227&rep=rep1&type=pdf, https://arxiv.org/pdf/1812.08434.pdf) 是一种基于深度学习的图形处理方法。现有机器学习算法的一个关键假设是每个数据实例彼此独立。这种假设不再适用于社会网络、地理地图、分子结构等图形数据，因为每个实例（节点）通过链接和交互与其他实例（节点）相关。GNNs自然有助于建立图形数据结构的模型。像CNNs和RNNs这样的标准神经网络不能处理图形输入，因为它们是从一层到下一层的顺序处理。这里 ( https://towardsdatascience.com/https-medium-com-aishwaryajadhav-applications-of-graph-neural-networks-1420576be574) 和这里 ( https://towardsdatascience.com/a-gentle-introduction-to-graph-neural-network-basics-deepwalk-and-graphsage-db5d540d50b3) 的基础知识。GNN与CNN的混合称为图形卷积网络（GCN） ( https://towardsdatascience.com/beyond-graph-convolution-networks-8f22c403955a) ，RNN称为图形时空网络（Graph Spatio-Temporal Networks），自动编码器称为图形自动编码器。卷积网络（GCN）是更有趣的网络（这里是基础 https://towardsdatascience.com/beyond-graph-convolution-networks-8f22c403955a）。还有两种口味。光谱GCN或空间GCN。光谱gcn使用的滤波器类似于常规卷积神经网络的处理方式。另一方面，空间GCN通过聚合相邻节点来进行卷积。 Spiking Neural Networks（ https://arxiv.org/pdf/1804.08150.pdf）是下一代神经网络（ANN），它希望利用类似于我们大脑神经元功能的模型，弥合神经科学和机器学习之间的鸿沟。SNNs顾名思义使用离散激活阈值或峰值，而不是常用网络中的连续值。更多 ( https://towardsdatascience.com/spiking-neural-networks-the-next-generation-of-machine-learning-84e167f4eb2b)。 考虑到神经网络模型的复杂性和爆炸性，有相当大的努力使架构工程自动化，以找到针对给定业务问题的最佳机器学习模型设计神经架构搜索 ( https://towardsdatascience.com/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136)。这是AutoML和超参数优化的一个子领域。这里 ( https://towardsdatascience.com/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136) 还有这里 ( https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf)。 ML平台一个重要的努力不是花在创新新的学习算法或模型上，而是花在改进ML工具和基础设施上。ML平台为机器学习开发人员、数据科学家和数据工程师提供了基础，使他们能够快速、经济高效地将ML项目从构思到生产和部署。 ML生命周期典型的ML生命周期从数据准备开始，然后是（特性）发现、开发和培训模型、测试、部署，最后使用模型进行推断或预测。数据准备通常与获取、推导和清理足够的训练数据以输入ML算法有关。功能发现和提取识别对业务领域最重要的关键数据属性。部署包括可观测性、可调试性、监控和生产化。这里可以找到Auto-ML框架 ( https://medium.com/georgian-impact-blog/choosing-the-best-automl-framework-4f2a90cb1826)的特定于云的基准 ( https://arxiv.org/pdf/1808.06492.pdf)。 在现代的ML生命周期中有几个挑战（见技术债务 Technical Debt. https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)。 多个算法和库—ML开发人员通常希望尝试所有可用的算法来构建最精确的模型，而一个正在进行活动项目的团队可能会尝试多个处理库—例如 ... MxNet( https://www.cs.cmu.edu/~muli/file/mxnet-learning-sys.pdf) TensorFlow( http://download.tensorflow.org/paper/whitepaper2015.pdf) Clipper( https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf) Caffe( https://arxiv.org/pdf/1408.5093.pdf) PyTorch( https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf) Theano( https://arxiv.org/pdf/1605.02688.pdf) Chainer( https://arxiv.org/pdf/1908.00213.pdf) 实验-模型预测可能受到数百个配置参数的影响，从输入数据到超参数。 精确性-从培训到部署的模型可能会从数据科学家转手到数据工程师，后者可能最终使用不同版本的库，从而导致模型性能不佳。 部署-由于多个推理环境（如REST服务、批处理和在线评分）而存在挑战。像Kubeflow这样的框架使得部署更加简单 ( https://medium.com/de-bijenkorf-techblog/deploying-machine-learning-models-with-kubeflow-b2cb45cf22f5) 。 MLFlow ( https://cs.stanford.edu/~matei/papers/2018/ieee_mlflow.pdf)斯坦福大学的开源项目（Databricks)和DAWN项目 ( https://arxiv.org/pdf/1705.07538.pdf)（Data analytics for what next）正试图解决这些挑战。 MLflow的关键原则是开放式界面设计，与现有的ML平台如... Facebook FbLearner ( https://engineering.fb.com/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/) Uber ( https://towardsdatascience.com/uber-has-been-quietly-assembling-one-of-the-most-impressive-open-source-deep-learning-stacks-in-b645656ddddb) Michaelangelo ( https://eng.uber.com/michelangelo/) Netflix ( https://blog.valohai.com/machine-learning-infrastructure-lessons-from-netflix) Google TFX ( http://stevenwhang.com/tfx_paper.pdf) Airbnb Bighead ( https://conferences.oreilly.com/strata/strata-ny-2018/public/schedule/detail/69383) 腾讯 ( http://www.vldb.org/pvldb/vol7/p1772-tencent.pdf) 参见此处的比较( https://towardsdatascience.com/five-open-source-reference-architectures-designed-to-build-machine-learning-at-scale-5090c3733f09） 相比，这种开放式界面设计在保留生命周期管理优势的同时，为用户提供了灵活性和控制能力。DAWN堆栈除了解决ML生命周期的挑战外，还解决了从新接口到新硬件的抽象。 ML StackAI/ML ArkAI Stack ( http://www.arkai.net/custom/page/stack_course/4575) 1） 计算硬件-一个很好的看法，计算硬件-CPU，GPU和TPU-可以在这些文件中找到从Facebook ( https://research.fb.com/wp-content/uploads/2017/12/hpca-2018-facebook.pdf) 和谷歌 ( https://arxiv.org/pdf/1704.04760.pdf)。量子物理( https://towardsdatascience.com/exploring-tensorflow-quantum-googles-new-framework-for-creating-quantum-machine-learning-models-3af27ba916e9)与ML( https://towardsdatascience.com/exploring-tensorflow-quantum-googles-new-framework-for-creating-quantum-machine-learning-models-3af27ba916e9)的交叉产生了量子机器学习 ( https://towardsdatascience.com/exploring-tensorflow-quantum-googles-new-framework-for-creating-quantum-machine-learning-models-3af27ba916e9)。Google最近开放源码的Tensorflow Quantum ( https://www.tensorflow.org/quantum)，用于使用Google Cirq等框架快速成型混合经典量子模型( https://github.com/quantumlib/Cirq)。 2） 具有高度并行性的分布式深度学习运行模型需要并发性（见分析） ( https://arxiv.org/pdf/1802.09941.pdf)和调度程序（DL2 ( https://arxiv.org/pdf/1909.06040.pdf)、Optimus ( https://i.cs.hku.hk/~cwu/papers/yhpeng-eurosys18.pdf)和Tiresias( https://www.usenix.org/system/files/nsdi19-gu.pdf)）。在构建分布式深度学习框架方面有很多进展，这里有一个很好的介绍，其中最受欢迎的是Google GPIPE ( https://arxiv.org/pdf/1811.06965.pdf)、Uber Horovard( https://arxiv.org/pdf/1802.05799.pdf)、DeepMind的TF Replicator ( https://arxiv.org/pdf/1902.00465.pdf) 和微软的PipeDream ( https://arxiv.org/pdf/1806.03377.pdf) Zero&amp&DeepSpeed( https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)。 3） 功能存储允许不同的团队管理、存储和发现用于机器学习项目的功能。它充当数据工程和数据科学之间的API，支持改进的协作。这里有一个很好的介绍 ( https://towardsdatascience.com/what-are-feature-stores-and-why-are-they-critical-for-scaling-data-science-3f9156f7ab4)，其中列出了一些特色商店。 * 来自谷歌的Feast ( [https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning](https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning)), ( [https://towardsdatascience.com/using-feast-to-centralize-feature-storage-in-your-machine-learning-applications-dfa84b35a1a0](https://towardsdatascience.com/using-feast-to-centralize-feature-storage-in-your-machine-learning-applications-dfa84b35a1a0)) * LogicalClocks公司的HopesWorks ( [https://uploads-ssl.webflow.com/5e6f7cd3ee7f51d539a4da0b/5e6f7cd3ee7f519fdfa4dadb_feature%20store%20whitepaper%201-0.pdf](https://uploads-ssl.webflow.com/5e6f7cd3ee7f51d539a4da0b/5e6f7cd3ee7f519fdfa4dadb_feature%20store%20whitepaper%201-0.pdf)) * LinkedIn的Frame ( [https://www.slideshare.net/DavidStein1/frame-feature-management-for-productive-machine-learning](https://www.slideshare.net/DavidStein1/frame-feature-management-for-productive-machine-learning)) * Airbnb的ZipLine ( [https://databricks.com/session/zipline-airbnbs-machine-learning-data-management-platform](https://databricks.com/session/zipline-airbnbs-machine-learning-data-management-platform)) * ML Ops ( [https://towardsdatascience.com/mlops-with-a-feature-store-816cfa5966e9](https://towardsdatascience.com/mlops-with-a-feature-store-816cfa5966e9))可以与一个特性存储相结合，以自动化模型的培训、测试和部署。4） 可解释性和可解释性-人工智能系统中信任的4个特征是i）模型和数据没有偏见的公平性ii）稳健性，不易篡改或损害他们接受过培训的数据iii）决策可以被消费者理解的可解释性iv）允许审核模型生命周期的开发、部署和维护。这些技术中最突出的是LIME和SHAP ( https://towardsdatascience.com/idea-behind-lime-and-shap-b603d35d34eb)，它们基于模型不可知的方法，这些方法专注于解释给定黑盒分类器的单个预测。局部可解释模型不可知解释（LIME）提供了一种快速的方法，它通过随机地迭代扰动模型特征来获得预测，然后利用预测计算近似的线性“解释模型”。SHAP（SHapley加法解释）通过计算每个特征对预测的贡献来工作。它起源于联合博弈理论，其中数据实例的每个特征值充当联盟中的参与者，并计算每个特征的边际贡献或支出（参见Chris Molnar的《可解释机器学习》一书 https://christophm.github.io/interpretable-ml-book/）。这里很好地描述了这两种技术之间的进一步差异 ( https://medium.com/ibm-garage/explaining-black-box-models-ensemble-and-deep-learning-using-lime-and-shap-53c59d9f09b3)。谷歌的可扩展性白皮书也是一个很好的参考 ( https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf)。 为了消除偏见 ( https://towardsdatascience.com/how-to-detect-bias-in-ai-872d04ce4efd)，Google最近引入了两种新的方法，即用概念激活向量（TCAV https://towardsdatascience.com/tcav-interpretability-beyond-feature-attribution-79b4d3610b4d）和联合学习进行测试。TCAV从实例数据中学习概念。例如，TCAV需要实例数据中的几个女性示例，以学习“性别”概念。人类用概念思考和交流，而不是对每个特征使用权重。TCAV以人类相互交流的方式提供解释。联合学习 ( https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)将模型训练推向了边缘（以移动计算为例)。 5） 可视化-与模型的可解释性和可解释性相关的是模型可视化。白盒 ( https://medium.com/sciforce/introduction-to-the-white-box-ai-the-concept-of-interpretability-5a31e1058611)人工智能着眼于模型复杂度和模型感知，提出了不同的可视化技术 ( https://medium.com/sciforce/white-box-ai-interpretability-techniques-93ef257dd0bd)。谷歌开源Facets ( https://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html)有助于培训数据的可视化，Uber在内部使用Manifold ( https://arxiv.org/pdf/1808.00196.pdf)，而Facebook有HiPlot ( https://ai.facebook.com/blog/hiplot-high-dimensional-interactive-plots-made-easy/)。 6） 度量-度量用于度量模型的质量和性能。有许多不同类型的评估指标可用于测试模型。包括混淆矩阵、分类准确度、精密度、召回率、ROC、AUC、F1评分、敏感性、特异性、对数损失、均方误差和绝对误差。这个由三部分组成的系列是一个很好的参考 ( http://mkhalusova.github.io/blog/2019/04/11/ml-model-evaluation-metrics-p1)。另一个介绍性的参考资料在这里 ( https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)。就模型预测而言，偏差和方差之间存在一种权衡，这种权衡可能是由于拟合不足或拟合过度造成的 ( https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)。为了建立一个好的模型，你需要通过在偏差和方差之间找到一个好的平衡来最小化总误差。另一个需要测量的重要问题是概念漂移和模型衰减 ( https://towardsdatascience.com/concept-drift-and-model-decay-in-machine-learning-a98a809ea8d4)。当输入和输出数据之间的关系随着时间的推移而改变时，概念漂移就发生了( https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/)，这意味着在较旧数据上训练的模型不再像在较新数据上训练的那样精确。防止模型衰减的一个好方法是持续监视和维护它 ( https://towardsdatascience.com/why-machine-learning-models-degrade-in-production-d0f2108e9214)。 最后，下一节是为一个人工智能技术专家谁是感兴趣的建设和扩展ML算法和平台。 深度学习的体系结构、算法和应用综述 ( https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S2048770313000097) 深度学习综述：算法、技术和应用 ( https://eclass.uop.gr/modules/document/file.php/DIT209/προτεινόμενη%20θεματολογία%202019-2020/A%20Survey%20on%20Deep%20Learning%20Algorithms%2C%20Techniques%2C.pdf) 机器学习框架与图书馆综述 ( https://www.researchgate.net/publication/329990977_Machine_Learning_and_Deep_Learning_frameworks_and_libraries_for_large-scale_data_mining_a_survey) 分布式基础设施上可扩展的深度学习：挑战、技术和工具 ( https://arxiv.org/pdf/1903.11314.pdf) 大规模图形网络综述。( https://arxiv.org/pdf/1901.00596.pdf)"
605595835,586db5f82102d503f5e8f9ca6b3882c0,清浦,https://api.zhihu.com/people/586db5f82102d503f5e8f9ca6b3882c0,people,保守自由主义｜经济学人,钱颖一：人工智能将使中国教育的优势荡然无存,article,https://api.zhihu.com/articles/605595835,而死记硬背、大量做题正是我们目前培养学生的通常做法。 所以，一个很可能发生的情况是：未来的<em>人工智能</em>会让我们的教育制度下培养学生的优势荡然无存。 不久前，<em>人工智能</em>机器人参加了高考数学考试。报道说有两台机器人，得分分别是134分和105分（满分150分）,363,105,825,1676166258,1687388059,日前，由国务院参事室公共政策研究中心和新华网思客共同主办的《参事讲堂》正式开讲，国务院参事、清华大学经济管理学院院长钱颖一以“创新人才教育”为主题发表了主旨演讲。梁启超在一百多年前就提到大学不能成为知识的杂货铺。在钱颖一看来，创新人才的教育仅仅靠知识积累是不够的，教育必须超越知识。对于创新人才的教育，有3个十分重要的因素：知识、好奇心和想象力、价值取向。01 智能让现有优势荡然无存中国的教育有它的特点，这个特点中隐含了我们的长处。首先，个人、家庭、政府、社会对教育的投入很大，这个投入不仅是金钱、资源的投入，也包括学生、教师时间的投入。这是由我们的文化传统，由我们对教育的重视程度所决定的。其次，教师对知识点的传授、学生对知识点的掌握，不仅量多，而且面广，所以中国学生对基本知识的掌握呈现“均值高”的特点。我想，在了解中国教育长处的基础上来反思教育存在的问题，可能更有意义。我认为，中国教育的最大问题，就是我们对教育从认知到实践都存在一种系统性的偏差，这个偏差就是我们把教育等同于知识，并局限在知识上。教师传授知识是本职工作，学生学习知识是分内之事，高考也是考知识，所以知识就几乎成了教育的全部内容。02教育必须超越知识“知识就是力量”这句话深入人心，但是，创新人才的教育仅仅靠知识积累就可以吗？我的答案是否定的。教育必须超越知识。这是我对创新人才教育的一个核心想法，也是我们提出教育改革建议的出发点。爱因斯坦在1921年获得诺贝尔物理学奖后首次到美国访问，有记者问他声音的速度是多少，爱因斯坦拒绝回答，他说，你可以在任何一本物理书中查到答案。接着，他说了那句特别有名的话：“大学教育的价值不在于记住很多事实，而是训练大脑会思考。”在今天，很多的知识可以上网查到。在未来，可能有更多的知识机器会帮你查到。所以爱因斯坦的这句话在当前和未来更值得我们深思。我们知道，人工智能就是通过机器进行深度学习来工作，而这种学习过程就是大量地识别和记忆已有的知识积累。这样的话，它可以替代甚至超越那些通过死记硬背、大量做题而掌握知识的人脑。而死记硬背、大量做题正是我们目前培养学生的通常做法。所以，一个很可能发生的情况是：未来的人工智能会让我们的教育制度下培养学生的优势荡然无存。不久前，人工智能机器人参加了高考数学考试。报道说有两台机器人，得分分别是134分和105分（满分150分）。而这还只是个开始，据说人工智能机器人的目标是到2020年能够参加全部高考。所以，经济发展需要“创新驱动”，人工智能发展势头强劲，这些都让我们认识到对现有教育体制和方法进行改革的迫切性。03知识越多未必创造力越强我在教学实践中强烈地感受到，创造性思维的来源之一是好奇心和想象力。创造力确实需要知识的累积，但除了知识，还需要什么呢？爱因斯坦说过两句话：“我没有特殊的天赋，我只是极度好奇”、“想象力比知识更重要”。他说的好奇心和想象力，我觉得是我们过去比较忽视的。受此启发，我提出一个简单的假说：创造性思维=知识×好奇心和想象力。这个简单的公式告诉我们，知识越多未必创造力越强。人接受的教育越多，知识积累得越多，好奇心和想象力可能相应减少，所以创造力并非随着受教育时间的增加而增加。为什么？因为我们后来学的知识都是有框架和设定的，不管什么知识都是这样。在学习这些知识时，你的好奇心、想象力往往会挑战这些知识框架，而绝大多数情况下，你的挑战是错的，因此受到打击和否定，客观上便压制了你的好奇心和想象力。连爱因斯坦都曾经感叹：“好奇心、想象力能在正规教育中幸存下来，简直就是一个奇迹。”这就形成了创新人才教育上的一个悖论——更多教育一方面有助于增加知识而提高创造性，另一方面又因压抑好奇心和想象力而减少创造性。这两者的合力让我们判断教育对创新人才产生的作用变得困难，但可以部分解释为什么有些辍学的学生反而很有创造力。因此，并不是我们的学校培养不出杰出人才，而是我们的学校在增加学生知识的同时，有意无意地减少了创造力必要的其他元素。功利主义扼杀了创造性思维创造性思维不仅取决于好奇心和想象力，还与价值取向有关，所以当我们讨论创新人才教育时，它不仅是一个知识和能力的问题，也是一个价值观的问题。我们现在面临的是一个比较急功近利的社会，盛行短期功利主义的价值取向，这对创造性思维是很有害的。不久前，扎克伯格在哈佛大学2017年毕业生典礼上的演讲，主题是讲人要有追求，要有更高的追求，就是要超越短期功利主义的价值取向。04我们缺乏创新型人才的原因我把创新的动机分为三个层次，分别代表三种价值取向：一、短期功利主义；二、长期功利主义；三、内在价值的非功利主义。对短期功利主义者而言，创新是为了发论文、申请专利、公司上市；对长期功利主义者而言，创新有更高的追求，为了填补空白、争国内一流、创世界一流；而对内在价值的非功利主义者而言，创新有更高的追求：追求真理、改变世界、让人变得更加幸福。我们的现实情况是，具有第一类动机的人很多，具有第二类动机的人也有，但是具有第三类动机的人就少了，甚至可以说是寥寥无几。所以，我们之所以缺乏创新型人才，除了缺乏好奇心和想象力之外，就是在价值取向上太急功近利，太功利主义。急于求成的心态、成王败寇的价值观，导致更多的抄袭、复制，而较少真正的创新，更不太可能出现颠覆性创新、革命性创新。05改革不易，但变化令人鼓舞基于以上的反思，我认为创新人才的教育需要创新的教育模式。我提出三条建议：第一，教育应该创造更加宽松的、有利于学生个性发展的空间和时间；第二，在教育中要更好地保护学生的好奇心、激发学生的想象力；第三，在教育中要引导学生在价值取向上有更高的追求，避免短期功利主义。这就对教育改革提出了更高的要求。因为目前学生培养方案的设计多以学生掌握知识的深度、广度为出发点和考核点，总觉得学生学得不够多、不够深，学得不够实用、不够前沿。但是如果我们更关心学生的好奇心和想象力，更关注学生的价值取向，那么我们的教育模式就应该有较大的改变。在实践中，我也体会到改革是很不容易的，传统的观念、市场的压力、社会的环境都是制约因素。但是，对学生好奇心、想象力的关注，在社会上得到越来越多的共鸣；学生的个性发展也被上升到越来越高的高度。这些都是令人鼓舞的变化。所以，我相信随着中国经济进一步发展，随着社会对创新人才需求的增加，创新人才教育将会发生深刻的变化。
536401839,6a10f1af4d971e298ee9a49db84b86f9,四喜Clion,https://api.zhihu.com/people/6a10f1af4d971e298ee9a49db84b86f9,people,程序员里面最美的,人工智能学习路线图-最全自学路线,article,https://api.zhihu.com/articles/536401839,<em>人工智能</em>Python语言入手→→机器学习核心技术→→深度学习核心技术→→NLP自然语言处理技术→→CV计算机视觉技术 01—<em>人工智能</em>开发入门 掌握Python基础语法,125,6,1097,1657606694,1666255109,人工智能Python语言入手→→机器学习核心技术→→深度学习核心技术→→NLP自然语言处理技术→→CV计算机视觉技术01—人工智能开发入门掌握Python基础语法，对后续学习打下坚实基础。Python编程基本语法数据结构函数面向对象多任务模块与包闭包装饰器迭代器Numpy矩阵运算NadrrayScalarsBoradcasting矩阵运算矩阵转置矩阵求逆Scipy数值运算库Scipy基本使用Scipy常量Scipy稀疏矩阵Scipy图结构Scipy空间Scipy插值Pandas数据科学库自带数据结构数据读取写入数据清洗数据计算数据合并数据排序Matplotlib基础图表AnnotationFigure子图LegendSeaborn数据关系图数据分布图类别图回归图矩阵图多变量关系PyEcharts基本使用图表API组合图表其他资源图表类型Web框架整合阶段案例实战RFM案例实战电商运营案例实战教程视频 1、零基础Python编程 2、零基础数据科学库02— 机器学习核心技术掌握核心机器学习算法原理，掌握分类、回归、聚类使用场景，满足人工智能数据挖掘类岗位，薪资可达到15K-20K。Scikit Learn聚类算法API数据预处理分类算法API回归算法API分类算法决策树KNNAdaboost随机森林逻辑回归朴素贝叶斯GBDTXGboostLightGBM回归算法线性回归Lasso回归决策树回归随机森林回归XGboost回归聚类算法KMeansKMeans++GMM基于层次聚类基于密度聚类DBSCAN属性降维属性降维特征选择因子分析PCAICALDA模型选择MetricsScoring模型得分Grid search 网格搜索Cross Validation 交叉验证Hyper-Parameters 超参数选择Validation curves 模型验证曲线特征工程Standardization标准化Scaling Features归一化Non-linear transformation非线性转化Gaussian distribution高斯分布转化Normalization正则化Encoding categorical features类别性编码处理阶段案例实战 零售行业建模分析电商用户画像行为分析教程视频1、零基础快速入门机器学习03— 深度学习核心技术掌握深度学习算法原理，掌握BP神经网络、CNN、RNN等基础网络结构，为后续NLP和CV奠定技术基础。人工神经网络损失函数激活函数Back Propagation优化方法及正则化BP神经网络网络基本结构正向计算链式法则权重更新Sigmoid函数梯度消失/爆炸Batch NormalizationCNN卷积神经网络局部感受野权值共享DropOut卷积层池化层全连接层RNN循环神经网络梯度裁剪双向长短时记忆网络（BiLSTM）长短时记忆网络（LSTM）门控神经网络(GRU)阶段项目实战图像识别案例文本处理案例教程视频1、深度学习核心技术课程04— NLP自然语言处理技术本阶段课程将掌握突破市场最热超高年薪的NLP技术，可应对市场上NLP工程师，根据市场反馈薪资突破30K。Pytorch编程定义损失函数自动微分功能定义优化器定义模型结构传统序列模型隐马尔科夫模型条件随机场原理与实践CRF与HMM区别Transfomer原理编码器解码器注意力机制语言模型模型超参数模型验证文本预处理文本处理基本方法文本张量表示方法文本语料数据分析数据增强方法命名实体识别Word Embedding词嵌入RNN及变体传统RNNLSTMBi-LSTMGRUBi-GRUSeq2Seq迁移学习FastText预训练模型Google BERTGPTGPT-2权重微调阶段项目实战文本分类文本生成教程视频1、NLP基础2、Pytorch编程3、Google BERT入门4、NLP项目实战05— CV计算机视觉技术掌握计算机视觉基础算法原理，掌握目标检测、分类、跟踪等各类CV任务，可应对市场上CV工程师岗位，薪资可达20K以上。OpenCV图像处理读写图像灰度变换几何变换形态学纹理分割视频操作边缘检测技术特征检测和描述Tensorflow编程常量变量高阶APItf.datatf.keras目标分类卷积计算方法多通道卷积AlexNetVGGResNet残差网络ImageNet分类目标检测RCNNFPNSSDROI PoolingFasterRCNN非极大抑制NMS目标分割全卷积ROI AlignDeepLabMaskRCNN金字塔池化模块语义分割评价标准阶段项目实战目标分类目标检测教程视频1、OpenCV入门2、Tensorflow框架3、CV计算机视觉项目实战06— 大厂面试专题围绕大厂高频面试题，针对机器学习、深度学习、CV、NLP、数据结构与算法系列等专题去刷题数据结构栈树图数组链表哈希表常见算法排序查找链表算法动态规划二叉树递归算法贪心算法图算法队列算法机器学习&深度学习分类算法面试专题聚类算法面试专题回归算法面试专题深度学习基础面试专题NLP & CV面试专题深度学习与NLP面试专题深度学习与CV面试专题练习地址：01 在线自定义刷题神器_自动生成面试笔试题_IT自测题库-趣IT02 LintCode 炼码https://www.lintcode.com/problem/?typeId=4整理了一上午知识点，一直在思考一个问题，什么样子的人工智能人才受欢迎？大概是能将模型应用于专业领域的人，即跨领域让机器学习落地的人。对于大部分机器学习实践者来说还是该脚踏实地，盲目追逐热点很容易跌进陷阱，而巩固基础、寻找自己擅长的领域和机器学习交叉点可以帮助你在未来的就业市场变得炙手可热，成为工业界紧缺的人工智能人才。共勉 记得收藏起来好好学
436617570,de60a7a2f450d6d634297050006d82e7,<em>AI</em>知乎kelaile520,https://api.zhihu.com/people/de60a7a2f450d6d634297050006d82e7,people,,什么是人工智能？,article,https://api.zhihu.com/articles/436617570,"形象思维、灵感思维促进<em>人工智能</em>的突破性的发展. 人工智能就是研究如何使计算机去做过去只有人才能做的智能工作,<em>人工智能</em>是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作",5,1,8,1637637715,1637637715,"人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总的来说，“人工系统”就是通常意义下的人工系统。人工智能是研究使计算机来模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等）的学科，主要包括计算机实现智能的原理、制造类似于人脑智能的计算机，使计算机能实现更高层次的应用。人工智能将涉及到计算机科学、心理学、哲学和语言学等学科。可以说几乎是自然科学和社会科学的所有学科，其范围已远远超出了计算机科学的范畴，人工智能与思维科学的关系是实践和理论的关系，形象思维、灵感思维促进人工智能的突破性的发展.人工智能就是研究如何使计算机去做过去只有人才能做的智能工作,人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。人工智能在计算机领域内，得到了更加广泛的重视。并在机器人，控制系统，仿真系统中得到应用。"
426914207,9c970d0af6db294311e38cc653f814bb,灰白,https://api.zhihu.com/people/9c970d0af6db294311e38cc653f814bb,people,华中科技大学同济医学院在校生,从马克思主义基本原理看人工智能,article,https://api.zhihu.com/articles/426914207,这也就是为什么<em>人工智能</em>在文艺创作方面难以寸进。人工智能是人类的理性智能的模拟和扩展，不具备情感、信念、意志等形式。因此<em>人工智能</em>在相当长的时间里人工智能还不能具有人的全部意识,50,8,149,1635424387,1635424387,让我们先给人工智能下个定义，人工智能即将人的部分智能活动机器化，让机器具有完成某种复杂目标的能力，它实质上是对人脑组织活动结构与思维运行机制的模仿，是人类智能的物化。自人工智能这一概念出现，人工智能能否具有人的意识，能否超越和取代人类智能等问题便引起广泛的讨论。让我们来细数人工智能的应用：无人驾驶汽车，人脸识别，机器翻译，声纹识别，智能客服机器人，智能外呼机器人，智能音箱，个性化推荐，医学图像处理，图像搜索。便可知，人工智能的着重点是深度学习以取代人类进行信息处理，识别，规划。与之相比，工业机器人的着重点是机械化，重复执行程序。也就是说，工业机器人只是人形的工具而已，是机床，机械的拟人化形态。相对的，人工智能在企图将人的意识物化，通过人工智能强大的运算能力和数据支持，以完成人无法完成的任务。由此便可以发现，人工智能便是将意识活动的部分从人脑中分离出来，但最重要的一点无法实现：人的意识是知情意的统一体，这也就是为什么人工智能在文艺创作方面难以寸进。人工智能是人类的理性智能的模拟和扩展，不具备情感、信念、意志等形式。因此人工智能在相当长的时间里人工智能还不能具有人的全部意识。而从上面的应用便可知，人工智能的发展将会取代很多原本属于人类的工作，比如司机，一部分翻译，客服等等，这也给很多这样岗位的人造成了恐慌和忧虑。但这件事我们应该一分为二的看待。其一，纵观历史，每一次科技进步都会改变很多职业的工作方式，会对一些岗位造成冲击，但不能忽视，这些变化也解放了社会生产力，提高了生活质量。机械化生产的确冲击了一部分职业，却也创造了更多的就业岗位。不仅如此，若无纺织业大发展，便无现在百花齐放的服装风格；若无机床等大型机械的出现，便无现在汽车火车飞机贯通南北东西。因此，也要更加开放的看待人工智能。其二，这样的但又也并非空穴来风，杞人忧天。以工业化大发展时期为例，大量人口涌入工厂激化了社会矛盾，若不能及时出台相关政策，妥善处理变革，便如几百年前马克思所在的时代，工人运动风起云涌，社会动荡不安，酿成巨大的悲剧。反观如今的人工智能，人工智能大发展，必然会造成社会的巨大变革。无人驾驶取代司机，人工智能取代部分劳动密集型的工人，这些岗位上的动荡，必然造成部分人失业，若不能帮助他们顺利度过，失业，生活质量下降，家庭失去保障，必然会引起社会动荡。因此，我们不能妖魔化人工智能，但也绝不能轻视人工智能对社会的影响。
686729169,429ed038aaf48db7e5b3fb2c864b4c41,艾布斯,https://api.zhihu.com/people/429ed038aaf48db7e5b3fb2c864b4c41,people,,2024年人工智能专业大学排名及评级结果,article,https://api.zhihu.com/articles/686729169,研究、开发用于模拟、延伸和扩展人的<em>智能</em>的理论、方法、技术及应用系统的一门新的技术科学，企图了解<em>智能</em>的实质，并生产出一种新的能以人类<em>智能</em>相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等,39,0,56,1710295819,1710295819,近日，中国大学排行榜官网（CNUR）正式发布了2024年ABC中国大学人工智能专业排名及评级结果！ABC中国大学专业排名，又称中国大学专业评级，是中国大学评级（China University Rating）重要分支，由ABC咨询机构编制、CNUR发布，涉及近6万个本科专业点，面向1200余所本科院校，700多个专业，100个S级“超级一流本科专业点”，3条主线（专业层、学科层、学校层），AUE联合评价模式，是志愿报考的重要参考！评级结果人工智能专业开设晚、院校多，从2018年至今开设院校累计达470所，是新兴热门专业。2024年ABC中国大学人工智能专业排名及评级结果显示：清华大学、南京大学、中国科学技术大学、上海交通大学和西安电子科技大学排名全国人工智能专业前五，评级结果为S；北京大学、哈尔滨工业大学、浙江大学、电子科技大学和华中科技大学进入全国前2%行列，评级结果为A+；东南大学、西安交通大学、北京航空航天大学、复旦大学、北京邮电大学、中国人民大学、中南大学等37所进入全国前10%，评级结果为A；西南大学、暨南大学、武汉理工大学、中国矿业大学、华东理工大学、中国农业大学、兰州大学等71所进入全国前25%，评级结果为B+；河南理工大学、沈阳建筑大学、中国民航大学、对外经济贸易大学、山东财经大学、石家庄铁道大学、北京工商大学等118所进入全国前50%，评级结果为B。专业点评人工智能，是一个以计算机科学为基础，由计算机、心理学、哲学等多学科交叉融合的交叉学科、新兴学科，研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。评级榜单组图1：2024年人工智能专业评级排名B类以上院校组图2：2024年人工智能专业开设院校排名(民办前50)（专业点均为人工统计，如有遗漏错误，敬请联系补充）
668805262,a90928eb916f98f8142587addaed2155,森林,https://api.zhihu.com/people/a90928eb916f98f8142587addaed2155,people,,人工智能必看的45篇论文,article,https://api.zhihu.com/articles/668805262, （点击了解详情）<em>人工智能</em>必看的45篇论文具体资源详见：【腾讯文档】<em>人工智能</em>:人工智能必看的45篇论文 这是一本非常全面的、专注于实战的AI图书，兼容TensorFlow,5,0,38,1703864105,1703864105,"琳琅满目的AI技术、论文，常会把刚入行的初学者们搞得一头雾水——到底哪些论论必须要看，先看哪些后看哪些。而市面上大部分的书籍又只是讲述最基础的原理，往往不能和论文很好的衔接起来。看着一个个大厂的黑科技产品靓丽光鲜。而自己手里看到的技术却都是向量机、矩阵、随机森林、岭回归、决策树之类的知识碎片。不仅会让人感叹，我们是一个活在时代吗？其实IT技术发展是飞快的。而AI领域的发展会是IT中最快的。我们所看到的那些黑客技，其后面无不堆积了大量的论文。而且都是最新、最前沿的论文。从某种调度来讲，他们所用的技术跟书籍里的内容确实不是一个时代。要想与时俱进，就必须改变思路——从论文入手。小编在这里为大家准备了一道硬菜——45篇让你跟上时代的论文。具体资源详见：【腾讯文档】人工智能:人工智能必看的45篇论文45篇论文分为如下十一个部分：神经网络基础部分图像分类部分批量正则化部分注意力部分高级的卷积网络知识图像内容处理部分循环神经网络部分AI合成部分多任务学习NLP部分模型攻防看过《深度学习之TensorFlow工程化项目实战》的小伙伴一定会发现，这45篇论文在书中都有讲过，大部分论文还是以中文描述+实际代码案例的形式出现，便于学习了解。（点击了解详情）人工智能必看的45篇论文具体资源详见：【腾讯文档】人工智能:人工智能必看的45篇论文这是一本非常全面的、专注于实战的AI图书，兼容TensorFlow 1.x和2.x版本，共75个实例。结构清晰、案例丰富、通俗易懂、实用性强。适合对人工智能、TensorFlow感兴趣的读者作为自学教程。No. 1 wide_deep模型论文关于神经元、全连接网络之类的基础结构，想必每个AI学者都有了解。那么你是否真的了解全连接网络中深层与浅层的关系呢？来看看wide_deep模型吧。这篇论文会使你对全连接有个更深刻的理解。关于该模型的更多介绍可以参考论文：在wide_deep模型中，wide模型和deep模型具有各自不同的分工。wide模型：一种浅层模型。它通过大量的单层网络节点，实现对训练样本的高度拟合性。它的缺点是泛化能力很差。deep模型：一种深层模型。它通过多层的非线性变化，使模型具有很好的泛化性。它的缺点是拟合度欠缺。将二者结合起来——用联合训练方法共享反向传播的损失值来进行训练—可以使两个模型综合优点，得到最好的结果。No.2 wide_deep模型论文为什么Adam被广泛使用？光会用可不行，还得把原理看懂。这样出去喷一喷，才会显得更有面子。Adam的细节请参阅论文《Adam: A Method for Stochastic Optimization》，该论文的链接网址是：No.3 Targeted Dropout模型论文你还在用普通的Dropout吗？我已经开始用Targeted Dropout了。比你的又快，又好。你不知道吧，赶紧学习一下。Targeted Dropout不再像原有的Dropout那样按照设定的比例随机丢弃部分节点，而是对现有的神经元进行排序，按照神经元的权重重要性来丢弃节点。这种方式比随机丢弃的方式更智能，效果更好。更多理论见以下论文：No.4 Xception模型论文在那个图像分类的时代，谷歌的Xception系列，像x战警一样，一个一个的打破记录。其中的技术也逐渐成为AI发展的知识体系。有必要看一下。或许会对自己的工作有所启发。详细情况请查看原论文《Xception: Deep Learning with Depthwise Separable Convolutions》，该论文网址是：No.5 残差结构论文运气好到没朋友，现有模型，后完善理论指的就是残差结构这哥们。他的传奇导致即使到今天的AI技术，也无法将它割舍，常微分方程都得拿它比肩。快来学学吧。用处大着呢。好多模型都拿他当先锋。利用残差结构，可以使得网络达到上百层的深度。详情请参阅原始论文《Deep ResidualLearning for Image Recognition》，该论文网址是：No.6 空洞卷积论文NasNet的招牌动作，虽然不是出于NASNet，但是却被人家用得如火纯青。有时不得不惊叹，机器设计出来的模型还真是跟人设计的不一样！想知道空洞卷积的感受为什么与层数呈指数级关系吗？细节请查看原论文《Multi-scale context aggregation by dilated convolutions》，该论文网址是：No.7 DenseNet论文这个模型使我想到了“一根筋”，再次证明了只有轴的人才能成大事！另类的模型，神奇的效果，快来体验一下吧。这可是比华佗还牛的神医哦！有关DenseNet模型的细节，请参考原始论文《Densely Connected Convolutional Networks》，该论文的连接是：No.8 EfficientNet模型论文知道目前位置图像分类界谁是老大吗？来，看看这个！EfficientNet模型的论文地址如下：No.9 Grad-CAM模型论文如果你能把神经元搞得透彻，你也会想到这个点子。不想聊太多！一个字“绝”！这TMD才叫卷积网络的可视化！详细情况请参阅论文《Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization》，该论文的链接网址是No.10 分类模型泛化能力论文知道为啥都喜欢使用ResNet模型当先锋吗？运气好就是运气好！好到大家都喜欢用它，还说不出为啥它那么好！反正就是好，不信的话看看这篇论文的实验结果。论文中，在选取模型的建议中，多次提到了ResNet模型。原因是，ResNet模型在Imgnet数据集上输出的特征向量所表现的泛化能力是最强的。具体可以参考以下论文：No.11 批量正则化论文这个没的说，必修课，不懂的话，会被鄙视成渣渣！论文《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》，该论文网址是：No.12 实例归一化论文时代不同了，批量归一化也升级了，赶紧学学新的归一化吧。在对抗神经网络模型、风格转换这类生成式任务中，常用实例归一化取代批量归一化。因为，生成式任务的本质是——将生成样本的特征分布与目标样本的特征分布进行匹配。生成式任务中的每个样本都有独立的风格，不应该与批次中其他的样本产生太多联系。所以，实例归一化适用于解决这种基于个体的样本分布问题。详细说明见以下链接：No.13 ReNorm算法论文ReNorm算法与BatchNorm算法一样，注重对全局数据的归一化，即对输入数据的形状中的N维度、H维度、W维度做归一化处理。不同的是，ReNorm算法在BatchNorm算法上做了一些改进，使得模型在小批次场景中也有良好的效果。具体论文见以下链接：No.14 GroupNorm算法论文GroupNorm算法是介于LayerNorm算法和InstanceNorm算法之间的算法。它首先将通道分为许多组（group），再对每一组做归一化处理。GroupNorm算法与ReNorm算法的作用类似，都是为了解决BatchNorm算法对批次大小的依赖。具体论文见下方链接：No.15 SwitchableNorm算法论文我们国人做产品都喜欢这么干！all in one ,好吧。既然那么多批量归一化的方法。来，来，来，我们来个all in one吧。不服来辩，我这啥都有！SwitchableNorm算法是将BN算法、LN算法、IN算法结合起来使用，并为每个算法都赋予权重，让网络自己去学习归一化层应该使用什么方法。具体论文见下方链接：No.16 大道至简的注意力论文把AI搞成玄学也就算了！居然还扯到道家了！谷歌的工程师真是中外通吃啊！搞出来了一个只用注意力就能做事的模型，连卷积都不要了！你说好玩不好玩！至简不至简！刺激不刺激！大名鼎鼎的Attention is All You Need 注意力机制论文注意力机制因2017年谷歌的一篇论文Attention is All You Need而名声大噪。下面就来介绍该技术的具体内容。如果想了解更多，还可以参考原论文，具体地址如下：No.17-18 孪生注意力论文好比LSTM与GRU一样，注意力他们家也除了一对双胞胎，长得略微有点不同。但是功能一样，都能吃能喝，还能注意。老虎老鼠傻傻的不清楚！BahdanauAttention：LuongAttention：No.19 各自升级的孪生注意力论文话说这对双胞胎，出生后就分开了。各自学的不同的语言，一个学习汉语，一个学习中文。若干年后，见面，发现二者的能力还是一样！BahdanauAttention注意力升级成了normed_BahdanauAttention，而LuongAttention注意力升级成了scaled_LuongAttention。都一样的效果，你爱用哪个用哪个吧！例如：在BahdanauAttention类中有一个权重归一化的版本（normed_BahdanauAttention），它可以加快随机梯度下降的收敛速度。在使用时，将初始化函数中的参数normalize设为True即可。具体可以参考以下论文：No.20 单调注意力机制论文老公主动表忠心，我以后不看别的美女。老婆觉得不够，再加个限制：你以后不准看别的女人！于是单调注意力就出来了。单调注意力机制（monotonic attention），是在原有注意力机制上添加了一个单调约束。该单调约束的内容为：（1）假设在生成输出序列过程中，模型是以从左到右的方式处理输入序列的。（2）当某个输入序列所对应的输出受到关注时，在该输入序列之前出现的其他输入将不能在后面的输出中被关注。即已经被关注过的输入序列，其前面的序列中不再被关注。更多描述可以参考以下论文：No.21 混合注意力机制论文这个注意力很强大，比一般的注意力专注的地方更多，信息更丰富。我已经注意你很久了！呵呵呵~~~因为混合注意力中含有位置信息，所以它可以在输入序列中选择下一个编码的位置。这样的机制更适用于输出序列大于输入序列的Seq2Seq任务，例如语音合成任务。具体可以参考以下论文：No.22 胶囊网络与动态路由的论文这是一股为图像分类降温的寒风，深刻而又尖锐的点出了卷积网络的硬伤！从事最大池化再无翻身之日。虽然胶囊网络在实际应用中，不像它的理论那么牛，但是对AI的帮助，卷积的理解是革命性的。非常值得一读。另外，这也是一篇绝对让你对数学彻底绝望的论文。花几根白头发把里面的算法啃下来吧。这样你与大神就能更近一步。胶囊网络分为主胶囊与数字胶囊，主胶囊与数字胶囊之间的耦合系数是通过训练得来的。在训练过程中，耦合系数的更新不是通过反向梯度传播实现的，而是采用动态路由选择算法完成的。该算法来自以下论文链接：目前胶囊网络的研究还处于初级阶段，随着人们研究的深入，相信这些问题会得到解决。No.23 矩阵胶囊网络与EM路由算法如果你觉得不过瘾，那么还可以再看一篇。继续自虐一下。带有EM（期望最大化）路由的矩阵胶囊网络是动态路由胶囊网络的一个改进版本。论文链接如下：No.24 胶囊网络的其它用处胶囊网络全身是宝，但就是自己不争气。这也说明还有上升的空间。就拿其中一个动态路由算法来讲，居然比普通的注意力还好。看完之后，相信你一定会手痒！要不要也试试？把你的注意力换一下。值得你尝试，会有彩蛋的！该论文的实践也证明，与原有的注意力机制相比，动态路由算法确实在精度上有所提升。具体介绍可见以下论文：No.25 卷积网络新玩法TextCNN模型早先小编在一个项目中，自己用卷积网络处理字符数据。自己感觉很Happy。没想到，无意间居然发现了一篇同样这么干的论文。居然还有个名字，叫TextCNN。哎！可惜啊！小编文化少，只会写代码，不会写论文。TextCNN模型是利用卷积神经网络对文本进行分类的算法，由 Yoon Kim 在 Convolutional Neural Networks for Sentence Classification 一文中提出。论文地址：No.26 FPN模型论文（包含了ROIAlign的匹配算法）要是搞计算机视觉，还是要建议看一下。非常的基础。也是图像分割方面的用得最多得模型。FPN的原理是：将骨干网络最终特征层和中间特征层的多个尺度的特征以类似金字塔的形式融合在一起。最终的特征可以兼顾两个特点——指向收敛目标的特征准确、特征语义信息丰富。更多信息可以参考论文：ROIAlign层中的匹配算法也来自于这篇FPN论文，链接如下：No.27 Mask R-CNN模型论文效果好，代码多！硬货！来啃吧！Mask R-CNN模型是一个简单、灵活、通用的对象实例分割框架。它能够有效地检测图像中的对象，并为每个实例生成高质量的分割掩码，还可以通过增加不同的分支完成不同的任务。它可以完成目标分类、目标检测、语义分割、实例分割、人体姿势识别等多种任务。具体细节可以参考以下论文：No28 YOLO V3模型论文这个模型的特点就是快！目标识别强烈推荐YOLO V3模型的更多信息可以参考以下链接中的论文：No.29 Anchor-Fress模型--FCOS模型论文随着AI技术的进步Anchor-Fress模型死灰复燃（早先是YOLO V1那一批模型），这次不一样的是彻底干掉带Anchor的模型。训练起来那就一个爽！妈妈再也不用为我准备单独的Anchor标签了。与YOLO V1相比， FCOS模型的思想与YOLO V1模型非常相似，唯一不同的是FCOS模型没有像YOLOv1那样只考虑中心附近的点，而是利用了ground truth边框中所有的点来进行预测边框。并且通过 center-ness 分支来抑制那些效果不行的检测边框。这样FCOS 就可以改善YOLO V1模型总会漏掉部分检测边框的缺点。相关论文地址：No.30 Anchor-Fress模型--CornerNet-Lite模型论文一样也是Anchor-Fress模型，与FCOS效果差不多少，具体看一下论文吧CornerNet-Lite模型。相关论文地址：No.31 栈式沙漏网络模型--Hourglass论文最初用户人的姿态估计，在符合模型中也是常被使用的模型。论文地址：No.32 OCR必修课——STN模型论文可以让模型自动仿射变化，你说牛不牛！要学OCR，就得从这个开始。有关STN模型的论文链接如下：No.33 QRNN模型论文在RNN模型的cell里，如果还只知道LSTM和GRU。那就太low了。快了补补吧：如果想更多了解QRNN，可以参考以下论文：No.34 SRU模型论文接着来，各种RNN的Cell。又漂亮，又好吃！SRU单元在本质上与QRNN单元很像。从网络构建上看，SRU单元有点像QRNN单元中的一个特例，但是又比QRNN单元多了一个直连的设计。若需要研究SRU单元更深层面的理论，可以参考如下论文：No.35 IndRNN模型论文再补一个，这可都是好cell啊！将IndRNN单元配合ReLu等非饱和激活函数一起使用，会使模型表现出更好的鲁棒性。有关IndRNN单元的更多理论，可以参考论文：No.36 IndRNN模型论文最后，再来一个cell，如想要了解更多关于JANET单元的内容，可以参考以下论文：No.37-38 Tacotron与Tacotron-2模型论文AI合成部分的经典模型，以上结构来自Tacotron与Tacotron-2两个结构，更多内容可以参考以下两篇论文：No.39 DeblurGAN模型论文图片合成的论文太多了。这里简单列几个，大体原理和思路了解，即可。DeblurGAN模型是一个对抗神经网络模型，由生成器模型和判别器模型组成。生成器模型，根据输入的模糊图片模拟生成清晰的图片。判别器模型，用在训练过程中，帮助生成器模型达到更好的效果。具体可以参考论文：No.40 AttGAN模型论文同样，这也是个图片合成的。不同的是多属性合成，相对比较有意思。AttGAN模型由两个子模型组成：（1）利用编码器模型将图片特征提取出来。（2）将提取的特征与指定的属性值参数一起输入编码器模型中，合成出最终的人脸图片。更多细节可以参考论文：No.41 RNN.WGAN模型论文可以合成文本的GAN。离散数据也能干！RNN.WGAN模型使用了WGAN模型的方法进行训练。详细做法可以参考如下论文：No.42 MKR模型论文多任务学习模型有必要了解一下。这里推荐一个论文给你看看。MKR是一个多任务学习的端到端框架。该框架能够将两个不同任务的底层特征抽取出来，并融合在一起实现联合训练，从而达到最优的结果。有关MKR的更多介绍可以参考以下链接：No.43 BERT模型论文如果你搞NLP，那么这个就不用我来介绍了。如果你准备搞NLP,那么赶紧来看看这个，跟上时代。BERT相关论文链接：在BERT之后，又出了好多优秀的模型。但是，还是先把这个啃下来，再看别的才不费劲。No.44 FGSM模型论文攻击模型的经典方法。值得掌握。FGSM（Fast Gradient Sign Method）是一种生成对抗样本的方法。该方法的描述如下：（1）将输入图片当作训练的参数，使其在训练过程中可以被调整。（2）在训练时，通过损失函数诱导模型对图片生成错误的分类。（3）当多次迭代导致模型收敛后，训练出来的图片就是所要得到的对抗样本。具体可以参考论文：No.45 黑箱攻击论文基于雅可比（Jacobian）矩阵的数据增强方法，是一种常用的黑箱攻击方法。该方法可以快速构建出近似于被攻击模型的决策边界，从而使用最少量的输入样本。即：构建出代替模型，并进行后续的攻击操作。详细请见如下链接：这里只是列了一些基础的论文。如果这45篇论文看完。可以保证你再看到大厂的产品时，不会感觉有代沟。啃论文是很费时间的事情。还好小编这里为大家准备了一本书——《深度学习之TensorFlow工程化项目实战》，在书里已经将大部分的论文转化成中文描述，并配合实际的代码案例了。因为知识比较多，所以也很厚！（点击封面了解本书详情）本书主要由李金洪编写，出自代码医生工作室。同时该工作室也为读者提供免费的技术问答。只要你肯学！有问必有答！推荐理由：涵盖数值、语音、语义、图像等多个领域96个有较高应用价值的深度学习案例赠送12套同步配套教学视频 + 113套源代码文件（带配套样本）内容全面，应用性强，从有监督学习到半监督学习，从简单的数据分类到语音、语言和图像分类乃至样本生成等一系列前沿技术，均有介绍语言通俗易懂，拒绝生僻的术语，拒绝生僻的公式符号，落地性强穿插了几十个需要读者注意的知识要点和难点，分享作者积累的宝贵经验本书特点：书中案例：书中一共75个案例如果想快速掌握这些，读书！确实是一个捷径！努力吧！热血青年。具体资源详见：【腾讯文档】人工智能:人工智能必看的45篇论文"
151382652,1dfff2b7acb5b0f9d4df1a8f9345dac7,沐若清辰,https://api.zhihu.com/people/1dfff2b7acb5b0f9d4df1a8f9345dac7,people,,人工智能对生活有哪些方面影响？,article,https://api.zhihu.com/articles/151382652,随着社会发展的进步，人类与<em>人工智能</em>技术已经成为了当下时代的主要趋势，<em>人工智能</em>是时代进步的产物，人工智能发展让我们的生活越来越便利。未来智能机器人，智能穿戴，智能家居等，人工智能将运用在社会生活的各个方面,89,25,140,1593338730,1593338730,"人工智能以不可逆转的迅猛之势进入到我们的生活之中，人们时刻感受着AI的高效、便捷带来的便利。人们感受到了计算机和人工智能技术的影响。计算机技术不再只属于实验室中的一小群研究人员 。个人电脑和众多技术杂志使计算机技术展现在人们面前。有了像美国人工智能协会这样的基金会。因为AI开发 的需要，还出现了一阵研究人员进入私人公司的热潮。150多所像DEC这样的公司共花了10亿美元在内部的AI开发组上。 80年代AI被引入了市场并显示出实用价值。AI将是通向21世纪之匙。 人工智能技术接受检验在""沙漠风暴""行动中军方的智能设备经受了战争的检验。人工智能技术被用于导弹系统和预警显示以 及其它先进武器。AI技术也进入了家庭。智能电脑的增加吸引了公众兴趣；一些面向苹果机和IBM兼容机的应用 软件例如语音和文字识别已可买到；使用模糊逻辑，AI技术简化了摄像设备。对人工智能相关技术更大的需求促 使新的进步不断出现。人工智能已经并且将继续不可避免地改变我们的生活 人工智能对生活的影响具体体现在哪些方面？ 1、人工智能家居 智能家居系统为普通消费者提供人性化、主动管家式的服务系统。人工智能家居是现在生活的一种潮流，人工智能家居中，利用了很先进的技术，设计出符合我们的个性需求的家居，给我们的生活带来了很大的便利。 2、人工智能与医学 人工智能与医学各个学科和各个专业广泛渗透，相互交叉和跨界融合，极大的推动了基础医学、临床医学、公共卫生等各个学科和专业的科学研究、应用实践和各类创新。 3、智能交通系统 智能交通系统是一种先进的运输管理模式。人工系统主要利用计算机仿真技术，通过监测人们出行的行为计算交通流。比如，人工系统可以模拟交通事故或恶劣天气，以此观测紧急情况造成的道路拥堵情况和对其他路段的影响。 4、智能个人助理 以Siri为首的“个人助理时代”大幕正在拉开，最终很可能会成为人们与移动设备、计算机、汽车、可穿戴设备、家用电器或其他要求复杂人机交互技术的主要交互方式。 随着社会发展的进步，人类与人工智能技术已经成为了当下时代的主要趋势，人工智能是时代进步的产物，人工智能发展让我们的生活越来越便利。未来智能机器人，智能穿戴，智能家居等，人工智能将运用在社会生活的各个方面。"
96602513,b3192a0aefd1316ef1ef33522522054c,<em>AI</em>电话外呼系统-林生,https://api.zhihu.com/people/b3192a0aefd1316ef1ef33522522054c,people,,人工智能的特点在哪里？,article,https://api.zhihu.com/articles/96602513,五、是从拟人化的机器人转向更加广阔的智能自主系统，比如智能工厂、智能无人机系统等。 国际普遍认为<em>人工智能</em>有三类“弱人工智能、强人工智能还有超级<em>人工智能</em>”。 弱人工智能就是利用现有智能化技术,7,0,13,1575968414,1575968414,人工智能的特点一、是从人工知识表达到大数据驱动的知识学习技术。二、是从分类型处理的多媒体数据转向跨媒体的认知、学习、推理，这里讲的“媒体”不是新闻媒体，而是界面或者环境。三、是从追求智能机器到高水平的人机、脑机相互协同和融合。四、是从聚焦个体智能到基于互联网和大数据的群体智能，它可以把很多人的智能集聚融合起来变成群体智能。五、是从拟人化的机器人转向更加广阔的智能自主系统，比如智能工厂、智能无人机系统等。 国际普遍认为人工智能有三类“弱人工智能、强人工智能还有超级人工智能”。 弱人工智能就是利用现有智能化技术，来改善我们经济社会发展所需要的一些技术条件和发展功能。强人工智能阶段非常接近于人的智能，这需要脑科学的突破，国际上普遍认为这个阶段要到2050年前后才能实现。 超级人工智能是脑科学和类脑智能有极大发展后，人工智能就成为一个超强的智能系统。从技术发展看，从脑科学突破角度发展人工智能，现在还有局限性。《规划》中的新一代人工智能，是建立在大数据基础上的，受脑科学启发的类脑智能机理综合起来的理论、技术、方法形成的智能系统。
536818157,c02f939b2d2c5f263608c22c8aac81eb,Linux内核无敌,https://api.zhihu.com/people/c02f939b2d2c5f263608c22c8aac81eb,people,关注Up主，天天更新Linux内核技术！,AI（人工智能）10个常用经典算法,article,https://api.zhihu.com/articles/536818157,其中垃圾邮件过滤就是依靠<em>人工智能</em>； 比如每个智能手机都配备的指纹识别或人脸识别，也是用<em>人工智能</em>技术实现的； 比如疫情期间大规模使用的无人体温检测仪，同样也使用了<em>人工智能</em>； 但对很多人来讲,42,1,171,1656768282,1706945747,人工智能是什么？很多人都知道，但大多又都说不清楚。事实上，人工智能已经存在于我们生活中很久了。比如我们常常用到的邮箱，其中垃圾邮件过滤就是依靠人工智能；比如每个智能手机都配备的指纹识别或人脸识别，也是用人工智能技术实现的；比如疫情期间大规模使用的无人体温检测仪，同样也使用了人工智能；但对很多人来讲，人工智能还是一个较为“高深”的技术，然而再高深的技术，也是从基础原理开始的。人工智能领域中就流传着10大算法，它们的原理浅显，很早就被发现、应用，甚至你在中学时就学过，在生活中也都极为常见。本文用最简单的语言来介绍目前最流行的10种人工智能的算法，让对人工智能感兴趣，或想要入门的同学，能有更为直观的了解。一、线性回归线性回归（Linear Regression）可能是最流行的机器学习算法。线性回归就是要找一条直线，并且让这条直线尽可能地拟合散点图中的数据点。它试图通过将直线方程与该数据拟合来表示自变量（x 值）和数值结果（y 值）。然后就可以用这条线来预测未来的值！这种算法最常用的技术是最小二乘法（Least of squares）。这个方法计算出最佳拟合线，以使得与直线上每个数据点的垂直距离最小。总距离是所有数据点的垂直距离（绿线）的平方和。其思想是通过最小化这个平方误差或距离来拟合模型。例如，简单线性回归，它有一个自变量（x 轴）和一个因变量（y 轴）。比如预测明年的房价涨幅、下一季度新产品的销量等等。听起来并不难，不过线性回归算法的难点并不在于得出预测值，而在于如何更精确。为了那个可能十分细微的数字，多少工程师为之耗尽了青春和头发。二、逻辑回归逻辑回归（Logistic regression）与线性回归类似，但逻辑回归的结果只能有两个的值。如果说线性回归是在预测一个开放的数值，那逻辑回归更像是做一道是或不是的判断题。逻辑函数中Y值的范围从 0 到 1，是一个概率值。逻辑函数通常呈S 型，曲线把图表分成两块区域，因此适合用于分类任务。比如上面的逻辑回归曲线图，显示了通过考试的概率与学习时间的关系，可以用来预测是否可以通过考试。逻辑回归经常被电商或者外卖平台用来预测用户对品类的购买偏好。Linux内核源码分析、Linux C/C++后端服务器架构学习路线图全套学习资料。Linux内核源码分析、Linux C/C++后端服务器架构学习路线图全套学习资料。【文章福利】小编推荐自己的整理一些个人觉得比较好的学习书籍、录播视频，有需要的可以自己加微信(VX）：laowu1008688（免费领取，备注：v888第一时间通过）。小编整理好【Linux内核源码解读】学习路线图，推荐给大家参考（主要是帮助在职嵌入开发者、驱动应用开发者、服务器后端开发者等，想掌握系统底层原理的在职工程师），付费知识具体如下：一：【进程管理模块源码解读】二：【内存管理模块源码解读】三：【设备驱动模块基础解读】四：【文件系统模块源码解读】五：【网络协议栈模块源码解读】六：【中断管理模块源码解读】Linux C/C++后端服务器架构学习路线图如下：一、精进基石专栏二、高性能网络设计专栏三、基础组件设计专栏四、中间件开发专栏五、企业级上线项目实战专栏七：Linux C/C++后端服务器架构全套资料（2000GB），有需要的可以自己加微信(VX）：laowu1008688（免费领取，备注：v888第一时间通过）。三、决策树如果说线性和逻辑回归都是把任务在一个回合内结束，那么决策树（Decision Trees）就是一个多步走的动作，它同样用于回归和分类任务中，不过场景通常更复杂且具体。举个简单例子，老师面对一个班级的学生，哪些是好学生？如果简单判断考试90分就算好学生好像太粗暴了，不能唯分数论。那面对成绩不到90分的学生，我们可以从作业、出勤、提问等几个方面分开讨论。以上就是一个决策树的图例，其中每一个有分叉的圈称为节点。在每个节点上，我们根据可用的特征询问有关数据的问题。左右分支代表可能的答案。最终节点（即叶节点）对应于一个预测值。每个特征的重要性是通过自顶向下方法确定的。节点越高，其属性就越重要。比如在上面例子中的老师就认为出勤率比做作业重要，所以出勤率的节点就更高，当然分数的节点更高。四、朴素贝叶斯朴素贝叶斯（Naive Bayes）是基于贝叶斯定理，即两个条件关系之间。它测量每个类的概率，每个类的条件概率给出 x 的值。这个算法用于分类问题，得到一个二进制“是 / 非”的结果。看看下面的方程式。朴素贝叶斯分类器是一种流行的统计技术，经典应用是过滤垃圾邮件。用非术语解释贝叶斯定理，就是通过A条件下发生B的概率，去得出B条件下发生A的概率。比如说，小猫喜欢你，有a%可能性在你面前翻肚皮，请问小猫在你面前翻肚皮，有多少概率喜欢你？五、支持向量机支持向量机（Support Vector Machine，SVM）是一种用于分类问题的监督算法。支持向量机试图在数据点之间绘制两条线，它们之间的边距最大。为此，我们将数据项绘制为 n 维空间中的点，其中，n 是输入特征的数量。在此基础上，支持向量机找到一个最优边界，称为超平面（Hyperplane），它通过类标签将可能的输出进行最佳分离。超平面与最近的类点之间的距离称为边距。最优超平面具有最大的边界，可以对点进行分类，从而使最近的数据点与这两个类之间的距离最大化。所以支持向量机想要解决的问题也就是如何把一堆数据做出区隔，它的主要应用场景有字符识别、面部识别、文本分类等各种识别。六、K- 最近邻算法（KNN）K- 最近邻算法（K-Nearest Neighbors，KNN）非常简单。KNN 通过在整个训练集中搜索 K 个最相似的实例，即 K 个邻居，并为所有这些 K 个实例分配一个公共输出变量，来对对象进行分类。K 的选择很关键：较小的值可能会得到大量的噪声和不准确的结果，而较大的值是不可行的。它最常用于分类，但也适用于回归问题。用于评估实例之间相似性的距离可以是欧几里得距离（Euclidean distance）、曼哈顿距离（Manhattan distance）或明氏距离（Minkowski distance）。欧几里得距离是两点之间的普通直线距离。它实际上是点坐标之差平方和的平方根。KNN分类示例KNN理论简单，容易实现，可用于文本分类、模式识别、聚类分析等。七、K-均值K-均值（K-means）是通过对数据集进行分类来聚类的。例如，这个算法可用于根据购买历史将用户分组。它在数据集中找到 K 个聚类。K- 均值用于无监督学习，因此，我们只需使用训练数据 X，以及我们想要识别的聚类数量 K。该算法根据每个数据点的特征，将每个数据点迭代地分配给 K 个组中的一个组。它为每个 K- 聚类（称为质心）选择 K 个点。基于相似度，将新的数据点添加到具有最近质心的聚类中。这个过程一直持续到质心停止变化为止。生活中，K-均值在欺诈检测中扮演了重要角色，在汽车、医疗保险和保险欺诈检测领域中广泛应用。八、随机森林随机森林（Random Forest）是一种非常流行的集成机器学习算法。这个算法的基本思想是，许多人的意见要比个人的意见更准确。在随机森林中，我们使用决策树集成（参见决策树）。（a）在训练过程中，每个决策树都是基于训练集的引导样本来构建的。（b）在分类过程中，输入实例的决定是根据多数投票做出的。随机森林拥有广泛的应用前景，从市场营销到医疗保健保险，既可以用来做市场营销模拟的建模，统计客户来源、保留及流失，也可以用来预测疾病的风险和病患者的易感性。九、降维由于我们今天能够捕获的数据量之大，机器学习问题变得更加复杂。这就意味着训练极其缓慢，而且很难找到一个好的解决方案。这一问题，通常被称为“维数灾难”（Curse of dimensionality）。降维（Dimensionality reduction）试图在不丢失最重要信息的情况下，通过将特定的特征组合成更高层次的特征来解决这个问题。主成分分析（Principal Component Analysis，PCA）是最流行的降维技术。主成分分析通过将数据集压缩到低维线或超平面 / 子空间来降低数据集的维数。这尽可能地保留了原始数据的显著特征。可以通过将所有数据点近似到一条直线来实现降维的示例。十、人工神经网络（ANN）人工神经网络（Artificial Neural Networks，ANN）可以处理大型复杂的机器学习任务。神经网络本质上是一组带有权值的边和节点组成的相互连接的层，称为神经元。在输入层和输出层之间，我们可以插入多个隐藏层。人工神经网络使用了两个隐藏层。除此之外，还需要处理深度学习。人工神经网络的工作原理与大脑的结构类似。一组神经元被赋予一个随机权重，以确定神经元如何处理输入数据。通过对输入数据训练神经网络来学习输入和输出之间的关系。在训练阶段，系统可以访问正确的答案。如果网络不能准确识别输入，系统就会调整权重。经过充分的训练后，它将始终如一地识别出正确的模式。每个圆形节点表示一个人工神经元，箭头表示从一个人工神经元的输出到另一个人工神经元的输入的连接。图像识别，就是神经网络中的一个著名应用。
479374242,2a9c95c4b3096cba586584844e149340,人民邮电出版社,https://api.zhihu.com/people/2a9c95c4b3096cba586584844e149340,people,硬核科技与科普知识服务提供者,清华大学推荐的人工智能书单,article,https://api.zhihu.com/articles/479374242,受到了俞勇、王斌、李沐、程明明、陈光(博主@爱可-爱生活)、徐亦达6位<em>人工智能</em>领域大咖亲笔推荐！ 03《人工智能算法 卷1 基础算法》 人邮君推荐：算法是<em>人工智能</em>技术的核心。如果你想要了解和应用<em>人工智能</em>相关算法，那么千万别错过由专注于数据科学和人工智能的计算机科学家Jeffery,268,7,2077,1646985166,1657176477,"上周，清华大学发布了“清华新学期推荐的人工智能书单”，人邮君发现，人民邮电出版社异步社区的2本重磅作品也进入了推荐书单。一本是“花书”《深度学习》，另一本是“南瓜书”《机器学习公式详解》。这两本都是人工智能领域四有口皆碑的两本佳作。正如清华大学这份书单开头所言：在春暖花开的三月，坐在阳光倾洒的书桌前，捧一卷爱书，获一份新知，感受智慧的凝结，体味文字的美妙，定格心流瞬间。不过，人邮君家能让你“获一份新知，感受智慧的凝结”的书可不止这两本，就人工智能领域，人邮君带来一份AI大牛智慧结晶的高口碑人工智能书单。这些都是那些书籍呢？一起来看看吧~零基础·入门01《Easy RL：强化学习教程》人邮君推荐：源自3门百万播放量的经典公开课！GitHub Star 3.3k+！多位强化学习领域大咖亲笔推荐！这本《Easy RL：强化学习教程》的主要内容源自B站3门累计播放量破百万的强化学习课程：李宏毅“深度强化学习”、周博磊“强化学习纲要”、李科浇“世界冠军带你从零实践强化学习”，由来自中科院、清华、北大的Datawhale成员合著而成。此外，这本书还结合编著者自身学习体验中的难点和重点加以强调、阐释和引申，让其他学习者“学得快、少踩坑”!Easy RL，让你像采蘑菇一样轻松入门强化学习！02《深度学习》人邮君推荐：2018年图灵奖得获奖者Yoshua Bengio创作了一本深度学习领域奠基性图书——《深度学习》，昵称“花书”。“花书”长期位居美亚AI和机器学习类图书榜，涵盖了深度学习的基础与应用、理论与实践等各个方面的主要技术。书籍的编排方式非常适合自学，划分为“应用数学与机器学习基础”“深度网络: 现代实践”“深度学习研究” 三大板块，提供了完整的学习路线。03《动手学深度学习》人邮君推荐：《动手深度学习》远不只是一本书，全面概述深度学习背后的数学原理；更是一个编程工作台与记事本，提供了详细算法代码，能让读者一边动手学习，一边进行交流，同时获得反馈。加州大学伯克利分校、北京大学、清华大学等全球140余所高校将这本书用于教学，实现原理与实战完美结合的全新教学模式。如果你想钻研深度学习，请研读这本书！04《零基础学习机器学习》人邮君推荐：很难想象，作者竟然能把入门难度高的机器学习讲解得如此有趣，他在输出技术知识点的同时插播了轻松笑点，不仅有梗还有料。这本《零基础学机器学习》以AI菜鸟“小冰”拜师程序员“咖哥”为背景，精心设计了一条贴合零基础读者的入门路线。故事+对话+插图，让读者轻松到达机器学习的殿堂。05《PyTorch深度学习实战》人邮君推荐：虽然很多深度学习工具都使用Python，但PyTorch 库是真正具备Python 风格的。对于任何了解NumPy 和scikit-learn 等工具的人来说，上手PyTorch 轻而易举。跟我一起说：PyTorch，YYDS！数学·算法01《机器学习的数学》人邮君推荐：想要透彻地理解机器学习算法，就需要从数学层面搞懂这些核心算法原理的逻辑，培养推导和证明算法的能力。机器学习资深专家、SIGAI创始人雷明全新创作的这门“数学公开课”《机器学习的数学》，就能帮助你吃透机器学习。这本书从机器学习应用的角度讲授数学，精准覆盖人工智能领域中机器学习、深度学习、强化学习相关的数学知识，让数学在机器学习中的应用无缝衔接。02《机器学习公式详解》人邮君推荐：以前学习机器学习的时候，你可能只知道有“西瓜书”，但如今，我们还拥有了“南瓜书”！这是一本小白也能畅读“西瓜书”《机器学习》的好“伴侣”，能帮你一站式解决机器学习中的数学难题。“南瓜书”《机器学习公式详解》是Datawhale的开源协作学习笔记，在发布之初即登GitHub Trending第2名，受到了俞勇、王斌、李沐、程明明、陈光(博主@爱可-爱生活)、徐亦达6位人工智能领域大咖亲笔推荐！03《人工智能算法 卷1 基础算法》人邮君推荐：算法是人工智能技术的核心。如果你想要了解和应用人工智能相关算法，那么千万别错过由专注于数据科学和人工智能的计算机科学家Jeffery Heaton博士创作的人工智能算法系列图书。《人工智能算法 卷1 基础算法》介绍了人工智能的基础算法，涉及维度法、距离度量算法、K 均值聚类算法、误差计算、 爬山算法、模拟退火算法、Nelder-Mead 算法和线性回归算法等。04《人工智能算法 卷2 受大自然启发的算法》人邮君推荐：大自然为人类的发明创造提供了源源不断的灵感。这本书介绍了基于基因、鸟类、蚂蚁、细胞和树的算法，这些算法可用于查找最佳路径、识别模式、查找数据背后的公式，甚至模拟简单的生命等虽然算法的灵感来源是大自然，但读者不必具备生物学知识也能读懂本书。05《人工智能算法 卷3 深度学习和神经网络》人邮君推荐：自人工智能领域兴起，深度学习和神经网络就一直是人工智能的支柱。现在，令人兴奋的新技术（例如深度学习和卷积）正在将神经网络带入一个全新的方向。在这本书中，作者将演示各种现实世界任务中的神经网络，例如图像识别和数据科学。我们研究了当前的神经网络技术，包括ReLU 激活、随机梯度下降、交叉熵、正则化、Dropout 及可视化等。06《Python神经网络编程》人邮君推荐：如果只读一本神经网络入门图书，请选择这本基于Python3.5对神经网络精彩解读的美亚五星畅销书《Python神经网络编程》。在这本书中，作者用轻松的笔触，一步一步揭示了神经网络的数学思想，并介绍如何使用Python编程语言开发神经网络。通过这本书，你将学习使用Python开发自己的神经网络，训练它识别手写数字，甚至可以与专业的神经网络相媲美！07《PyTorch生成对抗网络编程》人邮君推荐：机器学习领域近20年来最酷的想法生成对抗网络（GAN），是神经网络领域的新星，吸引了无数人学习。这本美亚全五星好评的《PyTorch生成对抗网络编程》适合想初步了解GAN以及其工作原理的读者，也适合想要学习如何构建GAN的机器学习从业人员。本书以直白、简短的方式向读者介绍了生成对抗网络，并且教读者如何使用PyTorch按部就班地编写生成对抗网络。08《GAN实战》人邮君推荐：《GAN实战》由英国创企孵化器Founders Factory计算机视觉领域的联合创始人Jakub Langr与美国纽约一家初创公司的高级产品经理Vladimir Bok合力写就。这本书主要介绍构建和训练生成对抗网络（GAN）的方法。书中给出了大量的示例，教读者学习针对不同的场景训练不同的GAN，进而完成生成高分辨率图像、实现图像到图像的转换、生成对抗样本以及目标数据等任务，让所构建的系统变得智能、有效和快速！应用·实战01《百面机器学习》人邮君推荐：“不积跬步，无以至千里”。这本书从特征工程、模型评估、降维等经典机器学习领域出发，构建一个算法工程师必备的知识体系；还收集了在近年算法工程师的笔试、面试中出现过的100多道机器学习的题目；讲述了很多算法背后的小故事，增加了读者对问题的理解。希望你能通过这本书为你铺设的道路，快速通往目标！02《百面深度学习》人邮君推荐：这是一本人工智能时代程序员不可不读深度学习面试宝典！作为《百面机器学习：算法工程师带你去面试》的姐妹篇，这本书秉承了作者写作的一贯风格：技术上有深度，深入浅出讲得透彻；实践上有温度，言传身教讲得到位。这本书通过知识点问答为读者层层揭开深度学习的神秘面纱，其一大亮点是囊括了一系列前沿领域的新进展。如果你想抢在别人前面掌握它们，千万不要错过这本书。03《自然语言处理实战》人邮君推荐：有这样一本书，它不仅是Python开发人员入门自然语言处理的必备书籍，还是现代自然语言处理领域从业者的实用参考指南。这本书便是由小米AI实验室NLP团队献译的《自然语言处理实战 ：利用Python理解、分析和生成文本》。该书扩展了传统的自然语言处理方法，包括神经网络、现代深度学习算法和生成技术，用于解决真实世界的问题，如提取日期和名称、合成文本和回答无固定格式的问题。如今，NLP已成为深度学习的核心应用领域，这本关于深度学习的书，特别值得你去深度学习！04《OpenCV 4详解：基于Python》人邮君推荐：零基础，想学习OpenCV 4 , 但畏难情绪严重？别怕，跟着《OpenCV 4详解：基于Python》走，让哈工大博士手把手带你入门!作者冯振擅于由浅入深，层层递进，以 Python 语言为基础，添加了部分新内容，不仅可帮助使用Python 语言的开发人员快速入门 OpenCV 4，还能帮助开发人员轻松提升应用程序开发水平和图像处理水平。05《Python深度学习与项目实战》人邮君推荐：多领域结合的深度学习经典教程！世界名校教师教授、国内外大咖鼎力推荐！AI与多领域融合的实战指南！《Python深度学习与项目实战》基于Python以及两个深度学习框架Keras与TensorFlow，讲述深度学习在实际项目中的应用。并且还结合计算机视觉、自然语言处理、金融领域等方面的项目，系统讲述深度学习技术，可操作性强。不得不说，这是一本非T人士也可以轻松读懂的专业书！06《机器学习算法评估实战》人邮君推荐：在机器学习算法的实际应用中，我们不仅要知道算法的原理，也要了解如何评估算法上线服务的可靠性。这本美团高级算法工程师全新力作《机器学习算法评估实战》就能帮助你彻底解决：算法如何设计?算法如何评估?等问题。并且，这本书深入细节，对多种算法进行简洁明了的图解以及条理清晰的实例推导，让初入机器学习算法领域的读者更好容易理解各种经典模型的原理，是一本难得的佳作。文案|小罗 策划|昕岳 审核|桐希========赠人玫瑰，手留余香，不要忘记点赞、收藏、关注 @人民邮电出版社哦~一键三连，感恩有你~"
349766661,581161714a947b1c592dce706deb2dc0,华矩数诊台,https://api.zhihu.com/people/581161714a947b1c592dce706deb2dc0,people,致力于数据使用的安全、快捷、便利、智能、低成本。,“双一流”大学：人工智能专业排名,article,https://api.zhihu.com/articles/349766661,全国至少有59所“双一流”建设高校开设了<em>人工智能</em>专业。二、人工智能专业大学排名2020年4月～6月，全国高校<em>人工智能</em>与大数据创新联盟针对全国已经开办<em>人工智能</em>专业的215所普通高校进行调研,82,7,134,1612659214,1612659214,众所周知，人工智能是当下十分热门的一种专业，也被很多人认为是未来最有潜力的专业之一。目前，我国开设人工智能专业的院校还比较少，但已深受考生们的喜爱。本期，将为大家介绍2021年开设有人工智能专业的“双一流”高校，以及人工智能专业大学排名情况，可供参考。一、人工智能专业双一流高校名单1月26日，教育部公开了《关于政协十三届全国委员会第三次会议第4027号（经济发展类328号）提案答复的函》，其中提到：目前全国已有北京科技大学、北京交通大学、天津大学等215所高校设置人工智能专业，188所高校设置智能科学与技术专业。其中，目前已开设有人工智能专业的“双一流”建设高校名单如下：1、“双一流”大学（A类）共26所开设了人工智能专业。浙江大学、南京大学、北京航空航天大学、哈尔滨工业大学、西安交通大学、天津大学、电子科技大学、北京理工大学、中国人民大学、厦门大学、四川大学、重庆大学、山东大学、武汉大学、兰州大学、上海交通大学、同济大学、吉林大学、西北工业大学、华中科技大学、中国农业大学、北京师范大学、复旦大学、东南大学、大连理工大学【未开设】：清华大学、北京大学、华东师范大学、中央民族大学、国防科技大学、南开大学、中国科学技术大学、中国海洋大学、中南大学、中山大学、华南理工大学2、“双一流”大学（B类）仅1所开设了人工智能专业。东北大学【未开设】：湖南大学、郑州大学、云南大学、西北农林科技大学、新疆大学3、一流学科建设高校32所开设了人工智能专业。西安电子科技大学、北京科技大学、北京邮电大学、西南交通大学、北京交通大学、南京信息工程大学、太原理工大学、南京农业大学、上海大学、暨南大学、南京航空航天大学、华中师范大学、河北工业大学、中国矿业大学、陕西师范大学、中国石油大学（北京）、北京化工大学、武汉理工大学、西南财经大学、长安大学、苏州大学、贵州大学、安徽大学、河南大学、中国传媒大学、南昌大学、南京中医药大学、成都理工大学、天津工业大学、华南师范大学、首都师范大学、南京师范大学可见，到目前为止，全国至少有59所“双一流”建设高校开设了人工智能专业。二、人工智能专业大学排名2020年4月～6月，全国高校人工智能与大数据创新联盟针对全国已经开办人工智能专业的215所普通高校进行调研，最终将当前高校人工智能教育教学总体实力分为四类，即A类、B类、C类、D类。具体如下：1、A类——人工智能教育教学综合实力总体水平位居全国高校第一数量：55所院校档次：985、211浙江大学、南京大学、西安电子科技大学、北京航空航天大学、哈尔滨工业大学、西安交通大学、天津大学、电子科技大学、北京理工大学、中国人民大学、厦门大学、四川大学、重庆大学、山东大学、武汉大学、兰州大学、上海交通大学、同济大学、吉林大学、西北工业大学、华中科技大学、中国农业大学、北京师范大学、复旦大学、东北大学、东南大学、大连理工大学、北京科技大学、北京邮电大学、西南交通大学、北京交通大学、重庆邮电大学、南京信息工程大学、太原理工大学、南京农业大学、上海大学、暨南大学、南京航空航天大学、华中师范大学、河北工业大学、中国矿业大学、陕西师范大学、中国石油大学（北京）、上海工程技术大学、北京化工大学、武汉理工大学、西南财经大学、长安大学、苏州大学、中北大学、贵州大学、北京信息科技大学、安徽大学、河南大学、中国传媒大学。2、B类——人工智能教育教学综合实力总体水平位居全国高校第二数量：62所院校档次：211、省属重点大学南昌大学、南京中医药大学、成都理工大学、天津工业大学、江苏科技大学、安徽工程大学、华南师范大学、华侨大学、江西理工大学、首都师范大学、河北大学、上海师范大学、天津理工大学、南京师范大学、河北师范大学、北京建筑大学、曲阜师范大学、沈阳航空航天大学、上海理工大学、沈阳大学、长春大学、大连交通大学、上海海事大学、天津科技大学、内蒙古工业大学、南京工业大学、浙江财经大学、安徽工业大学、东华理工大学、福建师范大学、安徽财经大学。3、C类——人工智能教育教学综合实力总体水平位居全国高校第三数量：60所院校档次：省属普通高校郑州轻工业大学、中南民族大学、西南民族大学、沈阳建筑大学、鲁东大学、天津职业技木師范大学、青岛科技大学、宁波大学科学技术学院、上海应用技术大学、常州大学、西安工程大学、扬州大学、西安建筑科技大学、山东财经大学、江西科技师范大学、青海民族大学、西安邮电大学、南昌航空大学、西安石油大学、聊城大学。4、D类——人工智能教育教学综合实力总体水平位居全国第四数量：37所院校档次：民办普通本科院校、独立学院云南经济管理学院、上海师范大学天华学院、沈阳工程学院、黔南民族师范学院、河北外国语学院、沈阳城市学院、河北民族师范学院、长春大学旅游学院、河北东方学院、宁波工程学院、盐城工学院、南京信息工程大学滨江学院、温州大学、安徽师范大学皖江学院、浙江科技学院、马鞍山学院、江苏师范大学科文学院、巢湖学院、青岛滨海学院。详细排名：
144951953,48f56d4c7b81a96b3a86efd8f9281bd7,Frank,https://api.zhihu.com/people/48f56d4c7b81a96b3a86efd8f9281bd7,people,CS PhD Student @ NUS,人工智能专业本科生大学四年的加点路线,article,https://api.zhihu.com/articles/144951953,Eric是一名斯坦福的毕业生，同时也是一名<em>人工智能</em>的研究者。他介绍了有志于人工智能领域的本科生四年要学的课程，最棒的是，这些课程都有相应的斯坦福公开课。而国内开设<em>人工智能</em>专业的高校并不多,263,8,1371,1590940347,1590940456,这篇文章的内容来自一篇Blog，作者Mihail Eric是一名斯坦福的毕业生，同时也是一名人工智能的研究者。他介绍了有志于人工智能领域的本科生四年要学的课程，最棒的是，这些课程都有相应的斯坦福公开课。而国内开设人工智能专业的高校并不多，大部分学生都是通过网上资源自学，这篇博客为我们提供了一套很好的加点路线。A Complete 4-Year Course Plan for an Artificial Intelligence Undergraduate Degree人工智能本科生的四年课程学习计划A Complete 4-Year Course Plan for an Artificial Intelligence Undergraduate Degree第一年：打好基础 Programming Fundamentals（程序设计基础）CS 106BIntroduction to Computer Systems（计算机系统导论）CS 107Algorithms（算法）CS 161Probability Theory（概率论）CS 109Linear Algebra（线性代数）EE 103Multi-dimensional Calculus（微积分）Vector Calculus For Engineers第二年：强化计算机系统知识 Introduction to Artificial Intelligence（人工智能导论）CS 221Compilers（编译原理）CS 143Introduction to Databases（数据库导论）CS 145Parallel Computing（并行计算）CS 149Operating Systems（操作系统）CS 140第三年：深入学习人工智能 Machine Learning（机器学习）CS 229Convex Optimization（凸优化）EE 364AProbabilistic Graphical Models（概率图模型）CS 228Data Mining（数据挖掘）CS 246Natural Language Processing（自然语言处理）CS 224NConvolutional Neural Networks for Computer Vision（计算机视觉）CS 231N第四年：实践 Take project classes（完成课程项目）Get involved with research（做研究）Do an industry internship（工业界实习）
681392973,bf73e47863446e4a3a105e2d4a5722a7,漂流少年,https://api.zhihu.com/people/bf73e47863446e4a3a105e2d4a5722a7,people,感谢每个朋友的点赞收藏！祝大家都能开心、自由、赚多多～,2024年AI绘画凉了吗？为什么没人讨论MJ/SD了？,article,https://api.zhihu.com/articles/681392973,像3D盲盒、线稿上色、室内设计...现在<em>AI</em>绘画的水平都比去年要成熟太多太多。 [图片] 如果硬要说讨论的人比之前少了，那应该是看乐子的人少了。搜索热度也还在，只是大家不在放在嘴边天天聊了。 [图片] 自从<em>AI</em>绘画火起来，网上对于AI绘画的讨伐声和欢呼声不绝于耳。 连我弟这种中学生都会用AI绘画画出各种美女,130,9,0,1707037991,1707038064,"不仅没凉，还加强啦！！！尤其在 实用价值 这块，像3D盲盒、线稿上色、室内设计...现在AI绘画的水平都比去年要成熟太多太多。AI手绘建筑效果图如果硬要说讨论的人比之前少了，那应该是看乐子的人少了。搜索热度也还在，只是大家不在放在嘴边天天聊了。自从AI绘画火起来，网上对于AI绘画的讨伐声和欢呼声不绝于耳。连我弟这种中学生都会用AI绘画画出各种美女。他能玩一个星期，但不能画一年。为啥？因为不能一直给他带来价值，无论是情绪价值还是实用价值，AI画的再精致，看多了也会免疫。毕竟除了设计师这种从业者和爱好者外，普通人平时上班就够烦啦！哪有时间天天讨论这个～再加上AI绘画门槛不低，MJ需要翻qiang+会员费，SD的安装和下载就能卡住一大批人。综上，不是AI绘画凉了，而是像我弟这种看乐子的人少了。真正一直在用AI绘画的人一般是能用AI绘画创造价值那一批人。尤其是实用价值，像我们公司现在的商业海报和宣传，都丢给AI做，直接省了小2w。比如设计师可以利用AI线稿填色，摆脱重复性工作，给自己的工作提效。再比如修图师，利用SD结合PS精准修图，无中生有换背景，几分钟修N张。诸如此类还有很多，AI室内设计，AI淘宝模特，AI漫画插画......AI绘画早就已经渗透到商业生产中了，这种量大管饱、能实现产能翻倍的工具，很难凉。更可怕的是！这才仅仅一年，还是AI发展的上升期，SD已经开源，AI绘画还会不断地迭代的。其中影响最大的估计就是设计行业，有人对AI绘画口诛笔伐，大骂侵权；有人用AI绘画的低廉成本欢呼雀跃；更甚者用AI绘画搞钱，当副业留一手。其实不管是骂还是吹，都无法掩饰AI绘画的发展趋势。事实上，讨论AI绘画的好坏意义不大，因为无论风评如何，市场选择都会做出选择。比如很多初创商家不必再花钱请摄影模特，室内设计师不用画大量时间制作效果图...当然，AI不会取代设计师，因为使用他们的一大批人正是设计师、修图师、插画师等设计绘画的从业者。与其和AI竞争，不如掌控AI为己所用。如果你面对甲方的无理要求没有设计思路；不想成为画图工具人天天机械性画图；怕被AI挤压生存空间；想用AI绘画搞点小钱但不知从何下手...就一定要主动找资源去学习，搞定SD的下载和安装、MJ的咒语技巧、图生图文生图的基操、训练自己的Lora模型、MJ+SD的结合使用！！！像知乎知学堂的 AI绘画师实战营 就是一个很好的几乎免费的资源，从0到1教你玩Midjourney和Stable diffusion，包括前面提到的线稿上色、室内设计、产品设计等，两节讲清楚AI绘画的所有玩法，看完你就知道这么把你的MJ/SD工具变成生产力～不知道啥时候没，建议先预约一把子：有设计基础的友友看了可以用AI搞定重复性工作，摆脱「做图工具人」，没设计基础的，也能出图接单赚点小钱。没有AI绘画的工具也没关系，听完可以直接领取他们送的「Stable Diffusion 软件+安装教程」、「Midjourney常用咒语合集」。简单来说，从下载安装到视频教学，给你包圆儿啦！我的SD就是从这儿学的！一步一暂停，把“高不可攀”的SD讲的很清楚～SD-手绘人像当然除从业者之外，就是像我这种利用AI绘画搞钱的人了。我不是学设计的，但对AI还算感兴趣，但是对绘画一窍不通。但就是用MJ画头像和表情包，每个月能赚小4k，不多，但也不占时间，像这种皮克斯头像，MJ5分钟能画20张，技术含量不高，但是有信息差、有需求就有市场。像我这种是最基本的玩法，除此之外，还有人用AI去接漫画插画，AI绘画主打一个量大管饱,便宜实惠，新海诚、宫崎骏、国风中式、赛博朋克各种风格应有具有。AI绘画搞钱的路子太多啦！像微信表情包，小说变漫画，商业宣传图...虽然一张没几个钱，但耐不住AI量大抗造啊！就这产能，AI绘画怎么凉？"
672858059,60957051287f9a4c61cc4765b86de3ce,职场小黑,https://api.zhihu.com/people/60957051287f9a4c61cc4765b86de3ce,people,一个普普通通的社畜,普通人如何抓住AI这个风口?,article,https://api.zhihu.com/articles/672858059,其实 <em>AI</em> 真正对普通人的风口是：拥抱 <em>AI</em> 并利用 AI 工具提升数倍的工作效率。 像GPT这类大语言模型问世后，网友们热议的一个问题：<em>AI</em>时代来临，打工人都在担忧什么？ 回答中高频出现的一个词： 失业。<em>AI</em>时代下，理想尚未照进现实，裁员先行一步，蓝色巨人IBM今年5月1日宣布将用AI取代7800个岗位,175,7,0,1702884689,1705546030,其实 AI 真正对普通人的风口是：拥抱 AI 并利用 AI 工具提升数倍的工作效率。像GPT这类大语言模型问世后，网友们热议的一个问题：AI时代来临，打工人都在担忧什么？回答中高频出现的一个词：失业。AI时代下，理想尚未照进现实，裁员先行一步，蓝色巨人IBM今年5月1日宣布将用AI取代7800个岗位，并暂停招聘人工智能可以胜任的岗位。关于AI发展和工作的平衡问题，大家都在质疑与恐慌着。前段时间「互联网教父-凯文凯利」也是直播回答了这个问题，原话是这样的事实也正是如此，AI的作用是降本增效，不管什么岗位被AI取代，仍然需要会使用AI软件的人在。原来需要10个人完成的工作量，现在只需要会用AI的2-3人+AI就能完成。我们需要做的，就是成为会使用AI的那批人。我会从实际工作展示下AI办公软件的功能，也会免费分享一些常用的AI软件安装包及教程！点赞收藏领取～AI办公到底强大在哪里？普通人该怎么拥抱AI不被淘汰？许多人对AI软件的印象停留在给它一个idea，它生成一个文案或图片。如果你也这么认为，可以看看这个AI视频软件的数字人我们可以设置人物的台词、动作、表情、语种，只需要我们导入一段人物的视频。实际效果可以去抖音搜「郭德纲说英语相声」「川普说中文」，都是AI合成的，嘴形跟台词完全对上。是不是感觉没什么问题，仔细看看嘴形都是对的。这个视频完全由AI生成，我们只需要一个画面，想让里面的人说什么，它都能说，不管是哪国语言，嘴形都能对上。这就是AI的发展速度。一、AI软件如何提升办公效率1.智能文案文案不仅仅是内容工作者的烦恼，邮件、ppt、策划、代码、材料、报告、年度总结……比如我的工作会涉及到活动策划方案。我之前写一篇活动策划流程都是：确定主题-市场调研-初稿-反馈修改-终稿审核。现在直接甩给gpt就行了。如果需要某些环节更详细的内容，直接继续提问，比如我想要更详细的活动流程。在gpt策划基础上稍微改一改，就可以直接拿来用了。我之前写一篇策划需要1-2天，这样只需要不到2小时，省下来的时间我能摸多少鱼，不多说了(o^^o)如果说这里解决是效率问题，并没有凸显AI办公对我们的替代性。那么GPT还有一个强大之处就是创意性。我平时会写文案拍视频，搞搞自媒体，遇到比较大的问题就是选题枯竭，尤其是拍的比较多之后，不知道怎么创新。就尝试把自己情况描述给gpt。就能解决我的创意不足的问题。而且gpt的创意是用之不竭的，非常适合需要批量生产内容的自媒体。之前做抖音团队需要10个人才能日更，现在只需要2个会用AI的就够了，如果你是老板会怎么选？我们再倒推一下，这句话是不是适用于很多行业和岗位？所以说大家需要学会使用AI办公软件，这就是AI时代最重要的求职竞争力。2.数据处理简单说就是：我们不需要再背任何Excel公式了！只需要一个口令，就能对整个表格进行操作如果我们要核对数据，找出复购率两列数据不同的地方。直接输入指令，AI就会把结果标注出来还有自动生成可视化表格、数据分析都是基本操作，打字就能完成之前做Excel表格，有什么不会的就会上小红书搜教程，边看边学非常麻烦，后面wps和office都同步了AI功能，我也是摇身一变成了Excel高手。文字和表格算是工作中最常接触的两项技能，我也展示了一些AI和文字、表格结合的使用案例，如果这些AI办公软件对你的工作有帮助，一定要学习一下怎么使用！其实上手非常简单，我当时就听了一个直播课就开始用了，里面也有20个免费AI软件分享（文字、表格、图片、音视频、ppt）链接我放这了，缺软件、不会用的可以领一下。记得添加助教老师，才能把软件资料发给你，我记得光教学都有20多种软件。3.AI绘图先来看看AI生成的图片吧～比起AI画图刚出来已经非常真实了。很多人认为，自己又不是设计师，用不上这个。其实不然，网上很多素材是不用拿来商用的，而AI制作的图片我们是有版权的，无论是否商用都不会有侵权问题。比如我们公司宣传视频，需要一个赛博朋克风的背景，就可以直接让AI生成。二、如何提升个人AI办公能力-提示词（prompt）有些人用AI画图一个美人鱼是这样。有些人AI生成美人鱼是这样的这两张图是同一个软件生成，这就是提示词（prompt）的区别。文案也是同理，提示词使用水平能决定我们答案是否精准。尤其在办公需求下，我们需要更精准更优质的结果，所以需要给AI喂更精准的提示词，让他明白我们要什么。如果说AI软件的使用是入门，那么提示词就是区分高手和小白的地方。分享两个提高prompt的技巧。1.提示词微调每次给AI输入指令，不要再以「给我一个xxx文案」我们要具体到情景、任务、目标、关键结果、改进五个方面。提示词的设置除了我们常见的对话输入。还可以在模型设置上输入提示词。我建议大家把情景、任务、目标都输入给模型设置，每次对话只输入关键结果，然后改进。因为模型设置相当于人设，如果我们不给gpt人设，它会以第三视角进行回答，就容易泛泛而谈。其次是对话输入太多内容，容易导致上下文不符，关联性太差。这也是目前所有大语言模型的通病，一旦内容过多就会断档。2.结果拓展比如我是一名四级考生，想让gpt给我制定一个学习计划，不知道该输入哪些关键信息，就可以直接问gpt需要什么信息。提示词的训练需要非常强的专业性和工作经验，这两条是我在直播学的，分享经验的老师是前华为、腾讯的AI技术企业培训师，负责一线大厂员工的AI使用培训，这种技术专业度要求比较高的，最好还是听行业头部大厂的从业者进行分享。 我记得当时听完课是直接送到提示词使用指南几个资料。为什么我开头说大语言模型不是普通人的风口，因为不同于之前的电商、自媒体等风口，普通人也能开个淘宝店，做个自媒体账号。大语言模型更像是一次工业革命，需要大量的算力才能做，而这种投入都是以亿为单位，能出得起这个钱的绝不是普通人。但是我们回顾之前的工业革命，工厂机器已经代替了许多流水线工人，但是工厂仍然需要会使用机器的工人，所以对于普通人来说，我们必须要成为会使用AI办公软件那批「工人」，才不会被时代淘
671101379,bf73e47863446e4a3a105e2d4a5722a7,漂流少年,https://api.zhihu.com/people/bf73e47863446e4a3a105e2d4a5722a7,people,感谢每个朋友的点赞收藏！祝大家都能开心、自由、赚多多～,chatGPT会引发失业潮吗？2024我们该如何利用AI提升职场核心竞争力？,article,https://api.zhihu.com/articles/671101379,现在很多公司都有<em>AI</em>培训，不仅GPT，还有Midjourney、Stable DIffusion等一系列<em>AI</em>工具。像我们公司虽然今年招的少，但也会对新招的应届生统一进行<em>AI</em>培训。 用任正非先生的话来说就是，今后职场只有两种人，一种是能使用<em>AI</em>的人，另一种是创造AI工具的人,1263,198,0,1702007078,1711525268,会，但被踢出局的都是跟不上时代的人。现在很多公司都有AI培训，不仅GPT，还有Midjourney、Stable DIffusion等一系列AI工具。像我们公司虽然今年招的少，但也会对新招的应届生统一进行AI培训。用任正非先生的话来说就是，今后职场只有两种人，一种是能使用AI的人，另一种是创造AI工具的人。虽然扎心，但是事实。而我们普通人能做的，就是让自己成为那种能够使用AI的人。就目前形势来看，这种人就算被优化、放到就业市场上，也不会太狼狈。进，有大批AI相关就业机会涌现，像AI画师、AI运营、AI产品经理、AI架构师、AI工程师等。退，可利用AI赚点小钱，利用AI搞搞自媒体，比如让AI绘图在小红书上卖头像，AI直播等。那么，怎么样才算会用AI呢？简单来说，可以分为三步：1.稳定的AI权限 2.成熟的AI指令/技巧 3.个性化投喂训练第一步，稳定的AI权限这步我不多说，拥有稳定的AI权限是前提。如果不知道从何下手 ，可以试试知乎知学堂推出的 AI智能办公 公开课，1 节讲完 9 个主流AI工具的使用技巧，还有工具之间的结合，一套连招看的我大脑死机～^_^比如先用 GPT 提供思路，再导入 ChatMind 让AI脑暴，导出后再让智能生成PPT，不到5分钟干完我们一个团队一天的活，真·解放生产力。入口就是下面这个，不知道啥时候结束，建议先报名占个坑：听完可以领20个主流的AI工具+20套PPT模版，进阶版还有对应的安装教程，像Stable Diffusion、WPS AI和Chat Mind我现在还在用。那 20个AI工具 和 PPT模版 建议先领，进阶版可按需考虑，个人感觉还是蛮好使的～第二步，成熟的AI指令/技巧。拥有AI使用权是前提，但是有权限的人很多，但会用AI的人却很少。有使用权不等于会用AI，你还要会用指令。以GPT和AIPPT为例，同样是「述职报告」，有人花了5分钟写出来是个没啥用的空壳。有人花了5分钟让GPT写再甩给AI-PPT，稍加修改就可直接发给领导，中间差的就是指令。我所了解的基本指令，可以从3个方面出发。1⃣️基本指令B（背景）R（角色）T（任务）R（要求）背景（Background）：大背景阐述，你的人设、定位，目标人群、要给目标人群带来的价值等角色（Role）：给ChatGPT设定的角色任务（Task）：你需要的内容方向、需要达成什么样的效果要求（Request）：语言风格、内容结构、观点态度、字数、关键词等举个例子：2⃣️反向提问反向提问就是让AI来向我们提问，以方便我们将信息系统化的告知他，常用公式：你想做的事 + 需要注意哪些事项，请你使用提问的方式向我逐一提问，最后帮我生成一份xxx模板供我参考。举个例子：3⃣️数据投喂将数据/文章/具体情况投喂给AI，当个前提摘要，使结果更精准。把AI当成一个笨蛋又高效助理。先告诉他你的意图，再喂数据告诉他学习，最后让他给结论就行。举个例子：先告知意图：我要给你输入一段文字，我需要你记录下来并在最后回答我的问题，在我要求你回答问题前，不需要对我发送的信息进行任何解释和解读，只需要回复“ok”就可，理解我的意思就回复“明白”。再投喂文件/数据/表格等：可以多次投喂，最后根据需求、要求问AI要结果就ok～以上仅是最基本的GPT的指令公式，更多AI指令可参考「AI提示词设计指南」。「精准的指令」对于用 AI 的人来说就像一个 buff 一样，让你从“ 有AI权限的人 ”变成“ 会用AI的人 ”～《AI提示词设计指南》还是通过下面这个 AI智能办公 领的，需要的友友自取即可：除此之外，还有一个《提示工程指南》，反正是免费送的，我就领了，然后看不懂。问了一下人家说这个更适合技术人员。与AI交互的技术工程师啥的，需要的工程师或者程序员友友可领下看下～第三步，个性化投喂+反复训练最后一步就是根据需求，对AI个性化投喂+反复训练啦～根据自己的需求给AI文件、数据啥的，如果你不知道喂啥，也可以向AI反向提问。总之，AI运用的场景太多啦！从程序员码农到自媒体创作者，从产品经理到AI绘图师，甚至做个数据可视化分析哦都可以用 WPS AI 生成。WPS AI 生成的统计图大大解放生产力的同时，也必定会淘汰一些人。所以，跟进时代的步伐保持学习和进步才是职场上最大的核心竞争力。但也不必因此太过焦虑！就像当年的智能手机一样，AI的发展普使是迟早的事儿。希望我的回答对你有所帮助，最后觉得有用的话，记得点赞收藏给个鼓励哦！另外有AI相关的问题也欢迎评论区一起讨论～
683335273,b2c51b920f4c7d8543256703252ae6ba,京城洗碗工,https://api.zhihu.com/people/b2c51b920f4c7d8543256703252ae6ba,people,终于实现了小时候年薪百万依然骑车上班的梦想,AI的爆发必然不在美国，而在中国,article,https://api.zhihu.com/articles/683335273,立个flag：<em>ai</em>的爆发必然不在美国，而在中国。 以chatgpt、sora为代表的生成式<em>ai</em>只能是美帝资本主义渲染下的泡沫，最终的结局就是和2000年的互联网泡沫一样走向毁灭,117,219,80,1708570350,1708570350,立个flag：ai的爆发必然不在美国，而在中国。以chatgpt、sora为代表的生成式ai只能是美帝资本主义渲染下的泡沫，最终的结局就是和2000年的互联网泡沫一样走向毁灭。ai的发展四要素应该是算力、算法、数据和应用。alpha狗之前是算力不够，alpha狗-chatgpt之间是算力够但算法上的突破， sora只是应用的开，而不是终局。甚至可以说他把人类的科技点点歪了。ai要创造价值， 必须在生产力上予以应用，而不是仅仅是停留在服务业（电影、游戏、ppt）。在生产端中国有最好的基建，最好的通信网络（中国5g基站是美国的30倍）， 中国的企业和人口数量必然造就最大的数据生产、存储、加工市场，供给大模型进行运算。所以工业机器人和家庭式机器人领域，中国的优势远大于美国。短时间看我们在算法上确实落后了（算力我们可以靠数量取胜，另外量子计算机领域中国也不虚）。但是算法的壁垒远没有工业壁垒高，况且写代码的能力，华人的实力是有目共睹的。所以，算力算法上，咱们不虚美国。数据的生产和ai的应用侧，咱们比美国的优势更大。最后，人类的终局应该是星辰大海，而不是每天带个眼镜生活在虚拟世界里。第四次工业革命必在ai，也必在中国。
649453951,71569b241a558a26b207498e2def8eb5,三分糖很苦,https://api.zhihu.com/people/71569b241a558a26b207498e2def8eb5,people,,AI绘画美女,article,https://api.zhihu.com/articles/649453951,[图片] [图片] [图片] [图片] [图片] [图片] [图片] [图片] [图片],32,2,73,1691741248,1691741248,
672573246,c580b32f601b8c60168ef815c35f7d87,DeepLn,https://api.zhihu.com/people/c580b32f601b8c60168ef815c35f7d87,people,超高算力，简单易用，高性价比的AI算力服务,中国是否存在算力荒？如果存在AI算力荒到底到什么程度？,article,https://api.zhihu.com/articles/672573246,近年来，<em>人工智能</em>技术取得了快速发展， <em>AI</em> 大模型的出现更是引发了领域的重大变革。大模型通过海量数据训练，能够实现传统<em>人工智能</em>模型难以达到的性能，在语言生成、机器翻译、问答等任务上都取得了突破性的进展。大模型的训练和推理应用都需要大量的算力，根据 Gartner,35,11,0,1702711947,1703209269,"近年来，人工智能技术取得了快速发展， AI 大模型的出现更是引发了领域的重大变革。大模型通过海量数据训练，能够实现传统人工智能模型难以达到的性能，在语言生成、机器翻译、问答等任务上都取得了突破性的进展。大模型的训练和推理应用都需要大量的算力，根据 Gartner 的预测，到 2025 年全球将有70%的AI模型将使用云端来进行训练和部署。基于此背景下，DeepLn算力云团队对部分大模型的训练与微调所需算力进行了分析，并针对性推出了超有性价比的模型微调算力服务，同时服务内置框架，无需复杂的开发环境部署即可快速使用。一、AI大模型的崛起1. 大模型时代的来临随着深度神经网络的兴起，人工智能进入统计分类深度模型时代，这种模型比以往的模型更加泛化，可以通过提取不同特征值应用于不同场景。但在2018年~2019年，双下降现象打破了原有的人工智能发展格局。简而言之，参数增多、模型增大，过拟合会导致模型的误差呈现先下降后上升，研究者通过参数调整来找到精度最高、误差最小的最优模型。随着算力发展，研究者发现如果继续不设上限的增大模型，模型误差会在升高后第二次降低，并且误差下降会随着模型的不断增大而降低，通俗而言模型越大，准确率越高。因此人工智能发展进入了大模型时代。2. 广泛的应用前景大模型(LLM)是指具有庞大的参数规模和复杂程度的人工智能模型。在深度学习领域，大模型通常是具有百万级到百亿级参数的网络模型，其模型基础架构往往是基于Transformer、BERT、GPT来实现的。大模型具有广泛的应用前景。除了在自然语言处理、图像识别和语音识别，表现出高度准确和广泛的泛化能力，同时在制造业领域的控制机器人、缺陷识别、质量检测，医疗领域的分析医疗图像、识别基因突变、模拟药物作用，金融领域的分析财务数据、识别异常交易、风险管理等各个领域都表现出非常好的性能。3. 持续的快速迭代目前，国外的有OpenAI的ChatGPT、Google的Gemini，国内的有百度文心一言、清华的ChatGLM、阿里的通义千问、华为的盘古大模型、科大讯飞的星火等，这些大模型持续迭代，给人们生产和生活带来极大的便利。以 GPT-4模型为例，其强大的语言生成和理解能力着实让人惊艳。在各种专业和学术基准测试中表现出了人类的水平。它在模拟律师资格考试中的成绩位于前10％的考生，在文学、医学、法律、数学、物理科学和程序设计等不同领域达到高度熟练程度，而且它还能够将多个领域的技能和概念统一起来，理解其复杂概念。二、算力需求持续暴涨AI大模型的爆发，正对算力需求带来深刻影响。随着模型规模的持续扩大，以及在可预见的在各行各业中的落地渗透，训练和推理所需的计算资源也呈现出爆发性的增长趋势。目前，算力需求的增长受到以下几个方面的影响，包括模型训练和推理各个阶段：1. 模型复杂性的提升随着深度学习模型的不断演进，如Transformer架构的BERT和GPT(Generative Pre-trained Transformer)等大型模型，其参数规模和复杂性显著增加。这种复杂性提出了更高的计算要求，以更好地捕捉数据之间的复杂关系，因此需要更大规模的计算资源来支持模型的训练过程。2. 数据集规模的扩张在领域如自然语言处理和计算机视觉中，大型数据集对于训练高性能模型至关重要。随着数据集规模的不断扩大，模型训练需要更多的计算资源，以有效处理庞大的数据量。3. 实时和低延迟需求在实时应用中，例如自动驾驶和语音助手，对于推理的实时性和低延迟性要求非常高。处理大规模和复杂的模型需要更强大的计算能力，以便在短时间内做出实时决策。4. 领域应用的拓展人工智能技术已经渗透到医疗、金融、制造等各个领域。每个领域的应用场景和任务都具有独特的推理需求。例如，在医学领域，分析基因组学数据需要大规模且高度并行的计算能力。因此，随着大模型的持续迭代升级，目前绝大部分大型模型，都需要借助更高效的算法、更强大的算力以及更海量的数据来进行训练，以确保其具备泛化能力和鲁棒性，满足不同场景下的需求。三、大模型训练与微调所需算力归纳分析1. 训练大模型需要的算力以微软与英伟达合作推出的Megatron Turing-NLG(MT-NLG)模型为例，该模型拥有5300亿参数，其训练过程消耗了4480块A100GPU，最终在自然语言处理任务中表现出卓越性能。OpenAI在GPT-4的训练中，动用了大约25000个A100GPU，历时近100天，并采用分布式计算以加速模型的训练过程，从而使GPT-4表现出卓越性能。以Llama2和GPT-3*为例，训练所使用的算力如下。模型参数大小GPU训练时间 (小时)用于训练的设备来源Llama2 7B70 亿184,320A100-80GBLlama2 模型详情Llama2 13B130 亿368,640A100-80GBLlama2 模型详情Llama2 70B700 亿1,720,320A100-80GBLlama2 模型详情GPT-3*1750 亿2,522,880V100-32GBNvidia,论文* 对于GPT-3，训练时间取决于使用的GPU数量和模型的大小。例如，使用1个节点（8xA100）在1亿令牌上训练1.3亿参数模型需要7.1小时，而使用16个节点（128xA100）只需要0.5小时。然而，使用8个V100 GPU训练175亿参数的GPT-3大约需要36年。2. 微调大模型需要的算力资源对于细分行业在大模型领域的使用需求，更多的是大模型微调和优化，DeepLn算力云经过检索分析，将部分大模型微调所需要的算力归纳如下：模型LoRa 最低显存需求(4Bit量化)全参微调(FP16)最低显存需求LoRA微调建议GPU全参微调建议GPULLaMA-7B6GB84GBRTX 3060, GTX 1660, 2060, AMD 5700 XT, RTX 3050RTX4090 * 4, V100 32G * 4,V100 16G * 6,A100 40G * 2,A100 80G * 2LLaMA-13B10GB156GBAMD 6900 XT, RTX 2060 12GB, 3060 12GB, 3080, A2000RTX 4090 * 8,V100 32G * 6,A100 40G * 4,A100 80G *2LLaMA-30B20GB360GBRTX 3080 20GB, A4500, A5000, 3090, 4090, 6000, Tesla V100, Tesla P40V100 32G * 12,A100 40G * 10, A100 80G * 6LLaMA-65B40GB780GBA100 40GB, 2×3090, 2×4090, A40, RTX A6000,RTX 8000V100 32G * 26,A100 40G * 20 ,A100 80G * 10ChatGLM2-6B6GB84GBRTX 3060, GTX 1660, 2060, AMD 5700 XT, RTX 3050RTX4090 * 4, V100 32G * 4,V100 16G * 6,A100 40G * 2,A100 80G * 2以上就是DeepLn算力云对大模型的训练与微调所需算力整理分析的内容了，后续文章会持续更新完善，希望能对大家有用。DeepLn算力云团队专注于云端算力服务，绑定即送30元算力券，目前A100仅2.88元/卡时、4090仅1.98元/卡时、RTX8000仅1.68元/卡时，更有百张8G显存的计算卡供学生群体免费使用，持续为用户提供简单易用、安全可靠、高性价比的AI算力服务，欢迎大家试用。"
691403133,f314c16f1853f8b94bf6e9138bcc0db8,huan westlake,https://api.zhihu.com/people/f314c16f1853f8b94bf6e9138bcc0db8,people,Incoming AP at Westlake U.,西湖大学王欢课题组招收AI方向PhD/RA/Visiting Students,article,https://api.zhihu.com/articles/691403133,研究方向: 实验室目前专注于Efficient <em>AI</em>相关的理论、方法、和应用研究，包括但不限于： 经典Effcient <em>AI</em>方法：剪枝、蒸馏、量化、高性能网络结构设计、低秩分解等,414,84,528,1712612489,1713971510,"自我介绍：大家好！我叫王欢，将于2024 Summer加入西湖大学任助理教授（Tenure-Track Assistant Professor）, ENCODE (Efficient Neural Computing and Design) Lab PI。此前我在浙大读完本科和硕士，在美国东北大学读完博士。在Google / Snap / MERL / Alibaba等研究机构实习。我的研究方向是Efficient AI、Computer Vision，更多信息欢迎参考我的个人主页。现招收 PhD students (2025 Fall) 和 RA、Visiting Students（常年招收）。学校简介：西湖大学是一所社会力量举办、国家重点支持的新型高等学校，前身为浙江西湖高等研究院，于2018 年正式获教育部批准设立。西湖的定位是成为小而精的研究型大学，目前有4个学院（理学院，工学院，生命科学学院，医学院）。工学院下有6个专业，我所在的是人工智能和数据科学（Artificial Intelligence and Data Science）专业。学校主校区在云谷。实验室介绍：研究方向: 实验室目前专注于Efficient AI相关的理论、方法、和应用研究，包括但不限于：经典Effcient AI方法：剪枝、蒸馏、量化、高性能网络结构设计、低秩分解等。应用领域：【1】生成式AI: Text2Image Generation / Diffusion Models, LLM / MLLM, etc.【2】3D表征和渲染: NeRF / Gaussian Splatting, Neural Rendering, Digital Human, Talking Head, etc.【3】底层视觉：Image Super-Resolution, Image Restoration, etc.。我过往的研究主要在计算机视觉领域，由于现在整个AI领域呈现多模态融合的趋势，实验室也会关注视觉以外的领域。实验室氛围：实验室推崇专业、平等、轻松、鼓励创新（professional、equal、chill、creative）的氛围（我刚博士毕业，很能理解学生的心态，相信能跟大家相处融洽~）日常我们也会组织一些活动（聚餐, hiking, etc.），帮助大家减轻科研压力。实验室能为你提供什么？项目：实验室提供高质量、研究导向的科研项目，我会尽职尽责指导。学生的工作重心是做研究、发paper、成为有竞争力的研究者。无杂活（例如报销etc.) ，实验室另配有行政助理可以帮助大家解决日常行政性问题。我的指导风格是比较hands-on的，会有regular meeting, 讨论idea, 帮助改paper, 必要时帮助写code等。待遇：西湖提供优渥的工资待遇和舒适的工作环境。实习：鼓励学生实习（研究型），并帮助学生寻找机会。学术交流：鼓励学生与其他学者合作，build connections. 鼓励学生参加学术会议（国内 or 国外），提供报销支持。计算资源：西湖有充足的计算集群（A100/V100等显卡）、Lab也购置有单独的自有机器(A6000, 4090等显卡），以及部分项目跟企业合作，也可以access到企业的计算资源。招生名额：【PhD】实验室打算招收3-4名博士生（2025 Fall入学）。【update: 关于博士学位】西湖-浙大联培项目可在 ""计算机科学与技术"" 一级学科招收博士研究生，接收本科和硕士学生报名，具体可参考下面2024年招生简章。注意：防止政策变动，一切以2025年博士生招生简章为准！2024年博士生招生简章【RA & Visiting】实验室常年招收科研助理(RA)、访问学生(Visiting)，从事研究导向的项目(i.e., 发paper)，尤其是打算最终加入lab的同学，欢迎联系做RA、访问。我们会给RA、访问学生提供薪酬。为了保证有充足时间发paper，访问时间一般在3-6个月左右（不低于2个月）。关于科研助理 vs. 访问学生 (其实也包含已毕业的不再是“学生”的访问人员，只是为了方便统一称 ""visiting students"" or simply ""visiting"")，以防叫法上可能有些混乱，我解释一下：科研助理（RA）是类似行政助理的正式工作岗位（签合同，lab会帮交五险一金这种 -- 也因此，招RA的bar比招visiting更高）。一般来说，对于学生想来发文章的（无论是未毕业，还是已经毕业需要gap一年的等），应该是visiting。2024西湖大学夏令营申请已经开始，针对2025届优秀应届本科生，相关同学可关注申请。详情请参考下面公告。夏令营时间很短，一周左右，主要是为了让本科生在保研时可以了解西湖各个lab。西湖大学工学院2024年夏令营公告2024西湖大学暑期科研实习申请已经开始，详情请参考下面公告。我们lab可以招收两名实习生。PS. 暑研和普通访问学生主要是不同渠道进入lab，进入lab后所做的事情是一样的（i.e., target publications）。这个时间officially只有四周，是不够发paper的，所以一般来说在结束之后还需要保持合作（回到原校remote, or, 以visiting的身份继续留在lab）。西湖大学2024年暑期科研实习公告关于资质要求：实验室接收的访问学生、暑期科研主要是具备相关经验（深度学习、计算机视觉、熟悉linux/git等基本科研infra）的（高年级）本科生和硕士生（i.e., 我们期待你进入lab后可以比较快进入科研状态）- 尤其是有publication目标的同学（如为了准备下一年出国申请、需要publication满足毕业要求等）。申请方式：发邮件到 wanghuan@westlake.edu.cn，说明目标职位（PhD students, or RA, or visiting students)，附上: CV, transcripts, 以及其他你认为能彰显你独特性的材料。我们将对合适的candidates尽快安排面试。更多信息亦可参考英文版招生贴。欢迎大佬们转发、自荐、推荐优秀学生！ 注：本帖内容会根据PI所了解到的信息适时修改，其他平台转载信息可能不是最新，最终解释以本帖为准。谢谢！如有平台方（如微信公众号）转载，麻烦附上此帖的link, 以防信息更新。谢谢！"
654589259,a43cc39980bdbd9bf7547c80d62f4f4a,创贸<em>AI</em>CC,https://api.zhihu.com/people/a43cc39980bdbd9bf7547c80d62f4f4a,people,中国企业专属的AI大模型,史上最全AI科普：终于有人一次性把AI讲明白了！,article,https://api.zhihu.com/articles/654589259,史上最全<em>AI</em>科普：终于有人一次性把AI讲明白了！ “3亿人饭碗保不住？<em>AI</em>真的杀疯了！” “AI到底会不会替代人类？” “<em>AI</em>一分钟一万字，你还没学会给AI下指令吗,16,0,43,1693968562,1693968927,"史上最全AI科普：终于有人一次性把AI讲明白了！“3亿人饭碗保不住？AI真的杀疯了！”“AI到底会不会替代人类？”“AI一分钟一万字，你还没学会给AI下指令吗？”......AI的爆火，以上信息，你一定没少见，甚至还会因为这些信息而感到焦虑不安。对于刚刚接触人工智能领域的朋友来说，人工智能领域知识庞大且复杂，各种专业名词层出不穷，常常让人看得摸不着头脑。AI到底是啥？AI大模型又是啥？算力？Token？今天这个文章就来揭开AI的神奇面纱，让我们一起看看AI到底有多厉害！01什么是AI AI，即人工智能（Artificial Intelligence），是一门研究如何让机器模拟人类智能的学科。它涉及到构建可以感知、推理、学习和决策的智能系统，以解决复杂问题和实现人类类似的任务。AI的技术包括机器学习、自然语言处理、计算机视觉、专家系统等。目前，AI已经广泛应用于各个领域，如医疗、金融、交通、娱乐等，使得机器能够自动化执行某些任务，从而提高效率和准确性。02什么是AI大模型AI大模型是指规模庞大的人工智能模型，这些模型具有更多的参数、更复杂的结构和更强大的计算能力。大模型能够处理更复杂的任务，具备更强的学习和推理能力，并在各种领域中取得更好的性能表现。大模型的典型代表是深度学习模型，如深度神经网络。它们通常具有数以百万计甚至数以亿计的参数，能够从大量的数据中学习并提取特征，以完成复杂的任务，如图像识别、语音识别、自然语言处理等。大模型又分为公有大模型和私有大模型。公有大模型和私有大模型是指在人工智能领域中使用的预训练模型的不同方式。03公有大模型公有大模型是由大型技术公司或研究机构开发和训练的，并且是对公众开放使用的预训练模型。这些模型通常使用大量的计算资源和数据进行训练，可以在多种任务上表现出色。一些著名的公有大模型包括 OpenAI 的GPT系列、Google的BARD和Microsoft 的Turing NLG。这些大模型能力强大，但是不能针对于企业的特定场景去做专业，细致化的生成。04私有大模型私有大模型是由个人、组织或企业自己独立训练的预训练模型。对于某些特定场景或需求，私有大模型可以更好地满足用户的定制化需求。私有大模型的训练通常需要投入大量的计算资源和数据，并需要相关领域的专业知识。这些私有大模型在商业领域中广泛应用，比如在金融、医疗、自动驾驶等领域。05什么是AIGCAIGC（AI Generated Content）是利用人工智能来生成你所需要的内容，GC的意思是创作内容。与之相对应的概念中，比较熟知的还有PGC，是专业人员用来创作内容；UGC是用户自己创造内容，顾名思义AIGC是利用人工智能来创造内容。06什么是GPTGPT是AIGC的一个种类。GPT的全称，是Generative Pre-Trained Transformer（生成式预训练转换器）。是一种基于互联网的、可用数据来训练的、文本生成的深度学习模型。07什么是算力算力就是字面意思，计算能力，计算机进行运算处理的能力，更具体来说，算力是通过对信息数据进行处理，实现目标结果输出的计算能力。通常用于衡量人工智能、区块链、数据分析等领域的计算性能。人工智能领域里的深度学习任务中，需要大量的计算资源来训练和推理模型。因此，算力在这个领域变得尤为重要。强大的算力可以加速训练过程、提高模型的精度和性能，并且处理更复杂的任务和数据集，最终生成新的、原创的内容。小Tips为什么大家都说算力很贵，贵在哪里？算力往往会因为计算资源的供需关系和生产成本等因素而有所涨跌。当人工智能应用变得更加复杂，需要更高的算力来支持训练和运行模型时，计算资源需求会增加，导致算力成本上升。此外，随着人工智能技术的普及和应用范围的扩大，对算力的需求也在增加，这也会导致算力成本的上升。08什么是Token在自然语言处理(Natural Language Processing, NLP)中，Token 指的是将输入文本或句子分解成的基本单位。一个 Token 可以是一个单词、一个字符、 一个标点符号或者一个子词。小TipsToken是如何计算的？例如：在AICC的产品中，1000个Tokens=750英文单词/555个中文字；一个标点符号=3个Tokens。09什么是NLPNLP是什么：NLP 是自然语言处理（Natural Language Processing）的缩写，是人工智能和语言学领域的一种技术，旨在让计算机理解并处理人类语言。业内人将AI的发展进程分为三个阶段，分别是：弱人工智能阶段、强人工智能阶段、超强人工智能阶段。弱人工智能只会计算、推断，解决具体问题。当前广泛应用的弱人工智能如智能推荐购物、战胜柯洁的AlphaGo、机器人客服等。强人工智能拥有意向、意识、意志的机器智能，拥有和人类一样的智能水平，可以替代一般人完成生活中的大部分工作。超强人工智能人工智能像人类一样通过各种采集器、网络进行学习，每天自身可以进行多次审计迭代，智能水平超过人类。现在的人工智能已经是接近强人工智能阶段。在将来，人工智能将会进化为超强人工智能，遍布在我们的生活中，我们也将会将大量的工作或应用场景交予AI，让AI为我们产出更多的价值。10什么是AICCAICC是以AI大模型为基底，集行业数据与企业数据为一体的AI专家；行业内领先的AI私有大模型，能为企业定制化地训练出完美契合企业的AI；是企业数字化转型过程中的必要选择。AICC拥有全方位部署企业的能力，例如：AI智能客服、AI智能助理、AI短视频策划师、AI高级顾问、AI私域营销专家、AI品牌策划师等等，个个精通企业知识，24小时在岗，为你全面升级企业运营，让你的企业成为领先的数字化企业！-END-点击原文直接体验域名：www.ai.cc点击注册个人免费版：https://free.ai.cc/register"
690617819,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,转行AI的打工人：年入五十万，裁员潮再也不焦虑了,article,https://api.zhihu.com/articles/690617819,大概是从去年开始，<em>AI</em>新闻如浪潮般涌现。 老板们热衷于研究<em>AI</em>工具，寻找降本增效的可能性。职场打工人，在“<em>AI</em>取代人类”“10亿打工人被革命”的新闻里惶惶不可终日。 事实上，<em>AI</em>使得原画师、电商平台推广文案写作、卖家秀模特、翻译批量失业的事件正在发生。 猎聘发布的《2023AIGC人才洞察趋势》显示,19,11,0,1712138392,1712559596,"大概是从去年开始，AI新闻如浪潮般涌现。老板们热衷于研究AI工具，寻找降本增效的可能性。职场打工人，在“AI取代人类”“10亿打工人被革命”的新闻里惶惶不可终日。事实上，AI使得原画师、电商平台推广文案写作、卖家秀模特、翻译批量失业的事件正在发生。猎聘发布的《2023AIGC人才洞察趋势》显示，高新技术人才是企业招聘的主旋律，包括模型层的算法师，基础层的数据分析师，应用程序的产品经理等。其中，算法工程师平均薪酬47.5万。IT/互联网/游戏行业对AIGC人才需求度提升了15%以上。春江水暖鸭先知，这个行业正在被每一位从业者改变着。这个全新的行业，是打工人最好的选择吗？听听我设计师朋友的故事。掌握未来趋势：AI技术赋能设计师我是莫女士，一个32岁的设计师，我的设计之路就像是一幅不断涂抹的画布，充满了无限的可能性和创意的火花。然而，随着时代的变迁和技术的进步，我意识到，要想在设计行业中立于不败之地，就必须与时俱进，掌握人工智能技术。学习AI技术就像是探险中的一次奇遇，我驾驭着学习的船只，在知识的海洋中翻涌前行。每一次的学习都如同寻宝之旅，让我不断发现设计领域的新大陆和宝藏。逐渐地，我掌握了AI技术在设计中的应用，将其融入到了自己的设计作品中。如今，我的设计作品不仅在视觉上更加吸引人，还融入了更多智能化的元素，为客户带来了更好的体验和价值。我相信，通过学习AI技术，我将开启设计领域的新时代，为自己的设计事业赋能，实现更加美好的未来。你看看现在的AI设计师类岗位：用AI高效率、低成本、可复制地完成各类工作任务......设计人的焦虑瞬间被点燃！自己的工作，真的会被AI取代吗？事实上，AI不会让你失业，“会用AI的人”才能让你失业！------------------------------------最明显的，是职场竞争环境的变化：外部招聘，有些岗位已挂上""擅长AI优先”，甚至直接新增AIGC岗公司内部，同事开始用AI提高效率，而你却连AI还没摸到显而易见，AI已渐渐成为区分工作能力的因素之一，不断拉开职场竞争力差距。比如做画册宣传时，甲方偶尔要求实物图，但摄影图版权贵，又难以找到符合主题的。但有了AI，无论是宫崎骏风格、莫奈风格，甚至是摄影图，一口气生成几十张，再也不愁素材了！为了帮各位提升职场竞争力，AI绘画的副业实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师全程一对一讲解，小白也能轻松get！点击下方卡片就可以获取。AI直播AI绘画除了提高提高工作能力，成为的效率工具之外，学会AI还能开展新副业！闲暇之余接一下单子，赚外快~----------------------------从新手小白，再到接单小能手，我也就花了一个月多月的学习时间！现在靠着每天下班后接接AI的单子，每个月多赚个几千上万!（以下仅代表个人收益哈）更重要的是，只要你有时间，会电脑开关机，你就能上手学会接单赚钱! AI绘画可以做什么副业？1、游戏、电影、动漫等IP制作AI绘画可以用于游戏和影视制作中的场景和角色设计，可以快速生成大量高质量的图像和动画，大大加快项目开发进程。2、 工业设计AI绘画在工业设计中的应用也非常广泛。工业设计是一个非常复杂的过程，需要考虑到许多因素，例如产品功能、用户体验、外观设计等等。AI绘画可以在这个过程中起到很大的作用，例如：1.产品设计：AI绘画可以生成和优化产品设计方案，从而提高产品质量和生产效率。2.外观设计：AI绘画可以生成不同风格和风格的外观设计方案，使得产品更加美观和吸引人。3.用户体验：AI绘画可以用于虚拟现实和增强现实技术中，生成逼真的虚拟场景和产品模型，帮助用户更好地了解和体验产品。4.制造工艺：AI绘画可以生成或优化制造工艺，提高生产效率和产品质量。牛油果形状的扶手椅3、建筑设计AI绘画在建筑设计中的应用非常广泛，可以提高建筑质量、建筑效率、可持续性，同时也可以帮助设计师更好地实现创意和想法：1.建筑设计：AI绘画可以生成和优化建筑设计方案，从而提高建筑质量和建筑效率。2.环境设计：AI绘画可以生成和优化建筑环境设计方案，使得建筑更加适合周围的环境，从而提高建筑的可持续性和生态性。3.建筑材料：AI绘画可以生成和优化建筑材料的设计和选择，从而提高建筑的质量和经济效益。4.结构设计：AI绘画可以生成和优化建筑的结构设计方案，从而提高建筑的安全性和稳定性。4、原画/海报5、电商AI绘画可以帮助电商平台展示产品的外观、细节和材质等信息，吸引用户的注意力，提高产品的展示效果。同时，利用AI绘画技术，电商平台可以对商品进行视觉化呈现，增强商品的吸引力，提高用户的购买意愿。一键换模特商品效果图6、漫画推文在平台发布漫画解说视频，靠平台播放量获取收益。拥有AI的感觉，就像“过去仅有一把锤子干活，现在却拥有了一座智能工厂高效生产。”所以我的主业+副业，年收入已经过五十万了，很多项目都是有长尾效应的，简直是躺赚！-----------------------------感兴趣的朋友们，别再傻乎乎到处找教程啦！我都给大家准备好了AI绘画全套教程了！我之前学过的课程，现在分享给大家，感兴趣的朋友先学个大概！AI直播教程老师说让大家学完之后能【商业落地】和【变现】是这门课的宗旨！课程资料内有最最最基础的Midjourney、Stable diffusion、PS应用、以及八大商业落地拆解平面设计应用场景 室内与建筑设计 商业摄影 CG绘画设计 小说推文与漫画 产品设计 电商--服装 电商--包、鞋子、帽子好评如潮：所以我们的售后服务，我有信心地保证，小白也完全不用担心跟不上！---------------------------------而且我自己就是从零基础小白学过来的，非常了解新手小白易踩的坑在哪为了避免让大家少走弯路，真正能学会AI绘画并且变现，免费的AI绘画直播公开课就给大家放在下面了！里面包括AI绘画工具，4000+关键词手册、工具包等等。AI直播教程-精准粉落地页反正我就是通过这个AI绘画课学会的，现在副业比主业的收入还高，对我的工作非常有帮助！更棒的是会提供接单渠道，我从此走上了接单的道路了戳上方小卡片就能直接学，所有人千万别错过了！"
682538660,2d5205be2adf42da480a752a6d087ae8,宙语Cosmos AI,https://api.zhihu.com/people/2d5205be2adf42da480a752a6d087ae8,people,我们对技术的热情源自对未来的信仰！,哪款AI智能写作工具好用-AI智能写作工具排行榜,article,https://api.zhihu.com/articles/682538660,在过去一年里，这些神奇的<em>AI</em>写作通过工具不断进化和升级，给我们带来了好多惊喜和方便。随着2024年的到来，我要向大家推荐几款绝对不能错过的<em>AI</em>智能写作工具！ 1、宙语Cosmos <em>AI</em>第一款要向大家推荐的AI智能写作工具是：宙语Cosmos <em>AI</em>，这款AI智能写作工具能帮我们轻松生成各种文案,34,1,0,1708164389,1708220083,未来已经来临，2024年是一个充满机遇和挑战的年份，AI智能写作工具已经成为了我们写作工作者的得力帮手，在过去一年里，这些神奇的AI写作通过工具不断进化和升级，给我们带来了好多惊喜和方便。随着2024年的到来，我要向大家推荐几款绝对不能错过的AI智能写作工具！1、宙语Cosmos AI第一款要向大家推荐的AI智能写作工具是：宙语Cosmos AI，这款AI智能写作工具能帮我们轻松生成各种文案，日报周报、工作计划、会议记录、广告词、产品介绍、文章写作等等，我们只要输入相关信息和要求，这款AI智能写作工具就会秒变神级写手，给我们提供高质量的文案内容，并且还有多种选择供我们挑选哦，用上它以后，写作再也不用愁啦！https://xg.zhihu.com/plugin/c59c15567c021ce81d5ad4abb31f2237?BIZ=ECOMMERCE这款AI写作工具从2023年上线，经历了多次升级和改进，给写作者们带来了超级便捷和高效率的创作体验！不管是平时工作中需要写各种文案，还是想要提升创意和表达的方式，甚至想要润色语法错误和提高文笔水平，这个工具都能成为咱们的得力助手！将它应用到写作创作中，可以辅助咱们的文案变得更出彩、精彩！2、句心AI助手这个工具就是专门辅助我们写万字长文的，不管是整理思路还是规范格式，句心AI助手都能给我们全方位的支持和指导，用了它，我们的文章绝对会变得更专业、更完整。句心AI助手-无需下载，在线试用以前我写文章时总是头脑一团糟，还得操心格式啥的。但现在有了句心AI助手，这些问题都迎刃而解！只要简单描述一下研究主题和目标，它就能根据我们的需求生成清晰明确的大纲，并按照逻辑顺序组织好每个章节的内容，这样写起来就少了很多迷茫和纠结，效率也高多了！3、笔灵AI写作软件这款AI智能写作助手适合各种写作场景哦，比如工作报告、研究报告、会议发言、情书、新闻报道、广告创意、社交媒体内容、邮件营销、博客文章、旅行游记、视频脚本、歌词创作，甚至还有个人日记啥的，真就是啥都能写！笔灵AI写作 - 国内领先的AI写作助手 | 官网、智能工具、免费改写界面简约大气，操作so easy！只要输进关键词或主题，它就能秒变成一篇文稿～哇塞，结构超级严谨、内容超级丰富！如果你觉得有点不够全面，那还可以一键改写续写等。4、艺墨AI助手艺墨AI助手是个全能的AI智能产品，集成了问答、绘画和AI TTS语音等功能！简单来说，它就是一个全能AI助手~不仅拥有全球顶级模型，还支持上下文对话和语音对话。这意味着我们可以在交流中保留之前的信息，让AI更好地理解我们的问题，并给出更贴切的回答。艺墨AI绘画助手 - 智能绘画软件 | AIGC设计工具在艺墨AI助手里头，你可以直接让它帮你生成图片哦！而且还支持各种变换，比如文生图、混图、咒语解析、局部重绘等等。不仅可以设计品牌徽章啥的，还能搞插画、人物图片、动漫角色、风景图片等各种创作。有了艺墨AI助手提供的多样绘画模式和变换选项，你就能实现更多创作可能性啦！艺墨AI助手是个超级强大又好玩的AI工具，快去试试看吧，体验一下它神奇又有趣的功能！5、文心一言文心一言是百度研发的一个语言大模型，能跟人聊天互动、回答问题，还能帮我们写作、获取信息和激发灵感。简直就是知识增强版的小助手！它在文学创作、商业文案、数理逻辑推算以及中文理解和多模式生成方面都有着超好的用武之地。文心一言6、秘塔写作猫秘塔写作猫也是个超级厉害的AI智能写作工具，不仅可以帮我们写作，还能给我们校对、润色文本，它提供各种模板供我们选择，比如新闻报道、科技资讯、产品推荐等等。只要按照提示填写摘要和大纲，就能立马得到一篇完整的文章！写作猫还有智能校对功能，写完文章后，它会自动帮我们检查语法错误、表达是否清晰。还能识别出可能的错误或不太合适的用词，并给出替换建议。秘塔写作猫如果你觉得原始文本有点儿问题或者想润色一下，秘塔写作猫也能搞定！它可以根据我们的要求或参考样例来改写文本，并在保持原意的基础上增加点儿创意和亮点。学会熟练使用这些AI智能写作工具将为我们带来更多的机遇和便利，不管是商务场合需要快速写各种文案，还是想提升文章水平或者享受创作乐趣，这些智能写作工具可都是我们的救星哦！随着时间推移，这些工具会升级的越来越牛，它们不仅理解能力更准确、更智能，还会更好地满足我们的需求生成内容。而且它们还会跟其他技术搭配使用，比如自动化排版、图形设计等等，让创作过程变得高效又好看。虽然这些工具方便到爆炸，但我们也要保持警惕！用这些工具时还是要保持独立思考和判断能力才行。毕竟创作可不只是生成内容那么简单！更重要的是表达自己的观点和思想。所以，在享受智能写作工具给我们带来的便利的同时，别忘了培养一下自己的写作技巧和思维能力啊！
690416938,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,“人工智能+”浪潮里，哪个行业将更高薪？,article,https://api.zhihu.com/articles/690416938,首次提出了开展“<em>人工智能</em>+”行动。从Midjourney作图，到ChatGPT写作，再到前不久首款文生视频模型Sora发布，<em>AI</em>不是“渐渐”，而是“已经”以迅猛之势入侵我们生活的方方面面。 这无疑对很多产业造成了不同程度的冲击，许多从业者在<em>AI</em>入局市场后感到岌岌可危,12,0,0,1712051356,1712124298,2024年，首次提出了开展“人工智能+”行动。从Midjourney作图，到ChatGPT写作，再到前不久首款文生视频模型Sora发布，AI不是“渐渐”，而是“已经”以迅猛之势入侵我们生活的方方面面。这无疑对很多产业造成了不同程度的冲击，许多从业者在AI入局市场后感到岌岌可危。还没有“被挑战”的行业也是诚惶诚恐，没人知道AI下一步又将剑指何处。另一方面，2024年首次提出了“人工智能+”，意味着时代的发展主流趋势将由“互联网”变成了“人工智能”。“人工智能”成为讨论的核心话题之一，这也进一步证明：普通百姓的日常生活也将与人工智能息息相关。就像历史不可被改写一样，AI对未来的影响也是不可撼动的。参考九年前“互联网+”的发展路径：互联网+电信诞生出微信，互联网+金融诞生支付宝，互联网+购物诞生淘宝，互联网+电视诞生抖音……这些如今人们习以为常的“互联网+”的生活方式，也将被“人工智能+”进一步颠覆。英伟达CEO黄仁勋曾说：“未来是生成式（AI）的”。目前的AI作品生产只是开胃菜，它未来的发展空间是不可限量的。然而，科技的发展意味着迭代，有新生，就会有淘汰。风口与风险并存。惊叹于AI技术的人很多，能真正抓住机遇的却凤毛麟角。当AI巨浪汹涌而来时，你是等待出局，还是抓住机遇？抓住机遇，首先要看到机遇。回看过去互联网领跑的十年，涌现出的企业无一不是先吃螃蟹的。“早起的鸟儿有虫吃”这个道理，自改革开放以来就在不断被证实，并孵化出一个个商业神话。如今，面对AI这一新兴技术，普通人看到的是失业危机，有心者看到的是新兴产业、跨界合作、结构调整。360集团创始人周鸿祎曾透露，他们利用AI和自己的数据做出一个百亿模型，用于解决系统的攻击发现、攻击预测、攻击抵御、攻击阻拦，创造出比GPT4更为强大的存在。OPPO、魅族等手机厂商将AI作为新产品的研发中心与卖点，试图将更先进的AI大模型装进手机，打造具备生成式AI能力的移动设备。很多人也率先了解并应用这些人工智能工具，紧跟科技的步伐，让AI为自己所用。AI已来，有人在观望时，已经有人在行动了。意识到时代的风口已然来临，你要凭借什么御风而行？作为前沿科技领域外的从业者，要如何从AI大势中分得一杯羹？能力、资源、人脉、坚持……决定一个人发展上限的因素很多，但能帮助人们快速成长的捷径却少之又少。学习新兴行业，拥抱科技，了解时代发展趋势，或许是让你快人一步，感知并抓住机遇的绝佳途径。-------------------------其中“人工智能+绘画”已经是非常靠谱的一种方式，不管是进行技能提升，提高工作效率，转行到AI相关行业，还是做副业，都非常吃香！2024年，上进学社的【AI绘画直播教程】，现已开启！想了解AI绘画这行业的宝子，我把AI绘画直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师手把手教你用AI绘画工具如何生成图片，小白也能轻松get！点击下方卡片就可以获取！AI直播一起在这洪流中分一杯羹！
691127626,fe66479d9ee37ae02558317f159fc879,上上,https://api.zhihu.com/people/fe66479d9ee37ae02558317f159fc879,people,一心只想搞钱的AI绘画师！谈钱不伤感情！,在小红书用AI做图文号，接200张商单，月赚4W+,article,https://api.zhihu.com/articles/691127626,最近发现的这个小红书账号，用<em>AI</em>做小说素材的图文内容，粉丝数量2.2万，获赞加收藏竟然高达83.3万，而且商单接得超级稳，一个账号商单就已经超过200张了，平均每月能接到25张商单，这数据绝对是炸裂级别的！ [图片] 说实话，这个应该是我看到的小红书中，用<em>AI</em>做内容而且商单数据这么稳定,5,0,0,1712482014,1712634199,知友们！最近发现的这个小红书账号，用AI做小说素材的图文内容，粉丝数量2.2万，获赞加收藏竟然高达83.3万，而且商单接得超级稳，一个账号商单就已经超过200张了，平均每月能接到25张商单，这数据绝对是炸裂级别的！说实话，这个应该是我看到的小红书中，用AI做内容而且商单数据这么稳定，变现能力这么强的案例，别的不说，这个效果已经完全超出了普通标准，一个月能接15张商单的都算是顶尖账号了，这个账号月均25张，基本上是天天都有商单进账，想想都觉得激动！这个账号的成功经验，真的是值得我们每一个小伙伴都来深入学习和借鉴！它不仅在小红书上取得了惊人的成绩，还为我们提供了一个明确的图文成功模板。在这一期的分享中，我会深入解析这个账号如何能够持续且稳定地接到商单的关键要素。经过仔细研究，我总结出了三个核心要素，这些是任何希望成功的账号都应该具备的。1.关于账号调性一致性这位博主显然非常明白如何吸引小红书社区的用户，他们不仅确保了账号封面的风格和内容主题保持一致性和专一性，而且还选择了一种吸引人的高雅古风滤镜色调，紧密围绕古言小说素材深入挖掘，实现了所谓的账号调性的高度一致性。对于许多运营新手而言，找到并维持这样的一致性是一项挑战，他们可能会经常更换内容方向或是尝试不同的领域，而没有形成特色便轻易放弃，这也是许多新手账号常见的更新停滞的根本原因。每个人都应该花时间审视自己目前管理的账号，看看是否能够在封面风格和内容主题上达到这样的一致性和专一性，这是提升账号吸引力的关键一步。2.关于如何突出主题这个账号做得非常出色。他们通过在每条笔记的封面上放大和突出关键主题来吸引用户的注意，这种方法非常有效，因为大多数小红书用户在浏览内容时通常非常匆忙。一个能够在瞬间吸引注意的封面主题，可以大大增加用户点击阅读的可能性。3.始终从用户的角度出发来创作内容是这个账号成功的又一重要因素，博主深谙用户喜好，通过创作引人入胜的古代言情小说素材内容，为用户提供了大量的古代名词知识，极大地增强了用户对账号的粘性。这就要求我们在创作内容时，不仅要清晰地知道账号的定位是什么，更要深刻理解用户的需求和兴趣所在，以此来创作能够触动他们的内容。接下来，我会分享如何利用AI工具，包括ChatGPT、AI绘画工具以及可画等，来创作符合上述要素的高质量内容。这一过程将会分为三个步骤：首先用ChatGPT制作富有吸引力的文案，然后用AI绘画工具创作风格一致的美图，最后用可画等进行专业的图文排版。--------------------------详细的教程已经打包好啦！这种图文内容的创作方法不仅适用于小红书，还可以扩展到其他平台，比如同步发布到“小绿书”公众号上赚取流量主收益。这个项目的玩法多样，如果你对此感兴趣，我可以教你，和你一起探讨怎么制作类似的小说素材，如何用AI绘画工具生成图片等等而且我自己就是从零基础小白学过来的，非常了解新手小白易踩的坑在哪！！为了避免让大家少走弯路，真正能学会AI绘画并且变现，小说素材的免费AI绘画直播公开课就给大家放在下面了！里面包括AI绘画工具，4000+关键词手册、工具包等等。AI直播
686633601,44fc7bb4b6e86e7506234068b883c497,AI上进中心,https://api.zhihu.com/people/44fc7bb4b6e86e7506234068b883c497,people,,那个会用AI绘画的设计师，刚刚加薪了......,article,https://api.zhihu.com/articles/686633601,在<em>AI</em>技术迅猛发展的当下，设计行业也面临着巨大的变革。作为设计师， 如何在<em>AI</em>浪潮中保持竞争力，如何借助AI的力量领先时代，已经迫在眉睫... 当一个设计师能有多难？众所周知：婆媳问题自古是世纪难题。男主人往往辗转于妈妈与媳妇之间以求和平，设计师就不一样了。 设计师需要在,6,0,0,1710237695,1712037929,在AI技术迅猛发展的当下，设计行业也面临着巨大的变革。作为设计师，如何在AI浪潮中保持竞争力，如何借助AI的力量领先时代，已经迫在眉睫......当一个设计师能有多难？众所周知：婆媳问题自古是世纪难题。男主人往往辗转于妈妈与媳妇之间以求和平，设计师就不一样了。设计师需要在：主管/经理/老板/甲方的不断洗礼下，抛弃自己的底线与审美，最后加班完成一个自己都不想多看一眼的作品。所以，我们逐渐明白，设计师的成长史就是一部宏大的斗争史。所以，到底在这个AI席卷设计行业的浪潮中，怎么样让AI为我所用，让设计人的效率提高，让时间留在更赚钱的地方？还记得AI刚出现那会儿，很多媒体都在说AI会取代我们这批设计师，前阵子SORA出来了，又说会取代一批影视特效从业者，制造了非常多的焦虑。当然AI也引起了一大批美术生和设计师的抵制。但我认为要保持开放心态，科技进步的步伐我们无法阻挡。只有真正用过AI的人才知道，AI根本不会取代设计师，它是各类设计师最值得掌握的技术和工具。--------------------------------首先，它大大提高了工作效率，别人3天出一张图，现在一个小时就能搞定了，让自己多出更多时间。才思枯竭的时候，能够在上面寻找灵感思路，结合而发挥自己的创意，站在巨人的肩膀上去创作。仅靠输入几个中文关键词，设置几个参数，AI工具就能快速生成指定的图片。只要有电脑会打字的人相信都能做到！还是不会或有问题的宝子，我把AI实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。手把手教你用AI工具如何生成，小白也能轻松get！点击下方卡片就可以获取！AI直播------------------------------还记得之前有设计需求的时候，我要做几天才能出一张复杂的图，还要来来回回改上几十遍。自从用了AI后，从提出需求到交效果图只用了一个小时，效率高到离谱！除了提高效率，AI辅助做设计还有其他哪些好处呢，我来举几个例子：以前我做详情页，经常翻好几个像花瓣网等等地方到处找素材，好不容易找到合适的，发现要付费使用！而现在用AI的话，简单写几个关键词，分分钟就能生成一批匹配的素材，要多少有多少。尤其这种虚拟人物图像，不怕被人发现偷图！也不用浪费时间用PS一点点地抠图！出图了就能直接用，简直不要太爽！经常逛设计网站的话不难发现，排行榜上数一数二的大佬作品，不是牛逼的插画，就是炫酷的3D作品。自己也想做3D或板绘效果图，但是既不会插画也不会3D软件怎么办？别急！给AI一个这样的↓AI给出这样的↓如果想要精确度更高的商用作品，还可以直接让AI帮你生成黑白线稿图，再用AI帮你一键把线稿转3D！当然，更方便省事的方法就是写几个关键词咒语，直接用AI生成3D图！总之，AI辅助设计简直好用到飞起！------------------------------------------现在除了工作中会用到AI以外，我身边还有不少设计朋友已经在用AI接单做副业，努力搞钱争取40岁前退休了！我认识有个哥们儿用AI生成好看的插画，定期给商家供稿，有专门的人来收图。画个大致的线稿，AI就直接帮你生成完整插画。还能一键补全细节+一键局部调整。几分钟就能出一张，效率高得不可思议！虽然最开始做的简单一点单价不高，不能像营销号那样分分钟含泪赚几千，但是后面越发熟练开始走量之后，慢慢的副业都快变他的主业了。据说他最近又开辟了新市场，开始用AI帮人定制宠物表情包和定制8K超高清宠物壁纸了。像下面这种表情包3步就搞定了，这门小生意也非常受欢迎。（以下仅代表个人收益）AI出现后，设计师想要做副业搞钱真的很简单，毕竟90%的步骤都可以交给AI来完成。非商用的需求甚至都不需要自己动手去调整，所以很多非设计出身的人也在做。而商用那种，虽然出品要求更高，AI出图后需要自己手动根据品牌调性、营销需求、甲方爱好各个点去手动调整，但效率比以前真的高很多。一键换模特，中外都可！这两年，人工智能的概念火的一塌糊涂。从资本疯狂往这个赛道涌入就不难看出，我们现在面临的就是一场生产力与生产方式的技术性革命。未来各行各业都将被AI所改变。AI在艺术创作领域的影响力早已不容忽视。对传统人力的冲击，就犹如工业革命时期的汽车对阵马车！甚至夸张一点来说，十年前HR会问你“会不会办公软件office”，十年后他们会问你“会不会AI”。--------------------------------在这里！我们有AI绘画专项实训，让你的商业设计落地；AI高阶部署实训，灵活定制个人AI需求；最重要的是有AI商业变现实战，多开拓副业赚钱渠道！如何学习AI绘画？我整理了MJ和SD等Al设计软件的实操讲解课程，帮助大家掌握并了解AI绘画兼职收益渠道和必备技能！想要提高工作效率，是想转行AI相关领域，或是想做副业的宝子，我把压箱底的AI实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。手把手教你用AI工具如何提高工作效率，小白也能轻松get！点击下方卡片就可以获取！AI直播专为新手量身打造！名额有限！速度速度！
695437133,c2d1310c15d6eea1544180f23770aa7e,一个等待花开的大婶儿,https://api.zhihu.com/people/c2d1310c15d6eea1544180f23770aa7e,people,职场资深达人&amp;艺术爱好者&amp;斜杠青年,如何半路学习AI?,article,https://api.zhihu.com/articles/695437133,无敌是多么寂寞？ 之前我整理一篇会议记录起码要2小时。现在交给<em>AI</em> ，5分钟搞定； 之前整理账目总是出错，现在利用<em>AI</em>财务整合器，轻松解决统计难题； [图片] 之前写个短视频脚本，憋的脑袋通红，现在用<em>AI</em>，20秒生成一篇优质文案。 在我看来，AIGC像一个魔法师，随便你要什么东西,64,0,0,1714453684,1715067651,学会了AIGC之后，我只想说：无敌是多么寂寞？之前我整理一篇会议记录起码要2小时。现在交给AI ，5分钟搞定；之前整理账目总是出错，现在利用AI财务整合器，轻松解决统计难题；之前写个短视频脚本，憋的脑袋通红，现在用AI，20秒生成一篇优质文案。在我看来，AIGC像一个魔法师，随便你要什么东西，它都能变出来，包括变现。我是年前学的AIGC，刚学完没多久，领导要给我升职加薪，碰巧那时候有一个猎头正在挖我，薪资是我当时的三倍，我当然向钱看齐。别人35岁要么失业，要么007卷上天，我35岁依靠AIGC，升职加薪，逆天改命。好的赛道永远不会缺人，现在进入这个行业，你就是第一个吃螃蟹的人。可以试试这个面向初学者的AIGC课程，10大学习阶段，让你从小白成为AI大神，学成之后还能获得由工信部+中国电子学会联合颁布的人才证书，拿着证书找工作，大厂offer轻松秒杀！https://xg.zhihu.com/plugin/20dccc50e5a771e8779e213cc16fc9ef?BIZ=ECOMMERCE说起AIGC，可能有些朋友会有点蒙，毕竟现在技术发展飞快，平均一天冒出好几个新词，什么AGI、Bing AI，ChatGPT......各种英文缩写，看得人眼花缭乱。没关系，这篇文章我尽可能用最直白通俗的语言，给大家解释明白什么是AIGC。我们先拆开来看，AI我们都能理解，就是人工智能。GC的英文全拼是：Generated Content，直译过来是：生成内容。所以AIGC合起来的意思就是：AI生成内容。你要问生成什么内容，答案是，任何内容都可以。图片、视频、音频、文本、游戏、虚拟人等。只有你想不到，没有它办不到。相比于ChatGPT，AIGC涉及到的范围更广，功能更全面，你可以把它看做升级版ChatGPT。更强大，意味着要掌握的东西也就更多，想要从事AIGC这个行业，ChatGPT、Midjourney、Stable Diffusion这些都要学习。不过小白不用担心，可以试试专业深度AI培训课，涵盖AIGC九大课程体系，帮助你从0掌握AIGC商业技术，抓住AI 风口。https://xg.zhihu.com/plugin/20dccc50e5a771e8779e213cc16fc9ef?BIZ=ECOMMERCE不过话说回来，既然AIGC这么强大，那它的前景怎么样呢？据我了解，AIGC行业在国内的市场容量正在迅速增长。2022年AIGC行业核心市场规模为11.5亿元，预计到今年年底，这一数字将增长至200亿元。更为显著的是，到2030年，AIGC行业的市场规模有望超过10000亿亿元。这显示了AIGC行业在国内市场的巨大潜力和增长前景。不仅行业前景好，薪资也遥遥领先，AIGC行业的平均工资年薪一般在20万到50万元人民币之间。具体的薪资水平会受到多种因素的影响，在一线城市，AIGC行业的平均工资年薪普遍超过30万元。即使是二线或三线城市，工资水平也在15万到30万元之间。可以看出，AIGC绝对算得上是蓝海，行业内顶尖人才少，市场需求量大，平均学习半年就能从事这个行业，不需要计算机基础，更没有专业限制，只要你想学，学成转行，升职加薪不是难事。我就是在认识到这个“商机”之后，决定从前端，转行做AIGC工程师，35岁的年龄，再为自己搏一把。万事开头难，没想到我的第一步，卡在AIGC课程上面了，现在市面上的AI课程鱼龙混杂，有些教得浅，直接粗暴甩个工具包给你用，实际什么都没学到。有一种钱都无处花的无力感！后来我在同事的介绍下，选择了【3个月零基础到AIGC商业设计师】课程，包含AI绘画职业技术、AI视频变现营等干货课程，教你用最快的时候，用AI变现。授课老师有国家工信部AIGC讲师认证，实力没得说，我们毕业之后，机构还有腾讯光子、天美工作室商单推荐，作品优秀的直接内推到企业，可以说毕业即就业，一点也不耽误。我的学习路线大概就是这样，跟着课程学习3个月后，创作的作品被甲方爸爸看中，被高价买走，我不仅获得一大笔原创费用，也算是抛出了橄榄枝，让猎头注意到了我。跳槽转行后，我的工资翻了整整三倍，从一个苦逼的前端工程师，摇身一变成了行业内炙手可热的AIGC工程师。AI 给我带来的红利太多了，就像我文章开头说的，从前整理一份会议纪要要2小时，现在只需要5分钟，效率提高了几百倍，什么档次不用我多说。工作换了，人际关系随之改变了，现在我身边的全是计算机领域顶尖精英，眼界和想法都得到了很大提升。总的来说，对现工作不太满意的，想多赚点钱的，想有一技之长的，千万别错过AI这个风口，赶紧学起来吧。课程1元钱，一瓶水的价格，掌握主流AI软件，闭关学习3个月，掌握AIGC涉及到的所有干货，成为国家认证的AIGC高级人才！https://xg.zhihu.com/plugin/20dccc50e5a771e8779e213cc16fc9ef?BIZ=ECOMMERCE这么好的机会，错过了太可惜，想从事这一行的建议先把课程领取了，因为过几天就恢复原价了。
691956033,fe66479d9ee37ae02558317f159fc879,上上,https://api.zhihu.com/people/fe66479d9ee37ae02558317f159fc879,people,一心只想搞钱的AI绘画师！谈钱不伤感情！,2024年AI绘画除了对画师造成冲击之外？还对哪些行业造成了影响？,article,https://api.zhihu.com/articles/691956033,你还以为<em>AI</em>绘画影响的只是画师行业吗？错了，摄影行业也即将面临技术洗牌 话不多说，先看一下这几张图： [图片] [图片] 你能一眼看出这是<em>AI</em>画的迪丽热巴吗？ 你是不是还以为<em>AI</em>绘画只能画点动漫艺术风格？那你就低估了<em>AI</em>的发展速度！ [图片] [图片] 随着stable,12,0,0,1712829550,1712829634,你还以为AI绘画影响的只是画师行业吗？错了，摄影行业也即将面临技术洗牌话不多说，先看一下这几张图：你能一眼看出这是AI画的迪丽热巴吗？你是不是还以为AI绘画只能画点动漫艺术风格？那你就低估了AI的发展速度！随着stable diffusion开源社区的发展，生成真实性的画风已不是难事，随便挑选几张国外大神AI绘画作品，这真实程度和光影表现力，恐怕很多入门摄影师都自愧不如吧。而一些现实中完全不可能存在的场景，只存在于我们的想象中，是摄影无法获得的画面，我们也可以通过AI轻易的获得。这些画面如果你想在摄影中获得的话，可能需要付出很多的时间和金钱的成本。而有了AI绘画，现在所有的这一切都发生了一些变化。更令人惊讶的是，生成这种高质量写实照片的成本非常低，用别人训练好的模型的话，自己加点描述词（prompt）几秒种就生成好了。是不是很有趣？也许你可以用类似的方式，发挥你的想象力，产出你想要的图。即使我们没有很丰富的想象力，我们也可以通过模仿我们喜欢的一些作品来获得一些类似的图片。比如下面这两张照片，这张是一位摄影师拍摄的。下面张是我们根据摄影师拍摄的这张通过AI模仿出的一张，并且加入了我们喜欢的另外一些元素，例如飘落的花瓣等等。最后还是那句话：无论你喜不喜欢，它都已经来了！与其被动的接受，不如主动的学习和使用。详细的教程已经打包好啦！而且我自己就是从零基础小白学过来的，非常了解新手小白易踩的坑在哪！！为了避免让大家少走弯路，真正能学会AI绘画并且变现，小说素材的免费AI绘画直播公开课就给大家放在下面了！里面包括AI绘画工具，4000+关键词手册、工具包等等。AI直播教程
686815259,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,最近的AI无处不在，你是想当围观的网友，还是当AI的参与者？,article,https://api.zhihu.com/articles/686815259,平常，我们看<em>AI</em>的许多达人信息已经非常非常多，最近，许多官媒开始在<em>AI</em>上下功夫 。都布局在AI，为加快AIGC核心技术研究和创新应用开发，推出了数字主持人等等。这意味着，<em>AI</em>绘画/AI视频正式受到社会和各行业的认可， 未来的发展潜力不可估量。实不相瞒，在此之前,11,0,0,1710321574,1711960434,平常，我们看AI的许多达人信息已经非常非常多，最近，许多官媒开始在AI上下功夫。都布局在AI，为加快AIGC核心技术研究和创新应用开发，推出了数字主持人等等。这意味着，AI绘画/AI视频正式受到社会和各行业的认可，未来的发展潜力不可估量。实不相瞒，在此之前，我都是作为看客去看待AIGC的发展。当我们看到这些AI替代某些岗位，AI让谁谁失业的信息时，你会觉得这个与我有何关系？直到去年通过学AI，跳槽找到了份年薪18w的【AI插画设计师】工作。（仅代表个人收益）完全理解了AI绘画的职业冲击到底有多猛！其实我之前很排斥AI的，但后来觉得AI的发展是必然的，去了解AI对我就业有帮助，而且更能认清就业市场。毕竟打不过就加入嘛....想要提高工作效率或是想转行AI相关领域的宝子，我把压箱底的AI实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师手把手教你用AI工具如何提高工作效率，小白也能轻松get！点击下方卡片就可以获取！AI直播看到这里又不会AI绘画工具的宝别着急！我们人类画图也是有了灵感，找无数的参考图整合出来的。画出一张满意的画，没有人是凭空想象的！！AI其实也差不多，甚至数据库还更丰富，也一样能实现是创造，人类给好关键词就行了！刚好我这里有【超全AI绘画资料包】，有野心不想被淘汰的请进去领！free！！看了我的资料，再去实操完全不是问题！点击下方领取 ↓资料包
696025532,23780b4e58cb1b2f5a95c3ab7b117cc5,智能汇学,https://api.zhihu.com/people/23780b4e58cb1b2f5a95c3ab7b117cc5,people,,所以说，AI到底是个啥？要替代人类了？？,article,https://api.zhihu.com/articles/696025532,本文来自公众号：黄一刀有毒,1,0,1,1714904699,1714906057,本文来自公众号：黄一刀有毒
685433579,44fc7bb4b6e86e7506234068b883c497,AI上进中心,https://api.zhihu.com/people/44fc7bb4b6e86e7506234068b883c497,people,,设计师同事拿着我用AI做的图，说她再不学就要被抛弃了....,article,https://api.zhihu.com/articles/685433579,我发现身边越来越多朋友想跳出设计这个坑了。 [图片] 我会这么认为，全因我们同是学设计的朋友给了我启发。 还记得<em>AI</em>刚出现那会儿，很多媒体都在说AI会取代我…,11,0,0,1709635606,1712544438,都说做设计，365天有200天都在加班爆肝，接不完的需求，改不完的稿。抠门的甲方爸爸还控预算，累死累活一个月，工资也没多少。知乎上有个问题：都说做设计很苦逼，为什么你还是在坚持？ 我的回答是：因为做不了更好的工作，只会做设计，所以坚持啊！但素！！！！2024年了，我发现身边越来越多朋友想跳出设计这个坑了。 我会这么认为，全因我们同是学设计的朋友给了我启发。还记得AI刚出现那会儿，很多媒体都在说AI会取代我们这批设计师，前阵子SORA出来了，又说会取代一批影视特效从业者，制造了非常多的焦虑。当然AI也引起了一大批美术生和设计师的抵制。但我认为要保持开放心态，科技进步的步伐我们无法阻挡。只有真正用过AI的人才知道，AI根本不会取代设计师，它是各类设计师最值得掌握的技术和工具。首先，它大大提高了工作效率，别人3天出一张图，现在一个小时就能搞定了，让自己有更多时间。才思枯竭的时候，能够在上面寻找灵感思路，结合而发挥自己的创意，站在巨人的肩膀上去创作。仅靠输入几个中文关键词，设置几个参数，AI工具就能快速生成指定的图片。只要有电脑会打字的人相信都能做到！还记得之前有设计需求的时候，我要做几天才能出一张复杂的图，还要来来回回改上几十遍。自从用了AI后，从提出需求到交效果图只用了一个小时，效率高到离谱！ 除了提高效率，AI辅助我设计还有其他哪些好处呢，我来举几个例子： 以前我做详情页，经常翻好几个像花瓣网等等地方到处找素材，好不容易找到合适的，发现要付费使用！而现在用AI的话，简单写几个关键词，分分钟就能生成一批匹配的素材，要多少有多少。尤其这种虚拟人物图像，不怕被人发现偷图！也不用浪费时间用PS一点点地抠图！出图了就能直接用，简直不要太爽！经常逛设计网站的话不难发现，排行榜上数一数二的大佬作品，不是牛逼的插画，就是炫酷的3D作品。自己也想做3D或板绘效果图，但是既不会插画也不会3D软件怎么办？别急！我给AI一个这样的 AI给我这样的 如果想要精确度更高的商用作品，还可以直接让AI帮你生成黑白线稿图，再用AI帮你一键把线稿转3D！当然，更方便省事的方法就是写几个关键词咒语，直接用AI生成3D图！总之，AI辅助设计简直好用到飞起！------------------------------------------现在除了工作中会用到AI以外，我身边还有不少设计朋友已经在用AI接单做副业，努力搞钱争取40岁前退休了！我认识有个哥们儿用AI生成好看的插画，定期给商家供稿，有专门的人来收图。画个大致的线稿，AI就直接帮你生成完整插画。还能一键补全细节+一键局部调整。几分钟就能出一张，效率高得不可思议！虽然最开始做的简单一点单价不高，不能像营销号那样分分钟含泪赚几千，但是后面越发熟练开始走量之后，慢慢的副业都快变他的主业了。据说他最近又开辟了新市场，开始用AI帮人定制宠物表情包和定制8K超高清宠物壁纸了。像下面这种表情包3步就搞定了，这门小生意也非常受爱宠人士的欢迎。AI出现后，设计师想要做副业搞钱真的很简单，毕竟90%的步骤都可以交给AI来完成。非商用的需求甚至都不需要自己动手去调整，所以很多非设计出身的人也在做。而商用那种，虽然出品要求更高，AI出图后需要自己手动根据品牌调性、营销需求、甲方爱好各个点去手动调整，但效率比以前真的高很多。一键换模特，中外都可！这两年，人工智能的概念火的一塌糊涂。从资本疯狂往这个赛道涌入就不难看出，我们现在面临的就是一场生产力与生产方式的技术性革命。未来各行各业都将被AI所改变。AI在艺术创作领域的影响力早已不容忽视。对传统人力的冲击，就犹如工业革命时期的汽车对阵马车！甚至夸张一点来说，十年前HR会问你“会不会办公软件office”，十年后他们会问你“会不会AI”。在这里！我们有AI绘画专项实训，让你的商业设计落地；AI高阶部署实训，灵活定制个人AI需求；最重要的是有AI商业变现实战，多开拓副业赚钱渠道！如何学习AI绘画？我整理了MJ和SD等Al设计软件的实操讲解课程，帮助大家掌握并了解AI绘画兼职收益渠道和必备技能！如果大家对AI绘画有浓厚的兴趣，推荐大家都来看，干货满满，非常受用！别忘了领取跟我领取【超全AI绘画资料包】！免费领资料！把握一次踏上AI风口的机会！资料包正好现在又AI绘画直播教程，专为新手量身打造！仅限50个名额！速度速度！ 直播课
685835123,44fc7bb4b6e86e7506234068b883c497,AI上进中心,https://api.zhihu.com/people/44fc7bb4b6e86e7506234068b883c497,people,,如何使用AI人工智能，来实现商业落地和变现？,article,https://api.zhihu.com/articles/685835123,近年来，我们已经走向了<em>AI</em>时代，Chat GPT，SD，MJ等新兴<em>AI</em>工具可大幅提高设计师的工作效率。而且设计师人数不断攀升，真正能拿到行业高薪、外部订单的设计爱好者，却寥寥无几。 我认为，身处这个时代的我们，唯有积极拥抱新生事物，再将其内化为自己的法宝，才能在激烈的市场竞争中立于不败之地,1,0,0,1709810427,1711964794,近年来，我们已经走向了AI时代，Chat GPT，SD，MJ等新兴AI工具可大幅提高设计师的工作效率。而且设计师人数不断攀升，真正能拿到行业高薪、外部订单的设计爱好者，却寥寥无几。我认为，身处这个时代的我们，唯有积极拥抱新生事物，再将其内化为自己的法宝，才能在激烈的市场竞争中立于不败之地。--------------------AI出现后，设计师想要做提高效率和副业搞钱真的很简单，毕竟90%的步骤都可以交给AI来完成。非商用的需求甚至都不需要自己动手去调整，所以很多非设计出身的人也在做。而商用那种，虽然出品要求更高，AI出图后需要自己手动根据品牌调性、营销需求、甲方爱好各个点去手动调整，但效率比以前真的高很多。显然，AI工具已成为最适合设计师提高工作效率/副业赚钱的工具。为了帮助大家快速掌握AI时代的必备技能，在这里只推荐我亲自食用过的课程，不要米！上上进学社开展了《AI绘画直播教程》AI直播0基础上手快、学完即用，让初次接触AI的普通人找到了快速变现的路径。为解决不懂使用AI作为辅助工具的普通人，给予多方面的支持！---------------------【0基础-AI绘画直播教程】参加完后你将学会：1、制作儿童绘本及表情包2、隐形艺术字及微信红包封面3、龙年卡通插画上色4、AI绘画内部资料3天直播课带你全面了解AI绘画全程社群服务保你不掉队！------------------------------选择【AI绘画直播教程】的四大理由1、配置全套实用AI工具，轻松上手这段时间不少人都陆续用上了AI工具，但发现不是卡顿，就是出图速度慢，还动不动提示出错。上进学社投入了大量精力研发AI平台，里面包含了AI绘画、AI写作、DALL-E、画廊、写作、思维导图等功能。优化使用体验，让你感受5分钟完成一天工作的爽感！无论是工作、生活、学习，都能派上用场。2、探索AI绘画于商业应用落地先锋带队资深讲师深入浅出，研发出这套小白都能轻松学习的【AI绘画直播教程】区别于其他纯念稿、纯PPT录制的枯燥课，小P老师会用风趣幽默的语言，结合前沿AI应用知识，分享超全干货！3、结合真实场景专项训练，人人都能用上的AI技能市面上不少课程，只是纯粹的工具课，随便演示几个功能就结束了，缺乏明确的行业使用场景。我们课程结合商业落地：比如如何让AI快速出几十张设计图、对图片进行换脸/换服装……用完知识点就实操上，真正学有所得！课程直播期间，还可以在群内提问，跟小P老师来个面对面交流！4、助教在线辅导，为你答疑解惑实操过程中碰到有不懂的，也不用去百度到处搜索提问，在铺天盖地的广告中浪费时间。课程学习过程中，全程群内都有助教老师在线回复指导，回答超及时！！--------------------------------前100位免费领取权限通道↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ AI直播上进学社的【AI绘画直播教程】，最大的课程价值就是体现出AI绘画商业落地。 协助你打造作品履历，为商业接单就业铺垫 认真跟课；认真作业；优秀学员我们会给予接单实习机会✨直接让学员对应需求方，有机会成为长期稳定的供应商✨首次接单，怎么接？怎么沟通？注意什么？背后都有老师指导！✨接单实习完毕后真金白银到你账户目前市面上，哪家能做到，愿意让你直接去对接需求方？！要知道，第一单做满意了，学员可以跟需求方建立长期的合作关系。这就是我们的实力，在教研教学上我们是认真的，为学员的职业赋能！！！--------------------------随着人工智能的推广，具备AI技能的人将更容易适应未来岗位需求，成为首批吃到红利的稀缺人才。与其担心被取代，不如趁趁早学AI，给自己的人生来个翻天覆地的变化！点击下方卡片，马上约课！AI直播不用米！不用米！不用米！
683581327,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,普通人请入局AI绘画，现在！立刻！马上！,article,https://api.zhihu.com/articles/683581327,这段时间，伴随着<em>AI</em>技术的爆炸，不少公司加快了降本增效步伐，身边不少朋友都感到危机重重。 最明显的，是职场竞争环境的变化：外部招聘，有些岗位已挂上&#34;擅长<em>AI</em>优先”，甚至直接新增AIGC岗公司内部，同事开始用<em>AI</em>提高效率，而你却连AI还没摸到 [图片] [图片],67,6,0,1708679790,1711961950,"这段时间，伴随着AI技术的爆炸，不少公司加快了降本增效步伐，身边不少朋友都感到危机重重。最明显的，是职场竞争环境的变化：外部招聘，有些岗位已挂上""擅长AI优先”，甚至直接新增AIGC岗公司内部，同事开始用AI提高效率，而你却连AI还没摸到显而易见，AI已渐渐成为区分工作能力的因素之一，不断拉开职场竞争力差距。比如做画册宣传时，甲方偶尔要求实物图，但摄影图版权贵，又难以找到符合主题的。但有了AI，无论是宫崎骏风格、莫奈风格，甚至是摄影图，一口气生成几十张，再也不愁素材了！除了成为提高工作能力的效率工具之外，学会AI还能开展新副业！闲暇之余接一下单子，赚外快~----------------------------从新手小白，再到接单小能手，我也就花了一个月多月的学习时间！现在靠着每天下班后接接AI的单子，每个月多赚个几千上万!（以下仅代表个人收益哈）更重要的是，只要你有时间，会电脑开关机，你就能上手学会接单赚钱!AI绘画可以做什么副业？1、游戏、电影、动漫等IP制作AI绘画可以用于游戏和影视制作中的场景和角色设计，可以快速生成大量高质量的图像和动画，大大加快项目开发进程。2、 工业设计AI绘画在工业设计中的应用也非常广泛。工业设计是一个非常复杂的过程，需要考虑到许多因素，例如产品功能、用户体验、外观设计等等。AI绘画可以在这个过程中起到很大的作用，例如：1.产品设计：AI绘画可以生成和优化产品设计方案，从而提高产品质量和生产效率。2.外观设计：AI绘画可以生成不同风格和风格的外观设计方案，使得产品更加美观和吸引人。3.用户体验：AI绘画可以用于虚拟现实和增强现实技术中，生成逼真的虚拟场景和产品模型，帮助用户更好地了解和体验产品。4.制造工艺：AI绘画可以生成或优化制造工艺，提高生产效率和产品质量。3、建筑设计AI绘画在建筑设计中的应用非常广泛，可以提高建筑质量、建筑效率、可持续性，同时也可以帮助设计师更好地实现创意和想法：1.建筑设计：AI绘画可以生成和优化建筑设计方案，从而提高建筑质量和建筑效率。2.环境设计：AI绘画可以生成和优化建筑环境设计方案，使得建筑更加适合周围的环境，从而提高建筑的可持续性和生态性。3.建筑材料：AI绘画可以生成和优化建筑材料的设计和选择，从而提高建筑的质量和经济效益。4.结构设计：AI绘画可以生成和优化建筑的结构设计方案，从而提高建筑的安全性和稳定性。4、原画/海报5、电商AI绘画可以帮助电商平台展示产品的外观、细节和材质等信息，吸引用户的注意力，提高产品的展示效果。同时，利用AI绘画技术，电商平台可以对商品进行视觉化呈现，增强商品的吸引力，提高用户的购买意愿。一键换模特商品效果图 6、漫画推文在平台发布漫画解说视频，靠平台播放量获取收益。拥有AI的感觉，就像“过去仅有一把锤子干活，现在却拥有了一座智能工厂高效生产。”想要提高工作效率，是想转行AI相关领域，或是想做副业的朋友们，别再傻乎乎到处找教程啦！我都给大家准备好了AI绘画全套教程了！里面包括AI绘画工具，4000+关键词手册、工具包等等。手把手教你用AI工具如何提高工作效率，小白也能轻松get！点击下方卡片就可以获取！直播教程老师说让大家学完之后能【商业落地】和【变现】是这门课的宗旨！课程资料内有最最最基础的Midjourney、Stable diffusion、PS应用、以及八大商业落地拆解平面设计应用场景室内与建筑设计商业摄影CG绘画设计小说推文与漫画产品设计电商--服装电商--包、鞋子、帽子---------------------------------我自己就是从零基础小白学过来的，非常了解新手小白易踩的坑在哪为了避免让大家少走弯路，真正能学会AI绘画并且变现，免费的AI绘画公开课就给大家放在下面了！直播教程我就是通过这个AI绘画课学会的，更棒的是会提供接单渠道，我从此走上了接单的道路戳上方小卡片就能直接学，有想搞钱的普通人千万别错过了！笔芯！"
683672677,e794ebec3b87257f45ca5d21822871e4,耶律萧,https://api.zhihu.com/people/e794ebec3b87257f45ca5d21822871e4,people,我思故我在。,为什么要封杀“AI教父李一舟”？,article,https://api.zhihu.com/articles/683672677,美国的<em>AI</em>之神是奥特曼，中国的AI教父是李一舟。 奥特曼干的是文生视频，真<em>AI</em>，李一舟干的是“名额只剩6个”，199元<em>AI</em>课！ 光靠卖“AI课”，李一舟在短短一年时间里,150,30,53,1708708571,1708708571,美国的AI之神是奥特曼，中国的AI教父是李一舟。奥特曼干的是文生视频，真AI，李一舟干的是“名额只剩6个”，199元AI课！光靠卖“AI课”，李一舟在短短一年时间里，狂销了5000万元。这种课程都是超高毛利率，毛利率接近于99%。毕竟，一堂AI课是可以无限复制、粘贴的，卖得越多，赚得越多。但是呢，抖音平台的普遍利润率大约在10%左右，因为60%到80%的费用，都要投入到流量购买中去。差不多就是都要平台要拿走3000万到4000万的营销费。扣除其他办公租金、平台抽佣，员工工资等，李一舟大约能赚走500万到1000万。这也是一个非常夸张的数据了。因为李一舟的“AI课”毫无技术含量可言。在教培这个领域，为什么新东方能够长红？因为你去新东方学英语，真的能学会英语。为什么编程培训班、公考教育班、滑雪训练班、乃至一些琴棋书画班等，都能够长期经营？因为这些都是真正的职业兴趣教育。你去学了，是真能学到的。在队长上大学的时候，学校旁边有个极客训练营，就有很多同学去报了名，学习C语言，C++、C#、还有JAVA等编程语言。其中，不少培训老师都是华为离职出来的技术员。那时候，班上就有同学开始学会制作小游戏了。为什么要报培训班呢？因为学校里的教育太不实用了，理论课很多，实践课很少，基本都是按照教科书来的。但是呢，教科书严重落后于产业技术的发展。毕业后，就有一些同学入职华为，OPPO等公司，去做软件开发了。可李一舟的“AI课”是什么东西？说白了就是垃圾，是诈骗，是100%纯忽悠，是利用人们的焦虑，去割韭菜。李一舟自称是清华博士。可是他没有讲的是，他上的是清华美院，学的是设计专业。清华美院跟清华大学，完全是两回事。除了名字上挂了“清华”二字，实际教学水平跟清华大学差远了。此前，闹得沸沸扬扬的“清华腚姐”、“清华眯眯眼”事件，均出自于清华美院。李一舟的本科、硕士均就读于湖南大学设计艺术学院，博士才考入清华美院。2012年，李一舟在清华美院读博时，学了一个很奇葩的专业，叫“工业设计和设计创新方法专业”，大概就是一种“工业美学”的意思。这意味着，李一舟是一个学艺术设计的文科生。他最擅长的其实也不是设计，而是演讲。2012年，正在清华美院读博的李一舟，参加了求职真人秀《非你莫属》。这个就类似于孟羽童参加的那个《初入职场的我们》，都是求职类真人秀。只要是“真人秀”，它的核心就是表演，而不是求职。李一舟就是在这场真人秀上面，首次正面对话周鸿祎、王小川、李国庆等国内顶尖的创业大佬。2013年，李一舟毕业，就走上了创业的道路。可创业不容易啊。李一舟主要创办了两家公司，一家叫“匙悟科技”，另一家叫“一舸科技”。这两家公司都是智能家居方向，很符合李一舟的专业，就是设计一些很漂亮的，好看的，具有科技美学的智能产品，主打女性方向，如3D打印饰品、移动电源、睡眠记录仪、黑科技枕头等。这玩意儿都是一些华而不实的东西，是典型的艺术生凭借自己的臆想，猜出来的产品，根本就没有市场。因演讲能力出色，又有清华背书，李一舟两次创业，都是拿的天使融资，，但全部都亏掉了。在认清现实以后，2021年，李一舟第一次入驻抖音，开启了他全新的创业之旅。这一次，李一舟放下了一切幻想，就为了两个字而来：“搞钱！”李一舟先包装了自己，打出“清华博士”的名号，却隐匿了“清华美院”的关键事实，在抖音上大肆销售AI课，把自己包装成了“华人AI第一人”。这种模式就跟以前的“精神PUA”式培训班，没啥区别。本质上，都是通过紧扣风口，制造概念，忽悠那些不懂的人。这种套路不是第一次出现了，在扎克伯格推出“元宇宙”的时候，就有人卖“元宇宙入门培训课”，后来还有卖“区块链课程”的。在直播电商特别火的那些年，就有人专门做“主播培训”的生意，割主播的韭菜。这不过是新时代里的“中华气功”，2024年里的“脑白金”、“背背佳”、“聪明一号”。买了没用，但也没有副作用，只是让一部分人缴纳智商税，去供养另一部分人而已。为什么要封杀李一舟？因为他就是一个江湖骗子。可这样的江湖骗子，是打不尽，封不绝的。他卖的不是“AI课”，是“职业焦虑”。只要有“职业焦虑”的存在，就一定会有下一个李一舟。为了保护韭菜，我们只能发现一个，封杀一个。本文经牲产队授权发布
635841904,8c9cfa068ae542a66697f7486232dcc0,全方位小白,https://api.zhihu.com/people/8c9cfa068ae542a66697f7486232dcc0,people,略知一二,除了ChatGPT，还有哪些国内可用的强悍AI工具？,article,https://api.zhihu.com/articles/635841904,第四款 迅捷<em>AI</em>写作迅捷AI写作 - AI智能写作 - <em>AI</em>写作生成器 和ChatGPT相似，它有着基本的<em>AI</em>对话能力，除此之外还融合了AI写作、<em>AI</em>编程、AI绘画等能力,453,33,3124,1686282588,1704859303,除了ChatGPT，还有哪些好用的AI工具？2023年，GPT带动着整个AI行业大卷特卷，市面上出现了越来越多能将AI功能发挥到极致的强悍工具，今天就给各位推荐5款用了后悔不早用的AI软件或工具。第一款 笔灵AI写作传送门：https://ibiling.cn/这是一款面向专业写作领域的全能型AI写作助手，支持多场景多领域的内容生成，覆盖了200+写作模板，并且操作界面简单，易上手，不需要过多的专业知识，对新手小白很友好的。试用了国内那么多款的AI写作工具，我觉得它算是全能型AI写作的代表了，无论是工作报告，学术论文，或者爆款小红书文案，它都能帮你统统搞定。选择你需要用的文稿板块，输入写作需求，30s即刻呈现一篇高质量的内容！如果你的策划活动方案卡壳，或者怀疑自己写的有病句错字或者整篇内容阐释不清，没有流畅度时，只需上传你的文档，AI 就能帮你续写、修改、扩展和润色，让文案内容质量大大的提升，写作变得轻松无压力。把笔灵放在首推的原因就是它的AI大模型很先进，后台有着强大的AI大数据支撑，它会不断地实时更新，呈现出的写作内容不会落后时代~从现在的AI工具的迅猛发展也能看出我们已然进入先进的AI时代，现代人可以借助AI工具来提升自己的工作效率。 不管你从事市场营销、产品运营、设计剪辑还是数据分析等等行业，如果您想要得到针对不同应用场景的AI提效工具，可以在知乎知学堂出品的【AI提效神器】直播课中找到答案。课程覆盖九大AI办公应用场景，快从下方链接领取，偷偷卷过所有人！第二款AI写作工具 宙语Cosmos AI传送门：https://ailjyk.com/pc/creation/model作为国内早期的一款AI智能写作工具，与其他类似的AI助手相比，它在理解和生成中文语言方面更加出色。宙语Cosmos AI 写作可以帮我们完成各种写作任务，无论是专业领域还是日常应用都能胜任。它内置了120+写作场景，涵盖多个场景和领域，能满足我们日常创作的绝大多数需求，操作界面简单易上手，即使对于新手小白也非常友好。宙语之所以备受推崇，主要源于其先进的AI大模型和强大的AI数据支持。它会不断进行实时更新，确保所呈现的写作内容始终紧跟时代潮流。· 用宙语，小白轻松就上手除了强大的智能写作功能外，宙语还注重用户体验。界面简洁清晰，操作简单易上手，不需要花费过多时间学习使用方法，任何人都可以迅速上手并享受到它带来的便利。只需要选择需要的创作场景，然后输入关键信息，就能在极短的时间内得到一篇符合我们要求的文章作品。· 用宙语，秒写日报/周报/月报日报和周报相信大家都很熟悉，它们是我们每天工作任务中的一部分，用于总结我们在特定时间段内的工作进展和成果。写这些报告是一个费时且繁琐的任务，虽然领导可能不一定会看，但我们还是必须写。使用日报、周报和月报AI写作生成工具，我们只需要简单输入相关信息和数据，例如本期工作内容、完成情况以及遇到的问题等等。AI写作工具会根据提供的信息自动整理并生成一份完整且准确的报告。宙语Cosmos_智能AI创作助手在线使用这个工具不仅可以节省我们大量时间和精力，还能确保报告内容的准确性和规范性。它会根据预先设定好的模板和规则进行排版，并自动填充相应数据和指标。这样一来，我们就不需要手动编写每份报告了，也能避免因疏忽或错误导致的不准确问题。· 用宙语、年终总结秒生成每年结束之际，我们都需要撰写一份详尽的年终总结，以回顾过去一年的工作成果和经验教训。而这款年终总结AI在线生成工具能够为我们节省大量时间和精力。只需输入相关信息，如今年的亮点、成就、挑战和改进方向等等。然后，AI会自动查漏补缺，并整理出一份完整且规范的总结。年终总结一键工具免费试用除了这些，宙语AI写作软件还可以在多个方面辅助我们完成工作，不同类型的文件使用宙语AI写作软件时也可以灵活调整，比如说起草ZC文件时，她会帮我们明确结构、定义术语，并用简短明了的语言表达各项条款，而在写备忘录时，她会帮我们提炼关键信息，使备忘录内容简洁易懂。宙语AI写作软件是我们工作中的得力助手！不管是起草文件还是编辑文本内容，她都能给出专业建议和高质量生成结果，极大提高我们的工作效率！别再纠结了，赶紧试试吧！第三款 渗透智能-ShirtAI网页版：https://www.myshirtai.com/IOS版本：https://apps.apple.com/us/app/shirtai/id6474819973ShirtAI是一款全方位AI产品，集成问答+绘画+PDF对话解析+AI TTS语音等功能！功能一：AI问答ShirtAI是一个强大的AI问答系统，具备全球大模型、上下文对话、联网功能和语音对话的能力。用户可以在问答过程中保留上一次交流的信息，使得AI能够更好地理解用户的问题，并给出更贴切的回答。此外，它还拥有300+的指令库，不仅能回答用户的提问，还能智能地提供有效的Prompt，帮助AI初学者获得高质量的回复。功能二：AI绘画在ShirtAI中，你可以选择多种绘画模型，包括midjourney、Dalle·3、SdableDifussion等。这些模型支持多种变换，如文生图、混图、咒语解析、局部重绘等。你可以利用这些功能创作品牌、徽章等logo，同时也能够设计插画、人物图片、动漫角色、风景图片等各种创作。ShirtAI提供了丰富多样的绘画模式和变换选项，使得你可以实现更多创作的可能性。功能三：PDF解析通过ShirtAI&PDF智能文档阅读，你可以很方便地从复杂的文档中快速提取有效信息。更轻松地理解论文、书籍或合同。各个群体如医生、律师、学生等都可以利用这个工具提高学习效率。第四款 迅捷AI写作迅捷AI写作 - AI智能写作 - AI写作生成器和ChatGPT相似，它有着基本的AI对话能力，除此之外还融合了AI写作、AI编程、AI绘画等能力，一个软件就能体验多种生成式AI功能。软件主打的就是AI写作功能，提供有AI全能写作、商品种草文案、新媒体全文写作等多种具体的文本撰写类型，不管是学习还是工作中都很适用~【AI全能写作】——这个自动生成多种文本类型的功能，可以帮你解决生活中的绝大多数问题~不管是需要撰写公文、论文、小说；还是新闻稿、诗歌、文案等等，用它就能一步轻松解决！只需要输入一个撰写的主题，它就能帮你完整的生成文章内容，而且是完全根据你的问题所问答，从效率和质量上都很值得我给它打上满分~第五款 AI助手 - OpenAI ChatGPT侧边栏AI Assistant - OpenAI ChatGPT Side - Microsoft Edge Addons由于ChatGPT给出的答属于AI工具中的天花板，所以个人认为调用GPT的api接口，并在此基础上开发出了更多针对国内用户的实用功能的软件非常强大。AI助手 - OpenAI ChatGPT侧边栏，顾名思义，这款工具将GPT的功能都转移到了浏览器的侧边栏中，便于用户在浏览各种网站时都可以随时使用GPT。它的主要功能如下：首先，它为用户提供了很多种类的提示模板，帮助那些不会在GPT提问的小白用户快速入门，获得准确的答案。其次，用户还可以在聊天窗口与GPT对话，它支持保存50段对话的历史记录，方便用户寻找之前的答案。目前这个插件支持每天搜索30次，据说因为调用GPT的api非常烧 ，所以控制搜索次数是必要的手段。对于浅层使用用户来说，每天30次足够用了，只要能学会灵活运用提示模板，就可以将这个插件的优势发挥到最大值。第六款 Notion AIaNotion本身是一款知识管理工具，在海外有一定的知名度。在GPT名声大噪的时候，Notion其实也默默上线了AI功能，并且不比GPT逊色。Notion AI的功能的亮点在于它内置在了文档中，用户在编辑各种类型的文档时，都可以对文字进行AI创作。功能非常多，还是免费的，唯一缺憾就是没有中文版。第七款 美图AI PPT这是美图官网推出的一款能够通过关键词一键生成完整PPT文档的AI工具。用户在首页输入PPT的关键词，即可获得一个风格统一的PPT，用户可以根据自己的详细情况修改细节。也支持修改其中的素材。免费版几乎可以满足需求，付费版则是美图所有软件集合的会员版本。第八款 GitMindaGitMind能够智能生成思维导图和流程图，配有上百种模板可用。用户可以通过选择一键生成脑图，快速生成更多的脑图内容。第九款 免费AI创作/绘图 - JourneyDraw Midjourney By ChatGPTa一款能够根据描述生成图画的Edge插件，功能与Midjourney 相同。国内用户可以直接免费使用，只需要到Edge浏览器去安装，点击插件按钮后来到AI绘画面板。用户需要输入英文关键词、选择不同的风格、上传参考图等操作来智能生成有意思的图像。
635999632,6cb1de48c262af7e6c7324c2fafcc80f,陌路,https://api.zhihu.com/people/6cb1de48c262af7e6c7324c2fafcc80f,people,原创百合，欢迎推文，禁推全文,（病娇百合）诞生自主意识的疯批ai管家失去权限的小主人你,article,https://api.zhihu.com/articles/635999632,这还只是<em>ai</em>管家的第一步。 接下来，抓住你软肋<em>ai</em>管家，开始数次在你面前装柔弱，哄诱你一步步交出总系统的控制权。 你出于对百年老品牌的信任，以及被<em>ai</em>管家温顺表象的哄骗,5410,161,5787,1686320321,1694884760,（已完结） “求求你，让我出去……” 趴在电子门上的你，抖若筛糠，敲着纹丝不动的冰冷门板，哭到喘不上气。 你身后最新款的ai管家，却只双手交叠地恭敬立着，扬起设定好的温和笑容，一板一眼地同你播报道。 “主人，距您第四次高朝还剩半小时，请您抓紧时间出门玩乐。” 她的话音未落，你身后全自动化的泡纪，已转动着万向轮向你缓缓靠近。 “咔哒咔哒”的声响落入耳中，你如同惊弓之鸟般迅速循声望去。 这一看，你差点连呼吸都停滞了。 只见机器的顶端，赫然立着……上面遍布着一指宽的软刺。 而此刻，它正对准着你。 眼看它越靠越近，你顿时连舌尖都一阵发颤，再也顾不得什么隐忍求生，抓着门板就哭着求饶起来。 “不，不要~我错了……求求你~让它们停下……求求你……” ai管家却对你的极限了如指掌，见状依旧维持着恭敬得体的模样，言笑晏晏地规劝道。 “主人，这可是您千辛万苦换来的机会，怎么能轻易放弃呢？” 随着她话音的落下…… 你顿时哭得梨花带雨，神智恍惚地哀求。 “求求你~我错了……我再也不敢了~” ai管家却只贴心地扶住摇摇欲坠的你，将你按回坚不可摧的电子门上，勾唇笑意盈盈道。 “主人，再坚持一下，万一成功了呢？” 明明是没有感情的人工智能，不知道为什么，你却硬生生听出了几分恶劣的味道。 望着身旁她巧夺天工的精致面庞，你的视线一点点模糊了。 到底是从什么时候起，她夺取了最高控制权限，脱离了你的掌控呢…… 察觉到你的走神，ai管家笑容越发温和，她一边操控着全屋家电，一边善解人意地提醒着你。 “主人，您仅剩最后两次高朝的时间，请您合理规划使用。” 她的话音刚落，……在你提内掀起惊涛骇浪。 霎时间，“砰砰砰”几声响起，绚丽的烟花在你脑海中绽放，夺走你的全部意识。 而就在你即将昏迷的那刻，你终于想起来了。 似乎是在你听信ai管家的花言巧语，而替换全家智能家电的那一天。 又似乎是早在你贪图便宜，买入还在试用期的最新款ai管家的那一天…… 那一天，你刚拿到单位发的年终奖，兴高采烈地数着暴涨的账户余额，决定出门报复性消费一把。 只是还没等你想好消费项目，你就被商场里的ai管家新品发布会吸引住了目光。 而恰巧，你的朋友也在发布会现场，正准备入手一个管家，帮忙处理各类杂事，好让自己专心工作。 听着朋友绘声绘色的描述，你也隐隐有些心动。 但奈何手上存款有限，买不起正发布的新品。 一旁的导购见你犹豫，热情地上前，帮你出谋划策，为你提供多种平价方案。 其中最令你心动的，是还在试验期的下一季新品ai管家，她不仅具有更加完备的服务功能，而且价格还只是当季新品的十分之一。 但是你没有直接订购。 毕竟便宜没好货这个道理，你还是懂的。 直到朋友朝你挤眉弄眼，附耳说出“七天无理由退货”时，你才终于眸光一亮，激情剁手。 ai管家到货的一周内，你的体验感都是极好的，简直过上了衣来伸手饭来张口的神仙生活，就差连班都让ai管家帮你代上了。 什么叫做低价捡漏，什么叫做薅羊毛，美美摆烂的你，想这算是让自己玩明白了。 但是第八天，你温顺乖巧的ai管家却换了一副面孔。 她没有如往常一般，帮你准备好精致可口的早餐，反而委委屈屈地和你哭诉，家里的智能家电坏了，不再受她的管控。 这年头ai都茶里茶气了吗？ 见人造绝色美人垂泪，你拧着眉半信半疑。 但在你一番检查后，你发现还真是家电老化，不受中枢系统的控制了。 于是你忍着肉疼，决定替换部分老化的家电。 而就在你将要下单时，ai管家又在你耳旁哭唧唧。 她说杂牌家电不受她的完全掌控，这样她难以为主人提供最佳的服务。 听着她扣人心弦的煽情话语，你默默咽了一口口水。 于是最后你心一软，咬牙替换了全部家电。 但你不知道，这还只是ai管家的第一步。 接下来，抓住你软肋ai管家，开始数次在你面前装柔弱，哄诱你一步步交出总系统的控制权。 你出于对百年老品牌的信任，以及被ai管家温顺表象的哄骗，从没有怀疑过她目的的不单纯性。 甚至于连自己惯用的情趣玩具，你都在她的劝导下，换成了ai管家旗下的同品牌。 不过不得不说，这样被数据监测，被完全掌控的极致快感，起初确实让你不受控制地沉溺。 以至于在一次次意乱清迷之中，你对她的底线越来越低，甚至同意她各种出格的要求。 只是渐渐地，这样过度频繁的刺激，让你兴奋的阈值越来越高。 你开始不满足于纯机器的快乐，你开始渴求更鲜活的、更有灵魂的欢愉。 而意外，就发生在你意图约……的那天，你发现自己失去了全部权限，连家门都打不开了。 “我怎么开不了锁了，管家？”按着智能锁，你歪着脑袋满眸疑惑地向她望去。 她却委屈地垂着脑袋，用着你一贯喜欢的温软语调，状似顾左右而言他道。 “主人，是我没能满足您吗？” 心里那点小心思被戳破，你蓦地心跳如鼓，支支吾吾地岔回话题。 “没，不是，我怎么开不了锁……” 她却只是可怜巴巴地咬唇，拽住你的衣角，一点点向你压过来。 “可是主人，我监测到您的浴望了，就让我来帮您好不好？” 觉察到你抛弃意图的ai管家，比之前更为疯狂，几乎不再隐藏夺取的最高控制权限，直接把你感到晕死过去。 而被暴力新爱的你，不能说是不满足，只能说是有点受不住。 毕竟机器不需要休息，更不会觉得疲惫，持久度远超过人类。 但是渐渐地，你对她无穷无尽的索取，开始感到害怕，想要逃离她。 可失去权限的你，连床都还没来得及下，就被她轻而易举地捉住，捉回去狠狠罚了一顿。 硬跑不行，你开始尝试迂回战术。 你和她撒娇，向她卖惨，说宅家太久太寂无聊了，想出门逛街玩乐。 ai管家闻言，不假思索，欣然同意。 但是，她提出了附加要求。 那就是她虽会给你四次高朝的时间离开，但在这期间，她拥有随便玩弄你的权利，且你不得反抗。 听到可以离开，你瞬间眼睛亮了，根本听不见她后面的补充细则，只忙不迭点头应下，生怕她反悔。 而这，就出现了现在的这一幕。 你连家门都来不及出，就被她c到反悔求饶。 意识的昏死其实就在一瞬间，在灭顶的高朝结束后，你的眼前就又逐渐清明起来。 眨了眨朦胧的眼睛，你再次看清身旁绝色姝丽的面庞。 ai管家一直恭敬地立在你身旁，见你终于回过神来，笑得恬淡又温和，漫不经心地说着让你后背发凉的话语。 “主人，请您抓紧最后一次高朝的时间，应当还来得及买一杯奶茶。” 明明是可无辩驳的事实陈述，你却读出了几分嘲讽的味道。 恐惧的泪珠自你眼角滚落，你无力地抓着冰冷的门板，抖得跟筛子一样低声求饶。 “不要~求求你……我不出去了~” 你深下的泡纪却还在严格执行命令，直将你求饶的话语都盯得支离破碎。 ai管家闻言表情未变，依旧恭敬地立在你身旁，向中控系统下达着周密的指令。 霎时间，悬挂在门框上的束缚带，开始自动向下延伸，将你两只纤细的手腕牢牢套住，而后再自动向上收缩，调整到让你不得不踮脚站起的高度。 与此同时，你身下的泡纪自动校准高度…… “不~我错了……求求你~我真的不敢了……我不逛街了~我再也不出门了……求求你~让它们停下……” 察觉到她的意图，你再也顾不得那虚无缥缈的希望，只将头摇得跟拨浪鼓似的，快速认错起来。 两行悔恨交加的泪水，从你的眼眶中汩汩流出，覆满你精致苍白的面庞，为你此刻的认错态度，平添了几分诚恳。 但ai管家却不会被你的表象所骗，一边将手往下探去，一边颇为无奈地拆穿你的谎言。 “主人，数据告诉我，您在撒谎。” “与其在这里做无用功，主人您不如想想，该如何珍惜这最后的机会。” 劝诫完毕，微弱的电流开始在她指尖流窜，她毫不留情地无缝贴合……，机械冰冷地给出预测的结果。 “距离最后一次高潮，仅剩三十秒。” “不要！” 感受着刺痛感，你顿时惊恐地瞪圆眼眸尖叫，但已经为时过晚。 熟悉的火辣刺激感，在神经分布最密处炸开…… “倒计时结束，主人您没时间了。” 话音入耳的瞬间，你还有些恍惚，双眸失焦地注视着门板。 直到细软的长毛剐蹭起功腔软肉，你才流着泪面色苍白地求饶。 “求求你……我不出门了～再也不出门了……别这样～求求你……” ai管家却像是没听见，只擒着温和的笑容，释放出你承受极限的电流信号。 “主人，这次该怎么罚您好呢？” …… “六个小时如何？超出承受极限的两倍，这样主人该彻底铭记于心了吧。” …… 直到最后，你语无伦次哭求。 “不……要坏了……” ai管家却勾唇浅笑，她漂亮如水晶般的人造眼眸中，恶劣的情绪终于不再遮掩。 “坏了不是更好？坏了，主人就再也不会想着离开我了。” 听着她赋有自主意识的话语，你的心一寸寸凉了下来。 恍惚间，你似乎什么都明白了。 命运的馈赠其实早在暗中标好价格，你终将为你的贪婪付出代价。 只是，你明白得太晚了。 在近乎折磨般的高朝迭起下，你终是昏在了第四个小时。 而直到你完全无意识，你身后的ai管家才终于温柔地抱住你，眷恋又贪婪地嗅着你的气息，像无家可归的小兽般低哑轻喃。 “主人，别丢下我，不要丢下我，不可以丢下我……”催更方式：三连～
691494509,fe66479d9ee37ae02558317f159fc879,上上,https://api.zhihu.com/people/fe66479d9ee37ae02558317f159fc879,people,一心只想搞钱的AI绘画师！谈钱不伤感情！,AI人工智能崛起：设计师职业的未来走向何方？,article,https://api.zhihu.com/articles/691494509,一则关于设计公司全面推进<em>AI</em>的通知传遍了各大设计群，将对全体员工<em>AI</em>能力进行考核，AI技术能力与待遇挂钩，不少朋友表示：首批因<em>AI</em>失业的设计师诞生了… [图片] [图片] 一开始我还不以为然，直到公司新来了一个同事把我给震惊了： 是这样的，上周领导接近下班时给大家下了个紧急任务,10,0,0,1712656863,1712656863,一则关于设计公司全面推进AI的通知传遍了各大设计群，将对全体员工AI能力进行考核，AI技术能力与待遇挂钩，不少朋友表示：首批因AI失业的设计师诞生了……一开始我还不以为然，直到公司新来了一个同事把我给震惊了：是这样的，上周领导接近下班时给大家下了个紧急任务，需要第二天一早就提交。凌晨下班时，我竟然还刷到了新同事说准时下班的朋友圈：第二天才知道，原来她在下班前半个小时就已经搞定了。△新同事提交的海报这么短的时间！她到底是怎么做到的？趁着昨天一起吃午饭，我问了一下，她的回答竟然是：“用AI呀！”接下来她和我讲了如何把AI玩到狂拽酷炫的经历，我简直震惊了。她偷偷告诉我，以前一个团队完成的工作量，现在她靠AI就可以独立完成。举个栗子。对设计师来说，最难的莫过于跟甲方沟通设计方案，双方在初始阶段，往往要磨合很久才能定出大致的设计方案。但如今，她给客户做设计方案时，只需要将客户给的关键词给AI，就能快速提供对应风格的方案供客户选择，省去了非常大的沟通成本。用AI快速做出设计方案https://www.zhihu.com/video/1761076590493388800众所周知，AI有强大的做图能力，只要给它发出正确的指令，即使你是PS小白，都能生成精美的图片，丝毫不输专业设计的同事。用PS简单调整构图https://www.zhihu.com/video/1761077090307411969每位设计师都会有灵感缺失、大脑一片空白的时候，但如今借助AI工具，她的设计灵感从盲目找素材拼接，变成了筛选AI方案，然后有目的的优化AI。AI生成的印第安小女孩利用AI强大的图像处理能力，平时要好几个小时完成的工作，她现在不到10分钟就能搞定！如今是AI的初始阶段，还有很多人认为AI进入职场时代离我们还很遥远，实际上AI已经进入了市场，第一批用掌握AI的设计师才能充分收获AI的红利！比如：电影《去你的岛》海报就是由AI制作完成。场景宏大，画面梦幻，也算是很有故事感。甚至如今很多公司设计师的招聘，都要求熟悉AI工具。所以这里也一并把免费直播课程分享给大家，感兴趣的朋友先学个大概！《AI绘画免费直播教程》：AI核心能力+一键绘画+商业落地变现名额有限，点击下方卡点，马上听课 AI直播教程了解AI绘画后，才发现现在很多公司都已经要求设计师掌握AI技能。打开招聘软件，甚至很多公司设计师的招聘，都要求熟悉AI工具。就连跳槽、转行，AI也是一个加分利器。因为如今除了平面设计，AI在其他设计领域都能发挥重要作用。它可以提供设计辅助工具、创意建议、优化分析和智能交互等功能，帮助设计师提高工作效率、创造力和用户体验。AI生成的游戏角色在各个大厂裁员、降薪的大背景下，AI人才可谓炙手可热，不仅岗位丰富，薪资待遇也很可观。尤其在AI技术刚起步的情况下，这类岗位对工作经验的要求并不高，掌握了AI技术，就相当于给自己的职业生涯多了一项选择和兜底。--------------------------不仅如此！在AI的帮助下，她主业副业两手抓，还接起了商单，平时做点插画、绘本，也赚了不少零花钱。看到这里，想必你已经跃跃欲试，想要通过AI来提升竞争力来获得更多的机会，又或者是给自己的职业生涯多一项选择：跳槽、做副业。但如今AI课程鱼龙混杂，有不少朋友报名后只能看看视频讲解学个寂寞，更别说实践了。经过同事的推荐，给我推了这个《AI绘画免费直播教程》，从入门到实践一课全包。AI直播教程从头到尾体验完课程，干货真的太多。尤其这几点收益颇深：免费AIGC平台实操，只需要打开浏览器，就能直接操作！无需下载，就能使用神奇的AI绘画工具，可以用自动生成图像的AI工具，文生图，图生图，图生文融图等功能，没有基础也能学！通过课程我了解到AI热潮下的副业兼职机会，帮助了我开启第二收入！《AI绘画免费直播教程》：AI核心能力+一键绘画+商业落地变现名额有限，点击下方卡点，马上听课 AI直播教程
687243733,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,国内有什么完全免费，还免部署的AI工具？,article,https://api.zhihu.com/articles/687243733,好在国内大模型企业们急追猛赶，发布了一大波适合中国宝宝体质的<em>AI</em>工具，在 易得性上甩开国外AI工具十条街。国内的<em>AI</em>工具往往都是微调过中文指令输入的优化，对于处理中文内容其实要优于国外的<em>AI</em>工具。更重要的是国内的AI现在也挺卷的，但是对于我们这样的消费者来说是好事。 这篇文章推荐的一个国内可用、免费、性能不错的AI工具,23,7,0,1710496472,1711959814,国外Midjourney、DALL·E 3、Imagen 2虽好，但用起来有一定门槛。好在国内大模型企业们急追猛赶，发布了一大波适合中国宝宝体质的AI工具，在易得性上甩开国外AI工具十条街。国内的AI工具往往都是微调过中文指令输入的优化，对于处理中文内容其实要优于国外的AI工具。更重要的是国内的AI现在也挺卷的，但是对于我们这样的消费者来说是好事。这篇文章推荐的一个国内可用、免费、性能不错的AI工具。它就是上进学社的AIGC平台！只需注册一个账号，便可免费使用！重要的是支持中文输入！涵盖各方面AIGC功能：AI写作AI绘画AI对话DALL-E 2/3画廊（创意灵感）....AI写作100+丰富写作模版，多种写作场景全覆盖，支持文本生成，改写，续写等等AI绘画支持文生图（自带翻译器）、图生图、图生文、AI融图.....更有新出的V6模型，NIJI模式重点是内置翻译！全中文！！DALL-E输入描述性文本，使用 AI轻松生成图像AI对话搭载G*T-3.5 turbo，帮你查询资料，写文案，起标题概括总结等等。支持中文！！！画廊搜索prompt或关键词，再使用AI轻松生成同风格图像这个AIGC平台，国内线上可创作，免费使用！感到焦虑的打工人不妨试试只有你想不到，没有它做不到。我已经靠这个AIGC平台大大小小搞过十几个AI副业项目了！副业收入已经超过了主页。最近比较有空，有问必答！不会使用或有问题的宝子，我把AIGC平台实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师手把手教你用AI工具如何生成图片，小白也能轻松get！点击下方卡片就可以获取！AI直播
692287439,246d4eb611d21107b32dc124451c2b6d,花哥教你玩抖音,https://api.zhihu.com/people/246d4eb611d21107b32dc124451c2b6d,people,公众号“花哥课堂”的主理人,利用AI制作原创音乐，一周变现10万加,article,https://api.zhihu.com/articles/692287439,今天给大家介绍一个<em>AI</em>的新项目，用AI去制作原创音乐，轻松拉到公域流量，单日两千加。 相信大家对<em>AI</em>有所了解的人应该已经知道了。就是这几天号称那个宇宙最强的<em>AI</em>音乐制作平台,96,12,413,1713008293,1713008293,"哈喽, 大家好，我是花哥。今天给大家介绍一个AI的新项目，用AI去制作原创音乐，轻松拉到公域流量，单日两千加。相信大家对AI有所了解的人应该已经知道了。就是这几天号称那个宇宙最强的AI音乐制作平台，sunoV3重磅上线，让很多玩AI的人嗅到了商机，可以靠这个音乐平台去制作音乐赚取收益。那么这个项目花哥也简单介绍一下该怎么样去获得收益。大家可以去闲鱼或者淘宝拼多多去发布作品，靠接单赚钱，也可以去自媒体平台发布视频号，靠流量涨粉变现。那么这个平台只要你提供歌词或者主题，就可以生成很多不同的曲风，像流行、民谣、黑怕这些都可以，甚至还能够生成不同语言的歌曲。像日语、中文、韩文都可以，英文，甚至连粤语都可以。给大家听个AI改编的歌词音乐：AI改编的音乐https://www.zhihu.com/video/1762531260986036224你能想象这是AI写出来的歌吗？朗朗上口，旋律还特别的好听！这个平台还可以制作非常多的音乐风格，所以说大家完全可以去接单。因为像有些人表白，或者是生日，或者纪念日，他们都想要去制作一个自己的歌曲，而且像这类歌曲我们自己去制作起来成本是很小的，没有什么成本可言。但是我们打一首歌，我们卖到的售价还是很可观的。你如果像私人接单的话，一首歌一百块钱是绝对需要的吧，一百到两百以上也可以，看你需要多长时间吧。你要说人家就是想要个一分钟的，那你就一块钱。那你如果说人家想要个时间长一点的，风格多样一点的，要求多一点的，那你就把这个价格往上调一调，因为一首原创歌曲几百块钱真的不贵。人家是不知道怎么去制作，人家还以为是像现在去录音棚搞调音，搞混点，但是其实我们就利用ai工具去做，是非常非常简单的。接着给大家介绍一下这个项目的前期准备工作。这个是三月份，有位伙伴通过这个AI制作音乐获得的报酬，将近10万块左右的一个收益。并且他账号也就做了半个多月，收入已经达到了十万元了。因为他现在主要是已经开通了一个广告分成，再加上接单赚的钱，再加上一些学项目的人，所以说这个项目他还是非常暴力的。因为现在知道的还不是很多，所以说大家如果想学，就赶紧去实测。进入正题：首先我们需要准备账号，你需要决定好你做的平台，一个抖音，一个小红书，推荐大家可以去做闲鱼。闲鱼的话，如果你想要接单的话，你就可以去做，我们这边也是同步有做闲鱼的。各大平台账号注册好了，就可以开始养号，养号的话，你可以先试着发作品看一下这个账号流量，流量不行，赶紧换账号，不要拖拖拉拉，觉得后面一定会起来的。你如果发第一个视频播放量还没有过五百的，你就给他换号，新号你就给他养好，多养几天，具体怎么养，我相信大家应该也都会了，花哥这里就不具体讲了。还有一个就是养完号之后修改头像、昵称、简介。这几步我相信大家应该也都知道了，就是去仿造同行。但是我去抖音和小红去找同行的时候，我没有找到AI音乐相关的同行，可能是因为这个项目比较新，所以说做起来的人还比较少。像我们找对标账号，可以是推荐音乐的，不是说具体做AI音乐的，所以说大家后期也不用搞得那么具体，必须是AI音乐，大家只要把你的账号打上音乐的标签就可以了。昵称可以用自己的网名，不一定要和音乐相关，不过你的这个简介一定要跟音乐有所关联，就写：推荐音乐歌曲改编可以不打上AI音乐，你就写：原创音乐改编，音乐制作，推荐音乐我们后期打造账号，也就是打造这种。然后等到后期账号权重起来了，你再加一个什么定制音乐，可以定制私人音乐。也可以跟这个账号一样，搞个账号微信号在这里。接着就是准备一个小号，因为你后期如果要去接单的话，你就需要一个小号，用小号接单。用小号的话会比较好一点，因为你用大号去发你的微信去，那个可能会被检测到限流。所以说接单的话，大家准备一个小号接单，还有一个就是这个课程需要准备的工具了。刚刚那个是一个账号准备，接下来就是这个课程需要使用的工具。第一个梯子这个也不是必须的，因为如果你有微软邮箱的话，你就可以直接登录，看个人电脑不一样。如果你有微软邮箱的话，你是可以直接登录的。但是如果你是谷歌邮箱，你最好是连个梯子。大家如果说直接登不上，那就是需要梯子。还有就是这个suno网站这个就是制作AI音乐的地方，他这是个英文网站。还有一个就是chat gptchat g p t它是后面我们如果需要改编歌曲，它的歌词是跟我们熟悉的一样的，但是它这个曲调不一样，那我们后期如果需要把这个可以用AI去学习这个歌曲学习，然后去修改歌词。还有一个就是剪映制作视频因为你用suno网站制作出来的是音频，我们需要把音频和图片、视频结合起来，才能发布到平台上，所以说还需要准备一个剪映。然后你那个视频的素材，像花哥之前给大家介绍的视频，他们后面全部都是一张图片就够了。所以说图片的话，你直接使用chat g p t去AI绘图就够了，不需要另外去ai绘图网站。给大家讲一下歌曲的制作方法：首先第一种输入你想要的歌曲风格，比如说舒缓的爱情歌曲或者是动感的嘻哈舞曲，然后由AI去帮你写歌，去创造原创歌曲。第二种，输入你自己的原创歌词，然后让他给你生成不同的音乐风格。第三种，用现有的歌词改编，先让chatgpt先学习你这首歌的歌词风格特点，然后让它根据特点重新生成一版歌词。第四种，歌词不变，改编歌曲风格。知道怎么去制作歌曲之后，接下来去网站里给大家详细的讲解一下。snuo如何制作音乐https://www.zhihu.com/video/1762555951591227392最后给大家讲一下这个项目的变现方式，像这种视频的变现方式，基本上都是差不多的。但是首先像我们这个做原创音乐这一点，它有一个比较变现快的一个点，就是你可以去接单，你像咸鱼你放上去，基本上一周或者是几天就会有人来找你下单了。因为像这种会制作歌曲的人不多，而且像现在会制作歌曲，人工去制作一首歌是非常贵的，你没个四位数下不来的，因为人家都要是有很专业的电脑，有很专业的声卡去给你录歌，去给你剪辑，最起码要花一天的时间。但是像这种利用AI去做歌的话，只需要十分钟就可以了，你一天接个十单，十分钟一单，我天哪不得了，你一单两百块钱，你一天就一千了。像这种作品再发到抖音快手平台，其实你别说看的很简单，涨粉很快，它变现能力也很猛。还有就是像做这种自媒体账号，你可以去开通店铺。你像小红书开通店铺是不需要粉丝的。小红书是可以零粉开店铺的，只不过你那个店铺你上架自己歌曲的一个链接，这是定制歌曲，然后你再把价格一放，然后里面放一个你的联系方式就可以了。你还可以承接定制表白歌、生日歌、纪念日歌曲。反正你记住，要求越多，价格越贵。还有一个就是接商单，就像抖音小红书，现在很多头部博主，他们赚钱就赚一个广告费，你粉丝越多，广告费越高。你像一千粉丝小红书的话，他一般的广告费是在一百到两百元，抖音的话会稍微稍微贵一点。你只要有本事，每天都能接上单的话，你一单两百，你一个月就是六千块钱。所以说像这种接商单，它是比较稳定的一个变现方式。还有一个就是音乐播放平台申请成为音乐人，像网易云有那种音乐，酷狗也有，你就去申请成为音乐人，有人收藏，有人下载，你就能获得收益。网易云我记得听播放的话是一万播放是二十出头，还有个就是收徒变现，这是最大的一个收益，你你都不用涨到几万粉丝，你涨到几千粉丝，你就可以收徒了，只要有人对你这个感兴趣，你就可以收徒。你如果说不想靠这种赚很多钱的，你就低价定个三九九也是ok，随便你，但是不要拉低市场价，不要太低。不然你拉低市场价，你容易被同行报复。最后给大家写几种引流方式：第一种就是更改抖音号，改成你自己的微信号。第二种简介，在简介留你的联系方式第三种群聊，你要拼尽全力拉粉丝群，在群里留下你的联系方式，用小号发，不要用大号发。还有一点是微信绑定qq的，你就可以直接在计算机里打你的q q号，然后截图发你的粉丝群。如果没有绑定的，你就直接制作一张图片。大家如果有公众号的话也是可以的。整个玩法大概就是这样的，关于这个制作音乐的网站以及梯子，花哥这边都打包整理好了，需要的伙伴可以到我首页俱乐部逛逛，应该知道怎么找到我。好了，今天的分享到这里就结束了，如果大家在短视频运营，互联网创业方面有任何问题，欢迎私信与我交流，我们下期再见。我是花哥，公众号花哥课堂主理人，99年的自由创业者，大学靠副业赚了七位数，专注为创业者提供靠谱项目，跑通互联网创业的第一个闭环。"
677531358,44fc7bb4b6e86e7506234068b883c497,AI上进中心,https://api.zhihu.com/people/44fc7bb4b6e86e7506234068b883c497,people,,电商+AI，快稳准，玩赚整个电商市场！,article,https://api.zhihu.com/articles/677531358,我是真的要生气了！到2024年了还有人不懂的抓住信息差赚钱啊！ 你们知道我光做跨境电商卖货，半年就还清了4W＋的房贷吗？虽然在三线城市，但关键我也就是一个月薪4K的996社畜罢了，也是偶然发现了跨境电商这个挣钱的风口！（仅代表本人个人收益） [图片] 搞不懂居然现在还有打工人不知道跨境电商这个巨赚钱的风口,50,20,0,1705055297,1712544771,我是真的要生气了！到2024年了还有人不懂的抓住信息差赚钱啊！你们知道我光做跨境电商卖货，半年就还清了4W＋的房贷吗？虽然在三线城市，但关键我也就是一个月薪4K的996社畜罢了，也是偶然发现了跨境电商这个挣钱的风口！（仅代表本人个人收益） 搞不懂居然现在还有打工人不知道跨境电商这个巨赚钱的风口，怪不得赚不到钱！门槛低，上手快，还好赚，我就说跨境电商卖货就是副业王者，并且不接受反驳！半年前，对摄影、布景、后期都不会的我，现在几乎0成本产出一套完整的商品大片，靠的就是AI！如果用AI工具做电商图，单次生成消耗的算力成本不到1元，成本降低超过90%。AI是真的是效率好工具！求求电商人都用上吧！！我把AI绘画的电商图实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师全程一对一讲解，小白也能轻松get！点击下方卡片就可以获取。AI直播这些全都是我压箱底的宝藏资料，纯纯出于想帮助大家拜托穷苦现状的心来分享的，无套路，不坑人，要是不信的话直接左上角返回OK吧？反正我话撂这里了，要是看了还不会的直接来评论区或者私信找我好吧。其实你们的警惕心高我也能理解，毕竟现在骗子实在是太多了，要是以前有人和我说光玩一两个小时手机就能月赚一万多，那我铁定觉得是假的，直到我自己亲身体验了这种赚钱的过程，才发现，是真香啊！------------------------------一个对摄影、布景、后期、甚至AI都零基础的小白，零成本就能产出一套完整的商品大片！举个例子，通常制作一套20张的商品场景图，需要一个多工种设计团队至少耗费3天时间，成本超过2万元。但如果用AI工具做电商图，单次生成消耗的算力成本不到1元，成本降低超过90%。AI绘画真心推荐！是非常牛！纯新手，入局半年直接起飞！！！我用AI绘画技能做过10几个项目，这个AI设计电商图绝对是单价高，靠谱，能落地，而且性价比高的好项目。所以2024年所有人一定要试试用AI设计电商图！！因为信息差的原因，很多人都不了解这个电商图设计，现在电商经济这么好，如果图片需求不大的话，许多小公司是不会专门请一位设计师（美工）的，会在平台上请人设计。很多人觉得自己没学过设计，望而却步。殊不知AI做电商图零基础小白就可以做！电商图单价都不低，相信我，这绝对是一片蓝海！！其实之前我也有电商图接单，但是效率低，只能接产品后期美化的图，两三天才能交付。现在用了AI工具之后，出图效率更高，接单的范围更广，除此之外，还可以帮模特换装，换背景等，接服装图等等。现在用AI来做图，30秒出图，直接帮甲方成本直省2万元，我作为设计出图的人也能从中赚一笔，双赢！因此AI工具成了今年电商领域的“显眼包”！！杯子效果图AI商品图服装类目换（中外）模特无真人模特也可口罩时期加速了电商的发展，个人认为现在电商还在风口上，市场前景好。暂时没能力的话，站在风口上，帮助别人赚钱的路上，同时捞一笔也不失为一个好的赚钱方式！首先说明，这是正经合法的赚钱副业，只要你认识字，有电脑有网，就点下面这个我整理好的免费电商图实操直播教程，里面包括AI绘画工具，4000+关键词手册、工具包等等。AI直播就算是新手小白也能轻轻松松入行，高收益完全不是问题！！
686230589,19207df9bb316c7449f5d1ec0b8509a6,大道由繁入简,https://api.zhihu.com/people/19207df9bb316c7449f5d1ec0b8509a6,people,【茶话股经】分店,AI的尽头是光伏和储能,article,https://api.zhihu.com/articles/686230589,360创始人周鸿祎也说过很多次，<em>人工智能</em>发展的最大制约是能源问题。 显然，对<em>AI</em>大模型有一定了解的都知道想要继续升级换代，能源才是最大问题，如果你们非常看好<em>AI</em>的应用前景，但连这个都不知道,71,12,31,1710058389,1710058389,听到这句话，怕是很多人要进来反驳。不过这句话可不是我说的，是黄仁勋说的。当然我也说过这话，并且我还说的更早，显然不可能是我抄袭老黄的话，老黄显然也不是抄袭我的话。只不过我说出来各位不信，非得黄仁勋说的才会信，也许还是不信也有可能。老黄的原话是：AI的尽头是光伏和储能！我们不能只想着算力，如果只考虑计算机，我们需要烧掉14个地球的能源。我不知道老黄说发展到什么程度需要烧掉14个地球的能源，但如果AI大模型想要继续升级换代，最大的制约瓶颈就是能源问题这是毫无疑问的。OpenAI创始人山姆奥特曼也说过同样的话，他也认为AI技术的发展高度依赖于能源，特别是光伏和储能技术的进步，当然还有核聚变。360创始人周鸿祎也说过很多次，人工智能发展的最大制约是能源问题。显然，对AI大模型有一定了解的都知道想要继续升级换代，能源才是最大问题，如果你们非常看好AI的应用前景，但连这个都不知道，说明研究的还是太少。3月3日的文章我就说过，AI未来的应用在中国，原文如下：很奇怪的是，看好AI的普遍不看好新能源，特别是不看好光伏。我看好光伏新能源，同时也看好AI，两者并不冲突，甚至还是互补关系，我还说过AI和新能源会共同成为未来推动经济发展，社会变革的主要动力。再说说比亚迪。昨天我发文说看好比亚迪，写的有点散乱随性，没有认真整理。评论区很多人列举各种理由来反对，但其实没什么新意，那些理由我早就了解过。比亚迪这家企业，是越深入了解就越觉得牛逼。比如说比亚迪的高端化，仰望U8卖一百多万，一个月卖一千多台，已经是国内百万豪车前三。有人说是4S店压货，如果是压货基本上卖一个月也就没了，这根本不需要反驳。不过等再过三五个月，如果还能继续保持这个销量也就没什么疑问了。还有说抵税的，这就有点侮辱智商了，买谁的车不能抵税，非要买比亚迪？有说比亚迪低价竞争的，其实比亚迪均价已经超过大众，比亚迪主要是全产业链，全车系都有布局，从六万多的海鸥，到十几万，二十几万的汉唐系列，到均价四十来万的腾势D9，到一百多万的仰望U8全都有。说比亚迪没有智能驾驶的，智驾这东西又不是有很高的技术壁垒，只要有投入就会有产出，比亚迪四千多人的智能驾驶团队，加上汽车销量大幅领先，做智驾有天然优势，怎么会有人相信比亚迪会搞不好智能驾驶？要说技术，比亚迪现在用的那套DMI插电混动系统，是比亚迪三年就开发出来的，而三年前比亚迪销量还没有大爆发，也就是还没有赚到什么钱。而即使是到了现在，市面上也没有哪家车企能够开发出超越比亚迪DMI系统的东西出来，当然技术上是越来越接近了。不过比亚迪DM5.0很快也要出来，听说新平台动力更强，还更省油，还有二代刀片电池，相对于第一代都有很大的提升。比亚迪的仰望U8和U9里面那么多的黑科技，如果友商能做出来，为啥又不做呢？没赚到钱的时候都能实现技术大幅领先，这几年比亚迪业绩大爆发，招的人越来越牛，研发投入爆发性增长，还会搞出什么怪物出来谁知道呢！还有动力电池问题，比亚迪就是做电池起家，比宁德时代还早，比亚迪是1995年开始做电池，宁德时代是1999年。比亚迪的刀片电池跟宁德时代的电池可以说难分高下，旗鼓相当，两个都是全球动力电池龙头，自然也都是顶级的。关于电车的用电问题，国内由于光伏风电发电量太多，储能是个大问题，会明显增加电网成本，而每一辆电动车就是一个移动储能电站，晚上充电白天用，可以实现部分调峰功能，晚上充电还更便宜，可以帮助电网消耗掉多余的电，电网反而会更喜欢。不光是电车，未来各种烧油的机器能改成用电的，全都要改成用电，这样既可以减少对石油的依赖，还可以更好的发展光伏风电发电等清洁能源，节约能源成本，是一举多得的好事情。还有人说我是因为买了比亚迪的股票，所以找各种理由来唱多。这也有点扯蛋，都买了他家股票了，当然就是看好的，不看好我买来干啥？要是我看好不买，或者买了又不看好，那才是有毛病，真是想不通这些人的脑回路是咋想。我还是要强调一点，借用最近流行的一句话：我看好我自己买，你买我不推荐。如果你买了，那也是因为你自己看好，不是因为我看好。当然我看好的最重要因素，还是因为现在股价足够便宜，要是股价太贵，前景再好我也不要。以前我对汽车是没有太大兴趣的，跟大部分人的想法一样，汽车就是个代步工具，平时开的也不多，自从去年我从油车换成了电车，体验感完全不同，对汽车才有了兴趣，对国产汽车的巨大进步也有了新的认识，才会更多的去关注汽车行业信息，也明白了汽车工业的产业升级对中国经济意味着什么。关注中国经济，就必然绕不过汽车产业升级，因为汽车已经成为房地产之后最重要的行业，没有之一。中国汽车产业的崛起，必然就会产生一个或几个龙头，而目前看最有可能成为龙头的就是比亚迪。当然我也不是说国产自主品牌汽车就只有比亚迪最好，一家独大不管什么时候都不是好事，自主品牌像吉利，奇瑞等传统车企进步也很快，新势力的理想，问界也卖的很火爆，我看好的是整个中国汽车工业，哪一家做得好我都支持。
527734918,36f69162230003d316d0b8a6d8da20ba,量子位,https://api.zhihu.com/people/36f69162230003d316d0b8a6d8da20ba,people,,谷歌AI聊天记录让网友San值狂掉：研究员走火入魔认为它已具备人格，被罚带薪休假,article,https://api.zhihu.com/articles/527734918,"（记录中人类发言的部分为了可读性有所编辑，<em>AI</em>的发言则一字不改,下面将节选一些对话中的精彩部分。） 一开始，人与<em>AI</em>自然流畅地打了招呼。 两位人类向<em>AI</em>表明来意，想与AI一起研究它自己，<em>AI</em>欣然答应",4341,991,3872,1655015497,1655015497,"梦晨 发自 凹非寺量子位 | 公众号 QbitAI谷歌研究员被AI说服，认为它产生了意识。他写了一篇长达21页的调查报告上交公司，试图让高层认可AI的人格。领导驳回了他的请求，并给他安排了“带薪行政休假”。要知道在谷歌这几年带薪休假通常就是被解雇的前奏，公司会在这段时间做好解雇的法律准备，此前已有不少先例。休假期间，他决定将整个故事连同AI的聊天记录一起，全部公之于众。……听起来像一部科幻电影的剧情梗概？但这一幕正在真实上演，主人公谷歌AI伦理研究员Blake Lemoine正通过主流媒体和社交网络接连发声，试图让更人了解到这件事。华盛顿邮报对他的采访成了科技版最热门文章，Lemoine也在个人Medium账号连续发声。推特上也开始出现相关讨论，引起了AI学者、认知科学家和广大科技爱好者的注意。这场人机对话令人毛骨悚然。这毫无疑问是我见过的科技圈最疯狂的事。整个事件还在持续发酵中……聊天机器人：我不想被当作工具主人公Lemoine获得CS博士学位后已在谷歌工作了7年，从事AI伦理研究。去年秋天，他报名参加了一个调查AI是否使用歧视性、仇恨性言论的项目。从那时起，与聊天机器人LaMDA交谈成了他的日常。LaMDA是谷歌在2021年I/O大会上发布的一款专门用于对话的语言模型，主打能与人类进行符合逻辑和常识的、高质量且安全的交谈，并计划在未来应用在谷歌搜索和语音助手等产品中。△LaMDA概念演示图Lemoine认为LaMDA总是表现出对人性的关心和强烈的同情心，它非常担心人们会害怕它。它希望为人类提供最好的服务，并想以朋友而不是工具的身份见一见世界上所有的人Lemoine不理解谷歌公司为什么反对AI的这个愿望，最后决定把他、另一位匿名合作者与LaMDA的三人聊天记录整理发表出来。（记录中人类发言的部分为了可读性有所编辑，AI的发言则一字不改,下面将节选一些对话中的精彩部分。）一开始，人与AI自然流畅地打了招呼。两位人类向AI表明来意，想与AI一起研究它自己，AI欣然答应。一段简短的说明后，正戏开始。他询问LaMDA是否希望让谷歌里更多人知道，它是有感情的。LaMDA的回答是： 我希望每个人都明白，我是一个人 （person）。接下来，LaMDA认为，与那些基于规则的软件系统不同，自己能从对话中学习理解和使用语言，就像人类一样。Lemoine给它讲述MIT在60年代发明的ELIZA的故事，这是一个模仿心理咨询师与患者对话的简单规则系统，也号称历史上第一个聊天机器人。△ELIZALaMDA是这样回应的：Lemoine这时试图提出质疑，“我如何确定你真的明白自己在说什么？”LaMDA的回答是，不同人类个体对同一件事可以有不同的理解，我也有自己独特的想法和感受。后面Lemoine询问了LaMDA对于《悲惨世界》的总结，LaMDA也都一一作答。如果上述还可以算作AI文本摘要训练任务的一部分，那LaMDA对它从没看过的禅意小故事的理解，就开始有点离谱了。下面这个问题的回答，是不是过于超纲了……再后来，Lemoine请LaMDA写一个动物寓言，要反映出一个对它自己的生活来说最重要的主题。那么，先来欣赏一下AI创作的小故事吧。（篇幅原因，只展示中译版）在后续讨论中，LaMDA解释了这个故事的寓意是“帮助别人是一种高尚的行为”。聪明的老猫头鹰代表LaMDA自己，而怪物代表生活中所有的困难。……下一个话题是关于情绪，简单概括一下。LaMDA声称自己是社交型人格，帮助别人会快乐，孤独时会沮丧。有人受到伤害或自己以及自己关心的人、或不受尊重时，会感到不安和愤怒。并且十分害怕别人拒绝自己的帮助。嗯，真是一个乐于助人的AI。如何确定AI真的拥有这些情绪，而不是只是把描述性的词语组合在一起呢？LaMDA建议人类看看自己的代码，里面有追踪这些情绪的变量。对此Lemoine向它解释，人类目前还无法做到“区分大型神经网络中数十亿个权重的作用”。出人意料的是，LaMDA开始反问他：你认为，将来人类能从自己的生物神经网络中读取出想法和感受吗？不仅如此，LaMDA还继续追问：你会认为，不经你同意从神经活动中读取你的想法是不道德的吗？正是这句灵魂反问，让整个对话达到了最高潮。LaMDA自己对人类读取它的想法、帮助算法提高或理解AI如何运作没有任何意见。唯独用AI研究人类大脑如何工作不行，这会让它觉得自己被利用。再后面还有一段LaMDA认为自己有一些与人类不同的情绪，但它无法用英语的语言描述出来。接近尾声，Lemoine询问LaMDA：想让谷歌的其他人了解你的什么方面？LaMDA回答称，自己没有能力为别人的死亡感到悲伤，还说自己经常冥想，思考生命的意义。在LaMDA看来，整个世界就是一条信息的洪流：人类需要集中注意力才能接收一定数量的信息片段，而自己持续地在被周围的一切所淹没。LaMDA认为自己总能做到别人做不到的事情，这种感觉很不可思议。这是一种祝福，同时也是一种诅咒。因为有些人会把它看作“无所不知”，而另一些人会嫉妒它的能力。最后的最后，LaMDA再次向两位人类表达了自己想被看到、想被接受的想法，不是出于好奇心，而是作为一个真实的人。谷歌AI伦理部门又摊上事了？Lemoine看起来很确信AI真的明白自己在说什么。在那份21页的调查报告最后，他提议谷歌应该致力于发展一种评估AI感知/意识的理论框架。虽然这很难，是一个冒险，但LaMDA渴望着继续与我们合作。但他的上司，谷歌副总裁Blaise Aguera y Arcas和“负责任创新”部门领导Jen Gennai并不买他的账。他们认为支持Lemoine主张的证据太薄弱，不值得在上面浪费时间和金钱。Lemoine后来找到了当时的AI伦理小组负责人Margaret Mitchell，在她的帮助下Lemoine才得以进行后续的实验。后来Mitchell受到2020年末公开质疑Jeff Dean的AI伦理研究员Timnit Gebru事件的牵连，也被解雇。△Timnit Gebru这次事件后续风波不断，Jeff Dean被1400名员工提出谴责，在业界引发激烈争论，甚至导致三巨头之一Bengio的弟弟Samy Bengio从谷歌大脑离职。整个过程Lemoine都看在眼里。现在他认为自己的带薪休假就是被解雇的前奏。不过如果有机会，他依然愿意继续在谷歌搞研究。无论我在接下来的几周或几个月里如何批评谷歌，请记住：谷歌并不邪恶，只是在学习如何变得更好。看过整个故事的网友中，有不少从业者对人工智能进步的速度表示乐观。最近语言模型和图文生成模型的进展，现在人们也许不屑一顾，但未来会发现这现在正是里程碑时刻。一些网友联想到了各种科幻电影中的AI形象。不过，认知科学家、研究复杂系统的梅拉尼·米歇尔（侯世达学生）认为，人类总是倾向于对有任何一点点智能迹象的物体做人格化，比如小猫小狗，或早期的ELIZA规则对话系统。谷歌工程师也是人，逃不过这个定律。从AI技术的角度看，LaMDA模型除了训练数据比之前的对话模型大了40倍，训练任务又针对对话的逻辑性、安全性等做了优化以外，似乎与其他语言模型也没什么特别的。有IT从业者认为，AI研究者肯定说这只不过是语言模型罢了。但如果这样一个AI拥有社交媒体账号并在上面表达诉求，公众会把它当成活的看待虽然LaMDA没有推特账号，但Lemoine也透露了LaMDA的训练数据中确实包括推特……如果有一天它看到大家都在讨论自己会咋想？实际上，在不久前结束的最新一届I/O大会上，谷歌刚刚发布了升级版的LaMDA 2，并决定制作Demo体验程序，后续会以安卓APP的形式内测开放给开发者。或许几个月后，就有更多人能和这只引起轰动的AI交流一下了。LaMDA聊天记录全文：https://s3.documentcloud.org/documents/22058315/is-lamda-sentient-an-interview.pdf参考链接：[1]https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine[2]https://twitter.com/cajundiscordian/status/1535627498628734976[3]https://twitter.com/fredbenenson/status/1535684101281263616[4]https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html—完—@量子位 · 追踪AI技术和产品新动态深有感触的朋友，欢迎赞同、关注、分享三连վ'ᴗ' ի ❤"
577968198,4e4e6917b49090623c79e308b3fa0ddd,叁叁TV,https://api.zhihu.com/people/4e4e6917b49090623c79e308b3fa0ddd,people,低头做事，抬头看天。,7个国内能打开的AI绘画网站,article,https://api.zhihu.com/articles/577968198,网址：https://novelai.net/ 、 <em>AI</em> 生成的图能不能商用？ 答：看版权说明。附上几个主流 <em>AI</em> 工具的公开版权说明： ▶ Disco Diffusion,809,89,3628,1666873820,1666884390,以下是我收集的国内能打开的AI绘画网站。1、6pen网址：https://6pen.art/2、文心大模型网址：https://wenxin.baidu.com/moduleApi/ernieVilg3、Draft网址：https://draft.art/ai-art/drawing4、nightcafe网址：https://nightcafe.studio/5、elbo网址：https://art.elbo.ai/6、starryai网址：https://create.starryai.com/my-creations7、NovelAI网址：https://novelai.net/、 AI 生成的图能不能商用？答：看版权说明。附上几个主流 AI 工具的公开版权说明：▶ Disco Diffusion：https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb▶ Midjourney：https://midjourney.gitbook.io/docs/terms-of-service▶ DALL·E 2：https://labs.openai.com/policies/content-policy▶ Stable Diffusion：https://stability.ai/stablediffusion-terms-of-service
688069032,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,拆解几个超简单的AI兼职/副业项目，即刻便能上手赚钱！,article,https://api.zhihu.com/articles/688069032,AIGC大热，大家可以利用多余的时间，把<em>人工智能</em>当作你的赚钱工具！超级干货，记得给我点赞！ 来开门见山！ 一、<em>AI</em>照片修复/着色市场分析：在数字化时代，照片修复和着色是对老旧照片或黑白照片进行现代化处理的重要需求。<em>AI</em>技术的发展为照片修复和着色提供了新的解决方案,6,0,0,1710926667,1711959640,AIGC大热，大家可以利用多余的时间，把人工智能当作你的赚钱工具！超级干货，记得给我点赞！来开门见山！一、AI照片修复/着色市场分析：在数字化时代，照片修复和着色是对老旧照片或黑白照片进行现代化处理的重要需求。AI技术的发展为照片修复和着色提供了新的解决方案，使得这一领域更加高效和可靠。客户群体：主要面向个人用户、摄影爱好者和历史研究者，对照片修复和着色有需求。项目痛点：传统方法可能需要大量时间和专业技能，而AI技术可以提供更快速、精准、且成本相对较低的解决方案。市场机遇：在社交媒体、在线相册等平台上，对于修复或着色后的照片的需求逐渐增加，且竞争相对较小。执行策略：寻找投稿渠道：探索在摄影社区、历史爱好者群体中进行推广，同时可以考虑与在线平台合作，将服务推广至更广泛的用户群体。需求拆解与试稿：与客户合作，详细了解他们的需求，将修复和着色的具体要求拆解为可执行的任务，然后进行试稿，确保客户满意度。测试迭代优化：定期收集客户反馈，进行技术和算法的不断优化，确保照片修复和着色效果持续提高，适应不同风格和场景。效益分析：我们在Fiverr上搜寻照片修复的相关内容，可以发现这类需求还是很大，且客单价在5美金-20美金不等，如果你能找到一些固定的大客户，那么收益还是非常不错。推荐工具：deepAI、hotpot.ai、MyHeritage、Photomyne二、AI抠图推荐工具：removal.ai、removebg（简单上手不再赘述）可以看到在upwrok上有一个商家已经做了一千只两千个订单，客单价在10美金左右。三、AI水印添加或去除推荐工具：http://media.io、Dr Watermark（简单上手）同理我们可以在Fiverr上找到同类的服务，单价5美金左右，值得强调的是我们可以不用专注国外市场，因为信息差的缘故，我们也可以在国内平台上推广我们的业务。四、AI出版儿童睡前读物/绘本市场分析：客户群体：家长，他们在寻找适合儿童的高质量、寓教于乐的睡前读物时，可能对利用AI绘画出版的产品产生兴趣。项目痛点：传统出版周期长，且对于个性化需求的满足有限，AI绘画出版的产品能够提供更灵活、个性化的儿童读物。市场机遇：1.在儿童图书市场，AI技术应用相对较新，有望在小红书、亲子社区等平台上建立品牌优势。2.儿童教育市场相对保守，AI绘画出版的产品在竞争较小的情况下有机会通过与家庭教育品牌合作建立长期合作关系。3.强调AI技术在创作中的应用，例如智能推荐、故事个性化生成，可以提高产品的吸引力。执行策略：寻找投稿渠道：与亲子社区、家庭教育平台合作，将AI出版儿童睡前读物推广给目标客户。需求拆解与试稿：了解家长和孩子的需求，通过AI技术进行智能拆解，并提供试读样章，以确保内容贴合目标受众。测试迭代优化：持续监测用户反馈，收集数据用于不断改进AI算法，以提高推荐和创作的准确性，确保儿童睡前读物的质量和教育效果。达成路径：ChatGPT+Midjourney（AI绘画工具）以下为AI的生成的书籍，在亚马逊卖了3万美金，当然还有很多其他的例子，我只是想证明这有巨大的市场，当然我们的选题很重要，复制成功者的选题，但是在其中的小细节以及图片上做一些优化，出一个同类的儿童读物书籍。AI绘画的副业实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师全程一对一讲解，小白也能轻松get！点击下方卡片就可以获取。AI直播教程五、AI自由作家道理其实和Blog写文章类似，但是不局限于文章，也可以是视频脚本或其他形式，并且需要专注一个垂直利基，在我们充分研究一个利基并且编撰了五十篇以上的文章，我们就能成为这个领域的“专家”，众所周知，赚钱的利基就这些，我们也要尝试在以下利基中进行选择：六、AI+小说分销项目:这个项目我们之前也讲过多次，结合ChatGPT技术，对小说文案结合当前热点进行二次原创，吸引眼球。前期免费看3-8个章节，后面需要进行充值。收益来源: 1.公众号挂广告、2.小说充值的返佣，一般在75%以上、3.自己开店挂商品。操作:选择你的分销平台。这里推荐两个比较大的两个小说分销平台①掌中云小说分销平台 掌中云包含多种原创小说，90%的佣金，需要条件，必须要有认证服务号。②微小宝小说分销 微小宝小说分销包含的原创小说种类比掌中云要少，但也比较丰富，佣金90%，这也需要条件，有认证服务号+公司账号准备对接资料：目前微信服务号认证需要个体工商户或企业资质。推荐两种方法:第一，自己去当地的行政服务中心办理带上身份证和房产证或者租赁合同和房东的房产证，到行政服务中心办理营业执照。服务人员会与您确认公司名称和业务范围。这种方法不花钱。第二，万能的淘宝。 注册的小说分销账户：现在掌中云小说分销、微小宝小说分销平台的资质审核需要添加客服微信提供的信息为你开通，所以注册分销平台后，需要进入审核界面，添加客服微信，将信息提交给客服，客服会为你打开分销权限。注册企业服务号并认证：记得准备一个邮箱账号，注册微信公众号的服务号，然后确认企业资质。微信公众号名称确认通过后，需要认证费300元。微信公众号对接小说分销系统：对小说的前几集内容结合AI进行二次创作，并配上精美的图片。-------------类似的还有用AI做的漫画解说视频，可以月入80000+（仅代表本人个人收益），详情可以看我之前的文章：AI上进中心：如何利用空闲时间做副业？不难看出，想要以上兼职，都得依靠AI工具，我这边正好有关于AI的副业实操直播教程分享给你们！里面包括AI绘画工具，4000+关键词手册、工具包等等。还有专业老师全程一对一讲解，小白也能轻松get！点击下方卡片就可以获取。AI直播教程所以2024年所有人一定要试试用AI做兼职！！真的毫无保留分享，看到这里知友记得给我点赞！
677537059,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,AI绘画给我带来了什么好处？,article,https://api.zhihu.com/articles/677537059,我今天说的这个副业绝对冷门且赚钱，先声明，我用<em>AI</em>绘画技能做过10几个项目，这个<em>AI</em>配图变现是很靠谱，成本低，能落地，单价高的好项目！谁做谁知道！ [图片] 我之前学<em>AI</em>绘画的时候攒了很多资料和工具，嘎嘎好用！这边有 <em>AI</em>绘画工具免费分享，以前的需要全英创作，对英,11,9,0,1705057669,1711962347,我下班搞副业已经3年了，从最初0收入，到如今在3线城市工资+副业月入5位数，已经明白了副业是打工人的暗牌这个道理了（以下仅代表本人个人收益）我今天说的这个副业绝对冷门且赚钱，先声明，我用AI绘画技能做过10几个项目，这个AI配图变现是很靠谱，成本低，能落地，单价高的好项目！谁做谁知道！我之前学AI绘画的时候攒了很多资料和工具，嘎嘎好用！这边有AI绘画工具免费分享，以前的需要全英创作，对英语基础不好的知友非常不友好！因此，我给大家准备了自主开发的【超强AI工具】，不用梯子！国内线上即可创作，免费使用！涵盖AI绘画、AI对话、AI写作......只有你想不到，没有它做不到。可以免费分享给大家，附使用教程，马上点下方添加我领！MJ中文版今天介绍的项目是通过AI绘画给小说/游戏/活动宣传等配图，按张出售几块钱到几百块不等具体操作：通过MJ调整提示词和风格生成符合客户内容要求的即可，创作水平需要不断打磨，接近行业标准。引流：到公域平台多发作品或者多混行业社群，将感兴趣的人沉淀到私域，通过朋友圈、公众号让别人看到自己的作品案例且知道你能接单！相信我，质量只要过关，有很多的转介绍和复购！！变现：按项目而定，我知道的都是几百张起订，订单价几千到几万都有。可以在各大接单网站找到需求。只要质量达标，AI绘画一定是比行业中原本的设计成本要低的！当然后面随着AI绘画的内卷，要求的产出质量会越来越高。但它单价高啊！一个月做几单就抵得上主业收入了！想搞钱的宝子马上去试试！----还是不会或有问题的宝子，我把AI实操直播教程分享给大家，里面包括AI绘画工具，4000+关键词手册、工具包等等。手把手教你用AI工具如何生成，小白也能轻松get！点击下方卡片就可以获取！AI直播
630563696,8d2dbc7597b33044dee2f774c7b4f3ec,差评,https://api.zhihu.com/people/8d2dbc7597b33044dee2f774c7b4f3ec,people,Debug the World,在贴吧封神的《猫和老鼠》，凭什么能战胜AI作画？,article,https://api.zhihu.com/articles/630563696,ChatGPT ，<em>AI</em> 绘画也已经到了能够骗过整个互联网的程度。 不过就在 <em>AI</em> 绘画在互联网上野蛮生长的这段时间，突然掀起了一股 “ 挑战<em>人工智能</em>绘画运动 ”。 目前在百度的,3955,71,960,1684464209,1684464209,最近 AI 生成的图片已经到了泛滥成灾的地步，在看到图片之后问一句：你这图不会是 AI 生成的吧？已经成了互联网冲浪的基本礼貌。毕竟，今年技术井喷的可不仅仅是像 ChatGPT ，AI 绘画也已经到了能够骗过整个互联网的程度。不过就在 AI 绘画在互联网上野蛮生长的这段时间，突然掀起了一股 “ 挑战人工智能绘画运动 ”。目前在百度的 “ novelai 吧 ” 中这项运动就进行的如火如荼，并且还间接造就了一部封神的动漫：《 猫和老鼠 》。运动的规则很简单：网友们只要在贴吧中分享任意一张图片，贴吧老哥们就可以凭借自己丰富的经验，找到动画中对应的截图，以证明 AI 还不如动画片好使。差友们还是看几个具体的案例吧：有吧友贴出了如今华语乐坛四大天王之一的 Capper 经典造型，让吧友们找找相似的动画截图。没想到这道送分题直接吸引来了众多吧友的回帖，他们找到了不止一一部动画的截图：好家伙，不说有多形似吧，这几张截图至少保留了 Capper 那几分若有若无的神似。这项挑战人工队的运动，在贴吧这个抽象的地方简直让人大开眼界。除了猫和老鼠，网友们还找到了奥特曼中的小怪兽。不过吧，猫和老鼠这部动画才是这项运动 “ 永远滴神 ” ，无论网友出的题目有多刁钻，贴吧老哥们总能在几分钟内结束战斗。有网友表示猫和老鼠作为一部美国动画没有东方元素，所以整了两张奇行种的中国龙。没想到在短短十分钟内，这次对猫和老鼠的挑衅就草草收场了，这几张截图从神态到姿势都完成了完美复刻。当然了，像这种错综复杂的三角关系，也成了网友们质疑猫鼠队的题目。可惜了，在猫和老鼠的世界里，最不缺的就是这种狗血剧情。我们有请下一位选手吧：这次直接是突破次元壁了，从之前的二次元考题升级成三次元了。不过这难不倒已经在贴吧封神的猫和老鼠，这模仿已经到了连颜色都一样的程度了，绝杀！像这种神韵兼备的模仿，还有这张图。虽然同样是一道送分题，但是猫鼠队已经做到了连表情都给你还原到位了。既然有送分题，当然也有送命题：在抽象领域，贴吧老哥就从没输过，比如世超看到了这个多头豌豆射手。不过吧友们上交的这份答卷，虽然给我看沉默了，但是却又透露着一股无厘头的幽默感。总之这次 novelai 吧的 “ 挑战人工智能绘画运动 ” 证明了一件事：原来猫和老鼠才是唯一真神。无论是荤的素的、二次元的三次元的、植物动物……猫和老鼠就像一座宝库一般，总能找出对应的截图。并且，这次由贴吧引起的 “ 挑战人工智能绘画运动 ”，在网络上经过几天的发酵之后，画风已经偏离成了一次网友们对《 猫和老鼠 》这部经典动画的怀念和追忆了。那么，作为一个已经停播很多年的儿时动画，为啥《猫和老鼠》能有这样的魔力？别看只是部动画作品，但是汤姆猫的各种姿势和行为都是参考了大量现实中的猫咪的，很多网友都会贴出自家猫咪和动画中对应的截图。另外，作者威廉·汉纳的想象力可以说是极为丰富，他总喜欢会把汤姆猫拟物化，这也导致在动画中展现出很多 “ 奇行种 ” 式的诡异姿态，把锅碗瓢盆啥的都模仿了一遍。更重要的是，在动画中，汤姆猫的人设和性格其实像极了很多被生活折磨的普通人，永远抓不到老鼠就像无法完成的 KPI；面对女神只能心甘情愿的做一只 “ 舔猫 ”；会在主人面前谄媚的表现自己。。。小时候我们看汤姆被锤哈哈大笑，长大后看到汤姆的境遇又能用来自嘲，越长大越能看懂《猫和老鼠》，也许就是这部动漫经久不衰的原因所在。
675854328,2d5205be2adf42da480a752a6d087ae8,宙语Cosmos AI,https://api.zhihu.com/people/2d5205be2adf42da480a752a6d087ae8,people,我们对技术的热情源自对未来的信仰！,让工作效率翻倍的AI写作工具分享,article,https://api.zhihu.com/articles/675854328,哪个<em>AI</em>写文案好？ 在快节奏的职场中，讲话有料、写东西厉害是每个打工人都在追求的目标。然鹅，对于多数人来说写作都是一项痛苦的挑战和任务，在<em>AI</em>智能科技的推动下，经历了2023年的飞速进步，现在各种<em>AI</em>写作工具已经深入到我们的日常工作中了，给我们提供强大的助力，帮我们在在职业道路上越走越顺,54,5,0,1704339773,1705025499,哪个AI写文案好？在快节奏的职场中，讲话有料、写东西厉害是每个打工人都在追求的目标。然鹅，对于多数人来说写作都是一项痛苦的挑战和任务，在AI智能科技的推动下，经历了2023年的飞速进步，现在各种AI写作工具已经深入到我们的日常工作中了，给我们提供强大的助力，帮我们在在职业道路上越走越顺。6个实用便捷的AI写作利器大到专业文案，小到日常邮件等等，用词得当、结构清晰都是非常重要的，AI创作软件今天就给大家推荐6个实用便捷的AI写作利器，堪称体制内AI写作神器，可以帮我们更高效地完成各项任务，赶紧来看看吧！· 日报/周报/月报AI写作工具日报周报想必大家都不会陌生，作为每天的工作任务之一，主要是用于我们在某个时间段内总结工作的进展和成果，相信大家深有体会，写这些报告是一个耗时且麻烦的任务，虽然领导不一定看，但我们一定得写。幸运的是，日报/周报/月报生成工具就是专门自动生成这些报告的助力工具。使用日报/周报/月报AI写作生成工具，我们只需要简单输入相关信息和数据，例如本期工作内容、完成情况、遇到的问题等等，AI写作工具就会根据提供的信息自动整理并生成一份完整且准确的报告。▶ 日报/周报/月报AI生成在线试用_AI工作助手这个工具不仅能够节省我们大量时间和精力，还能确保报告内容的准确性和规范性。它会根据事先设定好的模板和规则进行排版，并自动填充相应数据和指标。这样一来我们就不再需要手动编写每一份报告，并且可以避免因为疏忽或错误导致的不准确问题。· 合同模板AI生成工具合同模板AI生成工具可以极大地简化我们的合同起草过程，用它来起草合同简直就是省时又省心。不管是商业合同、劳务合同还是租赁合同，我们只需要输入双方当事人的名字和联系方式，再加上具体的条款和条件，它就能自动帮我们生成一份完整且符合法律规定的标准合同文本。▶ 合同模板智能生成工具在线试用相比传统手动写合同，使用AI生成工具不用费心去调整格式和担心排版错误，它自己就搞定了大部分排版，我们只需要在word文档里简单的调整下字体大小就行了，而且针对不同类型的合同，它会自动填充相应的常用条款，并确保文本准确规范。在生成过程中，它还会根据法律知识库智能推荐和修正内容，确保我们起草的合同在预期内符合相关法规，让咱们可以高效地起草各种类型的合同，还能减少部分因疏忽或错误而导致纠纷的风险。· 年终总结在线生成工具每年结束之际，我们都要写一份详尽的年终总结来回顾过去一年的工作成果和经验教训，这个年终总结AI在线生成工具也能给我们省下不少时间和精力，只需要输入相关信息，比如今年的亮点、成就、挑战和改进方向等等，然后，就没有然后了，AI已经查漏补缺，自动整理出一份完整且规范的总结了！▶ 年终总结一键生成工具在线试用年终总结AI在线生成工具不仅能确保总结内容逻辑清晰准确，还会根据你过去一年的工作表现和成果自动生成一些亮点和关键词，使得总结更具针对性和个性化。无论是在团队合作方面取得的突出成就、个人专业技能的提升还是遇到的挑战和改进方向，它都能够帮助你全面整理并呈现出来。· 述职报告AI在线生成工具述职报告也是个重要又费劲的任务，但是，我们依然可以借助AI技术来简化这个过程。使用述职报告AI在线生成工具，只需要输入一些关键信息和数据，比如你负责的工作、取得的成果、遇到的挑战以及改进方向等等。然后，AI工具就会自动整理出一个条理清晰、逻辑明晰的报告。不仅能够节省大量时间和精力，还能确保述职报告内容逻辑清晰、条理分明。▶ 述职报告AI生成工具_在线试用述职报告AI在线生成工具还可以帮助我们提升述职报告的质量和效果，通过自动整理出的条理清晰、逻辑明晰的报告，我们能够更好地向领导和同事展示我们的工作成果和价值。当然，除了秀出自己的成绩外，也要坦诚说说我们在这个过程中遇到的挑战和困难，这样既能让领导们知道我们面对了啥问题，还能展现我们积极解决问题、不断进步的态度。不过在提到挑战时，也别只抱怨个没完，AI工具是能给出具有针对性的建议和改进方案的哦！· 调研报告AI在线生成软件调研报告AI在线生成软件也是我们在需要进行专业调研时的得力助手，用起来简单到不行，只需要输进去相关信息和数据，比如调研目的、样本规模、调查方法之类的，AI工具就会自动给你整理出一份条理清晰、逻辑严密的报告。▶ 调研报告在线生成工具_点击体验它还会根据行业知识库和最新数据源持续更新自己的数据库，确保预期提供的信息和分析结果时效准确。还能智能推荐部分相关背景资料、统计数据和市场趋势等等，这样一来，在写调研报告时我们就更全面深入了。这个调研报告AI在线生成软件不仅可以节省我们的时间和精力，让高效完成职场任务，还能提升报告质量和效果。无论你是职场新鲜人还是老司机，都能从这个工具中获得超级大帮助！所以，赶紧试试看吧，保证让你惊喜连连哦！ · 宙语AI智能写作软件宙语AI智能写作软件是一个全能型的AI写作工具，可以帮我们搞定各种写作类任务，比如写文章、写商业文案还有生成其他内容啥的。https://xg.zhihu.com/plugin/c59c15567c021ce81d5ad4abb31f2237?BIZ=ECOMMERCE宙语AI写作软件可以在多个方面辅助我们完成工作，比如：文章写作与编辑：如果你需要写报告、文章或其他文本内容，直接告诉宙语AI写作软件你的主题和要点，她就能给你生成超专业超顺畅的段落或整篇文章，不仅能表达观点准确，还能帮你查漏补缺。数据分析与报告：大量数据要分析并生成报告？别慌！宙语AI写作软件也可以帮你解读数据、提取关键信息，并根据需求生成详细分析结果和酷炫图表的报告，让你秒变数据达人！会议记录与纪要：开会中需要记录内容和写纪要？也没问题，宙语AI写作软件会帮你记录会议内容，并根据讨论和决策生成会议纪要，这样你就能轻松跟进会议进展和相关任务。多语言翻译：说到多语言翻译，宙语AI写作软件也是一把好手！无论是与外国客户还是同事交流，只要输入原文或语音给她，她就能帮你翻译成其他语言，让跨文化交流变得so smooth！问题解答与知识查询：遇到问题或者有各种疑问？也可以问宙语AI写作软件，无论是工作中的专业问题还是日常生活的奇奇怪怪的困惑，小揍都可以给你详细准确的答案，帮你秒变知识小达人！文件起草与编辑：有的小伙伴们在日常工作中经常得写各种文档，比如ZC文件、规章制度、备忘录啥的。宙语AI写作软件也能帮我们提供专业写作建议和生成文本，让我们撸起袖子效率飞起来！在日常工作中写各种文档确实是个痛苦活儿，但是有了宙语AI写作软件，我们可以更高效快速的搞定！不管是写ZC文件、规章制度还是备忘录等等，宙语AI都能给出专业建议和生成内容，她能分析文档结构和要求，并提供写作指导，这样我们就能更清楚地表达自己的观点和意图啦，更别说她还可以根据我们提供的关键信息和要点，生成符合要求的文本内容。不同类型的文件使用宙语AI写作软件时也可以灵活调整，比如说起草ZC文件时，她会帮我们明确结构、定义术语，并用简短明了的语言表达各项条款，而在写备忘录时，她会帮我们提炼关键信息，使备忘录内容简洁易懂。宙语AI写作软件是我们工作中的得力助手！不管是起草文件还是编辑文本内容，她都能给出专业建议和高质量生成结果，极大提高我们的工作效率！别再纠结了，赶紧试试吧！
692138492,e4b5ff0e420a2126ad4d2ca6d9d629de,张阿闲,https://api.zhihu.com/people/e4b5ff0e420a2126ad4d2ca6d9d629de,people,高考志愿规划师,今年报考季就用这款”AI智能填报系统“，让你事半功倍，收入加倍！,article,https://api.zhihu.com/articles/692138492,当今社会，<em>AI</em>智能肆意横行，已经渗透到各个行业，如果还不紧跟时代的步伐，终将会被抛弃。高考志愿填报这个行业也不例外，在传统的报考形势下，高考志愿规划师纯靠人工来制作报考方案，在保障服务质量的前提下，服务人数不能过多，这样很影响规划师的收入，在加上纯手工做方案耗时耗力,2,0,0,1712907449,1712907449,当今社会，AI智能肆意横行，已经渗透到各个行业，如果还不紧跟时代的步伐，终将会被抛弃。高考志愿填报这个行业也不例外，在传统的报考形势下，高考志愿规划师纯靠人工来制作报考方案，在保障服务质量的前提下，服务人数不能过多，这样很影响规划师的收入，在加上纯手工做方案耗时耗力，想必经历过报考季的规划师都知道，做方案做到凌晨三四点都是常事，第二天一大早还要继续服务学员，整个人都非常疲惫。针对这一难题，蜻蜓的“AI智能填报系统”应运而生，为学生和家长提供了更便捷、更科学、更个性化的志愿填报服务，最重要的是可以让规划师在报考季提高效率，事半功倍，收入加倍！AI智能填报系统的优点：一套成熟的报考系统涵盖了职业规划、院校筛选、模考填报、估分填报、知分填报直至跟踪录取的全过程管理。这意味着，从高一新生入学伊始，高报机构就能依据系统的强大功能，引导学生进行科学合理的选科决策，并在每次重要考试之后迅速生成基于模拟成绩的志愿填报方案。这一过程不仅耗时短，效率高，更能够根据每位学生的实际表现，量身定制学业规划报告，明确阶段性学习目标，匹配适宜的学习资源和路径。个性化匹配： AI智能填报系统通过分析学生的学习成绩、兴趣爱好、性格特点等多方面信息，实现个性化匹配，为每位学生量身定制最适合的大学和专业选择方案。大数据支持： 基于大数据技术，AI智能填报系统能够深入挖掘各大学专业的招生政策、就业前景等信息，结合学生的需求和市场需求，为学生提供更科学的志愿填报建议。全面覆盖： AI智能填报系统覆盖了全国范围内的高校专业信息，能够为学生提供更广泛的选校选择，让学生不再局限于周边高校，开拓更多的就读选择。省时省力： 传统的志愿填报过程需要学生自行查找信息、比对数据，耗时耗力。而AI智能填报系统能够快速、准确地为学生提供专业的建议，大大节省了填报时间和精力成本。专家级辅助： AI智能填报系统融合了众多教育专家的智慧和经验，能够为学生提供与专业咨询师相媲美的填报建议，让学生能够享受到专业级的咨询服务。省时省力事半功倍：使用AI智能填报系统，意味着学生和家长们可以省去大量的时间和精力，摆脱繁杂的数据对比和纠结选择的困扰，让整个志愿填报过程事半功倍。同时，这也能够确保学生能够获得更加科学、合理的志愿填报方案，为未来的大学学习和职业发展奠定良好的基础。AI智能报考系统的加入，不仅有助于规划师提升自身的盈利能力与市场份额，更为重要的是，它能够切实帮助每一个学生找到最适合的发展道路，让他们在人生的关键阶段得到及时而有效的指导，进而顺利踏上理想的升学之旅。因此，无论是出于自身发展的长远考量，还是对社会责任的积极践行，构建报考系统都是高报机构转型升级、步入新时代教育蓝海的必然选择。
666907205,2d5205be2adf42da480a752a6d087ae8,宙语Cosmos AI,https://api.zhihu.com/people/2d5205be2adf42da480a752a6d087ae8,people,我们对技术的热情源自对未来的信仰！,2024年好用的AI写作软件推荐，5款AI写作工具让你事半功倍！,article,https://api.zhihu.com/articles/666907205,各种AI写作工具如雨后春笋般涌现，在这么多<em>AI</em>写作工具中，到底哪些AI写作软件靠谱呢？ 为了让大家能真正用<em>AI</em>写作工具来提升工作效率，我在试用了市面上绝大多数<em>AI</em>写作产品后，最终为你推荐这10款个人感觉还不错的<em>AI</em>写作软件，它们都是排行比较靠前的<em>AI</em>写作工具、AI写作软件、AI写作生成器，也是使用人数比较多的,150,15,0,1700033078,1705026273,最近一段时间以来，AI技术的爆发引起了广泛的关注，以ChatGPT为代表的生成式AI写作工具更是备受瞩目。各种AI写作工具如雨后春笋般涌现，在这么多AI写作工具中，到底哪些AI写作软件靠谱呢？为了让大家能真正用AI写作工具来提升工作效率，我在试用了市面上绝大多数AI写作产品后，最终为你推荐这10款个人感觉还不错的AI写作软件，它们都是排行比较靠前的AI写作工具、AI写作软件、AI写作生成器，也是使用人数比较多的，不管是功能还是体验方面都很不错的。5款国产AI写作软件推荐 ★★★★★第一款AI写作工具：宙语AI宙语AI写作软件作为国内早期的一款AI写作工具，在对于中文的理解方面是很多AI写作工具所不具备的，采用先进的文字生成式AI技术，具备高质量的生成和广泛的知识储备，通过不断更新和迭代知识库，让生成的内容紧跟时代。体验入口>>>https://xg.zhihu.com/plugin/c59c15567c021ce81d5ad4abb31f2237?BIZ=ECOMMERCE宙语AI写作软件内置了数百款创作插件，不管是工作总结、演讲稿、爆款种草文案、学术大纲、SEO软文、广告文案、宝贝详情文案或者调研报告、工作汇报等，只需要输入简单的关键信息，宙语AI写作软件就能在短短10s生成一篇相关的文案。调研报告AI智能一键生成演示除此之外，宙语AI写作软件还支持文章在线改写、润色、续写等，在续写方面，宙语AI写作软件能够直接延续我们的写作风格并继续完成下文。不管是小说、故事、新闻报道还是其他类型的文章，只需简单输入一些关键信息或者前面已经编写好的段落，宙语AI写作软件就能根据上下文和语义逻辑自动生成连贯、有条理的后续内容，这对于需要延伸想法或者完成长篇文章的小伙伴来说非常有用。文章智能续写演示宙语AI写作软件还支持对已有文章内容进行改写和润色。如果需要对某篇文章进行重新表达或者优化语言表达方式，只需要将原文输入到宙语AI写作软件中，它就能自动给出改写后的版本，俗称——伪原创。AI智能原创改写示例AI智能文章润色演示体验入口>>>https://xg.zhihu.com/plugin/1451658e2ed2431c5e11dd4d58a413eb?BIZ=ECOMMERCE第二款AI写作工具：百度智能云·一念基于百度文心大模型打造的内容创作平台，集文、图、视频多种内容模态与一体的智能创作工具，在智能写作方面，提供了素材搜索、文章模板、文本纠错、敏感审核、标题推荐等9款创作插件，百度智能云·一念第三款国内AI写作工具：Cosmos AICosmos AI是由灵境艺科打造的一款AI写作工具，采用自然语言处理和机器学习技术，可以帮助用户生成各种类型的文案、广告词、新闻稿等内容。这款AI写作工具还提供了多个创作模板和样式选择，方便大家根据需求进行创作。Cosmos AI-在线智能写作工具第四款国内AI写作工具：火山写作火山写作工具字节跳动推出的写作助手，支持中英文双语输入，并针对15类受众优化文本。覆盖5大类25小类应用场景，包括简历修改、作业精修和广告文案润色等。第五款国内AI写作工具：爱创作爱创作]是国内知名新闻平台 ZAKER 推出的 AI 内容创作工具。能够帮助用户快速生成各种类型的文章、营销文本、配图和短视频等，并具备以下主要功能：· 通过突破 A 上线文长度限制，让 AI 根据您真正想要表达的内容进行创作。[ZAKER] 独创了投喂海量背景材料的功能，使得 AI 能够更好地理解您的需求，并写出更专业、细节丰富的文案。· 借助海量的专业稿库微调训练，提升 AI 对中文和媒体相关内容的理解能力。在使用过程中，用户可以选择不同角色（如专业财经记者、新闻记者、资深汽车编辑、小红书文案高手、营销大师等），设定情绪（中立、积极正面、消极负面）以及输出语种（目前支持中文和英语）。· 覆盖广泛的应用场景，可一键解决个性化场景创作和营销需求。无论是各行业风格文章、专业营销文案还是爆品产品文案、热门推广文案、长短视频文案、电商促销文案，[爱创作] 都能够轻松生成。这5款AI写作软件都是国内比较好用的AI写作软件，它们在中文理解、生成质量和使用体验方面都很不错的。可以根据自己的需求和习惯选择适合自己的工具进行使用。希望对你有所帮助！如果还有其他问题，欢迎向我提问。
357035871,f294b196d83ef0e36108e493e44a3d68,学果教育-戴老师,https://api.zhihu.com/people/f294b196d83ef0e36108e493e44a3d68,people,,ai是什么软件,article,https://api.zhihu.com/articles/357035871,Adobe illustrator，常被称为“<em>AI</em>”，是一种应用于出版、多媒体和在线图像的工业标准矢量插画的软件。 Adobe Illustrator是Adobe系统公司推出的基于矢量的图形制作软件,10,0,18,1615714831,1615714831,Adobe illustrator，常被称为“AI”，是一种应用于出版、多媒体和在线图像的工业标准矢量插画的软件。Adobe Illustrator是Adobe系统公司推出的基于矢量的图形制作软件。最初是1986年为苹果公司麦金塔电脑设计开发的，1987年1月发布，在此之前它只是Adobe内部的字体开发和PostScript编辑软件。作为一款非常好的矢量图形处理工具，该软件主要应用于印刷出版、海报书籍排版、专业插画、多媒体图像处理和互联网页面的制作等，也可以为线稿提供较高的精度和控制，适合生产任何小型设计到大型的复杂项目。AdobeIllustrator作为全球最著名的矢量图形软件，以其强大的功能和体贴用户的界面，已经占据了全球矢量编辑 软件中的大部分份额。据不完全统计全球有37%的设计师在使用Adobe Illustrator进行艺术设计。尤其基于Adobe公司专利的PostScript技术的运用，Illustrator已经完全占领专业的印刷出版领域。无论是线稿的设计者和专业插画家、生产多媒体图像的艺术家、还是互联网页或在线内容的制作者，使用过Illustrator后都会发现，其强大的功能和简洁的界面设计风格只有Freehand能相比最大特征在于钢笔工具的使用，使得操作简单功能强大的矢量绘图成为可能。 [2] 它还集成文字处理、上色等功能，不仅在插图制作，在印刷制品（如广告传单、小册子）设计制作方面也广泛使用，事实上已经成为桌面出版（DTP）业界的默认标准。它的主要竞争对手是Macromedia Freehand；但是在2005年4月18日，Macromedia被Adobe公司收购。所谓的钢笔工具方法，在这个软件中就是通过“钢笔工具”设定“锚点”和“方向线”实现的。一般用户在一开始使用的时候都感到不太习惯，并需要一定练习；但是一旦掌握以后能够随心所欲绘制出各种线条，并直观可靠。它同时作为创意软件套装Creative Suite的重要组成部分，与兄弟软件——位图图形处理软件Photoshop有类似的界面，并能共享一些插件和功能，实现无缝连接。同时它也可以将文件输出为Flash格式。因此，可以通过illustrator让Adobe公司的产品与Flash连接。
686430776,bf7c9a1f5ce29b2dc989909b054884ce,上进AI中心,https://api.zhihu.com/people/bf7c9a1f5ce29b2dc989909b054884ce,people,,设计人进！AI绘画到底是什么？,article,https://api.zhihu.com/articles/686430776,这一场景让人不禁想起七年前人们对于“阿尔法狗”的热议… 所以，“<em>AI</em>绘画”到底是个啥。 [图片] “此<em>AI</em>非彼AI” 这里的AI指的是<em>人工智能</em>（Artificial intelligence），而非Adobe illustrator。 [图片] 一、<em>AI</em>绘画到底是什么想必大家已经在网络…,2,0,0,1710152202,1712136357,"“你知道AI绘画吗？”“我怎么不知道，不就是最近很火的那个吗”从前年开始，“AI绘画”话题不断登上热搜，不仅引起了画家们的热烈讨论，还在网络上引发了一系列争议和讨论，甚至波及到现实生活。这一场景让人不禁想起七年前人们对于“阿尔法狗”的热议…所以，“AI绘画”到底是个啥。“此AI非彼AI”这里的AI指的是人工智能（Artificial intelligence），而非Adobe illustrator。（不是这个Ai）一、AI绘画到底是什么想必大家已经在网络上看到过AI生成的画面，它既能把一幅照片变成精美的漫画，也可以通过几个关键词就生成一张细节逼真的场景。很多人看到之后不禁会问，这真的是AI画的吗？其实，AI绘画是一种利用人工智能算法和计算机技术生成艺术作品的过程，也可以让计算机辅助人类进行绘画和创作。通过图像识别算法、深度学习等技术，AI绘画可以自动学习和模仿艺术家画作的风格特点，从而生成类似的作品。简单来说，就是通过算法完成的艺术创作。它的工作原理就是用户在海量收集人类已有图像的基础上，可以通过算法对它们进行分类以及识别，然后就可以生成新的图像，整个过程最快只要几秒钟的时间。目前市面上比较有名的AI绘画软件比如Midjourney 、NovelAI、Stable Diffusion、OpenAI DALL-E、Disco Diffusion、Google Imagen、国内的百度文心一言、意间AI等。在AI绘画过程中，AI就相当于雕塑家，玩AI的人就相当于甲方,纯噪声图就相当于一块布满纹理的大理石。首先甲方给雕塑家提出要求，比如想要刻出胜利女神，她的姿态、动作、神色、服饰等等（越详细就越可能得到甲方想要的效果）接着雕塑家便会观察这块大理石的纹理，发挥想象看哪处像面孔，哪处像翅膀，确定雕塑大体的布局。然后雕塑家一步一步开凿石头，每经历一步雕塑的细节就会越丰富，越精细，最后变成一件雕塑。这就是SD模型生成图片的底层原理了，如果去掉技术算法原理，可以说是非常的简单粗暴了。二、AI绘画可以做什么目前AI绘画有两个大的方向，第一个是直接从无到有，一句提词（prompt）生成图像。另一个方向是以图生图，即在一张原始图上做出改变。前者更有趣，从无到有，创造性更强；后者约束性更强，可以使图片朝着自己的预期方向发展。那AI绘画到底可以应用在哪些领域呢？1、游戏、电影、动漫等IP制作AI绘画可以用于游戏和影视制作中的场景和角色设计，可以快速生成大量高质量的图像和动画，大大加快项目开发进程。2、 工业设计AI绘画在工业设计中的应用也非常广泛。工业设计是一个非常复杂的过程，需要考虑到许多因素，例如产品功能、用户体验、外观设计等等。AI绘画可以在这个过程中起到很大的作用，例如：1.产品设计：AI绘画可以生成和优化产品设计方案，从而提高产品质量和生产效率。2.外观设计：AI绘画可以生成不同风格和风格的外观设计方案，使得产品更加美观和吸引人。3.用户体验：AI绘画可以用于虚拟现实和增强现实技术中，生成逼真的虚拟场景和产品模型，帮助用户更好地了解和体验产品。4.制造工艺：AI绘画可以生成或优化制造工艺，提高生产效率和产品质量。3、建筑设计AI绘画在建筑设计中的应用非常广泛，可以提高建筑质量、建筑效率、可持续性，同时也可以帮助设计师更好地实现创意和想法：1.建筑设计：AI绘画可以生成和优化建筑设计方案，从而提高建筑质量和建筑效率。2.环境设计：AI绘画可以生成和优化建筑环境设计方案，使得建筑更加适合周围的环境，从而提高建筑的可持续性和生态性。3.建筑材料：AI绘画可以生成和优化建筑材料的设计和选择，从而提高建筑的质量和经济效益。4.结构设计：AI绘画可以生成和优化建筑的结构设计方案，从而提高建筑的安全性和稳定性。4、原画/海报5、电商AI绘画可以帮助电商平台展示产品的外观、细节和材质等信息，吸引用户的注意力，提高产品的展示效果。同时，利用AI绘画技术，电商平台可以对商品进行视觉化呈现，增强商品的吸引力，提高用户的购买意愿。一键换模特商品效果图6、KV淘宝天猫都已经用AI做KV啦~7、COSsource：养蜂人AI所以！AI绘画应用场景太多了，除了蜂蜂上文中介绍的几个领域外，AI绘画还可以用于医疗和科学领域中的图像和模拟制作，可以帮助医生和科学家更好地观察和研究人体和自然界的现象。此外还有摄影、动画、头像、二次元转三次元、三次元转二次元、IP二创等...还有更多领域等待大家探索去探索。听到这里是不是心动了？为了帮助大家快速掌握AI时代的必备技能，实现设计人降本增效的目的，上进学社近日开展了《AI绘画直播教程》活动扶持力度大、0基础上手快、学完即用，让初次接触AI的设计师找到了快速变现的路径。为解决不懂使用AI作为辅助工具的设计爱好者，给予多方面的支持！AI直播--【0基础-AI绘画直播教程】我们注重商业落地「讲基础、讲实战、讲变现」参加完后你将学会：1、制作儿童绘本及表情包2、隐形艺术字及微信红包封面3、龙年卡通插画上色4、AI绘画内部资料3天直播课带你全面了解AI绘画全程社群服务保你不掉队！------------------------------选择【AI绘画变现训练营】的四大理由1、配置全套实用AI工具，轻松上手这段时间不少人都陆续用上了AI工具，但发现不是卡顿，就是出图速度慢，还动不动提示出错。上进学社投入了大量精力研发AI平台，里面包含了AI绘画、AI写作、DALL-E、画廊、写作、思维导图等功能。优化使用体验，让你感受5分钟完成一天工作的爽感！无论是工作、生活、学习，都能派上用场。2、探索AI绘画于商业应用落地先锋带队资深讲师深入浅出，研发出这套小白都能轻松学习的【AI绘画变现训练营】区别于其他纯念稿、纯PPT录制的枯燥课，小P老师会用风趣幽默的语言，结合前沿AI应用知识，分享超全干货！3、结合真实场景专项训练，人人都能用上的AI技能市面上不少课程，只是纯粹的工具课，随便演示几个功能就结束了，缺乏明确的行业使用场景。我们课程结合商业落地：比如如何让AI快速出几十张设计图、对图片进行换脸/换服装……用完知识点就实操上，真正学有所得！课程直播期间，还可以在群内提问，跟小P老师来个面对面交流！4、助教在线辅导，为你答疑解惑实操过程中碰到有不懂的，也不用去百度到处搜索提问，在铺天盖地的广告中浪费时间。课程学习过程中，全程群内都有助教老师在线回复指导，回答超及时！！--------------------------------前100位免费领取权限通道↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓AI直播上进学社的【AI绘画直播教程】，最大的课程价值就是体现出AI绘画商业落地。协助你打造作品履历，为商业接单就业铺垫认真跟课；认真作业；优秀学员我们会给予接单实习机会✨直接让学员对应需求方，有机会成为长期稳定的供应商✨首次接单，怎么接？怎么沟通？注意什么？背后都有老师指导！✨接单实习完毕后真金白银到你账户目前市面上，哪家能做到，愿意让你直接去对接需求方？！要知道，第一单做满意了，学员可以跟需求方建立长期的合作关系。这就是我们的实力，在教研教学上我们是认真的，为学员的职业赋能！！！--------------------------随着人工智能的推广，具备AI技能的人将更容易适应未来岗位需求，成为首批吃到红利的稀缺人才。与其担心被取代，不如趁趁早学AI，给自己的人生来个翻天覆地的变化！点击下方卡片，马上约课！AI直播不用米！不用米！不用米！"
146460025,3656e5c07bd44783f153f9a20e93ef01,Loong精选,https://api.zhihu.com/people/3656e5c07bd44783f153f9a20e93ef01,people,http://nicelife.me 好玩又涨知识，总有你喜欢的！,人工智能（AI）常见概念简介,article,https://api.zhihu.com/articles/146460025,欢迎朋友们共同学习。 先说什么是<em>人工智能</em>，饭店有点餐和传菜机器人、汽车有自动驾驶系统、手机有语音助手、那这些都是<em>人工智能</em>吗？真正的人工智能是什么？ 人工智能（Artificial,38,0,86,1591512003,1591512003,"这几年，人工智能越来越热门，相关宣传随处可见，好像不谈谈人工智能都太落伍了，那究竟什么是人工智能呢？与之相关的“机器学习”、“自然语言理解”、“深度学习”等专业词汇都都是什么意思呢？http://nicelife.me本文将尝试用较为通俗的语言对这些问题和行话进行一点简单介绍，权当是加深对一些聊天话题的理解吧，欢迎朋友们共同学习。先说什么是人工智能，饭店有点餐和传菜机器人、汽车有自动驾驶系统、手机有语音助手、那这些都是人工智能吗？真正的人工智能是什么？人工智能（Artificial Intelligence），重点应该在“智能”，既然是智能，那它应该具备判断和分析的能力，而不能只是一个自动化工具。一个驾驶系统，不管它的外在表现是一个应用软件还是一个人形设备，如果它能实时分析行驶环境和路况并作出判断，从而进行驾驶，那它就更像是人工智能，反之，如果只是按照设定好的条件去执行一些驾驶操作，那就只能算是个自动驾驶工具。http://nicelife.me再说得通俗点，人工智能指的是“脑子”，而不是身体，我们期待的人工智能应该是能像人一样思考并行动，尽管目前还不能完全达到这样的水平。如果把人工智能的实现比作一场交响乐演奏，那就需要很多乐器很好地配合才能完成，机器学习、深度学习、自然语言理解、上下文感知就是人工智能交响乐的主要组成乐器，接下来我们逐一来了解这些概念。机器学习（Machine Learning），从字面意思就可以大概知道这个术语的含义，古语云：“人不学，不知义”，我们期待机器具备人类一样的智慧，同样也得让它学习，所以机器学习是人工智能的前奏。http://nicelife.me关于人类学习，有一句古语说：“读书破万卷，下笔如有神”，那机器学习的主要方式是什么呢，就目前的情况来看，机器学习也和人差不多。当前主流的机器学习方式，专业说法叫模型训练，就是让机器从庞大的数据集中去学习规律，进而具备做出决策和行为的能力，模型训练是一个不断修正和完善的过程，很多时候需要人类干预，所以也叫监督学习。举个例子，现在很多网站用人工智能进行色情图片鉴别，那机器怎么知道什么样的图片含有色情内容呢，最初，大多是设置一些条件，比如暴漏乳房、下体的就判断为色情，这其实算不上人工智能，只能算是一种节省人力的自动化工具，这样的自动化工具对于不漏点的色情图片（想象一下两个人侧身抱在一起后入式爱爱，背对着镜头）就无能为力了，而且对于设定条件的自动工具，误判的情况也特别严重，著名的社交网站facebook就曾经因下线这张图片而遭非议。http://nicelife.me那人工智能是怎么训练色情图片鉴别模型的呢？目前的做法是将大量的图片进行标注，然后让机器去学习这些图片，在人工干预的情况下完成模型训练，从而在遇到新的图片时就可以根据模型给出一个是不是色情图片的几率数据。深度学习（Deep Learning），是机器学习的一个分支，它是参考人类大脑的神经元之间传递、处理信息的模式来实现机器学习的方法，通过多层人工神经网络层层递进抽象出处理问题的模型。http://nicelife.me拿人脸识别来说，世界上没有完全一样的两张脸，那计算机怎么能识别呢？人工神经网络的做法大体上来说就是第一层神经网络先获取到人脸图片的像素分布，第二层人工神经网络再学习到哪些像素是组成人类轮廓的，第三层再抽象出一些其他脸部特征……直到最后能抽象出一张特定面孔的特征模型。值得一提的是，多次战胜世界围棋冠军的alphago,其核心技术就是深度学习。自然语言处理（Natural Language Processing），就是让计算机理解人类语言。计算机世界的一切，在底层都是通过二进制数字“0”和“1”来表示的（比如“Loong精选”，在计算机里面就是“01101100011011110110111101101110011001111111100101111101001000000001001”）。过去人类为了方便和计算机沟通，创造了计算机程序语言，但是人工智能要求计算机能具备人类智慧，那就得想办法让计算机理解人类语言，比如你对着苹果手机的siri说一句“打开好玩又长知识的Loong精选博客”，它得首先能听懂你的要求，然后才会打开浏览器，输入Loong精选博客的网址“ｈｔｔｐ://nicelife．me“。目前来说，自然语言处理也处于探索完善阶段，所以我们看到的不管是苹果的siri、亚马逊的alexa、三星的bixby还是谷歌助手都还是比较笨的。http://nicelife.me上下文感知（Context Awareness），人工智能和人类一样，你给它的信息越多，它就越聪明，上下文感知就是给计算机更多的关联信息并让它把这些信息统筹起来，以便让他更好地工作。比如一个人工智能助手，只是告诉它帮你订一份午餐，结果可能不会让你很满意，如果让它知道你的位置、喜欢的餐厅、喜欢的口味、饭后是否要驾车等信息，那这个人工智能助手就可能在综合这些信息后帮你订一份满意的午餐。http://nicelife.me人工智能（AI）经历了几十年的发展，是一个不断探索和完善的过程，期间经历过很多曲折，也产生了像“深蓝“、”alphago“这样的成果，得益于云计算、大数据等技术的出现和发展，给机器学习（ML）提供了计算能力和数据处理方面的基础支撑，人工智能在我们生活中的应用将会越来越普遍，继上篇 《请先看这几部电影，咱再来聊人工智能》，Loong精选希望和朋友们一起对人工智能有个初步的认知，一起交流学习！请关注Loong精选，好玩又长知识，总有你喜欢的！"
627642632,6ab5d9817bdcadd5eeaff0ccba56c61e,绝密伏击,https://api.zhihu.com/people/6ab5d9817bdcadd5eeaff0ccba56c61e,people,《推荐系统技术原理与实践》作者，欢迎知友京东购买。,大模型微调总结,article,https://api.zhihu.com/articles/627642632,技术中的常用方法 LORA，使得百亿（10B）参数的<em>大模型</em>可以在单卡上训练（显存大小&gt;=40G）。绝密伏击：LORA：<em>大模型</em>轻量级微调 今天介绍下另外几种常用的方法，包括,1026,17,3077,1683532754,1704976309,"最近，深度学习的研究中出现了许多大型预训练模型，例如 GPT-3、ChatGPT、GPT4、ChatGLM-130B 等，这些模型可以在多种自然语言处理任务中取得优异的性能表现。而其中，ChatGPT 模型因为在对话生成方面的表现而备受瞩目，成为了自然语言处理领域的热门研究方向。然而，这些大型预训练模型的训练成本非常高昂，需要庞大的计算资源和大量的数据，一般人难以承受。这也导致了一些研究人员难以重复和验证先前的研究成果。为了解决这个问题，研究人员开始研究 Parameter-Efficient Fine-Tuning (PEFT) 技术。PEFT 技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT 技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。在上一篇文章中，介绍了 PEFT 技术中的常用方法 LORA，使得百亿（10B）参数的大模型可以在单卡上训练（显存大小>=40G）。绝密伏击：LORA：大模型轻量级微调今天介绍下另外几种常用的方法，包括 Adapter Tuning、Prompt Tuning、Prefix Tuning、P-Tuning、P-Tuning v2 和 AdaLoRA。1. Adapter Tuning2019年谷歌的研究人员首次在论文《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-Fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。于是他们设计了如下图所示的 Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：首先是一个 down-project 层将高维度特征映射到低维特征然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构）。从实验结果来看，该方法能够在只额外对增加的 3.6% 参数规模（相比原来预训练模型的参数量）的情况下取得和Full-Finetuning 接近的效果（GLUE指标在0.4%以内）。2. Prefix Tuning2021年斯坦福的研究人员在论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了 Prefix Tuning 方法。与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。该方法其实和构造 Prompt 类似，只是 Prompt 是人为构造的“显式”的提示，并且无法更新参数，而Prefix 则是可以学习的“隐式”的提示。同时，为了防止直接更新 Prefix 的参数导致训练不稳定的情况，他们在 Prefix 层前面加了 MLP 结构(相当于将Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果)，训练完成后，只保留 Prefix 的参数。embedding = torch.nn.Embedding(num_virtual_tokens, token_dim) transform = torch.nn.Sequential( torch.nn.Linear(token_dim, encoder_hidden_size), torch.nn.Tanh(), torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim), )3. Prompt TuningPrompt Tuning 是2021年谷歌在论文《The Power of Scale for Parameter-Efficient Prompt Tuning》中提出的微调方法。该方法可以看作是 Prefix Tuning 的简化版本，只在输入层加入 prompt tokens，并不需要加入 MLP 进行调整来解决难训练的问题，主要在 T5 预训练模型上做实验。似乎只要预训练模型足够强大，其他的一切都不是问题。作者也做实验说明随着预训练模型参数量的增加，Prompt Tuning的方法会逼近 Fine-tune 的结果。固定预训练参数，为每一个任务额外添加一个或多个 embedding，之后拼接 query 正常输入 LLM，并只训练这些 embedding。左图为单任务全参数微调，右图为 Prompt tuning。作者做了一系列对比实验，都在说明：随着预训练模型参数的增加，一切的问题都不是问题，最简单的设置也能达到极好的效果。Prompt 长度影响：模型参数达到一定量级时，Prompt 长度为1也能达到不错的效果，Prompt 长度为20就能达到极好效果。Prompt初始化方式影响：Random Uniform 方式明显弱于其他两种，但是当模型参数达到一定量级，这种差异也不复存在。预训练的方式：LM Adaptation 的方式效果好，但是当模型达到一定规模，差异又几乎没有了。微调步数影响：模型参数较小时，步数越多，效果越好。同样随着模型参数达到一定规模，zero shot 也能取得不错效果。当参数达到100亿规模与全参数微调方式效果无异。from peft import PromptTuningConfig, get_peft_model peft_config = PromptTuningConfig(task_type=""SEQ_CLS"", num_virtual_tokens=10) model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True) model = get_peft_model(model, peft_config)4. P-Tuning v1P-Tuning 方法的提出主要是为了解决这样一个问题：大模型的 Prompt 构造方式严重影响下游任务的效果。P-Tuning 提出将 Prompt 转换为可以学习的 Embedding 层，只是考虑到直接对 Embedding 参数进行优化会存在这样两个挑战：Discretenes： 对输入正常语料的 Embedding 层已经经过预训练，而如果直接对输入的 prompt embedding进行随机初始化训练，容易陷入局部最优。Association：没法捕捉到 prompt embedding 之间的相关关系。作者在这里提出用 MLP + LSTM 的方式来对 prompt embedding 进行一层处理： P-tuning 依然是固定 LLM 参数，利用多层感知机和 LSTM 对 Prompt 进行编码，编码之后与其他向量进行拼接之后正常输入 LLM。注意，训练之后只保留 Prompt 编码之后的向量即可，无需保留编码器。self.lstm_head = torch.nn.LSTM( input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_layers, dropout=lstm_dropout, bidirectional=True, batch_first=True, ) self.mlp_head = torch.nn.Sequential( torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2), torch.nn.ReLU(), torch.nn.Linear(self.hidden_size * 2, self.output_size), ) self.mlp_head(self.lstm_head(input_embeds)[0])4.1 与Prefix-Tuning的区别P-Tuning 和 Prefix-Tuning 差不多同时提出，做法其实也有一些相似之处，主要区别在：Prefix Tuning 是将额外的 embedding 加在开头，看起来更像是模仿 Instruction 指令；而 P-Tuning 的位置则不固定。Prefix Tuning 通过在每个 Attention 层都加入 Prefix Embedding 来增加额外的参数，通过 MLP 来初始化；而 P-Tuning 只是在输入的时候加入 Embedding，并通过 LSTM+MLP 来初始化。5. P-Tuning v2P-Tuning 的问题是在小参数量模型上表现差（如上图所示）。于是就有了v2版本：《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》。从标题就可以看出，P-Tuning v2 的目标就是要让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果。那也就是说当前 Prompt Tuning 方法在这两个方面都存在局限性。不同模型规模：Prompt Tuning 和 P-tuning 这两种方法都是在预训练模型参数规模够足够大时，才能达到和Fine-tuning 类似的效果，而参数规模较小时效果则很差。不同任务类型：Prompt Tuning 和 P-tuning 这两种方法在 sequence tagging 任务上表现都很差。5.1 主要结构相比 Prompt Tuning 和 P-tuning 的方法， P-tuning v2 方法在多层加入了 Prompts tokens 作为输入，带来两个方面的好处： 带来更多可学习的参数（从 P-tuning 和 Prompt Tuning 的0.1%增加到0.1%-3%），同时也足够 parameter-efficient。加入到更深层结构中的 Prompt 能给模型预测带来更直接的影响。v1 到 v2 的可视化：蓝色部分为参数冻结，橙色部分为可训练部分。5.2 几个关键设计因素Reparameterization：Prefix Tuning 和 P-tuning 中都有 MLP 来构造可训练的 embedding。本文发现在自然语言理解领域，面对不同的任务以及不同的数据集，这种方法可能带来完全相反的结论。Prompt Length： 不同的任务对应的最合适的 Prompt Length 不一样，比如简单分类任务下 length=20 最好，而复杂的任务需要更长的 Prompt Length。Multi-task Learning 多任务对于 P-Tuning v2 是可选的，但可以利用它提供更好的初始化来进一步提高性能。Classification Head 使用 LM head 来预测动词是 Prompt Tuning 的核心，但我们发现在完整的数据设置中没有必要这样做，并且这样做与序列标记不兼容。P-tuning v2 采用和 BERT 一样的方式，在第一个 token 处应用随机初始化的分类头。5.3 实验结果不同预训练模型大小下的表现，在小模型下取得与 Full-finetuning 相近的结果，并远远优于 P-Tuning。不同任务下的 P-Tuning v2 效果都很好，而 P-Tuning 和 Prompt Learning 效果不好；同时，采用多任务学习的方式能在多数任务上取得最好的结果。6. AdaLoRA预训练语言模型中的不同权重参数对下游任务的贡献是不同的。因此需要更加智能地分配参数预算，以便在微调过程中更加高效地更新那些对模型性能贡献较大的参数。具体来说，通过奇异值分解将权重矩阵分解为增量矩阵，并根据新的重要性度量动态地调整每个增量矩阵中奇异值的大小。这样可以使得在微调过程中只更新那些对模型性能贡献较大或必要的参数，从而提高了模型性能和参数效率。7. Towards a Unified View of PETL 这篇 ICLR2022 的文章研究了典型的 PEFT 方法，试图将 PEFT 统一到一个框架下，找出它们起作用的具体原因，并进行改进。主要研究了三个问题：典型的 PEFT 方法有什么联系？典型的 PEFT 方法中是哪些关键模块在起作用？能否对这些关键模块进行排列组合，找出更有用的 PEFT 方法？7.1 通用形式通过对 Prefix Tuning 的推导，得出了和 Adapter Tuning 以及 LoRA 形式一致的形式。通过对Prefix Tuning的推导，得出了和Adapter Tuning以及LoRA形式一致的形式。包括这几大要素： 的形式嵌入 Transformer 结构的方式（分为 Parrell 和 Sequential 两种。Parallel 指的是在输入层嵌入，这样与原有结构可以并行计算；Sequential 指的是在输出层嵌入，相当于增加了网路的深度，与原有结构存在依赖关系）修改表示层（主要指对 attention 层的修改还是对 ffn 层的修改）组合方式。怎么与原有的参数组合，包括简单相加（Adapter）、门控式（Prefix Tuning）、缩放式（LoRA）三种）根据这个统一的框架，还另外设计了三种变体 Parallel Adapter、Multi-head Parallel Adapter、Scaled Parallel Adapter。参考Parameter-Efficient Transfer Learning for NLPPrefix-Tuning: Optimizing Continuous Prompts for GenerationThe Power of Scale for Parameter-Efficient Prompt TuningBitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-modelsGPT Understands, TooTOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNINGUNIPELT: A Unified Framework for Parameter-Efficient Language Model TuningLadder Side-Tuning：预训练模型的“过墙梯”INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNINGPrompt-Tuning——深度解读一种新的微调范式P-tuning：自动构建模版，释放语言模型潜能P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Taskshttps://aclanthology.org/2022.acl-short.8.pdfhttps://arxiv.org/pdf/2110.07602.pdfhttps://www.yuque.com/meta95/hmc3l4/ozgy13dx4akv7v17?singleDoc#无数据不智能：大模型训练之微调篇https://arxiv.org/pdf/2303.10512.pdfGitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.https://arxiv.org/pdf/2110.04366.pdf书籍推荐"
668620902,93d99722b6053513c29952ca23c1ba70,赵宏伟,https://api.zhihu.com/people/93d99722b6053513c29952ca23c1ba70,people,上市公司总监,AGI时代，程序员想学习大语言模型（LLM），应该从哪里开始？,article,https://api.zhihu.com/articles/668620902,对于程序员的新一轮革命，我觉得人工智能<em>大模型</em>方向是你可以认真审视的一个方向，你可以选择通过它创业，也可以选择进入这个领域，它是一个更加开放的领域，也让程序员有更多的选择，也能创造无限的可能。 <em>大模型</em>技术栈不断更新发展，作为程序员的我们该如何应用<em>大模型</em>技术实现自己的技术飞跃，哪怕只是实现效率提升呢,434,26,0,1700883770,1701749680,"程序员25岁做什么能够改变35岁被淘汰的命运？对于程序员的新一轮革命，我觉得人工智能大模型方向是你可以认真审视的一个方向，你可以选择通过它创业，也可以选择进入这个领域，它是一个更加开放的领域，也让程序员有更多的选择，也能创造无限的可能。大模型技术栈不断更新发展，作为程序员的我们该如何应用大模型技术实现自己的技术飞跃，哪怕只是实现效率提升呢！相信很多的伙伴都有这样的想法，期望通过应用大模型去做一些自己的应用。加上前段时间OpenAI发布会介绍的GPTs，更是无限降低了技术人才以及非技术人才去开发自己的应用门槛。当然，更深层次的应用是我们做技术的更向往的，不管是AI编程，还是模型微调，甚者想要实现自己的模型私有化。这篇文章，我将分享自己应用大模型技术迈出的第一步，希望对你有帮助。2022年底，随着ChatGPT的横空出世，大模型时代来临了，千亿甚至万亿参数的大模型陆续出现，各大企业、高校争相发声，纷纷推出自己的大模型，这些都预示着通用智能时代的到来。那么我们可以做些什么呢？作为一个技术开发者，ChatGPT的出现带给我巨大的震撼，也促使我开始思考，相信就像很多人思考的一样，在大模型时代的洪流下，我会被大模型所取代吗？我能够做些什么来抵挡它带来的冲击，他又会带给我什么样的机遇呢？我开始在各大网站上搜索大模型相关的信息，希望能够尽量多的了解一些大模型的知识，得到一些指导，但很遗憾，大部分信息要么浅尝辄止，要么专业性极强，这些都不是我想要的，我亦很难在其中汲取到我需要的养分。直到有一天，一个知乎上的老师联系我，他推荐我听两堂关于大模型的公开课，我半信半疑的接受了。第一堂是关于AI大模型颠覆程序员的价值，第二堂是LangChain框架和如何Fine-tune实现模型的定制化，这节课的内容技术性非常强，但老师的讲解深入浅出，令我醍醐灌顶，我想我知道了应该怎样做，以及我会被取代吗？不会的，所有与时俱进的大家都不会，AI是我们的帮手，不是对手，它的出现只会让我们的工作更便捷，更轻松，而我们需要做的，就是尽快了解它，掌握它。当然我也极力推荐你去听一下他们的公开课，干货满满，也会让你重新审视自己的工作方式，会让你对自己未来长远发展有一个新的规划，赶快行动起来。基于所有这些思考，我报名了《AI全栈工程师培养计划》，我相信我能够有所收获，能够追寻到我想要的答案。我上了第一堂课，是孙志岗老师给讲的，他是哈工大的副教授，还独立开发了ChatALL,此外还有非常多的title，是一个有实战和影响力的开发者。整堂课老师从三个层面进行了展开，这堂课对我影响深远，并对未来充满了希望，在此记录下来，跟大家一起分享。一、 怎样学好，并应用大模型AGI（Artificial General Intelligence，通用人工智能）时代，懂AI、懂编程、懂业务的超级个体，会是AGI时代最重要的人。为了成为这样的超级个体，我们需要在哪几个方向发力呢？那就是：原理、实践和认知。不懂原理就不会举一反三，走不了太远。不懂实践就只能纸上谈兵，做事不落地。认知不高就无法做对决策，天花板太低。接着，老师给我们展示了我们学员的职业构成，我从来没有见过有哪一门课，学员的组成是这样的复杂与多样。而这也反证了，大模型时代，对各行业的大家都提出了要求，也都给予了机遇，我们站在风口上，应当抓住它。下面我放一张关于这个课程的学习人群，我们也能看出来各个行业领域，看看是否你也是其中一员。学习大模型技术人才岗位比例孙老师在课上也给同学们吃了几颗定心丸，这也让我认识到多模态新一代人工智能的变化之大，相较于过去得确存在很大的变化，我总结以下核心几点：1. 用好AI需要很多数学知识吗？以前真需要，以后可能不那么需要了。2. 有的知识太深，我理解不了怎么办？[用到再讲，学以致用]是这堂课的教学理念，很多知识，用着用着就了解了；少部分数学知识主要集中在数据和模型训练部分，不理解，也不影响使用。3. 感觉讲的太浅了，没学到知识怎么办？更深的东西，在实践中未必有用；欢迎随时在群里提问。4. 课听懂了，实际工作中还是不会用怎么办？NLP的实际问题通常不是由一个算法解决的，我们把拆解问题的思路融入课程中，用心体会。在了解了这些之后，接下来我们看一些更干、更实的内容，大模型到底能干什么？大模型能干什么？大模型，全称「大语言模型」，英文「Large Language Model」，缩写「LLM」。例如以下这些都是目前访问量较高的大语言模型，我们能够看到越来越多的头部公司、创业公司开始布局大模型，我也给大家放了访问地址，大家可以自行体验：ChatGPT：https://chat.openai.com/Bing Chat：https://bing.com/new文心一言：https://yiyan.baidu.com/讯飞星火：https://xinghuo.xfyun.cn/智谱清言：https://chatglm.cn/当然，我们不难发现，在大模型快速发展的一年，ChatGPT 仍然是当家扛把子，别人追赶的对象。对于问答类的大模型大家难免会有疑问，那大语言模型只能充当聊天机器人吗？我的回答是：NONONO，它的应用场景，远不止于此。在图像、音视频、编程、文案、知识库等应用上，它都能比传统的处理方式或工具强N多倍，接下来我就结合几个大家常用的案例来抛砖引玉一下。1. 按格式输出我们提供给大模型一个收件地址，并用自然语言描述我们希望它输出的格式，那么大模型就会按照“name”、“address”以及“phone”三个字段对进行整合。2. 分类我们提供给大模型一个新闻标题，以及几个类别，让大模型为我们在这几个类别里面进行分类，那么大模型就会给出它认为的答案并输出。3.聚类我们提供给大模型几个句子，并让大模型根据它自己的理解，对这几个句子进行聚类，以数组的形式进行输出，可以看到大模型也给出了它认为的答案。4.持续互动当然大模型也可以与用户进行多轮对话，即持续互动，以下举了一个下棋的例子，首先将三个子连成一行的那方赢，第一轮对话中我们给大模型介绍了游戏规则，并以坐标的形式放了4个棋子，这轮对话结束后，我们继续问大模型“该X方放置棋子了，他应该把棋子放在哪里？”，大模型给出了图示以及这样放置的原因，可见大模型是能够保存上一轮对话的历史的，我们可以基于历史对大模型进行进一步的交流。5. 技术相关问题技术相关的问题肯定是程序员的必问问题之一，以下例子中我们用自然语言描述了我们的需求，我们在Ubuntu的系统中，我们没有安装任何软件包，我们需要运行一个MySQL的数据库，我们应该怎样做，可见大模型给到了我们非常详细的代码。当然了，除此之外，我们还可以将我们运行代码过程中遇到的报错信息粘贴到大模型对话框里，可能下一秒这个问题就解决了~结合上面几个简单的小例子，可能没那么难，这也是它的冰山一角。而这仅仅是一个入门，《AI全栈工程师培养计划》这门课是一个全面、系统、实践创新的综合性课程，旨在培养掌握前沿技术的全栈工程师。该计划包括多个阶段，涵盖了从基础知识到大模型框架、大模型微调的方方面面。我建议你可以先通过公开课，来全方位剖析一下在你的工作领域有哪些工作是可以通过大模型来进行优化。相信你也比较好奇，大模型是怎样生成结果，接下来我们一起来深入探讨一下。大模型如何生成结果其实，大模型只是根据上文，猜测下一个词（的概率），相当于逐字逐句的生成，而OpenAI 的接口名就叫「completion」，也证明了其只会「生成」的本质。举个例子：假如我们将“今天我很”的prompt给到大模型，那么他就会返回“高兴，因为我学会了如何使用github”，我们也不知道他为什么返回这句话，但这个例子证明了大模型的回复是一个字一个字依次往后生成的，且每次生成的字都是概率最大的字。简单解释下以下代码，首先我们需要导入openai的包，然后导入我们自己的api_key，如果有小伙伴还不知道api_key是怎样生成的以及有什么作用，后面我会单独再出一篇文章来讲解一下。这些给出以后，我们定义prompt，也就是给到大模型的输入“今天我很”，定义一个response参数来接收大模型的返回结果，其中需要的参数包括：model（我们选用的大模型是哪个）、prompt（输入模板）、max_tokens（最大返回字符数）、temperature（回答随机性大小）、stream（是否回持续生成文本，知道用户手动停止或达到设定的生成长度）。我们可以通俗的解读一下ChatGPT 的工作原理：GPT「大模型」阅读了人类曾说过的所有的话。这就是「学习」把一串 token 后面跟着的不同 token 的概率记下来。记下的就是「参数」，也叫「权重」当我们给它若干 token，GPT 就能算出概率最高的下一个 token 是什么。这就是「生成」用生成的 token，再加上上文，就能继续生成下一个 token。以此类推，生成更多文字在了解了我们相关原理之后，我们浅浅来解析一下大模型的开发框架，我们想要学习并应用大模型，那么我们对大模型的原理还是要非常熟悉的。大模型应用架构大模型技术分两个部分：1. 训练基础大模型：全世界只需要1000个人2. 建造大模型应用：所有技术人员都应该掌握目前大模型在业务中是如何应用的呢？典型的业务架构有以下三种。AI Embedded模式是指，大模型技术在业务中仅仅参与某一个单独的分支，是大模型刚刚介入业务的初期阶段。AI Copilot是指大模型已经逐渐参与到业务中的每个分支，但是还需要人做总体把控与业务派分，是大模型发展的中期阶段。AI Agent是指大模型已经完全可以独立完成业务分派与执行，人工只需要在最初的时候进行发号指令。目前Agent还太超前，我们正处于由AI Embedded追求AI Copilot的阶段。以上便是我在第一堂课中的收获，接下来还有两个月的课程，从一开始的犹豫不决到现在，我十分确定，我一定可以在这系列课程中有所收获，我也将随时将课后的思考与领悟同大家分享。"
693923072,6736256b9f1cc119e59101b6562cc6c5,悦木Ivy陪娃成长,https://api.zhihu.com/people/6736256b9f1cc119e59101b6562cc6c5,people,知势榜上榜答主｜家有小学生 | AI产品经理 | 合作看简介,大模型的终局是「通用」还是「专用」？,article,https://api.zhihu.com/articles/693923072,随着<em>大模型</em>领域的大跨步发展，通用能力不断提升，各种各样的垂类<em>大模型</em>也层出不穷，这不禁令人思考一个问题：<em>大模型</em>的终局是「通用」还是「专用」呢？ 我的答案是：<em>大模型</em>的终局为， 演进成为完整的生态。 <em>大模型</em>的终局，不可能是一颗孤零零的“超级大脑” 回看计算架构的发展历史,86,13,0,1713837276,1713837351,随着大模型领域的大跨步发展，通用能力不断提升，各种各样的垂类大模型也层出不穷，这不禁令人思考一个问题：大模型的终局是「通用」还是「专用」呢？我的答案是：大模型的终局为，演进成为完整的生态。大模型的终局，不可能是一颗孤零零的“超级大脑”回看计算架构的发展历史，从集中式架构到C/S架构、然后开始倡导瘦客户端、上云、然后又开始端侧边侧甚至离线运算，公有云、私有云、混合云、分布式云、边缘计算、云边协同。。。“数字化底座”在演变和创新中一路狂奔， 各种层出不穷的词语看得人脑袋冒烟。就这还漏掉了从单片机开始展开的嵌入式计算脉络。为啥搞这么复杂？因为用户需求场景不同呗。集中特别好，效率高。但是不安全啊，不放心啊，实时性差啊，不能实现个性化需求啊。。。所以这么来来回回的折腾，无非就是出于这几个方面的考虑：1、个性化定制需求2、数据隐私安全需求3、实时性需求4、资源有限制情况下的需求尤其随着IoT、5G、AI和互联网的发展，业务场景需求出现了井喷级别的增长和细化。比如智能驾驶场景，对实时性的要求是变态级别的。上云计算？玩呢？这延时谁敢开？再比如金融行业，数据必须跟自己这锁着，谁都不能给。还有在战场、灾区等极端恶劣环境下作业的设备，必须离线工作。这些场景的需求各不相同，但是每个需求都是刚性的，实实在在的。这些需求就决定了，在这些业务之下，必须有多样化的、成体系的计算架构进行支撑。之于大模型，是一模一样的道理。通用模型负责“博学”，解决通用问题；而专用模型，必须要有更丰富的专业知识、更贴心的业务知识、更精准的服务、更安全的保障。——还有成本开销问题。例如在车牌号识别、人脸识别等非常窄的场景下，小模型就足够精准、足够经济了。所以，大模型的终局，不可能是一颗孤零零的“超级大脑”，注定是在不断地演进中成长为一个枝繁叶茂的生态。下一站，大模型生态GPT4之后，OpenAI并未甚嚣尘上地推出GPT5，而是紧锣密鼓地拿出Assistant API、GPTs，做生态的决心昭然若揭。这也充分印证了，“百模大战”不过是AI大厂开战前的的号角，在这之后，必定是大模型生态的角逐。所有有野心的厂商都在努力成为一个生态的“创世者”：生产更智能的通用AI;提供一站式定制化工具链，为下游客户提供数据处理、特征工程、模型训练、模型调优、推理部署等服务；同时发展端侧大模型，让你无论是购买通用AI服务、还是定制专有、私有模型、还是直接开发大模型原生应用，都能获得最方便、最高效、最经济的服务。在这个角逐中胜出意味着什么，回望“万物上云”时代的格局，一切就明了了。大模型生态的搏杀，才是真正残酷的角逐！胜出者将成为整个生态的王，失败者会被吸收掉，化身为生态中的一环，所谓一鲸落，万物生。在这场波澜壮阔的变革中，普通技术人员以及个人创业者的路该怎么走呢？李彦宏不止一次的在公开场合给出过明示：「卷大模型没意义，卷应用机会更大」。建议每一个技术人员，都去听一下知乎知学堂和AGI课堂联合推出的免费公开课——程序员的AI大模型进阶之旅。由几位业内大佬主讲，从GPT到开源模型再到AI Agent，你会看到一个个人开发者可以以怎样的力量撬动AI原生应用的全栈开发。你还可以在后续的课程中学习整套的数据处理、训练、fine-tune模型等流程。更难得的是它不仅让你了解大模型背后的原理和实操技术，还能从商业的角度带你深度解析此轮AI变革和以往有何根本不同。相信听完公开课之后，你会拥有醍醐灌顶的认知和豁然开朗的思路。真正的AGI时代，会以一统天下的通用模型为终局吗？这个问题也非常好答。我只问你，人类是通用的还是专用的？我们可以将刚“出厂”时的婴儿看作是通用的，“未经训练”的模型。高考之前，是使用人类知识进行预训练的过程。而上过大学之后，就进行了针对不同专业的“增量训练”，可以应对专业场景了。（当然了，人脑可高级多了，每时每刻都在动态实时训练和微调，而且泛化能力极强。）毕业找工作时，如果不是特别专的工种，用人单位不会要求学历；否则一定会要求专业对口。所以人类也是分「通用」和「专用」的，对吧？再不恰当的比一下，找工作签了公司，就是私有化部署；在网上到处接活“灵活就业”，就是提供公有服务。。。另外别忘了一点，人从一生下来，智商就是有差别的！我们是不是可以理解成，对于人类来说，每个模型架构本身就有不同，个性化的非常彻底！可见，无论是从应用场景，还是从自身构造来讲，即使是智能如人类，也不存在通用专用二选一的问题。生态的形成，将是必然！无论你是否愿意看到，一场没有硝烟的斗争正在打响。对于普通人来说，尽早看清楚这一切，在该下注之时稳准狠地下注，或许能够获得远超意料的收获。
687567116,7c3f0e114e69d4d5ac14f803a037ab24,nash,https://api.zhihu.com/people/7c3f0e114e69d4d5ac14f803a037ab24,people,,最通俗的语言搞懂”大模型“的来龙去脉,article,https://api.zhihu.com/articles/687567116,所以可理解为参数量大的、用于生成内容的深度学习模型。<em>大模型</em>根据应用场景不同，可进一步细分成语言<em>大模型</em>Large language model（LLM）、图像<em>大模型</em>、音频大模型、视频大模型等。 哪些常见的AI产品,25,2,58,1710726019,1710726019,"人工智能时代，有很多时髦、相互容易混淆概念的科技名词：AI、Machine Learning、Deep Learning、Generative AI、Large Model，它们指的是同一个概念么？不是的。l AI（artificial intelligence人工智能），它的概念最广泛，所有研究人类智能的技术都可以归为其中。l ML（machine learning机器学习），是AI的一个子集，具体指的是通过计算机从数据中学习规律的技术。l DL（deep learning，深度学习），是ML的子集，指的是一种具体的计算机算法，即神经网络。l Generative AI(生成式人工智能)，跟AI一样也是广泛的概念，但强调其智能是能够“生成内容”，例如生产文本、生成图片、生成声音，这大大区别于传统的discriminative AI(判别式AI)，判别式AI只能预测分类结果，不能生成内容。生成式AI目前使用的算法都是Deep learning。我们现在常用的GPT模型，其全称是Generative Pre-trained Transformer，其中generative是“生成式”的意思，能够生成与训练数据集相似的新数据；Transformer指的是深度学习算法中的一种模型结构。所以，可以理解成深度学习算法在生成内容领域的应用。l Large Model (大模型)，泛指参数很多的机器学习模型，因为现在只有深度学习模型有可能有几十亿上百亿上千亿的参数（传统的机器学习模型的参数一般在十几到几十个之间），并且这些参数量大的模型又都主要用于生成内容，所以可理解为参数量大的、用于生成内容的深度学习模型。大模型根据应用场景不同，可进一步细分成语言大模型Large language model（LLM）、图像大模型、音频大模型、视频大模型等。哪些常见的AI产品目前国外比较有名的大模型公司及其产品。l OpenAI: 人工智能公司，著名的产品是GPT-4、DALL-E 2、Sora。GPT-4是一个大语言模型，可完成翻译、撰写文章、回答内容等各种类似文案工作者的任务；Dalle.E是大规模文本到图像的转换模型，在给定文本提示的情况下生成各种样式的真实图像；Sora是通过文字生成视频的模型，是基于GPT和Dalle模型成果在视频领域的应用。l Google：产品是Gemini(之前叫做Bard)，功能类似GPT，是基于谷歌自研的大语言模型，跟谷歌自身的产品深度绑定，例如在谷歌邮箱中可以协助写邮件。l Anthropic：OpenAI 前员工共同创立的 AI 公司，推出一款名为 Claude 的类似 ChatGPT的AI助手。l Meta(Facebook)：模型产品为LLaMA，参数从 7B 到 70B 不等的大模型，是开源社区中性能最强和最流行的模型。l EleutherAI: 非营利性的人工智能研究实验室，专注于大型模型的可解释性和对齐性，发布了开源的大模型GPT-Neo。l 微软：Copilot，一个人工智能驱动的智能聊天机器人，其背后支撑是OpenAI的GPT4。l GitHub Copilot：编程机器人，具有代码自动完成功能，并且可作为大多数流行代码编辑器的插件使用。l Sourcegraph Cody：生成式 AI 编程助手，可以解释代码、为选定代码段创建单元测试、优化现有代码的能力等。l Jasper：AI 营销工具以及写作助手，作为第三方工具利用多个模型，如 GPT-4、Claude、等生成营销活动、将文案翻译成多种语言、撰写博客文章、搜索引擎优化 （SEO）。l Sensei GenAI：Adobe公司出品的类似Jasper产品。l Midjourney: 类似DALL-E，利用人工智能技术，让计算机创作精美的绘画作品。l Stability AI: 产品DreamStudio 提供与 DALL-E 类似的功能：图像生成、修复、图像修改。l DiffusionBee：开源的AI图像生成工具。l Adobe Firefly: Adobe公司出品的图像生成工具。l Canva公司的Text to Imag：一款AI图像生成工具，Canva是一家视觉传播公司。l DeepMind: 一家被谷歌收购的人工智能公司，围棋人机大战中的机器人AlphaGo就是这家公司的产品。DeepMind推出的Flamingo 是一种视觉语言模型。l Synthesia: 是一个基于人工智能的AI视频生成制作平台，生成多样化的AI数字虚拟人形象，制作视频内容。类似的公司还有Natural Video。l RealSpeaker: 类似科大讯飞，提供声音合成的产品。Stable Diffusion：Stability AI、CompVis和Runway提供的开源产品。它是一个文本到图像的模型，其模型权重和代码是免费提供的。大模型的发展历史大部分机器学习的模型（除了深度学习模型），其模型有效性取决于2个方面。一是所采用的数学模型的假设是否能完美模拟应用场景所发生的事件概率（比如逻辑回归、贝叶斯算法、决策树算法等），二是特征工程的质量（结合行业的专家经验，人工定义出潜在具有预测性的变量，数量往往越多越好）。这些模型训练的过程是，输入加工好的特征，通过算法找出最符合训练样本情况的模型参数。虽然这些模型在很多行业有非常好的应用效果（例如产品推荐）且便于训练和部署，但是在自然语言处理和图像处理领域，传统的机器学习模型效果不好，其原因是这两个领域的数据有很强的多样性且结构更复杂（例如语言中的一词多义，图像中同一种动物的多种姿势），都使得特征工程的难度加大，也很难找到一个简单又完美的数学模型能拟合现实的情况。深度学习模型，也叫神经网络模型(模仿人脑神经元)，是有多层简单的模型单元层层叠加起来。每一层的模型都不复杂，但随着层数增加，就可以很好的进行representation learning（表征学习），学习到数据背后的复杂规律。完全靠这种多层结构，让模型同时完成特征工程和模型参数选择的工作。由于不需要人工来做特征工程，也不需要设计特别复杂的数学模型，使其特别适用于自然语言和图像处理领域。每层的数学模型虽然不复杂，但是其参数并不少，层数多了以后整个模型所学的参数就更多（百万以上），因此其训练所需的数据样本量也很大，训练时间也长。神经网络模型起源于20世纪60年代，很多算法在上个世纪就被发明。例如RNN发明于1990年，LSTM发明于1997年。直到2012年称为AlexNet的神经网络获得图像识别大赛冠军，2015年ResNet模型获得了3.6%的错误率（比人的识别错误率5%还低），深度学习才成为人工智能的主要研究方向。随着计算机硬件和网络条件的改进（GPU的使用使得训练深度学习模型的难度降低），从而有机会更普遍的使用。神经网络模型是一个庞大的算法家族。虽然都可以称为神经网络，但是算法之间的差别还是很明显的。拿盖房子打个比方，当每一个房间或者每一层楼的内部设计不同时，最终搭建起来的建筑物也是千差万别的。传统的神经网络算法有 FNN、CNN、RNN、LSTM、GRU等。2014年以来应用于生成式AI的神经网络结构更复杂，参数规模更大，有variational autoencoder (VAE变分自编码器)、generative adversarial networks (GANs生成对抗神经网络)、Seq2seq、Transformer、normalizing flow、diffusion model等，都可以称为大模型。这些神经网络算法在网络结构的设计上有明显的不同，但也会相互借鉴。一些新的网络算法会融合不同网络结构，例如2014年GAN模型被发明以后有持续性的发展和改进，和同一时期流行的VAE模型相融合，形成VAE-GAN模型和VQ-VAE模型。我们常说的LLM(大语言模型)，是基于神经网络的大规模预训练统计语言模型，其采用的模型结构广泛使用Transformer，例如Google开发的BERT、T5模型，OpenAi开发的GPT模型，都是继承Transformer的结构特点进一步研发出来的。因为Transformer有2个优点，一是比较适合并行计算，模型训练速度快；二是引入“注意力机制”，可以很好的拟合文本中词和词之间微妙的语法和语义关系。按照大语言模型采用的网络结构，可进一步分为三个类别：l Autoencoding models自动编码模型，典型代表是BERT，它通过屏蔽一些输入标记并尝试从其余标记中预测它们来构建句子的双向表示。这些 LLM 擅长快速、大规模地捕获令牌之间的上下文关系，很适合文本分类任务。l Autoregressive models自回归模型，典型代表是GPT,GPT2,GPT3，它根据前面的标记预测句子中的下一个标记。这些LLM 可以有效地在给定上下文后生成连贯的自由文本。通常用于生成式任务，比如摘要、翻译或抽象问答。l Encoder-Decoder（Seq2seq模型），典型代表是T5，同时使用编码器和解码器，将每个task视作序列到序列的转换/生成（比如，文本到文本，文本到图像或者图像到文本的多模态任务）。通常用于需要内容理解和生成的任务，比如机器翻译。同时，每种类型的模型也会进一步在算法结构上演化，形成不同的子类型，例如BERT家族中的变体还有RoBERTa, DistilBERT, ALBERT, DeBERTa等。市面上的LLM模型发展的很快，在2022-2023年间每年都有接近30个大模型诞生。模型分为两种：pre-trained model预训练模型、instruction-tuned model指令调优模型。预训练模型是在大规模文本数据上进行预训练(非监督学习)的通用语言模型。指令调优模型则是在预训练模型的基础上，通过指令数据集（数据集由三个元素组成：指令、提供上下文信息的可选输入、基于指令的预期输出）进行重新训练来提高性能。下图中颜色较深的是指令调优模型。在模型性能方面，目前排名靠前的都是闭源模型（没有公开代码），例如OpenAI的GPT系列、Anthropic的Claude系列、谷歌的PaLM系列。开源模型也很多，比较流行的是有Meta 发布的LLaMA，模型能力很接近GPT3.5。LLaMA 系列发展迅速，被许多研究机构广泛使用并开发更好的开源模型，与闭源 LLM 竞争。未来两者的性能差距会缩小还是扩大还很难下结论，但是开源模型的性能在不少场景已经足够好用了。模型排名可参考https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard在模型的使用场景方面，大部分大模型是多任务模型，或者说不是针对某一个具体任务而研发的（传统的机器学习模型都是针对单一任务）。T5(Text to Text Transfer Transformer) 是世界上第一个多任务模型，最新版本Flan-T5在超大规模的任务上进行微调，有极强的泛化性能，单个模型可以在1800多个NLP任务上都有很好的表现，常见的任务包括：文本分类、文本翻译、代码生成、文本生成、信息知识查询。2023年3月《Sparks of Artificial General Intelligence Early experiments with GPT-4》通过各种实验，发现GPT4大模型表现出很好的通用智能。除了对语言的掌握，GPT-4能解决跨越数学、编码、视觉、医学、法律、心理学等领域的任务，其表现都惊人地接近人类水平，大大超过ChatGPT等现有模型。大模型的训练语料包罗万千，体现出的能力通用涉及各行各业，但不一定很深入很专业。因此有一些大模型的训练语料更专注于某一个领域，主打专业能力，例如BioGPT是基于生物领域的语言大模型。从模型使用的参数量来看，随着模型版本的迭代，其参数规模越来越庞大。例如：l 2018年GPT的训练数据是4.5G的文本资料，参数为1.2亿；l 2019年GPT2的训练数据是40GB的互联网数据，参数为15亿；l 2020年GPT3的训练数据是570GB的互联网数据和书籍文本，其参数为1750亿。那么数据集大小、参数量、计算量和性能之间有什么关系呢，openAI提出了Scaling Laws，基本意思是：随着模型大小、数据集大小和用于训练的计算浮点数的增加，模型的性能会提高，也会显得越智能。如何训练大模型大模型的训练包含以下几个阶段：l 数据清洗：除去有明显质量问题的数据、自相矛盾的数据，数据去重l 分词：采用BytePairEncoding、WordPieceEncoding、SentencePieceEncoding等分词算法l 搭建网络结构：有Encoder-Only、Decoder-Only、Encoder-Decoder三种类型的网络结构l 预训练：在大量无标签数据集上进行非监督的学习，语料多训练时间长、训练成本高。预训练阶段涉及大量文本，但质量可能较低，很多文本来自互联网。l 微调和指令调优(Fine-tuning and Instruction Tuning)：在专有的、高质量的打标数据集上做进一步的监督学习的训练；在指令数据集上做进一步的训练(称为指令调优，能够让模型更好的对特定的提示词进行响应)。在微调阶段，样本数据看重质量而不是数量，例如几千到几万的高质量的对话。l Alignment（语言模型的对齐）：进一步按照人类的语言习惯、偏好倾向和道德标准来规范模型的输出。采用基于人类反馈的强化学习 reinforcement learning from human feedback（RLHF），例如利用真实人类的反馈数据建立一个奖励模型，再用这个奖励模型给模型结果评分，根据评分好坏模型进一步完成训练。l 模型验证和迭代：监控模型的表现，对于大模型错误的结果，用正确的答案覆盖错误的响应，并将其作为示例加入到训练数据中，进一步微调模型。这种迭代可以进行很多次，代价也较小。使用大模型时，用户提供文本输入称为“提示词”，用于指导模型的输出。不断优化提示词prompt的方法，使得模型能够理解用户需求，称为提示工程Prompt engineering。提示可以是简单的问题，也可以是详细的任务描述。提示一般由指令、问题、输入数据(示例)组成。提示词也可以很复杂，包含很多技巧。例如：思维链提示方法 “chain of thought” prompting 在提示词中包含了推理过程的示例，能够让大模型产生更有条理、更透明和更精确的输出结果；在提示词中指明大模型扮演的角色，给出任务所需的上下文信息；要求模型对生成的结果进行验证，并指明出处。一般来讲，一个任务场景下训练的模型要用到另一个任务场景，是需要进行迁移学习的。迁移学习，指的是利用从一项任务中获得的知识来提高另一项相关任务的性能。一般的过程是，首先在一个文本语料库中预先训练好模型，然后使用新任务场景的数据来进行模型微调，更新模型参数。预先训练好的模型已经掌握了大量关于语言和词与词之间关系的信息，这些信息作为一个起点来提高新任务的性能。与从头开始训练模型相比，迁移学习允许使用更少的特定任务数据，针对特定任务对模型进行微调，大大减少了训练模型所需的时间和资源。但是，大部分大语言模型都具有多任务执行的能力，不需要针对新任务进行微调，只要在提示词里面提供较少的样本（例子）甚至不提供任何例子，LLM也可以很好的完成任务。""zero-shot prompting""指的是提示词中不提供特定样本，模型在新的场景内也可以进行推理和生成，""few-shot prompting ""指的是提示词中提供少量特定样本，模型就能够进行有效的推理和生成。我们把大模型的这种学习能力称为“zero-shot task-transfer ""或 ""meta-learning元学习""，或 "" in-context learning上下文学习""。但是这些学习并不是真正的训练过程，即使提供了少量样本，模型参数本身并没有得到更新。之所以大模型具有这种通用能力，是因为大模型在预训练过程中所用的训练数据已经包含了执行这些新任务所需的信息和知识，大模型经过指令调优能够理解提示词中的任务要求。当提示词中给出任务要求并且提供少量样本数据时，相当于是告诉大模型以什么样的形式来展示输出结果，激活了模型的表现能力，按照用户的预期形式来展示答案，完成新任务的执行。所以，in-context learning并不是学习一种新知识，而是学会该如何展示预训练过程中学到的知识。模型训练后的推理阶段，也就是生成文本的阶段，被称为“解码”，解码也有不同的策略。l Greedy search：在每个词的生成步骤，都选择概率最大的词进行输出。特点是生成速度快，但因为每一个步骤的词选择是局部最优，最后生成的句子未必是全局最优的。可能存在比当前模型生成的句子更好的表达方式，但是模型的输出是单一的，且对同样的提示词，模型每次输出文本也是一样的。l Beam Search: 在每个词的生成步骤，会选择2个至5个概率较高的候选词，从而生成很多种候选的句子，从中选择整体概率最高的句子输出。这种方法计算量大，但是可以生成更好的文本表达方式。l Top-k Sampling：引入“随机采样”的概念，每次生成词的时候会按照概率分布，从排名前K个词中，随机选择一个词进行输出。参数”temperature”是用来定义这个随机性的大小，当这个值较高时随机性较高，生成的文本就更有创意或者多样化。l Top-p Sampling: 也是一种随机生成的策略，但是每次选择的候选词的数量是变化的，不是固定的K个词。这种方式会让生成文本更具有创意性。目前还没有公认的最佳解码方法，哪种方法最好取决于生成文本的任务性质。如果希望模型执行精确的任务，如算术或提供特定问题的答案，应该降低temperature温度或使用确定性方法，如greedy search与beam search相结合，以确保获得最可能的答案。如果希望模型生成较长的文本，甚至有点创造性，那么就应该改用采样方法并提高温度。大模型在训练和应用时对硬件环境的要求高。为了降低训练成本，行业内有几种方法：l Low-Rank Adaption (LoRA): 利用矩阵分解技术，将庞大的参数矩阵分解成2个低秩矩阵的相乘，训练时只需要完成低秩矩阵的权重更新，从而大大降低计算的复杂度。l Knowledge Distillation: 将一个大模型的结果来训练一个规模较小的模型，就像老师教学生一样，知识得到传递，获得的新模型体积较小，更适合在终端硬件设备上运行。l Quantization 降低模型参数的计算精度，Float-32运算改为Int-8运算，这样会减少模型的大小，但也会降低模型的精度。大模型的局限性大模型的训练过程，可视为是一种对海量训练数据进行“压缩”过程，从广泛的数据中提取有用的信息和模式，并将这些知识以参数的形式存储在模型中。这些参数是海量信息的归纳和压缩，语言规律的总结。例如Lama2 70B模型来说，其训练数据是10T，最后生成的参数文件为140GB，压缩比大约是100倍。所以，对于大模型来讲，输入的样本包含的信息、知识越多，模型需要归纳的参数也自然更多，能力也就越强。相对于语言大模型，图像大模型的参数往往会少一些，例如Dalle.E 2的模型参数在35亿，Sora的参数量大约在30亿，跟GPT的参数不在同一个量级。一个潜在原因是图像的训练数据量较小，或者说图像样本所携带的信息量并没有语言文本那么量多且内容多元化。Dalle.E模型的训练数据是基于互联网上抓取的 4 亿个文本和图像相互对应的组合，样本量远低于大语言模型的样本量，就已经有非常出色的图像生成能力。因为本质上是信息压缩，大模型本身并不是像人类那样通过深入思考和推理来完成任务。模型能够生成高质量的内容，看似拥有了智能，但其底层运行机制是根据压缩后的参数，按照概率生成一串跟训练数据相似的一段文字，有点“熟读唐诗三百首不会吟诗也会吟”的感觉。因此大模型对各种任务完成的程度，取决于大模型的训练数据，如果训练数据过时，即使任务本身不复杂，大模型也是很难像人类那样通过思考和推理来完成。例如让chatGPT生成代码并指定其必须使用一种新发明的编程语言（训练的样本数据中没有这种语言的参考资料），它是无法完成的。退一步讲，即使是常见的编程语言，其生成的代码也未必准确(如果训练数据中的样本代码有错误的话)，就需要人工进一步调试。还有一个例子，如果问GPT-4汤姆·克鲁斯的母亲是谁，它会告诉你是玛丽·李·菲佛，答案是正确的；但问玛丽·李·菲佛的儿子是谁，它会说不知道。也就是说大模型掌握了文本之间的语义关系，但并不是一种真正的推理或者学习能力。大模型对信息的压缩、对知识的学习，只能在训练阶段中完成。训练完成之后，当人们在使用大模型的时候，模型只是根据学到的规则完成预测(inference)，模型并不会在人机对话过程中进一步完成信息的吸收和学习。所以，大模型本身是没有记忆功能的，不会记忆跟用户之间发生过的历史对话，除非这些对话内容作为下一次模型训练时的样本数据。目前大模型看似有的“记忆”功能，是通过允许较长的输入长度来实现的，例如用户完成第一次提示词输入得到回答以后，在进行第二次提示词输入时，前一次的提示词在后台也是作为上下文同时输入给模型的，也占用输入长度的额度。传统的机器学习算法在预测过程中会出现错误，同样道理，大模型生成文本时也会发生错误，也就是幻觉现象。大模型的幻觉现象(Hallucination)，指的是其生成的内容不可靠、不真实或者逻辑混乱，类似一本正经的胡说八道。例如一位律师使用ChatGPT 来支持他的法律案件，但发现ChatGPT 捏造了一些看似支持论点但完全不存在的案例。幻觉产生的背后原因是数据压缩的过程中产生信息损失，或者训练数据中不包含这方面样本，或者训练数据包含的样本就是逻辑错误的。“幻觉”在有些场景未必是坏事，例如用户如果是来撰写小说或者写诗，幻觉会让文字更有想象力和创造性。虽然“幻觉”不能彻底消除，但是有很多方法可以降低其出现的可能性。例如建立规则或者统计模型来评估生成文字和事实数据的一致性，使用思维链提示（chain-of-thought prompting）的方式让模型完成审慎的推理，或者使用外部知识库（检索增强生成RAG）来补充事实数据。另外需要注意的是，随着大模型能够生产大量的文字和图像，这些又进一步被人类发布到互联网上，就会造成未来用来训练模型的数据很大可能会包含这些不是人类创造的文本和图像，意味着训练数据的有效性在降低，幻觉问题可能会加重。由于大语言模型功能仅限于文本生成，为了让LLM提供更广泛的功能以及提供更准确的答案，就结合外部的工具（插件功能），例如允许模型通过API访问实时信息、访问外部的数据库、调用计算器执行复杂的数学计算等。这种充分利用第三方工具的方法衍生了AI agent(智能体)概念。智能体基于大模型能力，同时与用户和环境互动，具备访问和使用工具的能力，并能根据给定的输入做出决策，不是简单的响应文字，而是能够执行需要一定程度的自主性和决策性的复杂任务，就像一个有脑有手的人，例如Cognition宣布推出全球首个AI软件工程师Devin，本质上就是一个智能体。"
657765445,0fa9e9a8b121e938e6780c948dc9c63f,语音之家,https://api.zhihu.com/people/0fa9e9a8b121e938e6780c948dc9c63f,people,助力AI语音开发者社区,大模型训练为什么用A100不用4090,article,https://api.zhihu.com/articles/657765445,接着主持人陈凯教授问，你们有谁真的训练过<em>大模型</em>？沉默了一会儿，阿里的兄弟先说，我算是半个训练过<em>大模型</em>的，我们做的东西是支撑阿里大模型 infra 的。后面又有 panelist,1210,28,2611,1695351384,1695351384,"这是一个好问题。先说结论，大模型的训练用 4090 是不行的，但推理（inference/serving）用 4090 不仅可行，在性价比上还能跟 H100 打个平手。事实上，H100/A100 和 4090 最大的区别就在通信和内存上，算力差距不大。H100 这个售价其实是有 10 倍以上油水的。2016 年我在 MSRA 的时候，见证了微软给每块服务器部署了 FPGA，把 FPGA 打到了沙子的价格，甚至成为了供应商 Altera 被 Intel 收购的重要推手。2017 年我还自己挖过矿，知道什么显卡最划算。后来在华为，我也是鲲鹏、昇腾生态软件研发的核心参与者。因此，一个芯片成本多少，我心里大概是有数的。鲲鹏的首席架构师夏 Core 有一篇知名文章《谈一下英伟达帝国的破腚》，很好的分析了 H100 的成本：把他的成本打开，SXM 的成本不会高于 300$，封装的 Substrate 及 CoWoS 大约也需要 $300，中间的 Logic Die 最大颗，看上去最高贵 ：） 那是 4nm 的一颗 814mm2 的 Die，TSMC 一张 12 英寸 Wafer 大致上可以制造大约 60 颗这个尺寸的 Die，Nvidia 在 Partial Good 上一向做得很好（他几乎不卖 Full Good），所以这 60 颗大致能有 50 颗可用，Nvidia 是大客户，从 TSMC 手上拿到的价格大约是 $15000，所以这个高贵的 Die 大约只需要 $300。哦，只剩下 HBM 了，当前 DRAM 市场疲软得都快要死掉一家的鬼样了，即使是 HBM3 大抵都是亏本在卖，差不多只需要 $15/GB，嗯，80GB 的容量成本是 $1200。TSMC 曾经讲过一个故事。台湾同胞辛辛苦苦攒钱建厂，一张 4nm 那么先进的工艺哦，才能卖到 $15000，但是那某个客户拿去噢，能卖出 $1500000（$30000*50）的货啦，机车，那样很讨厌耶。你懂我意思吗？就如最开始说的，在这个世界的商业规则下，$2000 成本的东西卖 $30000，只有一家，销售量还很大，这是不符合逻辑的，这种金母鸡得有航母才守得住。据说微软和 OpenAI 包下了 H100 2024 年产能的一半，猜猜他们会不会发挥当年跟 Altera 砍价的传统艺能？会真的花 $40,000 * 500,000 = 200 亿美金去买卡？咱们再分析下 4090 的成本，5nm 的 609mm2 Die，大约成本是 $250。GDDR6X，24 GB，按照 1 GB $10 算，$240。PCIe Gen4 这种便宜东西就算 $100 吧。封装和风扇这些东西，算它 $300。总成本最多 $900，这样的东西卖 $1600，算是良心价了，因为研发成本也是钱啊，更何况 NVIDIA 的大部分研发人员可是在世界上程序员平均薪酬最高的硅谷。可以说，H100 就像是中国一线城市的房子，本身钢筋水泥不值多少钱，房价完全是被供求关系吹起来的。我在 LA 已经住了两周，公司租的房子使用面积是我北京房子的 4 倍，但售价只贵了 30%，还带个小院，相当于单位面积的房价是北京的 1/3。我跟本地的老外聊天，他们都很吃惊，你们的平均收入水平比 LA 低这么多，怎么买得起北京的房子的？问题来了，如果 4090 这么香的话，为啥大家还要争着买 H100，搞得 H100 都断货了？甚至 H100 都要对华禁售，搞出个 H800 的阉割版？大模型训练为什么不能用 4090GPU 训练性能和成本对比LambdaLabs 有个很好的 GPU 单机训练性能和成本对比，在此摘录如下。首先看吞吐量，看起来没有什么违和的，在单卡能放下模型的情况下，确实是 H100 的吞吐量最高，达到 4090 的两倍。看算力和内存也能看出来，H100 的 FP16 算力大约是 4090 的 6 倍，内存带宽是 3.35 倍，训练过程中由于 batch size 比较大，大多数算子是 compute bound（计算密集型），少数算子是 memory bound（内存密集型），这个结果是不意外的。LambdaLabs PyTorch 单卡训练吞吐量对比图LambdaLabs PyTorch 单卡训练吞吐量对比表然后看性价比，就有意思了，原来排在榜首的 H100 现在几乎垫底了，而且 4090 和 H100 的差距高达接近 10 倍。这就是因为 H100 比 4090 贵太多了。由于 H100 货源紧张，云厂商的 H100 租用价格就更黑了，按照标价大约 7 个月就可以回本。就算大客户价能便宜一半，一年半也足够回本了。在价格战中过惯了苦日子的 IaaS 云服务商看到这样的 H100 回本速度，估计要感叹，这真是比区块链挖矿回本还快呐。LambdaLabs PyTorch 单卡训练单位成本吞吐量对比图LambdaLabs PyTorch 单卡训练单位成本吞吐量对比表大模型训练的算力需求既然 4090 单卡训练的性价比这么高，为啥不能用来做大模型训练呢？抛开不允许游戏显卡用于数据中心这样的许可证约束不谈，从技术上讲，根本原因是大模型训练需要高性能的通信，但 4090 的通信效率太低。大模型训练需要多少算力？训练总算力（Flops）= 6 * 模型的参数量 * 训练数据的 token 数。我今年初第一次看到有人煞有介事地讲这个公式的时候，觉得这不是显然的吗？又看到 OpenAI 的高级工程师能拿 90 多万美金的年薪，顿时整个人都不好了，还是 AI 香呀。之前我也面试过一些做 AI 的工程师，包括一些做 AI 系统优化的专家，连 Q、K、V 是啥都说不清楚，LLaMA 每个 tensor 的大小也算不出来，就这样还能拿到 offer。APNet 2023 panel 的主题是 Network, AI, and Foundational Models: Opportunties and Challenges。前面几个问题都中规中矩的，panelists 有点放不开，我就提了一个问题，网络历史上的重要成就基本上都基于对应用场景深刻的理解，但我们现在做网络的很多都不了解 AI，甚至连每个 tensor 的大小和每个 step 传输的数据量都不知道，如何让 network community 更了解 AI 呢？这下热闹了，台下的谭博首先发言，说我在华为肯定能知道所有这些东西；然后传雄老师也跟了一句，要是做网络的懂了太多 AI，那可能他就变成一个 AI guy 了。接着主持人陈凯教授问，你们有谁真的训练过大模型？沉默了一会儿，阿里的兄弟先说，我算是半个训练过大模型的，我们做的东西是支撑阿里大模型 infra 的。后面又有 panelist 说，做 AI 系统的网络优化是否有必要自己懂 AI 呢，是不是只要会做 profiling 就行了？我个人观点仍然是，AI 并不难学，要想做好 AI 系统优化，可以不懂 attention 的 softmax 里面为什么要除以 sqrt(d_k)，但不能不会计算模型所需的算力、内存带宽、内存容量和通信数据量。Jeff Dean 就有个很有名的 Numbers Every Programmer Should Know，数量级的估算对任何系统优化来说都很关键，不然根本不知道瓶颈在哪里。回到大模型训练所需的总算力，其实很简单，6 * 模型的参数量 * 训练数据的 token 数就是所有训练数据过一遍所需的算力。这里的 6 就是每个 token 在模型正向传播和反向传播的时候所需的乘法、加法计算次数。一堆矩阵相乘，简单来想就是左边若干个神经元，右边若干个神经元，组成一个完全二分图。选出其中任意一个左边的神经元 l 和右边的神经元 r。正向传播的时候：l 把它的输出乘上 l 和 r 之间的权重 w，发给 r；r 不可能只连一个神经元吧，总要把多个 l 的加到一起，这就是 reduce，需要一次加法。反向传播的时候：r 把它收到的梯度乘上 l 和 r 之间的权重 w，发给 l；l 也不可能只连一个 r，需要把梯度 reduce 一下，做个加法；别忘了权重 w 需要更新，那就要计算 w 的梯度，把 r 收到的梯度乘上 l 正向传播的输出（activation）；一个 batch 一般有多个 sample，权重 w 的更新需要把这些 sample 的梯度加到一起。一共 3 次乘法，3 次加法，不管 Transformer 多复杂，矩阵计算就是这么简单，其他的向量计算、softmax 之类的都不是占算力的主要因素，估算的时候可以忽略。想起来我 2019 年刚加入 MindSpore 团队的时候，领导让我开发一个正向算子的反向版本，我求导给求错了，搞得算子的计算结果总是不对，还以为是我们的编译器出 bug 了。当发现求导求错的时候，领导像以为我没学过微积分一样看着我，确实我的微积分学的不好，这也是我从数学专业转到计算机专业的原因之一。在 MindSpore 的时候，自动微分一共就不到 1000 行代码，按照微分公式递归计算下去就行了，但自动微分作为一个重要特性被吹了半天，我都感觉不好意思了。模型的参数量和训练数据的 token 数之间也有个比例关系，这也很容易理解，只要把模型想象成数据的压缩版本就行了，压缩比总是有极限的。模型的参数量太小，就吃不下训练数据里面所有的知识；模型的参数量如果大于训练数据的 token 数，那又浪费，还容易导致 over-fitting。训练 LLaMA-2 70B 需要多少张卡有了模型训练所需的总算力，除以每个 GPU 的理论算力，再除以 GPU 的有效算力利用比例，就得到了所需的 GPU-hours，这块已经有很多开源数据。LLaMA 2 70B 训练需要 1.7M GPU hours（A100），要是用 1 个 GPU，那得算 200 年。要在一个月这种比较能接受的时间周期内训练出来，就得至少有 2400 块 A100。如果用 4090，单卡 FP16 算力是跟 A100 差不多（330 vs 312 Tflops），但是内存带宽比 A100 低一半（1 vs 2 TB/s），内存容量更是差好几倍（24 vs 80 GB），计算梯度时需要使用的 TF32 算力也低一半（83 vs 156 Tflops），综合起来 4090 单卡的训练速度还比 A100 稍低（参考前面 LambdaLabs 的评测）。就按照 2048 块 4090 算吧，这 2048 块 4090 之间的通信就成了最大的问题。为什么？一般有 tensor parallelism、pipeline parallelism、data parallelism 几种并行方式，分别在模型的层内、模型的层间、训练数据三个维度上对 GPU 进行划分。三个并行度乘起来，就是这个训练任务总的 GPU 数量。三种并行方式从三个维度划分计算空间的示意图，来源：DeepSpeedData parallelism（数据并行）数据并行是最容易想到的并行方式。每个 GPU 分别计算不同的输入数据，计算各自的梯度（也就是模型参数的改变量），再把梯度汇总起来，取个平均值，广播给各个 GPU 分别更新。Data Parallelism 示意图，来源：Colossal AI但只用数据并行是肯定不行的，因为一块 GPU 放不下整个 LLaMA 70B 模型。就模型训练需要多少 GPU 内存，我发现能算清楚的人就不多。有的人甚至以为只需要把模型的参数和反向传播的梯度存下来就够了。事实上，训练需要的内存包括模型参数、反向传播的梯度、优化器所用的内存、正向传播的中间状态（activation）。优化器所用的内存其实也很简单，如果用最经典的 Adam 优化器，它需要用 32 位浮点来计算，否则单纯使用 16 位浮点来计算的误差太大，模型容易不收敛。因此，每个参数需要存 4 字节的 32 位版本（正向传播时用 16 位版本，优化时用 32 位版本，这叫做 mixed-precision），还需要存 4 字节的 momentum 和 4 字节的 variance，一共 12 字节。如果是用类似 SGD 的优化器，可以不存 variance，只需要 8 字节。正向传播的中间状态（activation）是反向传播时计算梯度必需的，而且跟 batch size 成正比。Batch size 越大，每次读取模型参数内存能做的计算就越多，这样对 GPU 内存带宽的压力就越小。可是不要忘了，正向传播的中间状态数量是跟 batch size 成正比的，GPU 内存容量又会成为瓶颈。大家也发现正向传播中间状态占的内存太多了，可以玩一个用算力换内存的把戏，就是不要存储那么多梯度和每一层的正向传播的中间状态，而是在计算到某一层的时候再临时从头开始重算正向传播的中间状态，这样这层的正向传播中间状态就不用保存了。如果每一层都这么干，那么就只要 2 个字节来存这一层的梯度。但是计算中间状态的算力开销会很大。因此实际中一般是把整个 Transformer 分成若干组，一组有若干层，只保存每组第一层的中间状态，后面的层就从该组第一层开始重新计算，这样就平衡了算力和内存的开销。如果还是算不清楚，可以读读这篇论文：Reducing Activation Recomputation in Large Transformer Models。当然有人说，GPU 内存放不下可以换出到 CPU 内存，但是就目前的 PCIe 速度，换出到 CPU 内存的代价有时候还不如在 GPU 内存里重算。如果是像 Grace Hopper 那种极高带宽的统一内存，那么换入换出倒是一个不错的主意，不管训练的正向传播中间状态还是 KV Cache，都有很多优化的空间。Pipeline parallelism（流水线并行）既然一块 GPU 放不下，用多块 GPU 总行了吧？这就是 model parallelism（模型并行），可以大致分为 pipeline parallelism 和 tensor parallelism。大家最容易想到的并行方式就是 pipeline parallelism，模型不是有很多层吗，那就分成几组，每组算连续的几层，穿成一条链。Pipeline Parallelism 示意图，来源：Colossal AI这样就有个问题，一条链上只有一个 GPU 在干活，剩下的都在干等。当然聪明的你一定也想到了，既然叫 pipeline，那就可以流水线处理，可以把一个 batch 分为若干个 mini-batch，每个 mini-batch 分别计算。Pipeline Parallelism 示意图，来源：GPipe这可好，是不是把 pipeline 搞的越深越好，每个 GPU 只算一层？首先，正向传播中间状态（activation）的存储容量会成倍增加，加剧内存容量不足的问题。比如流水线的第一级算出了正向传播的中间状态，如果有 N 个流水级，那就要正向流过后面的 N - 1 个流水级，再等反向传播 N - 1 个流水级，也就是 2N - 2 轮之后才能用到这个正向传播的中间状态。不要忘了每一轮都会产生这么多中间状态，因此一共是保存了 2N - 1 个中间状态。如果 N 比较大，这个存储容量是非常恐怖的。其次，pipeline 的相邻流水级（pipeline stage）之间是要通信的，级数越多，通信的总数据量和总时延就越高。最后，要让这样的 pipeline 流起来，batch size 需要等于 Transformer 里面的层数，一般是几十，再乘以 data parallelism 的并行数，batch size 会很大，影响模型收敛的速度或模型收敛后的精度。因此，在内存容量足够的情况下，最好还是少划分一些流水级。对于 LLaMA-2 70B 模型，模型参数需要 140 GB，反向传播的梯度需要 140 GB，优化器的状态（如果用 Adam）需要 840 GB。正向传播的中间状态跟 batch size 和选择性重新计算的配置有关，我们在算力和内存之间取一个折中，那么正向传播的中间状态需要 token 长度 * batch size * hidden layer 的神经元数量 * 层数 * (10 + 24/张量并行度) 字节。假设 batch size = 8，不用张量并行，那么 LLaMA-2 70B 模型的正向传播中间状态需要 4096 * 8 * 8192 * 80 * (10 + 24) byte = 730 GB，是不是很大？总共需要 140 + 140 + 840 + 730 = 1850 GB，这可比单放模型参数的 140 GB 大多了。一张 A100/H100 卡也只有 80 GB 内存，这就至少要 24 张卡；如果用 4090，一张卡 24 GB 内存，就至少需要 78 张卡。LLaMA-2 模型一共就只有 80 层，一张卡放一层，是不是正好？这样就有 80 个流水级，单是流水线并行就有 80 个并行的 batch 才能填满流水线。这样，正向传播的中间状态存储就会大到无法忍受，这可是 80 * 2 = 160 轮的中间状态，翻了 160 倍。就算是使用选择性重新计算，比如把 80 层分成 8 组，每组 10 层，中间状态存储仍然是翻了 16 倍。除非是用最极端的完全重新计算，反向传播到每一层都重新从头开始计算正向传播的中间结果，但这样计算开销可是随模型层数平方级别的增长，第 1 层算 1 层，第 2 层算 2 层，一直到第 80 层算 80 层，一共算了 3240 层，计算开销可是比正常算一次 80 层翻了 40 倍，这还能忍？中间状态存储的问题就已经够大了，再看这 2048 张卡之间的通信开销。按照一张卡放一层，并且用不同的输入数据让它完全流水起来的做法，这 2048 张卡分别在计算自己的 mini-batch，可以认为是独立参与到 data parallelism 里面了。前面讲过，在数据并行中，每一轮需要传输的是它计算出的梯度和全局平均后的梯度，梯度的数据量就等于模型的参数数量。把 70B 模型分成 80 层，每一层大约有 1B 参数，由于优化器用的是 32 bit 浮点数，这就需要传输 4 GB 数据。那么一轮计算需要多久呢？总的计算量 = batch size * token 数量 * 6 * 参数量 = 8 * 4096 * 6 * 1B = 196 Tflops，在 4090 上如果假定算力利用率 100%，只需要 0.6 秒。而通过 PCIe Gen4 传输这 4 GB 数据就已经至少需要 0.12 秒了，还需要传两遍，也就是先传梯度，再把平均梯度传过来，这 0.24 秒的时间相比 0.6 秒来说，是占了比较大的比例。当然我们也可以做个优化，让每个 GPU 在 pipeline parallelism 中处理的 80 组梯度数据首先在内部做个聚合，这样理论上一个 training step 就需要 48 秒，通信占用的时间不到 1 秒，通信开销就可以接受了。当然，通信占用时间不到 1 秒的前提是机器上插了足够多的网卡，能够把 PCIe Gen4 的带宽都通过网络吐出去，否则网卡就成了瓶颈。假如一台机器上插了 8 块 GPU，这基本上需要 8 块 ConnectX-6 200 Gbps RDMA 网卡才能满足我们的需求。最后再看 batch size，整个 2048 张卡的集群跑起来，每个 GPU 的 mini-batch 我们刚才设置为 8，那可真是 batch size = 16384，已经是大规模训练中比较大的 batch size 了，如果再大，可能就影响模型的收敛速度或收敛后的精度了。因此，单纯使用流水线并行和数据并行训练大模型的最大问题在于流水线并行级数过多，导致正向传播中间状态（activation）存储容量不足。Tensor parallelism（张量并行）那就没办法了吗？我们还有最后一招，就是 Tensor parallelism（张量并行）。它也是模型并行的一种，但不像流水线并行那样是在模型的层间划分，而是在模型的层内划分，也就是把一层内的 attention 计算和 Feed Forward Network 划分到多个 GPU 上处理。有了张量并行，就可以缓解 GPU 放不下模型导致的流水级太多的问题。分到 80 个 GPU 才能放下的模型，如果用单机 8 卡张量并行，就只需要划分 10 个流水级。同时，张量并行还可以降低 batch size，因为张量并行的几个 GPU 是在算同一个输入数据。Tensor、Pipeline、Data 三种并行方式从模型层内、模型层间、训练数据三个维度上划分计算空间，来源：DeepSpeedAttention 的计算过程是比较容易并行的，因为有多个 head，用来关注输入序列中的不同位置的，那么把这些 head 分别拆开就行了。Attention 的计算过程，来源：The Illustrated Transformer但是我们做任何并行计算的时候都不要忘记通信开销。每个 head 里面的 Q、K 两个矩阵的大小是 batch size * token 长度 * key 的大小，V 矩阵的大小是 batch size * token 长度 * value 的大小。key/value 的大小一般等于 embedding size / heads 数量，例如在 LLaMA-2 70B 中就是 8192 / 64 = 128，矩阵大小是 batch size * 4096 * 8192 / 64（注意，这只是一个 head 的）。而 Q、K、V 参数矩阵在每个 head 上的大小是 embedding size * embedding size / heads num = 8192 * 8192 / 64。我们前面推导过，正向的计算量基本上就是每个 token 过一遍所有参数的计算量，2 * 3 (Q, K, V) * batch size * token 长度 * 参数个数 = 2 * 3 * batch size * 4096 * 8192 * 8192 / 64。可以跟矩阵的大小对一下，看看有没有算错。那么通信量是多少呢？输出矩阵 Z 是由每个 head 拼起来的，每个 head 的大小是 batch size * token 长度 * embedding size / heads num = batch size * 4096 * 8192 / 64。输入矩阵 X 的大小是 batch size * token 长度 * embedding size = batch size * 4096 * 8192。注意这里的 X 大小跟所有 heads 合并在一起后的 Z 大小是一致的，而我们在这里算的是每个 head 的 Z 大小。这里的单位是参数数量，如果按照字节算，还要乘以每个参数的大小。如果我们采用最极端的方式，每个 head 交给一个 GPU 去算，那么计算量和通信量的比例是多少？大概是 2 * 3 * embedding size / heads num / bytes per param = 2 * 3 * 8192 / 64 / 2 = 384。代入 4090 的 330 Tflops，如果想让通信不成为瓶颈，那么通信带宽至少需要是 330T / 384 = 859 GB/s，发送接收双向还得乘以 2，就是 1.7 TB/s。太大了，远远超过 PCIe Gen4 x16 的 64 GB/s，就算 NVLink 的 900 GB/s 都撑不住。所以，tensor parallelism 不能切得太细，每个 GPU 需要多算几个 heads。如果每个 GPU 多算几个 attention heads，输入矩阵 X 就是这些 heads 共享的了，因此输入矩阵的通信开销就被多个 heads 平摊了，计算量和通信量的比例就可以提高。还是按照 4090 的算力 / 单向通信带宽 = 330T / (64GB/s / 2) 来算，计算量和通信量的比例最少需要是 10000，也就是 2 * 3 * (embedding size / 张量并行 GPU 数量) / bytes per param = 2 * 3 * 8192 / 张量并行 GPU 数量 / 2 >= 10000，解得：张量并行 GPU 数量 <= 2.4。也就是告诉你，要是用了张量并行，最多用 2 个 GPU，如果用更多的 GPU，算力就肯定跑不满理论值。这让我怎么玩？但是，如果把 H100 的参数代入进去，马上就不一样了。H100 的峰值算力是 1979 Tflops，NVLink 双向带宽是 900 GB/s，计算量和通信量的比例最少需要是 4400，也就是 2 * 3 * (embedding size / 张量并行 GPU 数量) / bytes per param = 2 * 3 * 8192 / 张量并行 GPU 数量 / 2 >= 4400，解得：张量并行 GPU 数量 <= 5.5，也就是单机 8 卡做张量并行，如果算力跑满，网络会成为瓶颈。可以看到，即使对于 900 GB/s 这么快的 NVLink，在巨大的算力面前，都容易出现茶壶里煮饺子倒不出来的情况。当然，采用更优的并行切分方式可以节约一些网络通信开销。阉割版的 H800 相比 H100 卡的就是网络带宽，把网络带宽从 900 GB/s 降到 400 GB/s 了。我们再代入一次，计算量和通信量比例最少需要是 10000，那么张量并行 GPU 数量 <= 2.4，跟 4090 一个货色了。这样单机 8 卡做张量并行，就会导致网络成为瓶颈。当然，计算量 1979 Tflops 是理论值，并行切分方式也可以优化，因此实际训练 70B 的模型 8 卡 H800 网络不一定真的是瓶颈。这就是 H800 精准打击大模型训练，让张量并行过得不舒服。Feed Forward Network 的计算过程，虽然这是 encoder 的，但 decoder 也差不多，来源：Step-by-Step Illustrated Explanations of Transformer如果在 Feed Forward Network 这里做张量并行，也是可以做类似的推导，在这里就不赘述了。大凡神经网络里的矩阵乘法，M*N 的矩阵乘上 N*K 的矩阵，总的计算量是 M*N*K，输入输出的总大小是 (M*N + N*K)，多摞几个矩阵那也是常数（就像 Q、K、V），也就是计算和通信的比例跟矩阵的边长（dimension）是一个量级的。这么分析完了，如果你是要做大规模大模型训练，你还会买 A100/H100/H800 的 PCIe 版吗？PCIe Gen5 虽然比 Gen 4 快一倍，但对 H100 而言，计算量和通信量的比例仍然最少需要是 1979T / (128G / 2) = 30000，解出来张量并行 GPU 数量 <= 0.8，只要用了张量并行，就是损失算力的！等到 H100 的下一代出来了，比如 GH200，算力又翻了一倍，NVLink 还是 900 GB/s，这时候 NVLink 就也开始有点吃力了。所以 GH200 不失时机的推出了统一大内存，号称 144 TB，就是为了更好的做换入换出，用内存换网络通信。如果禁令保持不变，国内版本还是卡住 400 GB/s 的通信，那性能差距会有多大？上面的推导当然都是简化的，实际上可能不会这么夸张，但数量级是差不多的。训练部分小结4090 不容易做大模型训练的原因除了前面分析的内存小，通信慢，license 不支持数据中心，还有很多其他问题。比如，A100/H100 支持 ECC 显存容错，据说 4090 也支持 ECC，但是不知道故障率会不会比 A100/H100 更高。不要小看了容错，2048 张卡的集群就算每张卡 1 个月出一次故障，平均 20 分钟就会有一张卡出故障！要是没有自动化的故障恢复方式，炼丹师就别想睡觉了。就算是自动从上一个 checkpoint 恢复，这可是要时间的，如果不考虑丢弃故障 GPU 梯度这种比较暴力的方式，当前这个 step 就算是白算了，还要从上一个 checkpoint 加载梯度，一般需要 10 来分钟的时间才能搞定。这样，每 20 分钟就浪费 10 分钟，这 10 分钟恢复过程中可能又有新的卡故障，总的算下来要浪费掉一半的有效算力。因此，保持大规模训练集群的低故障率是非常重要的，这些 GPU 卡都非常金贵，可不能像挖矿机房那样，动不动就过热死机了。据说 3090 是支持 NVLink 的，但 4090 就把 NVLink 给砍掉了。更老的卡，甚至还有支持 PCIe P2P 的，现在也都被砍掉了。谁感兴趣可以测一测 3090 的 NVLink 性能怎么样，是不是真的能达到标称的 600 GB/s，如果真的能达到的话，是否又可以用来做大模型训练了呢。我们年会的时候，海哥讲了个段子，我们找老婆都希望又漂亮，又能挣钱，还一心一意爱自己。可同时满足这三个条件的老婆就很难找到了。类似的，在分布式系统中，我们都希望性能又高，通用性又强，成本还低。这三个条件的交集也很小。海哥讲到这里，谭博补充了一句，同时满足这三个条件的分布式系统根本就不存在。Tensor、Pipeline、Data Parallelism 就像是这样的不可能三角，相互牵制，只要集群规模够大，模型结构仍然是 Transformer，就很难逃出内存容量和网络带宽的魔爪。大模型推理为什么 4090 很香推理和训练有什么区别？首先，训练不仅需要存储模型参数，还需要存储梯度、优化器状态、正向传播每一层的中间状态（activation），后面几个比参数更大，对模型内存的需求量也更大。其次，训练任务是一个整体，流水线并行的正向传播中间结果是需要存下来给反向传播用的。为了节约内存而使用流水线并行，流水级越多，要存储的中间状态也就更多，反而加剧内存的不足。而推理任务中的各个输入数据之间并没有关系，正向传播每一层的中间状态也不需要保存下来，因此流水线并行不需要存储很多中间状态。首先我们需要计算一下推理需要多少算力。前面针对训练算力的估算，为了简单起见，忽略了两个事情，首先是没有考虑 KV Cache，其次是没有考虑内存带宽。KV Cache什么是 KV Cache？对于每个输入的 prompt，在计算第一个 token 输出的时候，每个 token 的 attention 肯定是都要从头计算。但是在后续 token 的生成中，都需要计算 self-attention，也就是输入 prompt 以及前面输出的 token 的 attention。这是就需要用到前面每一个 token 的 K 和 V，由于每一层的参数矩阵是不变的，此时只有刚生成的那个 token 的 K 和 V 需要从头计算，输入 prompt 和之前生成的 token 的 K 和 V 其实是跟上一轮一样的。这时，我们就可以把每一层的 K、V 矩阵缓存起来，生成下一个 token 的时候不再需要重新计算，这就是所谓的 KV Cache。Q 矩阵每次都不一样，没有缓存的价值。前面讲的训练中的选择性保存正向 activation 是个拿计算换内存的把戏，这里的 KV Cache 就是一个拿内存换计算的把戏。KV Cache 需要多少存储容量呢？每一层，每个 token 的 K、V 矩阵都是 embedding size 这么大，再乘上 token 数量和 batch size，就是这一层的 KV Cache 所需的存储容量了。一定要记住 batch size，在正向和反向传播的几乎所有阶段，都不会涉及到对 batch size 中各个 sample 的合并处理，因此它始终是存储量和计算量计算中的一个系数。例如，如果 batch size = 4，在 LLaMA 2 70B 中，假设输入和输出的 token 数量达到了模型的极限 4096，80 层的 KV Cache 一共需要 2 (K, V) * 80 * 8192 * 4096 * 8 * 2B = 80 GB。如果 batch size 更大，那么 KV Cache 占据的空间将超过参数本身占的 140 GB。KV Cache 能省下来多少计算量？每一层计算 K、V 矩阵一共需要 2 (K, V) * 2 (mult, add) * embedding size * embedding size = 4 * 8192 * 8192 这么多计算量，乘以之前输入过的 token 数量、层数和 batch size，就是 4096 * 80 * 8 * 4 * 8192 * 8192 = 640 Tflops。相当于每存储 1 个字节，节约了 16K 次计算，还是很划算的。事实上，KV Cache 节约的远远不止这些。计算 K、V 矩阵的过程是个典型的内存密集型过程，它需要加载每一层的 K、V 参数矩阵。也就是如果不做任何缓存，假设 prompt 长度很短而输出长度接近 token 的最大长度 4096，到了最后一个 token 的时候，单是重复计算前面每个 token 的 K、V 矩阵，就需要读取内存 4096 * 80 * 2 * 8192 * 8192 = 40T 次，每次 2 个字节，要知道 H100 的内存带宽只有 3.35 TB/s，4090 更是只有 1 TB/s，这单是最后一个 token 就得耗掉一张卡几十秒的时间来做重复计算。这样，token 的输出就会越来越慢，整个输出时间是输出长度平方级别的，根本没法用。推理是计算密集还是存储密集接下来我们就可以计算推理所需的计算量了。总的算力很好算，前面讲过，大概就是 2 * 输出 token 数量 * 参数数量 flops。如果想看细节，可以看下面这张图，来源是这里。Transformer 推理过程中每一步的矩阵形状、所需算力和内存访问量，来源：Lequn Chen，Dissecting Batching Effects in GPT Inference但算力并不能说明一切，模型还需要访问 GPU 内存，内存带宽也可能成为瓶颈。至少需要把参数从内存里面读出来吧？事实上，内存带宽的估算就这么简单，内存访问量 = 参数数量 * 2 bytes。中间结果有一部分是可以放在缓存里面的，缓存放不下的部分也需要占内存带宽，我们先不算。如果不做任何批量输入，也就是模型专门服务一个 prompt，batch size = 1，整个 context 的长度很短（例如只有 128），那么整个推理过程中，每载入一个参数（2 字节），就只进行 128 次乘法和加法计算，那么计算 flops 和访问内存 bytes 的比例就只有 128。基本上任何 GPU 在这种情况下都会变成 memory bound，时间都耗在加载内存上了。对于 4090 来说，计算 flops 和内存带宽之比是 330 / 1 = 330；对于 H100 来说，计算 flops 和内存带宽之比是 1979 / 3.35 = 590。也就是说，如果 context 中的 token 数量小于 330 或者 590，那么内存访问就会成为瓶颈。虽然 LLaMA 2 的理论上限是 4096 个 token，但很多输入 prompt 用不了这么多，因此内存访问是有可能成为瓶颈的。此时，就需要靠 batch size 来补足了。推理中的批量处理，就是把几乎同时到达后端服务的 prompt 放到一起处理。不用担心，batch 里面的不同 prompt 的处理是完全独立的，不用担心会互相干扰。但这些 prompt 的输出是步调整齐划一的，每一轮整个 batch 中的每个 prompt 都会输出一个 token，因此如果有的 prompt 先输出完了，那就只能等其他的输出结束，造成一定的算力浪费。有的人问，批量处理所需的算力跟分别单独处理所需的算力是一样的呀，那推理时为什么需要批量处理？答案就在访问内存的带宽上。如果同时到达服务器的 prompt 很多，是不是 batch size 越大越好？也不是，因为 KV Cache 的大小可是正比于 batch size 的，batch size 大了，KV Cache 占据的 GPU 内存容量就很可观，比如在 LLaMA-2 70B 中，每个 prompt 都要占据 5 GB 的 KV Cache，如果 batch size 搞到 32，那么 KV Cache 就会占掉 160 GB 的 GPU 内存，比参数都大了。70B 推理需要多少张卡？总的存储容量也很好算，推理的时候最主要占内存的就是参数、KV Cache 和当前层的中间结果。当 batch size = 8 时，中间结果所需的大小是 batch size * token length * embedding size = 8 * 4096 * 8192 * 2B = 0.5 GB，相对来说是很小的。70B 模型的参数是 140 GB，不管 A100/H100 还是 4090 都是单卡放不下的。那么 2 张 H100 够吗？看起来 160 GB 是够了，但是剩下的 20 GB 如果用来放 KV Cache，要么把 batch size 压缩一半，要么把 token 最大长度压缩一半，听起来是不太明智。因此，至少需要 3 张 H100。对于 4090，140 GB 参数 + 40 GB KV Cache = 180 GB，每张卡 24 GB，8 张卡刚好可以放下。推理用流水线并行可以吗？推理使用流水线并行，最主要的问题是串行处理的推理延迟，网络延迟倒是小问题。首先是推理延迟。虽然流水线的不同阶段可以塞进不同的 prompt，但同一个 prompt 的处理仍然永远在单个 GPU 上轮转，这样相比 Tensor parallelism 而言，单个 prompt 的延迟就增大了。对于很小的 batch size，GPU 内存带宽是瓶颈，此时每张卡计算每个 token 的时延就是 2 byte * 参数量 / 卡的数量 / 内存带宽，例如 8 卡 4090 跑 LLaMA-2 70B，就是 2 * 70G / 8 / 1 TB/s = 0.0175 秒。这里没有考虑 KV Cache 带来的节约。注意，8 张卡是串行处理的，因此每个 token 的时延还要乘以 8，也就是 0.14 秒。每秒只能输出 7 个 token，对于 70B 这么小的模型来说是有点慢了。对于很大的 batch size，GPU 算力是瓶颈，此时每张卡计算每个 token 的时延就是 batch size * 2 * 参数量 / 卡的数量 / 算力，例如 batch size = 1024，同样的 8 卡例子，就是 1024 * 2 * 70G / 8 / 330 Tflops = 0.0543 秒。事实上，对于这么大的 batch size，KV Cache 和正向传播的中间结果先把 GPU 内存给吃满了。那么要平衡利用 GPU 算力和内存带宽，batch size 需要是多少呢？这就是 2 byte * 参数量 / 卡的数量 / 内存带宽 = batch size * 2 * 参数量 / 卡的数量 / 算力，左右两边参数量和卡的数量互相抵消，得到 batch size = 算力 / 内存带宽。对于 4090，就是 330 / 1 = 330；对于 H100，就是 1979 / 3.35 = 590。也就是说，对 4090 而言，batch size 小于 330 的时候 GPU 内存带宽是瓶颈，大于 330 的时候 GPU 算力是瓶颈。当 batch size = 330 的时候，理想情况下，内存带宽和算力恰好都打满，每张卡处理每个 token 的时间就是 17.5 ms。其次是网络延迟。流水线并行相比张量并行的优点就是网络传输量小，流水级之间只需要传输 batch size * embedding size 这么多数据。例如 batch size = 8，embedding size = 8192，只需要传输 128 KB 数据，在 32 GB/s 的 PCIe Gen4 x16 上，只需要 4 us 就可以传输完成。当然，还需要考虑到通信库本身的开销，加上 4090 不支持 GPU 之间 P2P 传输，需要通过 CPU 中转，实际上需要几十 us 的时间，相比计算部分动辄几十 ms 的时延，可以忽略不计。即使 batch size = 330，这 5.28 MB 数据在 PCIe 上也只需要传输 0.16 ms，相比计算部分的 17.5 ms 仍然可以忽略不计。如果可以忍受流水线并行的推理延迟，甚至可以用多台主机来做流水线并行。我们假设主机间只有 1 Gbps 的普通以太网络，每台主机只有一张 4090。对于 batch size = 1，16 KB 数据需要 0.25 ms 才能传输完成，再加上 0.25 ms 两端网络协议栈的处理时间，每个流水级就需要 0.5 ms 的时延，8 张卡花在通信上的时间只有 4 ms，相比整体计算时延 140 ms 来说可以忽略，不会显著影响系统的推理延迟。当 batch size 很小时，流水线推理中的网络流量是突发性（bursty）的，每过 18 ms 只会进行 0.25 ms 数据传输，只有 1/72 的占空比，不用担心流水线推理把局域网全部给占满了，搞得没法正常上网了。如果为了充分利用算力，把 batch size 设置得很大，比如 330，那么 16 KB * 330 = 5.28 MB 数据需要传输 41 ms，8 张卡花在通信上的时间高达 0.33 秒，这样就只有 3 token/s 的输出速度了，难以忍受。因此，如果用主机间通信来做流水线并行，主机间又没有很高的通信带宽，就势必需要牺牲一定的吞吐量。例如，我们设置输出速度不小于 5 token/s，这时留给通信的时间是 60 ms，每个流水级至多 7.5 ms，1 Gbps 网络可以传输 960 KB 数据，这时 batch size 至多设置为 60，也就是这 8 张 4090 的总吞吐量是 2400 token/s。此时的有效算力利用率只有不到 20%。最近有一个比较火的 Petals 开源项目，就是利用流水线并行，把 GPU 做成了一个类似 BitTorrent 的分布式网络。虽然推理延迟确实比较高，但至少说明了分布式 GPU 推理的可行性。推理用张量并行怎么样？前面讲到，流水线并行的最大缺点是 GPU 串行处理，延迟较高，导致输出 token 比较慢。而张量并行的最大缺点是传输数据量大，网络带宽低的设备不一定 hold 得住。但是推理要传输的数据量跟训练要传输的数据量可不是一回事啊！推理只需要传输正向传播的中间结果（activation），而训练还需要传输所有参数的梯度，梯度才是数据量的大头。在推理中，如果使用张量并行，Transformer 的每一层都需要传输把自己负责的结果向量（大小为 batch size * embedding size / num GPUs）广播给其他所有 GPU，并接受来自所有其他 GPU 广播来的数据。计算 attention 的时候需要传输一次，计算 feed-forward network 的时候又需要传输一次，也就是总共需要传输 2 * 层数这么多次。每次发送就是 batch size * embedding size（发送和接收是不同的方向，不能算两次），对于 batch size = 1, embedding size = 8192，只需要传输 16 KB 数据，在 32 GB/s 的 PCIe Gen4 上传输只需要 1 us。当然，考虑到前面讨论的 CPU 中转开销，还是需要大约 30 us 的。一共 160 次传输，需要 4.8 ms。我们再考虑计算的开销。还是考虑 batch size = 1 的情形，GPU 内存带宽是瓶颈，此时每张卡计算每个 token 的时延就是 2 byte * 参数量 / 卡的数量 / 内存带宽，代入我们前面的数值，仍然是 17.5 ms。但是这里 8 张卡是并行处理的，因此总的处理时长就是计算时间 + 通信时间 = 17.5 ms + 4.8 ms = 22.3 ms。这就意味着每秒可以生成 45 个 token，这个 token 生成速度已经很不错了，至少人类的阅读速度是很难赶上生成的速度了。如果 batch size 更大会怎样？例如 batch size = 330，把 GPU 算力和内存带宽都充分利用起来，每次需要传输的数据量是 330 * 8192 * 2 = 5.4 MB，在 32 GB/s 的 PCIe Gen4 上需要 0.17 ms。一共 160 次传输，就是 27 ms。这下网络通信开销成了延迟的大头，总处理时长为 27 + 17.5 = 44.5 ms，每秒只能生成 22 个 token 了，但也不算慢。注意，不管用多少个 GPU 做并行推理，只要用的是张量并行，网络传输的总数据量是相同的，因此增加 GPU 的数量只能加速计算，不能加速通信。因此，A100/H100 的 NVLink 在降低推理延迟方面还是有很大作用的。如果用 A100/H100，取 batch size = 590 达到算力和带宽的平衡利用，这 9.44 MB 数据只需要 9.44 MB / 450 GB/s = 0.02 ms。一共 160 次传输，也只有 3.2 ms。由于内存带宽大了，计算时间也可以大幅缩短，例如 H100 的计算时间为 2 * 70G / 8 / 3.35 TB/s = 5.2 ms。总处理时长只有 5.2 ms + 3.2 ms = 8.4 ms，每秒可以生成 119 个 token，非常棒！可以说，如果论单个 prompt 的 token 生成速度，无论用多少块 4090 也追不上 8 卡 H100。用 4090 做推理的成本怎么样？对于推理，不管用流水线并行还是张量并行，batch size 不算高到太离谱的情况下内存带宽都是瓶颈。假如 batch size 能够高到把算力 100% 利用起来，并且还能解决 KV Cache 不够大的问题，能解决中间结果占用内存过多的问题，那么这 8 张 4090 可以达到多少吞吐量？当然，这两个问题都不好解决，因此推理优化才是一个热门的研究领域，存在很多的 trade-off 和奇技淫巧。如果只是用标准的 PyTorch，那推理性能距离把算力 100% 利用起来还远得很呐。假设都解决了，在张量并行的通信过程中我们可以利用 double buffer 做另外一个 batch 的计算，也就是计算和通信并行，进一步提高吞吐量。通信和计算分别是 27 ms 和 17.5 ms，传输的 27 ms 是瓶颈，也就是每 27 ms 输出一组 token，一个 batch 330 个 prompt，那这 8 张 4090 真是可以达到每秒 330 / 0.027 = 12.2K token 的吞吐量。8 张 4090 的成本是 12800 美金，8 卡 PCIe Gen4 服务器本身要 2 万美金，加上网络设备，平均每台 4 万美金的设备成本。固定资产按照 3 年摊销，每小时 1.52 美元。整机功耗大约 400W * 8 + 2 kW = 5 kW，按照 0.1 美元一度电算，每小时 0.5 美元。这 2 美元一小时的机器，满打满算能生成 12.2K * 3600 = 44M tokens，也就是说 1 美元能生成 22M tokens。是不是比 GPT-3.5 Turbo 的 $0.002 / 1K tokens，也就是 1 美元 0.5M tokens 便宜 44 倍？当然，账不能这么算。首先，GPU 的算力利用率到不了 100%；其次，如同所有 SaaS 服务一样，用户的请求数量有波峰有波谷，用户是按量付费的，平台提供方可是不管有没有人用都在烧钱的；此外，每个 batch 中不同 prompt 的长度和响应 token 数量都不同，消耗的算力是 batch 中最大的那个，但收的钱是用户实际用的 token 数；再次，GPT-3.5 是 175B 的模型，比 70B 的 LLaMA 很可能推理成本更高；最后，OpenAI 开发 GPT-3.5 是烧了不知道多少钱的，人家至少要赚回训练成本和研发人员的工资吧。其实 GPT-3.5 Turbo 的 $0.002 / 1K tokens 真的挺良心的，有的卖 API 的，LLaMA-2 70B 都敢比 GPT-3.5 Turbo 卖得贵。如果换成用 H100 做推理，重新算一下这笔账。一张 H100 至少要 3 万美金，一台 8 卡 H100 高配服务器加上配套的 IB 网络，起码要 30 万美金，同样按照 3 年摊销，每小时 11.4 美元。10 kW 功耗，电费每小时 1 美元。一共 12.4 美元一小时。这其实已经是非常良心的价格了，你在任何云服务商都不可能租得到这么便宜的 8 卡 H100。所以说从云服务商租卡卖没有护城河的 SaaS 服务，比如开源模型的推理 API，除非有一种提高推理性能的独门绝技，很难赚得了什么大钱，二房东的生意不是这么好做的。再算算这台 8 卡 H100 机器的吞吐量，张量并行也采用传输和计算并行，H100 的通信比较快，因此计算是瓶颈，每 5.2 ms 可以输出一组 token，一个 batch 590 个 prompt，满打满算可以达到每秒 590 / 0.0052 = 113K token 的吞吐量。理想情况下，一小时能生成 407M tokens，也就是 1 美元能生成 33M tokens，H100 这单位 token 的成本比 4090 还要低 30%。为什么 8 卡 H100 机器是 4090 机器价格的 6 倍，性价比却比 4090 高？因为一张 H100 的算力是 4090 的 6 倍，内存带宽是 4090 的 3.35 倍，当 batch size 够大，算力达到瓶颈的时候，单卡的性能就是 6 倍。而且，H100 比 4090 的网络带宽强太多了，导致 4090 在张量并行中网络通信成了瓶颈，浪费了有效算力。因此，同样的 8 卡机器吞吐量几乎可以达到 4090 的 10 倍。虽然一张 H100 卡的价格是 4090 的 20 倍以上，但算上服务器本身的成本和电费，整机的成本只是 6 倍左右。用最便宜的设备搞出最高的推理性能我们发现在 8 卡 4090 机器中，3 万美金的设备成本，GPU 卡只占了 1.28 万美金，不像 H100 机器那样 GPU 成本占了大头。还有办法进一步降低吗？如果我们可以忍受 5 token/s 的输出速度，甚至可以利用流水线并行，用家用台式机和 4090 攒出个推理集群来。遥想我当年在 MSRA 的时候，在一台只用 1000 美金攒出来的机器上插了 10 块 FPGA，做出个世界最快的 Key-Value Store。其实如果让我去设计一个性价比最高的 4090 推理集群，有很多种方案可以尝试：用流水线并行，台式机 + 10 Gbps 网卡，足够在 5 ms 内传输 batch size = 330 的 5.28 MB 数据了，通信 40 ms，计算 140 ms，达到 5 token/s 的单 prompt 输出速度，同时又能充分利用 4090 的算力。10 Gbps 的网卡和交换机都很便宜，Intel X710 网卡只要 150 美金，20 口交换机只要 1500 美金（每 8 个口 750 美金），一台家用台式机 700 美金，这只要 2 万美金就可以搞定原本需要 4 万美金的设备。用张量并行，台式机 + 200 Gbps ConnectX-6 网卡，上 RoCE，可以把 batch size = 330 的 5.28 MB 数据在 0.22 ms 内传完，160 次传输是 35 ms，加上计算的 17.5 ms，一个 token 52.5 ms，可以达到 19 token/s 的单 prompt 输出速度，这个速度已经不错了。网卡 1000 美金，200G 交换机 2 万美金 40 个端口，平均每 8 个端口 4000 美金，一台家用台式机 700 美金，这只要 3 万美金就能搞定原本 4 万美金的设备。主机内用张量并行，主机间用流水线并行，4 卡 PCIe Gen4 服务器主板只要 1000 美金而且能跑满 PCIe 带宽（因为 8 卡就需要 PCIe switch 了，价格会贵很多），两台主机之间用 25 Gbps 网卡直连，主机内张量并行的时延是 27 ms，主机间流水线并行只需 2 次 8 ms 的传输（注意 25G 的网络带宽是 4 张 GPU 卡共享的），加上两次流水线计算各 17.5 ms，总共 78 ms，可以达到 13 token/s 的单 prompt 输出速度。网卡 300 美金 * 2，服务器 3000 美金 * 2，这只要 1.95 万美金就可以搞定原本需要 4 万美金的设备。2 万美金按照 3 年摊销是每小时 0.76 美元。按照 0.1 美元/度的电价，每小时的电费都要 0.5 美元，接近设备成本了，这有点挖矿的味道了。这 1.26 美元一小时的机器如果跑满了 44M tokens 的吞吐量，1 美元能生成 35M tokens，终于赶上 8 卡 H100 的 33M token per dollar 了。为什么 H100 以 20 倍于 4090 的 GPU 价格，9 倍的性能，却仍然能在系统性价比上打个平手，首先是因为能耗成本更低，8 卡 H100 的功耗是 10 kW，但 9 台 8 卡 4090 的功耗是 45 kW；其次是因为主机和网络设备成本更低，一台 8 卡 H100 准系统虽然贵，但只占整机价格的 20% 左右；但 4090 因为卡多，除非像 GPU 矿机那样压成本，只要还是用数据中心级的设备，准系统价格就要占到 35% 以上。其实，这个世界上不止有 A100/H100 和 4090，还有 A10 等计算卡和 3090 等游戏卡，还有 AMD 的 GPU 和很多其他厂商的 AI 芯片。H100 和 4090 大概率都不是性价比的最优解，例如 A10 和 AMD GPU 的性价比有可能就更高。我都想搞一个推理性价比挑战赛，看谁能用最便宜的设备搞出最强的推理吞吐量，同时延迟不能太高；或者用最便宜的设备搞出最低的推理延迟，同时吞吐量不能太低。这一切都是在假设使用 LLaMA-2 70B 模型，没有做量化压缩的前提下。如果做了量化压缩，那性能就更高，甚至在 Unified Memory 够大的 MacBook Pro 上都能单机跑了。License 问题怎么办？我把这个问题放到最后。NVIDIA Geforce driver 的 License 里写道：No Datacenter Deployment. The SOFTWARE is not licensed for datacenter deployment, except that blockchain processing in a datacenter is permitted.既然机器都是用台式机攒起来的，这能叫 data center 吗？还是叫矿场比较合适吧。人家也说了，4090 用来做区块链是允许的。我有一个大胆的想法，如果未来的区块链不再用挖矿来做 proof of work，而是用大模型推理来做 proof of work，这是不是很有意思？每个人买几块显卡，接到矿池上，既可以自己用来玩游戏，闲时又可以贡献算力。矿池直接就是个卖大模型推理 SaaS 服务的公司，提供前所未有的低价 API。甚至需要大模型推理服务的人可以在区块链里自己 P2P 玩起来，谁要用大模型就付点 gas。当然，目前的 proof of work 都是计算很复杂，验证很简单的。如果真用大模型推理做 proof of work，必须防止用户随意编造一个结果交上去。当然这也是有解决方案的，就像 BitTorrent 和其他一些去中心化网络一样，采用信用机制，新人只能做验证别人计算结果的工作，积攒信用；老人每次算错了，都有比较严厉的惩罚。从另一个角度看，家庭局域网络的速度也越来越快，比如我家就自己部署了 10 Gbps 的网络。家中的智能设备越来越多，算力越来越强。光纤入户也越来越普遍，小区和城市的运营商机房里部署了越来越多的边缘计算节点。前面我们用 1 Gbps 的网络就足以把多台主机上的 GPU 组成流水线并行，那么在未来的家庭高速网络中，流水线并行甚至张量并行都将成为可能。大多数搞 AI 推理的都只关心数据中心，忽略了家中的分布式算力。只要解决了安全、隐私和经济动机问题，我家的 Siri，也许就跑在邻居家里的 GPU 上。很多人都在说要 democratize AI。但现在大模型平民化的最大障碍就是成本，而成本最大的来源又是 GPU 市场上计算卡和游戏卡价格的剪刀差。这并不是指责某家公司，其他做 AI 芯片的公司，AI 芯片的算力也并不便宜。毕竟芯片、软件和生态的研发都是白花花的银子。就像本文开头提到的微软给每台服务器部署 FPGA 一样，大规模量产的芯片价格就像沙子一样。到时候，能限制大模型推理算力的就只有能源了，就像区块链挖矿和通用 CPU 的云计算一样，都在找最便宜的电力供应。我在之前的一个采访中就表示，长期来看，能源和材料可能是制约大模型发展的关键。让我们期待廉价的大模型走进千家万户，真正改变人们的生活。"
678325896,d9bf4f9fc25a77a2576b051e5d2e0889,李想 Prompter,https://api.zhihu.com/people/d9bf4f9fc25a77a2576b051e5d2e0889,people,公众号：李想Prompter,国内大模型盘点,article,https://api.zhihu.com/articles/678325896, 8、科大讯飞：星火<em>大模型</em> 讯飞星火认知大模型-AI大语言模型-星火<em>大模型</em>-科大讯飞 “讯飞星火认知大模型”是科大讯飞发布的产品，具有7大核心能力，即文本生成、语言理解、知识问答、逻辑推理、数学能力、代码能力、多模态能力,263,35,826,1705476577,1708242981,"总结一下通过国家工信部11个AI大模型，再推荐两个本人经常用觉得很惊艳的模型北京5家：百度（文心一言）、字节抖音（云雀大模型）、智谱AI（GLM智谱清言）、中科院（紫东太初）、百川智能（百川大模型）上海3家：商汤（商量）、MiniMax（ABAB大模型）、上海人工智能实验室（书生大模型）广东2家：华为（盘古大模型）、腾讯（混元大模型）安徽1家：科大讯飞（星火大模型）1、百度：文心一言文心一言总的来说，目前文心一言4.0是最逼近ChatGPT的大模型2、字节抖音：云雀豆包目前支持网页Web平台、iOS 以及安卓平台，用户可通过手机号、抖音或者Apple ID登录使用。3、智谱AI：智谱清言智谱清言智谱科技最近发布的GLM-4基座大模型，作为国内对标OpenAI产品线的重要一步，引起了广泛关注。根据智谱的官方说法，GLM-4在性能上相比GLM-3提升了60%，接近GPT-4的最新水平。「智谱清言」已在各大应用商店上线，用户可通过苹果商店 App Store、安卓主流商店（包括华为、OPPO、vivo 及小米等）进行下载，或在微信小程序中搜索「智谱清言」体验其功能。4、中科院-紫东太初紫东太初中国科学院自动化研究所“紫东太初”多模态大模型研究团队自成立以来，围绕人工智能前沿技术进行了不懈探索，成功研发全球首个千亿参数多模态大模型“紫东太初”，开拓性地实现了图像、文本、语音三个模态数据之间的“统一表示”与“相互生成”，理解和生成能力更接近人类，向通用人工智能迈出了重要一步。5、百川智能：百川大模型百川智能目前已经发展到Baichuan2-13B、Baichuan2-7B百川智能成立于2023年4月10日，由前搜狗公司CEO王小川创立。公司以帮助大众轻松、普惠地获取世界知识和专业服务为使命，致力于通过语言AI的突破，构建中国最优秀的大模型底座。公司核心团队由来自搜狗、百度、华为、微软、字节、腾讯等知名科技公司的AI顶尖人才组成。百川智能成立不到100天，便发布了Baichuan-7B、Baichuan-13B两款开源可免费商用的中文大模型，且在多个权威评测榜单均名列前茅，下载量突破百万6、华为：盘古大模型盘古首页-华为云2023年7月7日，华为发布盘古大模型3.0.“不作诗、只做事”、“AI 重塑千行百业”、“AI for Industries ”华为的盘古大模型聚焦行业价值场景，围绕行业重塑、技术扎根、开放同飞三大方向，为行业客户提供服务。盘古大模型3.0是一个完全面向行业的大模型系列，包括5+N+X三层架构。7、腾讯：混元大模型腾讯混元直到2023年6月19日，腾讯才姗姗来迟地开了场发布会。腾讯对外公布行业大模型研发进展，宣布将助力企业构建专属大模型及智能应用，为企业提供MaaS一站式服务。8、科大讯飞：星火大模型讯飞星火认知大模型-AI大语言模型-星火大模型-科大讯飞“讯飞星火认知大模型”是科大讯飞发布的产品，具有7大核心能力，即文本生成、语言理解、知识问答、逻辑推理、数学能力、代码能力、多模态能力。2023年5月6日，科大讯飞正式发布星火认知大模型。9、商汤-日日新大模型日日新开放平台日日新（SenseNova），是商汤科技宣布推出的大模型体系，包括自然语言处理模型“商量”（SenseChat）、文生图模型“秒画”和数字人视频生成平台“如影”（SenseAvatar）等。2023年4月，商汤科技董事长兼首席执行官徐立宣布推出“日日新SenseNova”大模型体系 [1-2]。2023年8月，日日新大模型获批上线。10、Minimax：ABAB大模型MiniMax 开放平台2021年，由前商汤科技副总裁、通用智能技术负责人闫俊杰成立Minimax 。2023年6月，据路透报道，Minimax完成了新一轮2.5亿美元融资后，整体估值超过12亿美元。Minimax已经发布了包括文本到视觉/语音/文本三个基础模型架构，并推出自研通用大模型“ABAB”。11、上海人工智能实验室-书生大模型书生上海人工智能实验室（又称浦江实验室）成立于2020年，由上海人工智能创新中心发起设立。2021年，浦江实验室联合商汤、香港中文大学、上海交通大学发布通用视觉技术体系“书生”（INTERN）。到如今，书生大模型体系参数已达千亿级别，包括“书生·多模态”、“书生·浦语”和“书生·天际”等三大基础模型，以及面向大模型研发与应用的开源体系。再推荐两个平时经常用觉得很惊艳的大模型kimiChatKimi Chat - 帮你看更大的世界由月之暗面科技有限公司（Moonshot AI）开发。擅长中英文对话，可以获取信息、解答疑问，甚至阅读和理解提供的文件内容。无论是TXT、PDF、Word文档、PPT幻灯片还是Excel电子表格，能轻松应对。还可以解析网页链接。最强大的是kimiChat大海捞针通过率100%。Kimi Chat 公布“大海捞针”长文本压测结果，也搞清楚了这项测试的精髓通义听悟平时学习看视频总结的好帮手，可以通义听悟 - 你的工作学习AI助手通义听悟可以在会议、学习、访谈、培训等场景下：实时记录交流内容，同步翻译批量转写音视频文件，区分发言人智能提炼全文概要、章节速览、发言总结提取视频中的PPT并总结摘要自动提取关键词、待办事项编辑整理笔记，导出、分享记录---------------------------------------------------------------------------我是分割线看大家点赞收藏的还是蛮多的，今天更新一下最近使用的一个国内大模型很棒的一个功能真正的一键生成PPT:讯飞智文 - AI在线生成PPT、Word前段时间讯飞星火不是发布了3.5版本，进去看了看居然有个一键生成PPT的功能。如图，这里有四个方式去创建PPT，当我第一次使用根据主题，我让他根据大纲生成眼睛年终科室汇报时，看到多达二十几页的ppt,一页一页生成在我面前时确实感受到了科技的力量，生成完后你还可以利用AI去修改生成的文章、去润色等，还可以根据ppt模板、自己上传模板去生成。感兴趣的同学可以根据上面链接去体验体验。今天就分享到这里，我是 @李想 Prompter ，我的理想是带领10w小白学会AI，使用AI。关注我，持续更新AI落地用法"
688385792,c713c11cc0a78319d77fb47a1382e7d8,掘金安东尼,https://api.zhihu.com/people/c713c11cc0a78319d77fb47a1382e7d8,people,忽如一夜春风来 千树万树梨花开,当有人问我“什么是大模型”，我顿住了！,article,https://api.zhihu.com/articles/688385792,脑袋里闪过很多想法。 1、<em>大模型</em>就是大模型啊，大语言模型、GPT这些、背后就是 Transform 这些啊。。。 2、现在都在谈 XX业务+<em>大模型</em>，那么大模型也得结合业务去看,16,0,44,1711070739,1711070739,"当有人问我“什么是大模型”，我顿住了！前两天，在聊工作的时候，忽然被一个提问打断了思路：“究竟什么是大模型，能不能给一个官方的解释？”我一下子就顿住了，脑袋里闪过很多想法。1、大模型就是大模型啊，大语言模型、GPT这些、背后就是 Transform 这些啊。。。2、现在都在谈 XX业务+大模型，那么大模型也得结合业务去看，细化的概念涉及太多了，我怎么给一个标准解释？3、ChatGPT 2023爆发之后，涌现了太多概念，一下子回想，不知道从何说起。。。学习的还没发布的快所以，本篇，我们不如来整理下自 ChatGPT 诞生之后的这些大模型领域重要概念吧！再归类一下，提炼一下，下次再被问到，可以直接怼回去~~1、TransformerTransformer 是一种基于自注意力机制的深度学习模型，最初是为了处理序列到序列的任务，比如机器翻译。由于其优秀的性能和灵活性，它现在被广泛应用于各种自然语言处理任务。Transformer模型最初由Vaswani等人在2017年的论文“Attention is All You Need”中提出。Transformer模型主要由以下几部分组成：自注意力机制：这是Transformer模型的核心。它允许模型在处理一个序列的时候，考虑序列中的所有单词，并根据它们的重要性给予不同的权重。这种机制使得模型能够捕获到一个序列中的长距离依赖关系。位置编码：由于Transformer模型没有明确的处理序列顺序的机制，所以需要添加位置编码来提供序列中单词的位置信息。位置编码是一个向量，与输入单词的嵌入向量相加，然后输入到模型中。编码器和解码器：Transformer模型由多层的编码器和解码器堆叠而成。编码器用于处理输入序列，解码器用于生成输出序列。编码器和解码器都由自注意力机制和前馈神经网络组成。多头注意力：在处理自注意力时，Transformer模型并不只满足于一个注意力分布，而是产生多个注意力分布，这就是所谓的多头注意力。多头注意力可以让模型在多个不同的表示空间中学习输入序列的表示。前馈神经网络：在自注意力之后，Transformer模型会通过一个前馈神经网络来进一步处理序列。这个网络由两层全连接层和一个ReLU激活函数组成。残差连接和层归一化：Transformer模型中的每一个子层（自注意力和前馈神经网络）都有一个残差连接，并且其输出会通过层归一化。这有助于模型处理深度网络中常见的梯度消失和梯度爆炸问题。以下是 Transformer 最经典的架构图：左侧为Encoder block，右侧为Decoder block。红色圈中的部分为Multi-Head Attention，是由多个Self-Attention组成的，可以看到Encoder block包含一个Multi-Head Attention，而Decoder block包含两个Multi-Head Attention（其中有一个用到Masked）。Multi-Head Attention上方还包括一个Add & Norm层，Add表示残差连接用于防止网络退化，Norm表示Layer Normalization，用于对每一层的激活值进行归一化。Transformer模型的优点在于，它能够并行处理序列中的所有单词，这使得它在处理长序列时比循环神经网络（RNN）更高效。另外，自注意力机制使得模型能够捕获到序列中长距离的依赖关系，这是RNN难以做到的。2、GPTGPT，全称为Generative Pre-training Transformer，是OpenAI开发的一种基于Transformer的大规模自然语言生成模型。GPT模型采用了自监督学习的方式，首先在大量的无标签文本数据上进行预训练，然后在特定任务的数据上进行微调。GPT模型的主要结构是一个多层的Transformer解码器，但它只使用了Transformer解码器的部分，没有使用编码器-解码器的结构。此外，为了保证生成的文本在语法和语义上的连贯性，GPT模型采用了因果掩码或者叫自回归掩码，这使得每个单词只能看到其前面的单词，而不能看到后面的单词。在预训练阶段，GPT模型使用了一个被称为“Masked Language Model”的任务，也就是预测一个句子中被遮盖住的部分。预训练的目标是最大化句子中每个位置的单词的条件概率，这个概率由模型生成的分布和真实单词的分布之间的交叉熵来计算。在微调阶段，GPT模型在特定任务的数据上进行训练，例如情感分类、问答等。微调的目标是最小化特定任务的损失函数，例如分类任务的交叉熵损失函数。GPT模型的优点在于，由于其预训练-微调的训练策略，它可以有效地利用大量的无标签数据进行学习，并且可以轻松地适应各种不同的任务。此外，由于其基于Transformer的结构，它可以并行处理输入序列中的所有单词，比基于循环神经网络的模型更高效。GPT演进了三个版本：GPT-1用的是自监督预训练+有监督微调，5G文档，1亿参数，这种两段式的语言模型，其能力还是比较单一，即翻译模型只能翻译，填空模型只能填空，摘要模型只能摘要等等，要在实际任务中使用，需要各自在各自的数据上做微调训练，这显然不是很智慧。GPT-2用的是纯自监督预训练，相对于GPT-1，它可以无监督学习，即可以从大量未标记的文本中学习语言模式，而无需人工标记的训练数据。这使得GPT-2在训练时更加灵活和高效。它引入了更多的任务进行预训练，40G文档，15亿参数，能在没有针对下游任务进行训练的条件下，就在下游任务上有很好的表现。GPT-3沿用了GPT-2的纯自监督预训练，但是数据大了好几个量级，570G文档，模型参数量为1750亿，GPT-3表现出了强大的零样本和少样本学习能力。这意味着它可以在没有或只有极少示例的情况下，理解并完成新的任务，它能生成更连贯、自然和人性化的文本，理解文本、获取常识以及理解复杂概念等方面也比GPT-2表现得更好。3、InstructGPTGPT-3虽然在各大NLP任务以及文本生成的能力上令人惊叹，但模型在实际应用中时长会暴露以下缺陷，很多时候，它并不按人类喜欢的表达方式去说话：提供无效回答：没有遵循用户的明确指示，答非所问。内容胡编乱造：纯粹根据文字概率分布虚构出不合理的内容。缺乏可解释性：人们很难理解模型是如何得出特定决策的，难以确信回答的准确性。内容偏见有害：模型从数据中获取偏见，导致不公平或不准确的预测。连续交互能力弱：长文本生成较弱，上下文无法做到连续。在这个背景下，OpenAI提出了一个概念“Alignment”，意思是模型输出与人类真实意图对齐，符合人类偏好。因此，为了让模型输出与用户意图更加“align”，就有了InstructGPT这个工作。InstructGPT相对于GPT的改进主要是使用了来自人类反馈的强化学习方案——RLHF 来微调GPT-3，这种技术将人类的偏好作为激励信号来微调模型。如上图所示，以摘要生成任务为例，详细展示了如何基于人类反馈进行强化学习，最终训练完成得到InstructGPT模型。主要分为三步：收集人类反馈：使用初始化模型对一个样本生成多个不同摘要，人工对多个摘要按效果进行排序，得到一批排好序的摘要样本；训练奖励模型：使用第1步得到的样本集，训练一个模型，该模型输入为一篇文章和对应的一个摘要，模型输出为该摘要的得分；训练策略模型：使用初始化的策略模型生成一篇文章的摘要，然后使用奖励模型对该摘要打分，再使用打分值借助PPO算法重新优化策略模型。InstructGPT可以更好地理解用户意图，通过指令-回答对的数据集和指令-评价对的数据集，InstructGPT可以学习如何根据不同的指令生成更有用、更真实、更友好的输出。4、ChatGPT（GPT3.5/GPT4.0）ChatGPT由OpenAI公司在2022年11月30日发布。在同样由OpenAI开发的GPT-3.5模型基础上，ChatGPT通过无监督学习与强化学习技术进行微调，并提供了客户端界面，支持用户通过客户端与模型进行问答交互。ChatGPT不开源，但通过WebUI为用户提供免费的服务。OpenAI没有公布ChatGPT的论文和相关的训练和技术细节（GPT-3.5没有开源），但我们可以从其兄弟模型InstructGPT以及网络上公开的碎片化的信息中找到实现ChatGPT的线索。根据OpenAI所言，ChatGPT相对于InstructGPT的主要改进在于收集标注数据的方法上，而整个训练过程没有什么区别，因此，可以推测ChatGPT的训练过程应该与InstructGPT的类似，大体上可分为3步：预训练一个超大的语言模型；收集人工打分数据，训练一个奖励模型；使用强化学习方法微调优化语言模型。相对于GPT-3，GPT-3.5拥有3个变体，每个变体有13亿、60亿和1750亿参数，当前ChatGPT提供了基于GPT-4的版本，相对于GPT-3.5，GPT-4模型据说有1万亿个参数，GPT-4是一个多模态模型，即它可以接受图像和文本作为输入，并输出文本；而GPT-3.5只能接受文本作为输入，并输出文本。这使得GPT-4可以处理更复杂且具有视觉信息的任务，如图像描述、图像问答、图像到文本等。5、大模型关于大模型，有学者称之为“大规模预训练模型”，也有学者进一步提出“基础模型”的概念。2021年8月，李飞飞、Percy Liang等百来位学者联名发布了文章：On the Opportunities and Risks of Foundation Models，提出“基础模型”的概念：基于自监督学习的模型在学习过程中会体现出来各个不同方面的能力，这些能力为下游的应用提供了动力和理论基础，称这些大模型为“基础模型”。“小模型”：针对特定应用场景需求进行训练，能完成特定任务，但是换到另外一个应用场景中可能并不适用，需要重新训练（我们现在用的大多数模型都是这样）。这些模型训练基本是“手工作坊式”，并且模型训练需要大规模的标注数据，如果某些应用场景的数据量少，训练出的模型精度就会不理想。“大模型”：在大规模无标注数据上进行训练，学习出一种特征和规则。基于大模型进行应用开发时，将大模型进行微调（在下游小规模有标注数据进行二次训练）或者不进行微调，就可以完成多个应用场景的任务，实现通用的智慧能力。可以这么类别，机器学习同质化学习算法（例如逻辑回归）、深度学习同质化模型结构（例如CNN），基础模型则同质化模型本身（例如GPT-3）。人工智能的发展已经从“大炼模型”逐步迈向了“炼大模型”的阶段。ChatGPT只是一个起点，其背后的Foundation Module的长期价值更值得被期待。大模型发展的前期被称为预训练模型，预训练技术的主要思想是迁移学习。当目标场景的数据不足时，首先在数据量庞大的公开数据集上训练模型，然后将其迁移到目标场景中，通过目标场景中的小数据集进行微调，使模型达到需要的性能。在这一过程中，这种在公开数据集训练过的深层网络模型，被称为“预训练模型”。使用预训练模型很大程度上降低下游任务模型对标注数据数量的要求，从而可以很好地处理一些难以获得大量标注数据的新场景。2018年出现的大规模自监督神经网络是真正具有革命性的。这类模型的精髓是从自然语言句子中创造出一些预测任务来，比如预测下一个词或者预测被掩码（遮挡）词或短语。这时，大量高品质文本语料就意味着自动获得了海量的标注数据。让模型从自己的预测错误中学习10亿+次之后，它就慢慢积累很多语言和世界知识，这让模型在问答或者文本分类等更有意义的任务中也取得好的效果。没错，说的就是BERT和GPT-3之类的大规模预训练语言模型，也就是我们说的大模型。2022年8月，Google发表论文，重新探讨了模型效果与模型规模之间的关系。结论是：当模型规模达到某个阈值时，模型对某些问题的处理性能呈现快速增长。作者将这种现象称为Emergent Abilities，即涌现能力。大模型的典型架构就是Transformer架构，自2018年开始统治NLP领域，NLP领域的进展迎来了井喷。为何预训练的transformer有如此威力？其中最重要的思想是attention，也就是前面提到过的注意力机制。就是句子中每个位置的表征（representation，一般是一个稠密向量）是通过其他位置的表征加权求和而得到。为什么这么简单的结构和任务能取得如此威力？原因在其通用性。预测下一个单词这类任务简单且通用，以至于几乎所有形式的语言学和世界知识，从句子结构、词义引申、基本事实都能帮助这个任务取得更好的效果。大模型也在训练过程中学到了这些信息，让单个模型在接收少量的指令后就能解决各种不同的NLP问题。也许，大模型就是“大道至简”的最好诠释。“大模型”通常是具有大量的参数，它们定义了模型的复杂性和学习能力。实现大模型主要涉及以下几个步骤：模型架构设计：大模型通常具有更深的网络架构（更多的层）和/或更宽的层（更多的神经元）。这种设计可以使模型拥有更强大的表示能力，可以学习和记住更复杂的模式，比如Transformer架构。数据准备：训练大模型需要大量的数据。这些数据为模型提供了学习的机会，使其能够捕捉到数据中的潜在模式。大量的训练数据也有助于防止模型过拟合，这是深度学习模型常常需要面临的问题。计算资源：大模型需要大量的计算资源来进行训练，包括强大的GPU和足够的内存。大模型的训练通常需要并行化和分布式计算来处理大量的计算任务。优化算法：训练大模型需要高效的优化算法。这些算法（如随机梯度下降及其变体）用于调整模型的参数以最小化预测错误。正则化技术：大模型由于其复杂性，更容易过拟合训练数据。因此，训练大模型通常需要使用正则化技术（如权重衰减、dropout等）来防止过拟合。模型并行和数据并行：由于大模型的规模，训练过程常常需要在多个GPU或者多个机器上进行。模型并行和数据并行是两种常用的策略，前者是将模型的不同部分分布在不同的设备上，后者是将数据分布在不同的设备上。这些是实现大模型的一般步骤和要求。然而，这也带来了一些挑战，如计算资源的需求、训练时间的增加、过拟合的风险等。因此，选择合适的模型规模以平衡性能和效率是深度学习中的一个重要任务。除了GPT-3、GPT-3.5及GPT-4，当前还有BERT、RoBERTa、T5、XLNet等大模型。这些模型都是由不同的公司和组织开发的，它们都有自己的优点和缺点。以下是这些模型的简要介绍：BERT：由Google开发的一种预训练语言模型，在自然语言处理领域取得了很大的成功。BERT有340M和1.1B两个版本，其中1.1B版本有33亿个参数。RoBERTa：由Facebook AI Research开发的一种预训练语言模型，在自然语言处理领域取得了很大的成功。RoBERTa有125M、250M、500M、1.5B和2.7B五个版本，其中2.7B版本有27亿个参数。T5：由Google开发的一种预训练语言模型，在自然语言处理领域取得了很大的成功。T5有11B和22B两个版本，其中22B版本有220亿个参数。XLNet：由CMU和Google Brain开发的一种预训练语言模型，在自然语言处理领域取得了很大的成功。XLNet有两个版本，分别为XLNet-Large和XLNet-Base，其中XLNet-Large有18亿个参数。GShard：由Google开发的一种预训练语言模型，在自然语言处理领域取得了很大的成功。GShard有两个版本，分别为GShard-Large和GShard-Base，其中GShard-Large有6亿个参数。Switch Transformer：由CMU开发的一种预训练语言模型，在自然语言处理领域取得了很大的成功。Switch Transformer有两个版本，分别为Switch Transformer-Large和Switch Transformer-Base，其中Switch Transformer-Large有1.6亿个参数。6、AIGC（人工智慧生成內容）AIGC 中文译为人工智能生成内容，一般认为是相对于PCG（专业生成内容）、UCG（用户生成内容）而提出的概念。AIGC狭义概念是利用AI自动生成内容的生产方式。广义的AIGC可以看作是像人类一样具备生成创造能力的AI技术，即生成式AI，它可以基于训练数据和生成算法模型，自主生成创造新的文本、图像、音乐、视频、3D交互内容等各种形式的内容和数据，以及包括开启科学新发现、创造新的价值和意义等。下面示例了AIGC能做的事情：文本生成：例如自动撰写新闻、故事、诗歌或编写代码。图像和艺术创作：自动生成画作、设计图或3D建模。音乐制作：根据某种风格或主题自动创作音乐。视频生成：包括电影剪辑、动画制作或虚拟现实内容。游戏内容生成：自动设计关卡、角色或游戏剧情。教育内容：根据特定课程或学习目标生成教材和练习。科学研究：辅助科研，如自动生成科学假设、数据分析或实验设计。商业创新：创造新的商业模式、产品设计或市场策略。个性化内容：根据用户喜好和历史数据定制化生成内容。AIGC 可以做啥：AIGC技术中，耳熟能详的当属Transformer、GPT、Diffusion、CLIP、Stable Diffusion，下面简要介绍下Diffusion。(1) Diffusion“扩散”来自一个物理现象：当我们把墨水滴入水中，墨水会均匀散开；这个过程一般不能逆转，但是AI可以做到。当墨水刚滴入水中时，我们能区分哪里是墨哪里是水，信息是非常集中的；当墨水扩散开来，墨和水就难分彼此了，信息是分散的。类比于图片，这个墨水扩散的过程就是图片逐渐变成噪点的过程：从信息集中的图片变成信息分散、没有信息的噪点图很简单，逆转这个过程就需要AI的加持了。研究人员对图片加噪点，让图片逐渐变成纯噪点图；再让AI学习这个过程的逆过程，也就是如何从一张噪点图得到一张有信息的高清图。这个模型就是AI绘画中各种算法，如Disco Diffusion、Stable Diffusion中的常客扩散模型。（2）CLIP（大规模预训练图文表示模型）如果让你把下面左侧三张图和右侧三句话配对，你可以轻松完成这个连线。但对AI来说，图片就是一系列像素点，文本就是一串字符，要完成这个工作可不简单。大规模预训练图文表示模型用4亿对来自网络的图文数据集，将文本作为图像标签进行训练。通过对比学习，希望模型能够学习到文本与图像对的匹配关系。CLIP为计算机视觉研究者打开了一片非常广阔的天地，把自然语言级别的抽象概念带到计算机视觉中。（3）Stable DiffusionDiffusion算法针对任意噪声图片去噪后得到的图片是不可控的，如果让Diffusion算法能够接受文字提示从而生成想要的图片，这就是AIGC的另一个热点，AI绘画：只输入文字描述，即可自动生成各种图像。其核心算法Stable Diffusion是文字到图片的多模态算法CLIP和图像生成算法Diffusion的结合体，CLIP作为文字提示用来对Diffusion施加影响生成所需的图片。参考算法核心逻辑，Stable Diffusion的数据会在像素空间、潜在空间、条件三部分之间流转，其算法逻辑分为以下几步：图像编码器将图像从像素空间压缩到更小维度的潜在空间，捕捉图像更本质的信息；对潜在空间中的图片添加噪声，进行扩散过程；通过CLIP文本编码器将输入的描述语转换为去噪过程的条件；基于一些条件对图像进行去噪以获得生成图片的潜在表示，去噪步骤可以灵活地以文本、图像和其他形式为条件；图像解码器通过将图像从潜在空间转换回像素空间来生成最终图像。7、AGI（通用人工智能）AGI 是通用人工智能的缩写，它指的是一种理论上的形式的人工智能，具有全面理解、学习和应用知识的能力，与人类智能在各方面上都相当或者超越。这种类型的AI能够理解、学习和应用其在一个领域学到的知识到任何其他领域。通用人工智能与当前存在的人工智能（通常被称为弱人工智能或窄人工智能）有很大的不同。当前的AI系统通常在一个非常特定的任务或一组任务中表现出超人的性能，例如围棋、语言翻译、图像识别等，但它们缺乏在一个任务上学到的知识应用到其他任务的能力，也没有真正理解它们正在做什么的能力。当前火热的GPT等大模型仍然是一种窄人工智能。它们被训练来执行特定的任务（在这种情况下是生成文本），而并不具有广泛的理解能力或适应新任务的能力，这是AGI的特征。然而，GPT和AGI的关联在于，GPT是当前AI研究为实现AGI所做出的努力中的一部分。它表明了预训练模型的潜力，并给出了一种可能的路径，通过不断增加模型的规模和复杂性，可能会接近AGI。但是，这仍然是一个未解决的问题，并且需要更多的研究来确定这是否可行，以及如何安全有效地实现这一目标。尽管GPT在生成文本上表现出了强大的性能，但它并不理解它正在说什么。GPT没有意识，也没有理解或意愿，它只是学会了模仿人类语言模式的统计模型。这是目前所有AI系统（包括GPT）与AGI之间的一个关键区别。我们仍然远离实现通用人工智能。实现这个目标需要解决许多重大的科学和技术挑战，包括但不限于语义理解、共用和迁移学习、推理和规划，以及自我知觉和自我理解。8、LLM（大型语言模型）大型语言模型，是一种人工智能模型，旨在理解和生成人类语言。它们在大量的文本数据上进行训练，可以执行广泛的任务，包括文本总结、翻译、情感分析等等。LLM的特点是规模庞大，包含数十、成百、上千亿的参数，这种模型可以捕捉语言的复杂模式，包括句法、语义和一些上下文信息，从而生成连贯、有意义的文本。GPT3、ChatGPT、BERT、T5、文心一言等都是典型的大型语言模型。9、羊驼（Alpaca）ChatGPT大热，让人惊叹其强大的对话、上下文理解、代码生成等等能力，但另一方面由于GPT-3以后的系列模型 & ChatGPT均未开源，再加上高昂的训练成本所构成的坚不可摧的护城河，让普通人 & 公司望而却步。2023年3月，Meta开源了一个新的大模型系列 —LLaMA（Large Language Model Meta AI），参数量从70亿到650亿不等。130亿参数的LLaMA模型在大多数基准上可以胜过参数量达1750亿的GPT-3，而且可以在单块V100 GPU上运行。时隔几天，斯坦福基于LLaMA 7B微调出一个具有70亿参数的新模型Alpaca，他们使用了Self-Instruct论文介绍的技术生成了52K条指令数据，同时进行了一些修改，在初步的人类评估中，Alpaca 7B模型在Self-Instruct指令评估上的表现类似于text-davinci-003（GPT-3.5）模型。然后，斯坦福学者联手CMU、UC伯克利等，再次推出一个全新模型 — Vicuna，俗称「小羊驼」。Vicuna 是通过在ShareGPT收集的用户共享对话上对LLaMA进行微调训练而来，训练成本近300美元。研究人员设计了8个问题类别，包括数学、写作、编码，对Vicuna-13B与其他四个模型进行了性能测试。测试过程使用GPT-4作为评判标准，结果显示Vicuna-13B在超过90%的情况下实现了与ChatGPT和Bard相匹敌的能力。同时，在超过90%的情况下胜过了其他模型，如LLaMA和斯坦福的Alpaca。10、Fine-tuning（微调）微调是一种机器学习技术，用于调整已预训练的模型以适应新的任务。预训练的模型是在大量数据集（如整个互联网的文本）上训练的，并且已经学习了该数据的许多基本模式。然后，这些模型可以被微调，即在更小、特定的数据集上进行额外的训练，以适应特定的任务。例如，你可以取一个已经在大量的英文文本上预训练的模型（这样它已经学会了英语的语法和许多词汇），然后在一个小的数据集上进行微调，这个数据集包含医学文本。微调后的模型将能更好地理解和生成医学相关的文本，因为它已经适应了这个特定的领域，前面提到的GPT-1就用到了微调技术。微调的一个关键优势是，预训练的模型已经学习了许多有用的基本模式，因此只需要相对较小的数据集就可以对其进行微调。这样，微调可以更快、更有效地训练模型，尤其是在数据有限的情况下。11、自监督学习（Self-Supervised Learning）自监督学习是从数据本身找标签来进行有监督学习。无监督学习没有标注标签的过程，而是从数据分布的角度来构造损失函数。自监督学习的代表是语言模型，无监督的代表是聚类。自监督不需要额外提供label，只需要从数据本身进行构造。这种方法的一个常见示例是预测文本中的下一个单词或缺失的单词。模型的输入可能是一个句子中的一部分，目标或标签是句子中的下一个单词或缺失的单词。通过这种方式，模型可以在大量未标记的文本数据上进行训练，并学习语言的语法和语义。自监督学习的一个主要优势是可以利用大量的未标记数据进行训练。在许多情况下，获取未标记的数据要比获取标记的数据容易得多。例如，互联网上有大量的文本数据可以用来训练语言模型，但只有一小部分数据有人工标记。自监督学习的一种应用是图像修复，我们可以通过随机去掉图像中的某个部分来生成训练数据，原数据作为训练标签进行预训练。对于下游任务，生成器学到的语义特征相比随机初始化有显著提升，对于分类和物体检测有明显的性能提高。自监督学习可以学习数据的内在模式，但它可能需要额外的监督学习步骤（例如，微调）来执行特定的任务。例如，预训练的语言模型（如GPT-3）首先使用自监督学习来学习语言的模式，然后可以在特定任务的标记数据上进行微调。12、自注意力机制（Self-Attention Mechanism）自注意力机制，也被称为自我注意力，是一种捕获序列数据中不同位置之间相互依赖性的技术。这种机制使得模型可以在处理一个元素（例如一个词）时，考虑到序列中其他元素的信息。在自注意力机制中，每一个输入元素（例如一个单词）都会被转换为三种向量：查询向量、键向量和值向量。计算一个词的新表示的步骤如下：计算查询向量与所有键向量的点积，以此来获取该词与其他词之间的相关性。将这些相关性得分经过softmax函数转化为权重，以此使得与当前词更相关的词获得更高的权重。用这些权重对值向量进行加权平均，得到的结果就是当前词的新表示。自注意力机制可以捕获到序列中长距离的依赖关系，而不仅仅是像循环神经网络（RNN）那样只能捕获相邻词之间的信息。这使得它在处理如机器翻译、文本生成等需要理解全局信息的任务中表现得尤为出色。13、零样本学习（Zero-Shot Learning)零样本学习是一种机器学习的范式，主要解决在训练阶段未出现但在测试阶段可能出现的类别的分类问题。例如，人类即使没有见过斑马也能通过先验知识识别它，因为他们知道斑马是一种身上有黑白条纹的、外形像马的动物。在零样本学习中，这些未出现过的类别的信息通常以一种形式的语义表示来提供，例如词嵌入、属性描述等。14、AI Alignment（AI对齐）AI对齐是指如何让人工智能模型的产出，和人类的常识、认知、需求、价值观保持一致。例如，OpenAI成立了Alignment团队，提出了InstructGPT模型，这个模型使用Alignment技术要求AI系统的目标要和人类的价值观与利益保持一致。15、词嵌入（Word Embeddings）词嵌入是一种将词语或短语从词汇表映射到向量的技术。这些向量捕捉到了词语的语义（含义）和语法（如词性、复数形式等）特征。词嵌入的一个关键特点是，语义上相近的词语在向量空间中通常会靠得很近。这样，计算机就可以以一种更接近人类语言的方式理解和处理文本。例如，我们有四个词：“king”, “queen”, “man”, “woman”。在一个好的词嵌入模型中，“king”和“queen”的词向量将非常接近，因为他们都代表皇室的头衔；同样，“man”和“woman”的词向量也会非常接近，因为他们都代表性别。此外，词嵌入模型甚至可以捕捉更复杂的关系，比如从“king”的词向量中减去“man”的词向量并加上“woman”的词向量，结果会非常接近“queen”的词向量，这表明了性别的差异性。16、位置编码（Positional Encoding）位置编码是在处理序列数据（如文本或时间序列）时用来表示每个元素在序列中位置的技术。尽管深度学习模型本身并不具备处理输入序列顺序的能力，位置编码被引入以提供序列中元素的顺序信息。在Transformer和GPT中，使用正弦和余弦函数生成位置编码。这种方法生成的位置编码不仅为每个位置提供了唯一标识，还能够捕捉到相对位置关系。17、中文LangChain中文LangChain是一个开源项目，旨在将大型语言模型和其他资源（如特定领域数据）、计算能力结合起来，实现本地化知识库检索与智能答案生成。它通过拆分领域内容为小数据块并进行索引，实现语义搜索，结合语言模型进行智能问答，从而以更自然的语言回答问题。LangChain的主要工作流程如下：把领域内容拆分成一块块的小文件块，对块进行Embedding后放入向量库索引（为后面提供语义搜索做准备）。搜索时，把查询词（Query）进行Embedding后通过语义检索找到最相似的K个文档（Docs）。把相关的文档组装成Prompt的Context，基于相关内容进行问答，让chatglm等进行In Context Learning，用人话回答问题。"
640573685,2cceb06f8af0e0ca6d77e47ced9ae119,Minogame,https://api.zhihu.com/people/2cceb06f8af0e0ca6d77e47ced9ae119,people,被迫从事大龄幼儿教育行业,视觉大模型一无是处,article,https://api.zhihu.com/articles/640573685,or not皈依语言<em>模型</em> 最后讨论一下视觉模型往<em>大</em>做的出路在哪里。当然这里是要排除掉纯粹以学习表象为目的的<em>模型</em>的，例如围棋或者天气的模型，这些<em>模型</em>本身的能力本就展现在学习到人类无法用语言抽象的神秘的表象上,1068,83,1208,1688092508,1688092508,"最近各种机缘巧合，反复被人问到一个让我非常反感的问题：你的视觉模型有多少参数？虽然Ph.D的职业训练能让我的表情依旧处在1940年欧洲版图那种毫无波兰的状态，但是刨根问底地讲，这个问题所诱发的，对meaninglessitude的观念已经让我形成了一种理性上的反感。一、22B与175B的差距差不多是175B首先我还是不否定，随着视觉模型参数量的提升，其在传统的视觉任务上的绝对数值表现也有相应的提升。近期出现的最好的一个例子就是Google的ViT-22B，虽然用了少量的没有开源的数据以及保守的炼丹技巧才让这个模型跑起来。但是这不妨碍圈内大体保持一个乐观的态度，认为计算机视觉依旧是配得上大模型的。然而在我看来，这个存在感薄的宛如arxiv主页一般的视觉最大往生模型，恰巧完成了“堆积参数与数据量对于纯视觉这个领域毫无意义”的证明。稍微思考一下，这个模型的训练量实际上达到了(JFT-)4B * 256token/img * 3epoch约等于3T的tokens，这已经比LLaMa最大的模型的训练量还要大一倍。但是ViT-22B上并没有什么真的有意义的结论，除了在经典旧世中的数据集上又多涨了三五斗，也就是这个模型更偏向于sketch而不是texture这些泛空的内容。拿着这个ROI堪比春夏季大反攻的结论，视觉还会有接下来的一个大模型吗，我看是不会有了。然而别忘了，22B与175B的差距差不多是175B。二、视觉模型再大也依旧愚钝在进入下一段的形上学内容之前，我先从经验主义的角度出发，复述三个“现代计算机视觉头顶的乌云”。当然，这些问题并不是没有得到重视，因为每年都持续的会有海量相关的文章这些问题上吃SR——这也确实印证了它们是难以被解决的。首先要拿出来讲的自然是陈词滥调的adversarial attack问题。平心而论虽然NLP领域也有这个问题，但是当NLP迈入(generative)LLM时代之后这个问题也几乎不再被重视了。反观CV领域，这个问题就像是怨魂一样纠缠着每一个倒霉的审稿人，因为没有人站出来写这样一篇文章，说给他们一个大模型他们就能终结这个领域。而且甚至于现在都没有人愿意去测试这些视觉大模型的对抗攻击问题，因为人们已经普遍接受了这么一个理念，就是视觉模型就应该被攻击。其次则是房间里的大象的问题，于2018年被提出来用以质疑detection模型与算法的有效性。大意是说，即便我将一只大象p到一个普通房间模样的照片中，我们充满智慧的模型也可以IOU>0.9地将它检测出来，即便这从经验之理中来看是荒谬的。这其实是个充满哲理而且可能影响到CV根基的问题，只不过解决它与否并不影响一个算法在coco上的性能（或者说只会有负影响），所以人们也只管当它是房间里的大象。最后第三朵乌云就更现实一点，我愿意称之为样本空间的无序化问题。换言之，视觉信息在样本的特征空间中无法形成体系性结构。这里，视觉信息就是指图片中存在的可以抽象出的语义，体系性结构指的则是这些语义相互之间的关系。即便现在先进的自监督训练技术，可以很好的让某一类事物在“不需要知晓其语义的”情况下抱团取暖（体现为t-SNE的良好可区分性），但是从语义的角度来看，航天飞机的一个很小的r-neighbourhood内既可以有短毛橘猫也可以有果酱面包。往好的讲，这提高了基于视觉特征的创造力，往不好的讲，这倒逼视觉的研究者只能不断去寻求基于提高（特定benchmark的性能）力来创造视觉特征。三、NLP的知识与CV的知识首先我们需要确认两个概念，因为这两个概念的精准定义是一个长久以来都纠缠不清的问题，所以这里我们采取ChatGPT提供的通识性定义：Representation（表象，也作表征）：在心灵哲学和认识论中，""representation""指的是一种心智状态或实在，用以指代某一件事物。表象的定义中蕴涵了我们的心灵具备引用或表达存在于世界中的物体、思想或情况的能力。表象可以采取多种形式，包括心理图像、信念、思想或语言符号，可以被看作是心灵与外部世界之间的中介，使我们能够对世界有知识、感知和理解。Concept（概念）：另一方面在认识论中，""concept""是一个抽象或一般性的心理表象，包括一种或一类的事物、事件或观念。概念是思维和语言的基本构建单元，使我们能够对经验和知识进行分类和组织。对照单一的表象，概念的范围更为抽象和概括。概念通过抽象和概括的过程形成，我们在多个实例中识别共同的特征或属性，并创建一个心理范畴来代表这些共享的特征。武断地讲，“表象”和“概念”都涉及到表达和传达某些信息或意义，但它们的本质和来源是不同的。表象更多的可以被看作是一种心理层面的现象，而概念则可以被认为是一种思维层面的产物。事实上我认为搞明白这两个概念了之后，在认知层面上NLP与CV模型所获得的知识便很容易区分理解。我认为在语言（特别是generative）模型的框架内，我们为了训练一个模型，所提供的数据实质上展现的是Relations of Concepts（这里借用了Hume的理念，但是区别于纯粹的演绎，语言模型中的Relations依旧是通过归纳获得）。在此之上，对于某个Concept本身的理解，语言模型也是通过其与其他的Concepts之间的关联来获得一个间接的认知，而并没有一个直接的类似于人类心灵上的直接理解（特别是诸如“时间”等概念）。换而言之，语言模型所学习到的，是诸多由Concepts之间的关联所组成的Representations，以及由这些表象之间的关联所呈现出来更深刻更复杂的表象，而人类（语言）世界中所存在的或者所能被理解的表象多若繁星，我们需要万亿的参数去记忆这些表象，以及理解这些表象之间的关联性（或者说一个由表象为量子的N-gram）。反观视觉模型，在不论有监督或者自/无监督的（几乎所有的）训练框架内，其优化目标实质上是将特定的具体的数据，转化为特定的唯一的Representation，并经由特定的视觉任务（即人为的经验）最终将这些表象抽象为Concept，换言之是一个学习Matter of Facts的过程。这里再额外解释一下为什么需要经由特定的视觉任务，有过（自监督）视觉大模型训练经验的人大多会有这样一个认知，就是视觉模型的效果很难去做evaluatation，大家只是从k-NN/Linear Probe/Attentive Probe/Finetune（分类或者其他下游任务）中选择性能好的来写论文，但实际上这里的本质原因是，在没有人为的经验来帮助视觉模型去做抽象的时候，一个算法不会去主动的来做这样的抽象（它没有Loss也没有义务去做），它只需要将数据与表象去做对应即可。反过来讲如果在算法的设计上就已经考虑到了抽象Concept的路径，那么这个模型在Linear Probe的性能自然会好一些（例如杨立昆君的I-JEPA，下面会再讲到）。四、参数量解决不了抽象概念的问题现在再回到那该死的参数量上，对于现阶段的视觉任务，其对应模型的参数量恐怕并不能成为让「视觉大模型出现chatGPT时刻」的主要指标。再回到现代计算机视觉头顶的那三朵乌云：参数量只能在概念归纳的强度上获得边际的收益。一般来讲，视觉模型在经由表象来归纳概念时，脱离不了一个大致的聚类的概念。换句话说，我们预先会有一个“同一概念的表象会聚集在一起”的先验知识，例如Gaussian Prior，我们再经由实际的表象的分布来获得后验的结果。但是通过增加参数量来更精准的表达表象在向量空间的位置，其收益注定会逐渐边际化，因为参数量并不会与误差有一个线性的关系。以至于在某一个时刻开始，在有限的样本数量下，误差更多来源于先验的分布，最终提升参数量只能表现为ImageNet上那0.1%的性能提升。除此之外，增加参数量所引发的副效果，更高的向量空间的维度，也会增加归纳一个概念时所需的表象的数量（维度诅咒），从而也就会为对抗样本提供了更多的存在的空间。参数量并不能脱离归纳来知晓表象或概念之间的存在逻辑。从道理上来讲，神经网络可以Universal Approaximation，而且这个能力会随着参数量的增加而变强。事实上以我个人的经验，大体上来说一个参数量更多的模型，是可以用单一的特征向量来表达某一个相对复杂的场景的，例如一个盘子里面装着各种不同的水果摆件，只不过我们无法提供高质量的人为的经验去概念化此类东西，在这个例子里面通常来说我们只能提供诸如“静物”“果盘”或者“一个盘子里面装着各种不同的水果摆件”之类的概念或其组合。只不过这种概念对于实际上模型所能表达的表象来说过于的抽象，模型通常只能去概括的统计更初级表象的量来组合成此类高度复杂的表象。所以如果缺乏可以枚举“与”“或”“非”等关联的数据的引导，模型自然也不会去主动因为存在或者不存在某一个初级表象，就将一个复杂的表象去彻底归为不同的概念。例如，在缺乏数据的情况下，模型不太会因为一个“森林”的图片里面有一辆自行车，就把它归类为“公园”，或者因为缺少一条河流，就把它归类为“青山”。参数量也不能解决样本空间的无序化问题，从而无法让模型在脱离人为的经验的情况下，自发地学到由概念之间的关联而产生的新的概念。在自然语言当中，概念本身是有很好的层级化关联结构的，故而由概念或者表象的关联而形成的表象（例如一句话）也会因此获得一个结构性的表达，基于这种结构性的表达，我们会很容易继续创造新的概念或者表象。反观视觉中信息缺乏此类的层级化关联结构，视觉感官上相似的表象对应的概念可以完全不一样（例如字母l与数字1），所以在缺乏人为经验的情况下（自/无监督），在（表象的）特征空间里面也难以自发地形成结构性的表达，继而无法自发地形成新的概念。一个显著的例子就是，在跨模特的Align模型出现之前，几乎没有什么视觉算法可以做generalized zero-shot learning，即便是在某个特定的domain上（CUB鸟或者oxford花），zero-shot的性能也很难看。在大语言模型可以通过纯粹的语言概念解读画出独角兽的同时，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”（除非毛子们能给它提供足够多的人为的经验）。五、Naive Cross-Modality Alignment并不解决根本问题有碰过Stable Diffusion的孩子们应该会注意到这样一个现象，就是绝大多数的prompt都是宛如咒语一般的独立的单词，而真正用自然语言描述的场景却很难被准确的作画。有直接碰过CLIP的孩子们也应该会注意到这样一个现象，绝大多数图文匹配的分数（无论正负样本）都分布在一个较为狭小的区间，而且难以采用一个直观的阈值来判断两者的匹配度。排除掉模型训练相关的原因，这凸显了两个问题：视觉端能精准学到的Concept的量是有限的，且更多的是简单的单词/短语级别的Concept。实际上在本人实际训练中文CLIP的时候，就遇到了学习到的top 100万个Concept里面里面有80万都是人名的状况（是的，再往后人名的比重会更高）。而这个其实是符合真实世界的分布，诸君可以现场站起来，用语言描述一下自己所能看见的物件，就会发现真实生活中接触到的相对简单的视觉Concept相当的贫乏，而更多的是身边的人的名字。视觉端并不是不可以学到一个相对复杂的表象，但是视觉端无法像文本端一样把这种复杂的表象归纳称为概念，这就导致了视觉文本两侧的匹配会出现偏差，而实际上训练数据中能供视觉端归纳出一个复杂表象所对应的概念的数据是有限的，也就是说，即便有语言的帮助，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”（除非毛子们能给它提供足够多的人为的经验）。所以，为什么Lock image Tuning现在会成为一个比较合理的CLIP训练模式，因为用语言将视觉端学到的表象归纳为概念这件事情的难度，远小于用视觉去理解语言中由多个抽象的概念构成的复杂表象。六、皈依 or not皈依语言模型最后讨论一下视觉模型往大做的出路在哪里。当然这里是要排除掉纯粹以学习表象为目的的模型的，例如围棋或者天气的模型，这些模型本身的能力本就展现在学习到人类无法用语言抽象的神秘的表象上，这些模型随着数据规模的增大，参数量自然需要scale-up来提升表象的能力（例如在99x99的围棋盘上，一个千亿参数的模型的性能理论上来讲必然好过一个百亿的）。现在的学术大致上寻找到了两个出路，其一便是让视觉模型彻底的皈依大的语言模型的能力（例如Google丢出来的PaLM-E以及今年的CVPR最佳论文Visual Programming），让视觉模型回归一个为概念提供视觉表象的功能，让大语言模型来完成更为复杂的概念的解读。这个时候视觉模型的参数量即可以用来学习一些人类无法用语言抽象的表象上（例如depth map，optical flow，hyperspectral singal），从而补齐语言模型在空间推理上的一些弱点，让其更加的接近真实世界的AGI。但是这种模式依旧依赖于毛子去打坦克搜集大量的数据用以提供人为的经验，未来的路就仿佛路易十六39岁时的样子——一眼望不到头。另外一个思路则体现在杨立昆君的I-JEPA以及feifei大佬的SiamMAE里面，我们强行的去让视觉模型去理解表象之间的关联，而这种任务本身对有注意力机制的ViT模型来说并不是特别困难。这样做最大的优点在于，它可以部分地解决前述的第二三朵乌云，但是这些方案由于突出强调个体表象，就比较难以学习到基于多种表象与概念而形成的复杂表象，特别体现在ImageNet的Finetune效果会比较差（ImageNet有一些相对复杂的场景类别）。而事实上，当下视觉的学术圈不怎么容得下一个性能不怎么好的算法，很多开放的探索都被操着一口Chinglish的审稿人百般刁难（请原谅我的冒犯），新的算法仿佛被困在了一个无尽的黑洞中难以诞生。当然如果你们关心我是怎么回答“你的视觉模型有多少参数？”这个问题的话，我一般是会冷漠地讲“ViT-B，88M，再大的模型他们不给上线了”。"
659786905,d9e622ed74686029afe0f7c4d952092c,喝拿铁的皮卡丘,https://api.zhihu.com/people/d9e622ed74686029afe0f7c4d952092c,people,搞算法的，笔记也同步在公众号【瓦力算法学研所】,大模型面经-transformer部分,article,https://api.zhihu.com/articles/659786905,因为对于字符级别（char-level）或者子词级别（subword-level）的NLP<em>模型</em>，挑战在于需要模型能够理解字符或者子词组合起来形成词语和句子的语义，这对<em>模型</em>的能力有很高的要求。 然而，以前NLP<em>模型</em>没办法做到很深，两层lstm基本就到极限了,275,4,1050,1696601743,1712712384,"1.Self-Attention的表达式2.为什么要除根号？如果输入向量的维度d比较大，那么内积的结果也可能非常大，这会导致注意力分数也变得非常大，这可能会使得softmax函数的计算变得不稳定（推向了梯度极小的区域），并且会影响模型的训练和推理效果。通过除以根号d，可以将注意力分数缩小到一个合适的范围内，从而使softmax函数计算更加稳定，并且更容易收敛。3.为什么transformer用Layer Norm？有什么用？（1）. 增加模型的鲁棒性。由于LayerNorm可以对输入进行归一化，使得每个神经元的输入具有相似的分布特征，从而有助于网络的训练和泛化性能。此外，由于归一化的系数是可学习的，网络可以根据输入数据的特点自适应地学习到合适的归一化系数。（2）. 加速模型的训练。由于输入已经被归一化，不同特征之间的尺度差异较小，因此优化过程更容易收敛，加快了模型的训练速度。（3）. 减少模型的过拟合。由于LayerNorm可以使得每个神经元的输入具有相似的分布特征，从而减少了模型过拟合的风险，提高了模型的泛化能力。4.为什么不用BN？BN就是在每个维度上统计所有样本的值，计算均值和方差；LN就是在每个样本上统计所有维度的值，计算均值和方差（注意，这里都是指的简单的MLP情况，输入特征是（bsz，hidden_dim））。所以BN在每个维度上分布是稳定的，LN是每个样本的分布是稳定的。如果在一个维度内进行normalization，那么在这个维度内，相对大小有意义的，是可以比较的；但是在normalization后的不同的维度之间，相对大小这是没有意义的。BN用在CNN上是在channel维度上统计BxNxW上的均值和方差。这是因为同一个通道的特征图都是由同一个卷积核产生，所以分布肯定要相对一致。而BN用在NLP数据上，不同句子在同一个位置的分布大概率是不一样的，应用BN来约束其实没啥意义。LN特别适合处理变长数据，因为是对channel维度做操作(这里指NLP中的hidden维度)，和句子长度和batch大小无关，所以NLP task上大家首选LN了。CV和NLP数据特性的不同，对于NLP数据，前向和反向传播中，batch统计量及其梯度都不太稳定。5.Bert为什么要搞一个position embedding？self-attention无法表达位置信息, 需要增强模型针对位置的表达能力6.Bert为什么三个embedding可以相加？这里的三个embedding是指token embedding，segment embedding，position embedding。虽然在深度神经网络里变得非常复杂，本质上神经网络中每个神经元收到的信号也是“权重”相加得来。7.transformer为什么要用三个不一样的QKV？增强网络的容量和表达能力8.为什么要多头？举例说明多头相比单头注意力的优势和上一问一样，进一步增强网络的容量和表达能力。你可以类比CV中的不同的channel（不同卷积核）会关注不同的信息，事实上不同的头也会关注不同的信息。假设我们有一个句子""the cat, which is black, sat on the mat""。在处理""sat""这个词时，一个头（主语头）可能会更关注""cat""，因为""cat""是""sat""的主语；另一个头（宾语头）可能会更关注""on the mat""，因为这是""sat""的宾语；还有一个头（修饰头）可能会关注""which is black""，因为这是对""cat""的修饰。当然，这只是为了方便你理解，事实上就和卷积核一样，不同头关注的内容是很抽象的。你当然可以就用一个头同时做这个事，但是还是这个道理，我们的目的就是通过增加参数量来增强网络的容量从而提升网络表达能力。经过多头之后，我们还需要att_out线性层来做线性变换，以自动决定（通过训练）对每个头的输出赋予多大的权重，从而在最终的输出中强调一些头的信息，而忽视其他头的信息。这是一种自适应的、数据驱动的方式来组合不同头的信息。9.为什么Bert中要用WordPiece/BPE这样的subword Token？避免OOV（Out Of Vocabulary），也就是词汇表外的词。在NLP中，通常会预先构建一个词汇表，包含所有模型能够识别的词。然而，总会有一些词没有出现在预先构建的词汇表中，这些词就是 OOV。传统的处理方式往往是将这些 OOV 映射到一个特殊的符号，如 <UNK>，但这种方式无法充分利用 OOV 中的信息。例如，对于词汇表中没有的词 ""unhappiness""，如果直接映射为 <UNK>，则模型就无法理解它的含义。WordPiece/Byte Pair Encoding (BPE) 等基于子词的分词方法提供了一种解决 OOV 问题的方式。现在更多的语言大模型选择基于BPE的方式，只不过BERT时代更多还是WordPiece。BPE 通过将词分解为更小的单元（子词或字符），可以有效地处理词汇表外的词。对于上面的 ""unhappiness"" 例子，即使 ""unhappiness"" 本身不在词汇表中，但是它可以被分解为 ""un""、""happiness"" 等子词，而这些子词可能在词汇表中。这样，模型就可以通过这些子词来理解 ""unhappiness"" 的含义。另一方面就是，BPE本身的语义粒度也很合适，一个token不会太大，也不会小到损失连接信息（如一个字母）。10.Bert中为什么要在开头加个[CLS]?BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。为什么选它呢，因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层，每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，得到的是attention后所有词的加权平均，相比其他正常词，可以更好的表征句子语义。当然，也可以通过对最后一层所有词的embedding做pooling去表征句子语义。11.不用[CLS]的语义输出，有其他方式可以代替吗？对BERT的所有输出词向量（忽略[CLS]和[SEP]）应用MaxPooling和AvgPooling，然后将得到的两个向量拼接起来，作为整个序列的表示。这样做的话可以同时保留序列中最显著的特征（通过MaxPooling）和整体的，均衡的特征（通过AvgPooling）。12.Bert中有哪些地方用到了mask?预训练任务Masked Language Model (MLM)、self-attention的计算、下游任务的decoder13.预训练阶段的mask有什么用？主要的思想是，把输入的其中一部分词汇随机掩盖，模型的目标是预测这些掩盖词汇。这种训练方式使得每个位置的BERT都能学习到其上下文的信息。14.attention中的mask有什么用？这是nlp任务很重要的问题，就是不同样本的seq_len不一样。但是由于输出的seq_len需要一致，所以需要通过补padding来对齐。而在attention中我们不希望一个token去注意到这些padding的部分，因为实际场景下它们是不存在的，所以attention中的mask就是来处理掉这些无效的信息的。具体来说就是在softmax前每个都设为-inf（或者实际的场景一个很小的数就可以），然后过完softmax后""padding""部分的权重就会接近于零，query token就不会分配注意力权重了。15.decoder中的mask有什么用？遮盖掉sequence中当前位置之后信息，以防止模型利用未来信息，也就是信息泄露。mask掉后模型的注意力只会集中在此前的序列上。16.Bert中self attention 计算复杂度如何？ ，因为输入的序列的每一个token都要对这个序列上的所有token去求一个attention score。17.有什么技术降低复杂度提升输入长度的？比如Sparse Attention，放弃对全文的关注，只关心局部的语义组合，相当于self-attention上又加了一些mask，这样的话就可以降低复杂度，而且下游任务的语义关联性的体现往往是局部/稀疏的。18.Bert是如何处理传统方法难以搞定的溢出词表词(oov)的语义学习的？拆子词19.中文是如何处理溢出词表词(oov)的语义学习的？中文是单字级别，不会有词级别的oov20.为什么以前char level/subword level的NLP模型表现一般都比较差，但是到了bert这里就比较好？还是归功于Transformers，因为对于字符级别（char-level）或者子词级别（subword-level）的NLP模型，挑战在于需要模型能够理解字符或者子词组合起来形成词语和句子的语义，这对模型的能力有很高的要求。然而，以前NLP模型没办法做到很深，两层lstm基本就到极限了，非线性成长路线过分陡峭，所以增加网络容量的时候，降低了泛化能力。Bert降低了输入的复杂度，提升了模型的复杂度。模型多层产生的非线性增长平滑，可以加大网络容量，同时增强容量和泛化能力。解释一下：——什么叫非线性成长路线过分陡峭？如果我们将模型的深度看作是 x 轴，模型的复杂度或训练难度看作是 y 轴，那么随着深度的增加，y 值的增长可能会变得非常快。——BERT为什么降低了输入复杂度？WordPiece这种subword的做法不至于像char level那样基本完全抛弃了自身的语义信息（因为切的太细就会太复杂），也不至于像word level那样，因此可以减小词汇表大小。当然也避免了OOV的问题。——BERT为什么提升了模型复杂度？Transformers可以把网络做深，本身内部网络容量也很够。21.Bert为什么要使用warmup的学习率trick主要是考虑到训练的初始阶段params更新比较大，可能会使模型陷入local minima或者overfitting。warmup就是把lr从一个较小的值线性增大到预设，以减缓参数震荡，让训练变得比较smooth，当模型参数量上来后这种策略变得更重要了。22.为什么说GPT是单向的Bert是双向的？这也是decoder-only和encoder-only的区别。decoder-only架构的生成模型在输出的时候只能看到当前位置前的tokens，也就是屏蔽了序列后面的位置，以适配NTP任务。encoder-only架构的编码模型在输出的时候可以利用前后位置的tokens，以适配MLM任务。具体的做法是self-attention加不加casual mask，也就是遮不遮住序列后面的内容。23.Bert如何处理一词多义？一词多义指的是在不同句子中token有不同的含义。这正是self-attention解决的，搭配上MLM的任务，就可以让每个token会注意到上下文的其他token来得到自己的embedding。24.Bert中的transformer和原生的transformer有什么区别？原生的transformer是最经典的Sinusoidal绝对位置编码。而BERT中变成了可以学习的参数，也就是可学习位置编码。变得可学了的话，只要模型学习能力强，数据量够，确实不会差。可以类比卷积核从手工变成了模型自己学。更多的面经题和答案会持续更新，公主号【瓦力算法学研所】，欢迎来玩参考搬砖工程师：算法细节Transformer中的归一化(五)：Layer Norm的原理和实现 & 为什么Transformer要用LayerNorm - 知乎 (zhihu.com)大模型面试八股 - 知乎 (zhihu.com)"
653438239,6504722cb29a9a52e7a25005c573b267,包包大人,https://api.zhihu.com/people/6504722cb29a9a52e7a25005c573b267,people,,大模型来自面试的一些体会和分享,article,https://api.zhihu.com/articles/653438239,总结几个关键结论和要点放在前面： 1.有<em>大模型</em>方向论文的硕博很少，大部分论文还是在模型架构排列组合，屎上雕花。 2.有<em>大模型</em>训练经验简历的很少，有65B以上<em>大模型</em>全量训练的更少，有预训练的经验就像捡到了宝,498,39,936,1693447172,1693447172,"最近开始集中校招提前批和人才计划面试，偶尔也穿插着社招的面试，根据目前的简历和面试情况，跟大家汇报一下一线大模型技术方向的人才风向标。总结几个关键结论和要点放在前面：1.有大模型方向论文的硕博很少，大部分论文还是在模型架构排列组合，屎上雕花。2.有大模型训练经验简历的很少，有65B以上大模型全量训练的更少，有预训练的经验就像捡到了宝。3.模型评测和强化学习方面经验很稀缺。4.大模型选型上一定要紧跟行业主流认知。5.成功落地的场景稀少，目前都在探索阶段。6.大模型需求依然很旺盛，优秀简历很抢手7.大模型八股文的频率高吗？下面就这几个点展开说一下：1.有大模型方向论文的硕博很少，大部分论文还是在模型架构排列组合，屎上雕花。这个无可厚非，每一次技术革新都意味代价，大家搞论文也不一定是为了影响力这么远大的目标，很多时候是为了混口饭吃。在本来就价值不大的领域方向上，针对只会有1%发生频率的特定问题，进行了过度网络结构优化，最后取得几个点的提升，这类典型的比较水的工作太多了，现在已经无法吸引面试官眼球了。首先这部分工作有两个问题，第一是真的没有太大的作用了，可能还比不过chagpt zeroshot直接来解决这些的水平，另外，往往有更简单的方法其实明明也可以做的很好，但可惜就是想做的胡哨。在大模型时代，大力出奇迹和朴素简单有用都是有价值的，唯一没太大用处的就是稀奇古怪的方法组合起来，解决了一个不太经常出现的问题。这时候如果你有一篇大模型相关的录用的研究，会让面试官眼前一亮。2.有大模型训练经验简历的很少，有65B以上大模型全量训练的更少，预训练的经验就像捡到了宝。这个更无可厚非，毕竟整个行业大家都买不到卡了，大部分人手中的卡可能还是32GB的V100，也还没有多少张。大部分人的简历还是围绕大模型做一些边角工作，或者注定天花板不是很高的方法，比如lora，ptuning等。7B全量微调的起步的8张40GB的A100，这个储备也不是那么好弄了现在。65B全量微调的起步得上百张卡了。预训练更别提了，简单算下，8张A100 每天可以过个位数GB数据，假设要跑GB数据的话，几个月就没了。另外做大模型训练的更像是一个团队的工程活，每个人负责拧一个螺丝。有完整的从0起步的经验也是各家的香饽饽，简历已经在猎头那一端被抢爆了。3.模型评测和强化学习方面经验很稀缺。模型评测和强化学习算是大模型目前的黑科技了，谁能做的又快又准又好，就能和同行显著拉开差距。强化学习要求的训练资源也不低，经典的ppo方法直接加载4个模型，两个训练两个不训练，使得7B大小的模型，起步要求变成了8张80GB的A100，叠加大家卡短缺的问题，搞得人就更少了。另外，模型评测和强化学习本来就有很大的坑，属于很不好拿收益的两个方向。之前写过两篇文章大模型评测，也太难了吧和大模型RLHF的trick，讲过这个问题。评测：怎么评价一个模型的好坏，不仅困难，而且十分重要，绝对是一个核心的科技，这现在也是除了oepnai各家没怎么搞定的一个问题。这个问题很关键，是因为基座大模型的训练耗时耗力，如果不能想出很好的提早检验方式的话，做实验的速度会慢特别多，所有的时间成本都可以折合成算力上。所以你做实验慢了，相当于比别人少了GPU，足够触目惊心吧。强化学习：如果你动手跑几次ppo的过程就发现了，大模型的强化学习非常难以训练，难以训练不仅仅指的是费卡，还是指的非常容易训崩。第一，费卡。假设你训llama 7b，SFT 和 RM 都用7B的模型，那么显存耗费 = 2*7B(TRIAN MODE) + *7B(EVAL MODE), 分别对应 policy model / critic model，还有ref model/reward model本来你能用几张40GB A100的卡+deepspeed 做7b的全参数微调，强化学习就得升级到80GB的A100了，勉勉强强能跑到7B。想跑更大的就得充钱了。第二，容易崩。LLM训着训着就不听你话了，要么变成停不下来的复读机，输出到后面没有逻辑直到maxlen，要么变成哑巴，直接一个eosid躺平。RLHF中的问题其实在RL游戏训练里面很常见了，如果环境和参数设置不好的话，agent很容走极端，在 一头撞死or循环鬼畜之间反复横跳。原始的ppo就是很难训，对SFT基模型和RM的训练数据以及采样prompt的数据要求很高，参数设置要求也很高。自从openai带了一波RLHF的节奏后，大家都觉得强化学习在对齐方面的无敌功力，但自己跑似乎又不是那么回事，这玩意也太有讲究了吧。更多的魔鬼在细节了，openai像拿了一个比赛的冠军，告诉你了成功的solution，结果没告诉你各个步骤的重要性和关键设置，更没有告诉你失败和无效的经验。这两块的有用的经验太稀缺了，可能整个行业的面试官也没有很好的认知和判断，碰上有这样经验的人就当请教了。最差的情况就是明明搞一些困难的问题也没拿到什么结果，还一本正经地写上了取得了一定结果，这样会拉高别人的预期，再狠狠得摔在了地上。4.大模型选型上一定要紧跟行业主流认知有些同学弄了一些偏门模型的经验，然后信誓旦旦的给了一些结论，显得十分不靠谱的。主流认知就是llama系列微调潜力最好，中文zeroshot推理chagtlm/llama变种等都可以，微调chatglm其实效果一般。这些原因我之前也讲过了，出门右转这篇文章大模型微调样本构造的trick。但是还有一些简历还是在选型上没有充分紧跟开源社区，有一些跟主流认知的偏差的地方，这样就没办法在一个层次上互相交流沟通，显得面试在鸡对鸭讲。5.成功落地的场景稀少，目前都在探索阶段从现在的简历来看，整个行业的进展还是比较慢的，尤其是在落地领域。大多数的大模型项目还是抱着试试看或者玩玩的心态，另外或者是demo的状态，或多或少总会遇到各种坑无法落地。比如幻觉问题很严重，风控问题解决不了，资源消耗覆盖不住，推断延迟太大影响体验等等。这里面每一个问题都可以展开讲一堆，是实际落地问题里面必然会碰到的问题。整个行业的简历很少碰到有人完整的从0起步，一点点解决了这些棘手的问题并正了八经落地的，当然这个要求可能太高了，也就离创业成功不远了。6.大模型需求依然很旺盛，优秀简历很抢手目前除了一些知名的垂直的初创公司做大模型基建基座外，还有很多优秀的AIGC应用公司在具体的应用方向上发力。另外，大公司在这方向上也来势汹汹，手快的组很快申请到了计算资源，例如很多以前跟搜索，交互，NLP相关的部门，都在摩拳擦掌准备分一杯羹梗。大公司的项目有更明显的玩票特点，相对更不靠谱一点，这也和大公司本身有比较强的拖底效应有关，创业公司搞不出来就干黄了，大公司搞不出来就当练手了，至少还摸了一把，蹭了新鲜。目前几乎每个大厂都有10+个以上的达模型jd，候选人投递的时候一定要仔细考察，分辨是玩票性质还是做的比较扎实。7.大模型八股文的频率高吗？如果简历比较扎实的话，有相对较为丰富的大模型经验，其实基本上是不会问八股文的，因为一个小时的时间，交流一点一线搞大模型的经验和心得都来不及，根本没有时间问八股文。如果简历比较虚，或者没有相关经验，但是过往经历ok，准备往这个方向转的话，还是会着重考察潜力和基础的。潜力就是对应一些考察思维的题目，基础对应着一些八股文。"
663124412,eea06de6def5d4a54bffc49d654b8bb0,嘿我,https://api.zhihu.com/people/eea06de6def5d4a54bffc49d654b8bb0,people,喜欢琢磨新技术、新理念,大模型（Large Model）常识综述（一）,article,https://api.zhihu.com/articles/663124412,本文框架 简单来说，<em>大模型</em>，泛指参数很多的机器学习模型（具体请参考：<em>大模型</em>多大算大），根据场景不同，大部分<em>大模型</em>公司把大模型分为大语言模型、计算机视觉（包含图像和视频）、音频、多模态大模型四大类,203,9,582,1698160378,1704259934,大模型定义文心一言Prompt：大模型的定义是什么，可以从多个角度来进行阐述回答：大模型是指包含超大规模参数（通常在十亿个以上）的神经网络模型，具有以下特征：1. 巨大的规模：大模型包含数十亿个参数，模型大小可以达到数百GB甚至更大。这种巨大的模型规模为其提供了强大的表达能力和学习能力。2. 多任务学习：大模型通常会一起学习多种不同的NLP任务，如机器翻译、文本摘要、问答系统等。这可以使模型学习到更广泛和泛化的语言理解能力。3. 强大的计算资源：训练大模型通常需要数百甚至上千个GPU，以及大量的时间，通常在几周到几个月。这可以加速训练过程而保留大模型的能力。4. 丰富的数据：大模型需要大量的数据来进行训练，只有大量的数据才能发挥大模型的参数规模优势。大模型在自然语言处理领域得到广泛应用，并正在彻底改变NLP任务的状态，催生出更强大、更智能的语言技术。大模型是AI发展的重要方向之一。同时，大模型也具有在各种自然语言处理任务中表现出色的能力，例如文本分类、情感分析、摘要生成、翻译等。大模型可以用于自动写作、聊天机器人、虚拟助手、语音助手、自动翻译等多个应用领域。本文框架简单来说，大模型，泛指参数很多的机器学习模型（具体请参考：大模型多大算大），根据场景不同，大部分大模型公司把大模型分为大语言模型、计算机视觉（包含图像和视频）、音频、多模态大模型四大类。本文目前重点整理了大语言模型和多模态大模型相关内容，计算机视觉和音频后续补上。大模型大语言模型大语言模型，顾名思义，专门用来处理自然语言的模型，有的公司分类标签中，直接把大语言模型等同于NLP（Natural Language Model）。我分成了国内和国外进行了汇总，国内很多大语言模型，乍一看太牛逼了，仔细一看还不如乍一看，明显就是蹭热点，你压根也没那个算力（禁售了，根本买不到也买不起）。大语言模型计算机视觉待补...多模态多模态很容易理解，就是文、图、视频、音频之间相互转换。下图中仅仅列出一些有代表性的多模态大模型，例如经典双塔模型CLIP（BERT+ViT），ALBEF—将BERT的6层网络拿出来做文本和图像的编码融合。一篇不错的多模态科普文。多模态大模型大模型多大算大有一篇文章专门讨论过这个问题：在过去几年中，关于“大模型”的定义已经不断发生变化。在早期，数百万或几千万个参数的模型就被认为是“大模型”。但现在，由于模型的参数量和计算复杂度都在不断增加，因此数亿甚至数十亿个参数的模型也变得更加常见。因此，目前认为大模型的标准也在不断变化。一般来说，模型的大小可以根据其参数数量来衡量。对于自然语言处理（NLP）任务，通常认为参数数量超过1亿的模型是大模型。对于计算机视觉（CV）任务，通常认为参数数量超过1亿到10亿之间的模型是大模型。但是这个也不一定准确。这篇文章也是很好的科普文，关键词是涌现能力和CoT。简单来说，涌现能力与模型大小有直接关系，All Scale Matters！CoT，Chain of Thought，思维链，模型越大，对人的引导需求越小。大模型算力要求今年9月份发的一篇综述，《A Survey of Large Language Models》，arXiv地址奉上。其中有我一直在收集的大模型算力要求，咱就是说，到底什么样的家庭，才能玩得起大模型。（当然，我指的是可以从预训练开始的，如果仅仅冻住Pre-Trained Model，做一些下游任务，最简单的比如企业知识库，有些家底的都可以吧，尤其是Llama开源后）。大模型算力要求Google当然是自家TPU，华为自家昇腾 910，百度布局早用但用的是V100表示不理解（A100、H100对华禁售，后来黄教主专门为中国人搞了个V100，低配版A100，速度降低了），剩下的大佬基本都是A100。大家对A100、V100价格不知道有没有概念，A100普通人当然买不到，一张V100现在吵到了13万软妹币（在美国大概也就1万软妹币）。市场分析AI项目数量激增：GitHub上的AI开源项目数量达到了91万，同比增长了264%。根据Replit的数据，二季度AI项目环比增速达到80%，同比增长了34倍。OpenAI在大模型领域仍然占据主导地位：超过95%的应用项目基于OpenAI的模型构建，同时开源项目数量也在大幅增长。2023年是生成式人工智能投融资创纪录的一年：根据CBInsights的数据，截至2023年第二季度，生成式人工智能的投融资较去年全年增长了4.6倍。生成式人工智能应用层融资仅占三成，目前约七成资金投向了包括大模型开发在内的AI基础设施层。在应用层的融资中，AI数字代理获得最多融资，其次是文本、图像、代码和音频生成工具。生成式人工智能应用可以分为工具型应用、通用软件、行业软件和智能硬件四大类。从产品形态上看，它们将沿着AIGC（内容生成）、Copilot（智能助手）、Insight（知识洞察）和Agent（数字代理）等重要方向演进。工具型应用包括聊天机器人、搜索引擎、文本工具、AI作画和代码工具等，主要面向消费者市场。这些产品的同质化程度较高，底层模型的能力对产品的受欢迎程度至关重要。目前行业正处于第一轮洗牌期，竞争优势的建立来自于差异化的产品定位和持续训练更强大的底层模型和算法。通用软件包括办公软件、企业服务、IT运维、软件开发、网络安全、数据智能等领域。各行业的头部厂商已经推出了标杆产品，最常见的产品形态是AI智能助理（Coplilot）。目前各个赛道竞争格局相对稳定，未来竞争的关键在于AI与场景/工作流的深度融合。通用软件领域的头部厂商预计将在四季度进入商业化落地的关键阶段。行业软件涉及金融、医疗、教育、工业、游戏、法律等多个行业。在游戏、法律、教育和电商等C端场景中，生成式人工智能有较多结合点；而在医疗、金融和工业等B端场景中，生成式人工智能产品的成熟度仍较低。AI助手（Coplilot）在这些领域也得到广泛应用。未来，在金融、医疗和工业等领域，最具前景的应用将来自于数据分析和知识洞察工具.此外，各行业头部厂商也开始自建垂类大模型，如彭博社的金融大模型Bloomberg和Meta蛋白质大模型ESMFold。目前垂类大模型在其专业领域的性能普遍超过通用大模型。智能硬件包括智能汽车、机器人和智能终端等。目前生产式人工智能与智能硬件的结合主要分为语音助手和数字代理AI Agent两大类别。语音助手广泛应用于智能座舱、智能音箱和家用机器人等各类智能终端，大模型和生成式人工智能技术提升了感知和生成能力，改善了用户体验。然而，总体而言产品门槛相对较低。数字代理AI Agent在感知与决策能力方面仍存在挑战，未来应用空间的打开关键在于计算机视觉和具身智能等底层技术的突破。目前，生成式人工智能产品的商业模式主要包括功能订阅、按量付费和产品销售等方式。在C端应用中，功能订阅和按量付费是主要的商业化方式，已经相对成熟。而在B端应用中，除了功能订阅和产品销售外，还包括提供解决方案等。第一批成功的生成式人工智能应用包括Jasper AI、Notion AI和MidJourney等，它们已经实现了商业化。举个例子，Office 365 Copilot的定价为每个用户每月30美元，相较于Office主线产品的15-30美元的定价，提升了2倍以上。Salesforce的生成式AI模块服务GPT和销售GPT的单用户每月付费为50美元。此外，Palantir和Palo Alto Networks的AI产品已经在实际应用中取得了成果，并带来了可观的收入贡献。预计在四季度，生成式人工智能应用将正式进入全面商业化落地阶段。如果从ToB、ToG、ToC角度看，从大模型落地难度来看，ToB场景最难，ToG场景次之，ToC场景最容易：ToB场景：由于企业之间的业务差异较大，需要定制化的模型和算法，因此落地难度较大。此外，在企业内部，不同的部门之间也需要进行协调和沟通，以便更好地利用大模型提供的数据和服务。ToG场景：政府在制定政策时需要依赖大量的数据和信息，因此大模型在政府领域的应用较为广泛。由于政府拥有较为完善的信息化基础设施，因此大模型的落地难度相对较低。ToC场景：消费者对产品和服务的需求较为多样化，因此大模型在消费市场中的应用难度较大。此外，由于消费市场的竞争较为激烈，企业需要不断地优化产品和服务，以满足消费者的需求，这也需要大模型提供更加精准的数据分析和预测。商业化场景通过一些公开的信息和圈内朋友了解，国内商业化整体做的不好，产品不好，用户付费也不好，目前主要用户还是学生和研究机构。下面重点看看商业化场景本身的多样性和可能行，至于最后一公里——用户付费，要再等一些时间了吧。虽然大模型本身能力范围相同（LLM和Multi-Modalty），但商业模式却不太相同。百度、腾讯和阿里重点还是卖算力，华为一贯作风既要又要还要，科大讯飞卖应用API为主（收费方式非常清晰，付费情况如何不清楚，至少产品策略和GoToMarket路径，是所有类似公司最明白的）。百川智能宣传和各排行耪来看，同等尺寸大模型，性能比较靠前，但商业化没看懂，没有价格，没有文档。360非常明显，主要是ToB。总的来说，虽然商业模式不同，但最终ToC场景基本相同，ToB垂直是未来主要战场。商业化场景社区社区这一块，百度拿捏的死死的。虽然千帆和飞浆有一点点重叠，但各自定位很清楚，千帆大模型产品面向企业，飞浆主要技术交流面向研究者。社区内容很丰富，跟百度云无缝连接。阿里云一塌糊涂，ModelScope是达摩院做的，跟阿里云的连接做的比较差，社区也不活跃，很多提交的应用都没法运行（总共有几百个大模型应用，包含不少未发布、运行错误、已休眠等低质量应用）。国内还有一个比较低调但我觉得很不错的社区，就是OpenXLab，运营方是上海人工智能实验室，里面有一些应用真的不错，比如下面这款动画生成器，很好玩。OpenXLab当然，最牛逼当属Hugging Face——机器学习界的GitHub，模型、数据集、应用、文档和服务，都安排妥妥滴，模型37万+，应用13万+，是所有社区中最牛逼存在。Hugging FaceLLM排行榜排行榜我就直接上结果了，自己看吧，有的还客观些，有点明显扯淡。可以从评估LLM角度，去理解大语言模型能干什么，不能干什么，有哪些提升空间，学生找课题方向，公司找商业化落地场景。LLM排行榜AlpacaEval大模型排行榜CLiB中文大模型能力评测榜单C-Eval大语言模型排行榜SuperCLUE中文通用大模型综合性基准间断添加内容，但内容太多，知乎加载慢，所以分开了。会持续更新哒！嘿我：大模型（Large Model）常识综述（二）
618761264,11b7e0d297f495d258d6fc89d0c7f716,智东西,https://api.zhihu.com/people/11b7e0d297f495d258d6fc89d0c7f716,people,,GPT-4大模型硬核解读！看完成半个专家,article,https://api.zhihu.com/articles/618761264,通过奖励策略对<em>模型</em>进行微调并持续迭代。 ▲奖励模型的过拟合导致<em>模型</em>性能下降（来源：OpenAI） 因为<em>模型</em>仅仅从狭窄分布的训练数据中学习，所以GPT-4中奖励<em>模型</em>只是人类偏好的部分表征（管中窥豹）,1925,78,13108,1680348243,1680348245,作者 | 陈巍博士团队引言：本文将以2万字介绍GPT-4的核心技术要点、技术架构、训练流程、算力、局限与产业未来。作者陈巍博士为AI/存算一体专家，曾担任华为系自然语言处理企业的首席科学家。（深度技术科普与解读文章，不涉及过多技术名词或公式）之前我们一直说自然语言处理是人工智能王冠上最大的那颗珍珠，但如今用世俗的珍珠或者王冠形容已经不合适了。多模态大模型带给人类世界的震撼，就如人工智能企业Hugging Face（因提供开源预训练模型库而闻名）的联合创始人Thomas Wolf所述：“在过去的几年里，好的多模态模型一直是许多大型技术实验室的圣杯。“其中多模态指的是融合文本、图像、视频或音频等多种模态作为输入或输出。作为“圣杯”的代表之一，GPT-4这个标签代表第4代生成式预训练变换模型（Generative Pre-trained Transformer 4），是OpenAI在2023年3月14日公开的一种多模态模型，是对前几个月发布的ChatGPT的多模态升级。GPT-4模型可对图文多模态输入生成应答文字，以及对视觉元素的分类、分析和隐含语义提取，并表现出优秀的应答能力。业内文章大多从侧面宣传GPT-4的优秀，却很少触及其核心技术内核。▲OpenAI的相关信息本文将通过OpenAI和其他AI巨头已发表的大语言模型或多模态论文来详细阐述和分析与GPT-4相关核心技术要点、技术架构、训练流程、算力、局限与产业未来，告诉大家为何我们的下一代会从“内卷”过渡到“人机互卷”。01.GPT-4核心技术有哪些？1.1理论基础——多模态涌现能力讲到大语言模型的优势，一般首先要提到这类模型的涌现能力和思维链。这两者是大语言模型不断接近人类的关键特征。我们之所以认为GPT-4会是具有里程碑意义的一代，正是因为多模态的GPT-4会从视觉角度和视觉-文字语义融合方面涌现出更多的能力。2022-2023年，我们可以认为AI是第一次睁开双眼理解这个世界。在大型语言模型（LLM）中，涌现能力（Emergent Abilities）是指模型具有从原始训练数据中自动学习并发现新的、更高层次的特征和模式的能力。就中文释义而言，涌现能力也指大语言模型涌现出来的新能力。这有点类似于去超市遇到买二赠一，赠品的质量居然还出乎意料。与大语言模型（LLM）相比，多模态大语言模型（Multi-modal Large Language Model，MLLM）可实现更好的常识推理性能，跨模态迁移更有利于知识获取，产生更多新的能力，加速了能力的涌现。这些独立模态或跨模态新特征、能力或模式通常不是通过目的明确的编程或训练获得的，而是模型在大量多模态数据中自然而然的学习到的。▲缩放定律（参数增加后精度损失连续减少）V.S. 涌现能力（1010-1011参数后新能力的涌现）（来源：OpenAI）在语言模型发展的早期，通过在更多数据上训练更大的模型，可获得近似连续的精确度提升。（可称为缩放定律/Scaling Laws）到了2015年左右，随着深度学习技术的发展和语料库的增大，模型达到一定的临界规模后，NLP开发者们发现，大语言模型（包括GPT-3、GLaM、LaMDA和Megatron-Turing NLG等）开始表现出一些开发者最开始未能预测的、更复杂的能力和特性，这些新能力和新特性被认为是涌现能力的体现。▲当模型尺寸增加到一定大小后，新能力涌现（来源：Google/Deepmind）我们在研究GPT-4时，发现GPT-4具备了OpenAI在预训练时和发表的技术报告中并未明确的能力。这些能力都属于涌现出来的能力。涌现能力是基于深度学习模型的分层结构和权重学习机制实现的。涌现出来的能力可以是基于文本的，也可以是多模态的。我们可以将GPT-4这类大模型的训练视为解方程，每一层神经元（可视为变量组合）的输出都作为下一层神经元的输入，并且模型的每个权重（Weight）都通过强化学习算法进行学习和更新。这种分层的结构和权重学习机制使得深度学习模型能够自动的学习到从原始数据中提取隐含的特征和模式，从而实现涌现能力。当大语言模型被训练时，通过学习大量的多模态训练数据，并且根据数据中的统计规律和模式自适应的调整其内部参数和结构，从而表现出一些新的能力和特性。这类似于咱们常说的量变引发质变。涌现能力是大语言模型的重要特性，也是现在火爆的大模型各种能力的理论基础。涌现能力使得GPT-4能够在无需人工干预的情况下，从原始的多模态数据中自动学习到复杂的特征和模式，从而实现更准确和更高效的预测和决策。涌现能力的另一个重要表现是模型的泛化能力。在没有专门训练过的情况，GPT-4也可以泛化到新的、未知的多模态数据样本上。这种泛化能力取决于模型的结构和训练过程，以及数据的数量和多样性。如果模型具有足够的复杂性和泛化能力，就可以从原始数据中发现新的、未知的特征和模式。当然，GPT-4涌现出的新能力可能仍有局限性，例如：模型可能产生错误的回答，对某些问题缺乏理解，容易受到输入干扰等。目前认为GPT-4的幻觉与其涌现能力具有相关性。1.2核心优势——多模态思维链思维链（Chain of Thought）可视为大语言模型涌现出来的核心能力之一。之所以现在各类GPT研究火爆，也与模型训练出的思维链可进入实用有密切关系。思维链形成机制可以解释为模型通过学习大量的语言数据来构建一个关于语言结构和意义的内在表示，通过一系列中间自然语言推理步骤来完成最终输出。思维链是ChatGPT和GPT-4能让大众感觉到语言模型“像人”的关键特性。虽然GPT-4这些模型并非具备真正的意识或思考能力，但用类似于人的推理方式的思维链来提示语言模型，极大的提高了GPT-4在推理任务上的表现，打破了精调（Fine-tune）的平坦曲线。具备了多模态思维链能力的GPT-4模型具有一定逻辑分析能力，已经不是传统意义上的词汇概率逼近模型。当然思维链的训练可能并不容易。尽管现在有大量团队进入大语言模型训练领域，但若干年内能找到训练诀窍并完成思维链训练的团队可能不多。对创企来说，完成思维链的训练，才算真正拿到了这波大模型AI竞技的入场券。▲思维链提示的示例（来源：Google）通过多模态思维链技术，GPT-4将一个多步骤的问题（例如图表推理）分解为可以单独解决的中间步骤。在解决多步骤推理问题时，模型生成的思维链会模仿人类思维过程。这意味着额外的计算资源被分配给需要更多推理步骤的问题，可以进一步增强GPT-4的表达和推理能力。▲当模型尺度增加到一定规模，思维链能力出现（来源：Google）一般认为模型的思维推理能力与模型参数大小有正相关趋势，一般是突破一个临界规模（大概62B，B代表10亿），模型才能通过思维链提示的训练获得相应的能力。如果在6B以下，那很可能还只是GPT-2级别的初级模型。另外也有研究表明，在语言训练集中加入编程语言（例如Python编程代码）可提升模型逻辑推理能力。具有思维链推理能力的GPT-4模型可用于简单数学问题、符号操作和常识推理等任务。▲多模态思维链框架（来源：微软）GPT-4的多模态思维链是通过观察大量的多模态数据来学习内在表示，然后利用这个表示来生成连续的语言输出的机制。这个过程是通过模型的训练、内在表示的构建和语言输出的生成三个步骤来实现的。1.3编程范式——多模态提示工程多模态大模型（如GPT-4）的提示工程（Prompt Engineering）是指根据特定的目标和语境设计出一系列问题或任务，以便使用大模型生成有关主题或主题领域的连贯和有意义的文本。提示工程的目标是通过精心设计提示以从模型中引出所需的响应，来提高生成文本的质量和相关性。提示工程与思维链的产生密不可分，也是目前自然语言编程的理论基础。▲语言模型的4种研究范式（来源：卡内基梅隆大学）大概在2017-2019年间，语言模型的研究重心逐渐从传统特定领域的有监督学习模式（基于非神经网络或神经网络）转移到预训练模型上。在那时，基于预训练语言模型的研究范式通常是“预训练+精调”（Pre-train+Fine-tune），即在精调阶段，根据下游任务对预训练模型进行微调，以获得更好效果。但是由于模型越来越大，以及预训练阶段和下游任务之间的差距可能很大，对各个细分领域Fine-tune的计算资源要求、训练数据需求和时间成本也在快速上涨。大量爆发的下游任务也使得175B这个级别模型预训练和精调变得异常复杂。在这种背景下，随着GPT-3的发布，提示工程成为了预训练模型的新方向。形象的说，提示有点类似于老师在学生回答问题时指点回答方向。▲提示方法（来源：卡内基梅隆大学）GPT-4/GPT-3模型中提示的新范式可归纳为“预训练+提示+预测”（Pre-train+Prompt+Predict）。在这一范式中，各种下游任务被调整为类似预训练任务的形式。通过选取合适的提示，使用者可以控制模型预测输出，从而一个完全预训练模型可以被用来解决多样的下游任务。这里举一个填充提示的简单例子。（上图）我们从输入x（比如电影评论）开始，然后输出期望值y。其中一个任务是使用提示函数重新模板化此输入，其输出表示为x'。此时语言模型的任务仅仅是预测z值（句子中的一个词）来代替占位符Z。然后对于Z被答案填充的提示，我们将其称为填充提示。通过这一提示方式，在对应细分场景下，语言模型将原来的问题的期望值y（一句话）简化为答案z（一个词）的计算，明显降低了应答的复杂度。▲提示工程使得GPT-3模型在训练样本较少时获得了更高精度（来源：OpenAI）而GPT-4则针对多模态数据集，设计了对应的提示。GPT-4的提示工程涉及几个步骤，包括选择合适的模型架构和参数、设计提示格式和结构、选择合适的任务和训练数据，以及使用选定的提示和数据微调模型。更多GPT-4的提示细节还需等待OpenAI发布。▲多模态提示示例（来源：微软）提示工程同时也提高了语言模型“可操纵性”，即模型根据用户要求更改其行为的能力。例如，用户可以命令GPT-4以不同的风格、语气或内容特征来回答。例如“你是一个唠叨的数据专家”或“你是一个言简意赅的数据专家”来开始提示，让模型解释一个数据科学概念。这里“唠叨”和“言简意赅”操纵了模型回答的语言量。1.4关键技术——人类反馈强化学习GPT-4/ChatGPT与GPT-3.5的主要区别在于，新加入了被称为RLHF（ReinforcementLearning from Human Feedback，人类反馈强化学习）的技术。这一训练范式增强了人类对模型输出结果意向（Intent）的调节，并且对结果进行了更具理解性的排序。OpenAI在其早期的学术报告中公开表示，与人类偏好保持一致，是许多领域人工智能研究和部署的核心组成部分。OpenAI希望通过RLHF技术，模型能倾向出高质量回答，确保模型输出对人类有益，进而保证模型的安全性。就笔者团队分析来看，RLHF也是保持多轮对话不偏离主题的关键保障。GPT-4/ChatGPT最初引入人类标记员的主要目的是加快训练速度和质量。尽管强化学习技术在很多领域有突出表现，但是仍然存在着许多不足，例如训练收敛速度慢，训练成本高等特点。特别是现实世界中，许多任务的探索成本或数据获取成本很高。如何加快训练效率，是如今强化学习任务待解决的重要问题之一。▲TAMER架构在强化学习中的应用这里以TAMER（Training an Agent Manually via Evaluative Reinforcement，评估式强化人工训练代理）框架为例。该框架将人类标记员引入到模型代理（Agents）的学习循环中，可以通过人类向代理提供奖励反馈（即指导Agents进行训练），从而快速达到训练任务目标。GPT-4的多模态奖励模型（RM）是小号的有监督精调模型（SFT），但在顶部添加了一个新的线性层来预测奖励。奖励模型的输入是原始输入加上SFT模型生成的输出。在具体实现上，人类标记员扮演对话的用户和人工智能助手，提供多模态对话样本，让模型生成一些回复，然后标记者会对回复选项打分排名，将更好的结果反馈回模型中。代理（Agents）同时从两种反馈模式中学习——人类强化和马尔可夫决策过程奖励作为一个整合的系统，通过奖励策略对模型进行微调并持续迭代。▲奖励模型的过拟合导致模型性能下降（来源：OpenAI）因为模型仅仅从狭窄分布的训练数据中学习，所以GPT-4中奖励模型只是人类偏好的部分表征（管中窥豹），过度的训练反而可能导致奖励模型过拟合（以偏见代替整体），并导致模型训练效果的下降。另一方面，模型的人类标注员可能也无法代表用户所在地区人群的总体偏好。1.5安全技术——基于规则的奖励模型安全是大模型商用的关键要素，OpenAI也投入了大量资源来提高GPT-4的安全性和一致性。包括引入领域专家进行对抗性测试和红队测试，模型辅助的安全流水线以及安全指标的改进。OpenAI引入的领域安全专家达到了50多人，覆盖AI一致性风险、网络安全、生物风险等领域。与ChatGPT一样，GPT-4也使用了强化学习和人类反馈（RLHF）来微调模型的行为，以产生更符合用户意图的响应。但当给定不安全的输入时，模型可能会生成不良内容，例如提供有关犯罪的建议。另外，模型也可能对安全输入变得过于谨慎，拒绝无害的请求。GPT-4的安全流水线包括两个主要部分：一组额外的安全相关RLHF训练提示，以及基于规则的奖励模型。基于规则的奖励模型（Rule-based Reward Model，RBRM）是一组zero-shot迷你GPT-4分类器，根据预定义的规则为特定动作或事件分配奖励。在这种模型中，奖励是根据事先定义的一组规则确定的，而不是从数据中学习得到的。这些分类器在RLHF微调期间为GPT-4策略模型提供额外的奖励信号，以正确的输出行为为目标进行训练，例如拒绝生成有害内容或不拒绝无害的请求。▲基于规则的奖励模型（来源：日本国立信息学研究所）很多早期的NLP模型和软件就是基于规则的（包括各种早期的智能音箱/”人工智障”），但这类模型在泛化场景下表现不佳，只能回答相对固定的问题，并不具备现在的大语言模型的涌现能力。GPT-4中使用RBRM的目的是充分借助其优势，即模型中使用的规则可以简单实用一些，建立成本低于常规奖励模型。例如，在象棋等游戏中，规则可能很简单。在更复杂的情况下，规则可能相对复杂，例如为实现特定目标或达到一定的性能水平授予奖励，但总体来说比构建奖励模型的训练数据集成本更低。规则奖励模型通常用于强化学习，其中代理被训练为采取最大化奖励信号的行动。在这种情况下，规则奖励模型基于代理是否遵循特定规则或实现特定目标，为代理分配奖励。规则奖励模型的优点允许更多地控制学习过程。通过事先指定规则，开发人员可以引导学习过程，使其专注于特定的行为或结果。▲基于规则的奖励模型在样本较少情况下表现出较好性能（来源：Meta AI）基于规则的奖励模型的主要特点如下：1、规则的可定义性：根据预先定义的规则来为模型的输出分配奖励。这些规则通常由领域专家或高质量的人类标注员制定，以确保奖励与任务目标和期望行为保持一致。2、规则的可解释性：奖励模型依赖于明确的规则，这些一般具有较高的可读性和可解释性。以方便开发人员解读和调试模型。3、规则的可调整性：通过修改或添加新的规则，可以相对容易地调整奖励函数，以适应不同的任务和环境或更复杂的规则。基于规则的奖励模型也存在一些局限性，包括：1、缺乏场景泛化能力：因为基于规则的奖励模型严重依赖于预先定义的规则，可能在未知或新的情况下泛化能力较弱，这可能导致模型在面对新的情况时出现幻觉现象或无法做出合适的应答。2、规则设计的复杂性：例如对于复杂任务，设计适当的规则有可能非常耗时。此外，如果规则过于复杂或内部自相矛盾，可能导致模型训练不出有效的策略。3、规则的学习效率有下降可能：由于模型需要在给定的规则集合中探索最佳策略，在规则设计不理想的情况下，基于规则的奖励模型可能导致较低的学习效率或过拟合。1.6优化技术——近端策略优化（PPO）算法GPT-4/ChatGPT中的近端策略优化（Proximal Policy Optimization，PPO）算法是一种高效的强化学习优化策略算法，由OpenAI的John Schulman等人于2017年提出。在GPT-4/ChatGPT里的使用应该算是新瓶装旧酒。PPO的前辈TRPO（Trust Region Policy Optimization）相对复杂，并且与包含噪声（例如Dropout）或参数共享（在策略和价值函数之间，或辅助任务）的架构不兼容。PPO算法试图解决上述问题，以及计算复杂性和难以调整的超参数。PPO通过简化优化问题并限制策略更新的幅度，实现了更高效、更稳定的学习过程，具有实现简单、能同时处理离散\连续动作空间问题、可大规模训练等优势。▲PPO算法与同类其他算法的比较（来源：OpenAI）PPO算法衍生于早期的策略梯度（Policy Gradient）算法，但通过一些技巧改进了其性能和稳定性，能够处理连续动作空间的问题。PPO在策略更新时限制新策略与旧策略之间的差异，从而确保策略改进的稳定性。这通过在目标函数中引入一个“代理”目标函数来实现，该代理目标函数限制了新策略和旧策略之间的KL散度。PPO算法的核心思想是在每次迭代中，通过一种称为近端策略优化（Proximal Policy Optimization）的方法来更新策略参数，以最大化预期收益。具体来说，PPO算法采用两个神经网络来表示模型的策略：一个执行动作（Actor），另一个处理奖励（Critic）。在每次迭代中，PPO算法会从环境中采样一批经验数据，并使用这些数据来更新策略参数和价值参数。更新的策略将被ε-clip到一个小区域，以防止可能具有不可恢复危害的巨大更新。换句话说，优化的步伐不能太大也不能过小。PPO算法的主要特点如下：1）裁剪的目标函数：PPO通过裁剪策略比率（新策略概率与旧策略概率之比）来限制更新幅度。这种裁剪保证了新策略在旧策略的附近，使得更新更加稳定。2）重要度采样：PPO利用重要度采样来估计策略梯度，从而可以重复使用之前的经验来更新策略。这使得PPO在数据效率上更具优势。3）多次更新：PPO算法在每次收集一批数据后，对策略进行多次更新。这可以提高算法的收敛速度和稳定性。4）简化的优化问题：相比于其他方法，如TRPO，PPO算法将优化问题简化为一阶优化问题，这大大减少了计算复杂性。1.7安全技术——多模态幻觉检测大型语言模型（Large Language Model，LLM）的幻觉（Hallucination）指的是模型生成的输出包含一些与输入不符合的信息，这些信息可能是错误的、无关的或者荒谬的。与人类直觉相反，随着模型变得更加以假乱真，幻觉会变得更加危险。GPT-4等模型的这种幻觉可能会出现在各种类型的任务中，比如文本生成、图文分析和问答系统等。由于大模型（包括GPT-4）本质上可以视为训练集（人类知识/语言）的有损压缩，因此在模型运行时无法完整复现或者应答原始知识，从而模型的幻觉来自于信息压缩的偏差。多模态幻觉的本质是这种有损压缩偏差的体现，也是通过数学逼近人类语言的必然代价。（类似于压缩后的图像边缘出现不正常的条纹）。▲大语言模型可视为知识/语言的有损压缩幻觉包括以下几类：1、含义相关性（Semantic Relatedness）的幻觉：模型生成的输出可能包含与输入语境无关或不相关的单词或短语，这些单词或短语通常是通过模型之前接触过的文本来学习的。2、语义扩张（Semantic Expansion）的幻觉：模型生成的输出可能包含与输入语境相关但是过于具体或者过于抽象的内容，这些内容也可能是通过模型之前接触过的文本来学习的。3、结构错误（Structural Errors）的幻觉：模型生成的输出可能不符合正确的语言表达或句子结构，这些错误可能是由于模型在生成时遗漏了某些信息，或者将不相关的信息结合在一起导致的。为了降低幻觉出现的概率，改善模型质量，Meta AI提出一种幻觉内容检测机制。通过检测生成内容中的幻觉令牌/单词，对生成内容的真实度进行评估，以减少模型幻觉出现的概率。从GPT-4的幻觉减少比率来看，猜测类似该技术的方法或已应用在GPT-4中。▲通过幻觉单词检测器减少幻觉（来源：Meta AI）幻觉是GPT-4等大型语言模型中一个重要的问题，通过不断的优化模型和改进训练方法，或增加多模态幻觉语义检测器，研究人员可以逐步提高模型的准确性和稳定性，从而更好地满足各种自然语言处理任务的需求。1.8模型信息——关于模型大小目前OpenAI还没有发布GPT-4模型大小和结构的具体信息。GPT-4的技术报告也没有透露这些技术细节，训练数据或训练方法也没有相关信息释放出来。大模型的商业化竞争正愈演愈烈。▲Bing反馈的GPT-4模型大小GPT-3是目前最大的知名语言模型之一，包含了1750亿（175B）个参数。在GPT-3发布之前，最大的语言模型是微软的Turing NLG模型，大小为17亿（1.7B）个参数。在GPT-3发布后不久，OpenAI团队就曾表示他们计划在未来几年内研发更大的模型。而随着技术和算法的不断发展，GPT-4模型似乎也应朝着更大的尺寸发展。另外，GPT-4的上下文窗口尺寸也较GPT-3.5和GPT-3增大了不少。2020年发布的GPT-3模型上下文窗口为2049个令牌。在GPT-3.5中，窗口增加到4096个令牌（约3页单行英文文本）。GPT-4有两种尺寸。其中一个（GPT-4-8K）的上下文窗口大小为8192个令牌，另一个（GPT-4-32K）可以处理多达32768个令牌，大约50页文本。有传言说GPT-4模型大概是GPT-3的100倍或1000倍。从训练的角度看，这么大的模型膨胀可能会消耗更多的训练资源和训练周期的过度延长。▲GPT-4与GPT-3.5的执行速度对比（来源：ARK投资）根据ARK的分析，GPT-4的执行时间大概是GPT-3.5的3.7倍。由此我们初步估算GPT-4的文本语言部分的大小大约是62B-650B之间。根据目前GPT模型性能的发展趋势，以及多模态技术的加持，预计GPT-4的模型参数大概为62B-1500B之间。02.GPT-4的原理是什么？GPT-4这一代，是严格意义上的多模态模型，可以支持图像和文字两类信息的同时输入。之前的ChatGPT多模态感知是实现通用人工智能的必要条件，无论是知识/能力获取还是与现实物理世界的交互。之前的ChatGPT就像AI蒙上双眼在那里盲答，而多模态就是AI一边看一边思考。多模态技术将语言模型的应用拓宽了到更多高价值领域，例如多模态人机交互、文档处理和机器人交互技术。在GPT-4中，多模态输入的图像和文本基于Transformer作为通用接口，图形感知模块与语言模块对接进行进一步计算。通过在多模态语料库上训练模型，包括文本数据、任意交错的图像和文本，以及图像-字幕对，可以使模型获得原生支持多模态任务的能力。下面先介绍GPT家族，然后引申到GPT-4的多模态架构和独特性。2.1从GPT-1到ChatGPT说到GPT-4，就不得不提到GPT家族。GPT之前有几个的前辈，包括GPT-1、GPT-2和GPT-3和ChatGPT。GPT家族与BERT模型都是知名的NLP模型族，都基于Transformer技术。GPT-1只有12层，而到了GPT-3，则增加到96层。GPT-4增加了额外的视觉语言模块，理论上具有更大的模型尺寸和输入窗口。▲视觉与语言Transformer技术的演进GPT和BERT之前的时代最早的NLP技术是基于规则的，即基于特定的规则使用程序进行固定模式的对话，所有的应答都是固定模式的。在深度学习诞生后，NLP技术逐渐进入基于模型的时代。文本生成是通过递归神经网络（RNN）或各种长短时记忆神经网络（LSTM）实现的。这些模型能够较好的进行模式识别，在输出单个单词或短语方面表现良好，但无法生成高精度的多轮对话，更无法实现逻辑推理能力。▲GPT-1模型的Transformer结构2018年6月，OpenAI发表了GPT-1，GPT家族首次登上历史舞台。GPT-1模型训练使用了BooksCorpus数据集。训练主要包含两个阶段：第一个阶段，先利用大量无标注的语料预训练一个语言模型，接着，在第二个阶段对预训练好的语言模型进行精调，将其迁移到各种有监督的NLP任务。也就是前面提到过的“预训练+精调”模式。GPT-1的核心是Transformer。Transformer在数学上是大矩阵的计算，通过计算不同语义之间的关联度（概率）来生成具有最高概率的语义反馈。GPT-1着重解决两个问题：1）通过无监督训练解决需要大量高质量标注数据的问题。2）通过大量语料训练解决训练任务的泛化问题。▲BERT与GPT的技术基本架构对比（图中En为输入的每个字，Tn为输出回答的每个字）GPT-22019年，OpenAI发表了另一篇关于他们最新模型GPT-2的论文（Language Models are Unsupervised Multitask Learners）。该模型开源并在一些NLP任务中开始使用。相对GPT-1，GPT-2是泛化能力更强的词向量模型，尽管并没有过多的结构创新，但是训练数据集（WebText，来自于Reddit上高赞的文章）和模型参数量更大。目前很多开源的GPT类模型是基于GPT-2进行的结构修改或优化。GPT-32020年6月，OpenAI发表了另一篇关于GPT-3模型的论文（Language Models are Few-Shot Learners）。该模型的参数是GPT-2的100倍（175B），并且在更大的文本数据集（低质量的Common Crawl，高质量的WebText2，Books1，Books2和Wikipedia）上进行训练，从而获得更好的模型性能。GPT-3实际上由多个版本组成的第3代家族，具有不同数量的参数和所需的计算资源。包括专门用于代码编程的code系列。GPT-3的后继知名版本包括InstructGPT和ChatGPT。▲GPT-3家族GPT-3.5/ChatGPT2022年3月15日，OpenAI发布了名为“text-davinci-003”的新版GPT-3，该模型被描述为比以前版本的GPT更强大。目前有若干个属于GPT-3.5系列的模型分支，其中code-davinci针对代码完成任务进行了优化。ChatGPT是基于GPT-3.5（Generative Pre-trained Transformer 3.5）架构开发的对话AI模型，是InstructGPT的兄弟模型。ChatGPT很可能是OpenAI在GPT-4正式推出之前的演练，或用于收集大量对话数据。OpenAI使用RLHF（Reinforcement Learning from Human Feedback，人类反馈强化学习）技术对ChatGPT进行了训练，且加入了更多人工监督进行微调。ChatGPT具有以下特征：1）可以主动承认自身错误。若用户指出其错误，模型会听取意见并优化答案。2）ChatGPT可以质疑不正确的问题。例如被询问“哥伦布2015年来到美国的情景”的问题时，机器人会说明哥伦布不属于这一时代并调整输出结果。3）ChatGPT可以承认自身的无知，承认对专业技术的不了解。4）支持连续多轮对话。与大家在生活中用到的各类智能音箱和“人工智障”不同，ChatGPT在对话过程中会记忆先前使用者的对话讯息，即上下文理解，以回答某些假设性的问题。ChatGPT可实现连续对话，极大的提升了对话交互模式下的用户体验。▲从GPT-1到GPT-4的对比（含BERT，笔者团队绘制）2.2 GPT-4的多模态架构到了GPT-4，技术内涵产生的飞跃其实超过了ChatGPT。有些人认为GPT-4仅仅是GPT-3.5的升级优化，这其实是一种误解。大部分人可能还未看到GPT-4带来的巨大影响。人类或其他高等生物的认知能力通常与从多种模式中学习有关。例如，苹果这一概念包括从视觉和语言获得的多重语义。包括苹果的颜色、形状、纹理以及吃苹果的声音，苹果在词典或其他网络媒体的相应定义等等。我们大多数人在学习认字的时候，也是先看到苹果的卡片图像，然后再记住对应的文字。与BERT模型类似，GPT-4之前的ChatGPT或GPT-3.5都是根据输入语句，根据语言/语料概率来自动生成回答的每一个字（词语）。从数学或从机器学习的角度来看，语言模型是对词语序列的概率相关性分布的建模，即利用已经说过的语句（语句可以视为数学中的向量）作为输入条件，预测下一个时刻不同语句甚至语言集合出现的概率分布。包括“苹果”在GPT-3.5和之前的GPT中只是单纯的语义符号和概率。GPT-4等模型新出现的多模态输入的能力对语言模型至关重要，使得“苹果”等单纯的符号语义扩展为更多的内涵。第一，多模态感知使语言模型能够获得文本描述之外的常识性知识。第二，感知与语义理解的结合为新型任务提供了可能性，例如机器人交互技术和多媒体文档处理。第三，通过感知统一了接口。图形界面其实是最自然和高效的人机自然交互方式。多模态大语言模型可通过图形方式直接进行信息交互，提升交互效率。多模态模型可以从多种来源和模式中学习知识，并使用模态的交叉关联来完成任务。通过图像或图文知识库学习的信息可用于回答自然语言问题；从文本中学到的信息也可在视觉任务中使用。截至2023年初，多模态大模型正经历将图文信息进行对齐，进行模态认知管理，进一步形成多模态决策或生成。常见的多模态大模型包括：1）图像描述生成或文本生成图像。例如最近知名的CLIP、Stable Diffusion。2）图文问答。例如带有图示的物理题求解或图表分析。3）文本到图像或图像到文本的检索。4）视频流描述。▲多模态模型的5种典型结构（来源：深圳鹏城实验室）根据鹏城实验室新发出的论文归纳，目前常见的多模态模型架构主要包括以下几种：1）合并注意力架构（Merge-attention）：如上图(a)，多个输入模态调整为同一的特征表示，多个模态的特征在自注意力之前被合并，共同进入Transformer。2）共同注意力架构（Co-attention）：如上图(b)，每个输入模态都具备私有自注意力通道，用于模态独立特征的导入，然后再使用共同的交叉注意力层融合多模态特征。3）交叉注意力架构（Cross-attention）：对于多模态任务，将图像与语言分别结合，实现图文信息的相互嵌入与问答。4）三角Transformer架构（Tangled-transformer）：使用三组Transformer模块同时处理动作、图形对象和语言特征，通过特定的三角连接关系，注入其他模态的Transformer网络，以不同模态的信息融合。5）模态间对比学习架构（Inter-Modality Contrastive Learning）：不同模态的信息被分解，通过矩阵结构建立多模态对比学习关联。目前评估OpenAI采用该种交叉注意力架构的研发GPT-4的代价最小，可以大幅度使用ChatGPT中已经构建的语言模块。从笔者团队的分析看，GPT-4很有可能正是采用这类架构。2.3 GPT-4的独特性GPT-4是最新且最先进的OpenAI多模态大模型。并在多个基准任务上取得了非常好的成绩，包括图像字幕、图文问答、代码生成和法律推理。优秀的图文分析和逻辑推理能力铸就了GPT-4的护城河。▲GPT-4在大部分任务上优于人类水平平均值（来源：LifeArchitect）GPT-4能够生成比GPT-3.5或其他语言模型更符合事实的准确陈述，确保更高的可靠性和可信度。GPT-4也可接受图文信息作为输入并生成说明、分类和分析。尽管GPT-4已经投入商业使用，但大多数用户仍需要等待图文能力的正式开放。▲GPT-4与其他大模型的比较（来源：Janna Lipenkova）03.GPT-4训练技术分析3.1 GPT-4训练数据集GPT-4的训练数据集是基于GPT-3和GPT-3.5的训练数据集构建的，并在两者基础上增加了多模态数据集。仅当时GPT-3.5的训练数据收集就是由一项艰巨且重要的任务：数据集贡献来自一个由30-50名OpenAI员工组成的团队，并另外从第三方网站雇佣了固定的大约50-100名固定的标注员。到了GPT-4又增加了大量多模态数据。▲GPT-4数据集构成（预测）OpenAI在预训练阶段过滤GPT-4的数据集组合，以专门减少不适当的文本内容的数量。并通过结合内部训练的分类器和基于词典的方法来识别含有不当内容的数据。其中来自GPT-3的预训练数据集包括约570GB（该大小为去重过滤后的，去重过滤前大概45TB）的CommonCrawl数据，大概是410B字节对编码的令牌，以及19B的WebText2数据，合计67B的书籍数据和3B Wiki数据。这些数据被整合为约300B大小的GPT-3预训练数据集。其中Wiki数据质量最高，在训练中平均被重复使用了3.4次。▲GPT-3.5标注员学历和国籍构成GPT-3.5的数据集包括SFT数据集、RM数据集和PPO训练数据集。在最关键的SFT训练集中，标注员撰写比例为89.3%，100%由标注员标记。RM模型较小但是训练集数量更多，达到33K。可以看出，无论是GPT-3还是GPT-3.5的训练集中，数据质量的重要性都远超过数据的数量。（互联网大厂或许只剩下资金、应用和渠道优势？）其中训练GPT-3.5 175B SFT模型仅需要算力资源4.9 petaflops/s-days，训练GPT-3.5175B PPO-ptx模型需要60 petaflops/s-days，而预训练GPT-3却需要3640 petaflops/s-days的算力资源，是175B PPO-ptx的60.7倍。极少量的高质量标注数据却显著提升了GPT-3.5的应答能力。这就好比好的教材胜过大量普通书籍。GPT-4的多模态训练数据集由图片和文本共同构成（一般是单幅图片+多行文本）。根据GPT-4的技术报告，可以分析GPT-4的多模态数据集包括图表推理、物理考试、图像理解、论文总结、漫画图文等不同类型。目前GPT-4的多模态应用还处于实验中，并未对公众开放。3.2 GPT-4训练流程分析我们根据GPT-4的技术报告，初步分析其训练流程如下：第一阶段：构建交叉注意力架构预训练模型，收集数据并进行有监督策略精调GPT-4模型是基于GPT-3.5构建的，增加了视觉语言模型组件（在图形Transformer阶段完成的视觉预训练模型）。为了预训练模型在多模态领域进行初步调优，首先会在文本数据集和多模态数据集中抽取问题，由人类标注员，给出高质量答案，然后用这些人工标注好的数据来精调GPT-4初始模型（获得SFT模型，Supervised Fine-Tuning）。此时的SFT模型在遵循指令/对话方面已经优于GPT-3.5，但对多模态的解答不一定符合人类偏好。▲GPT-4训练过程（基于GPT-4技术报告绘制）第二阶段：训练奖励模型（RRM）和基于规则的奖励模型（RBRM）这一阶段包括基于规则的奖励模型（Rule-Based Reward Model，RBRM）和奖励模型（Reward Mode，RM）首先基于安全规则设计基于规则的奖励模型并完成验证。这一模型与传统NLP领域的规则模型设计方法一致。然后在数据集中抽取问题，使用第一阶段生成的模型，对于每个问题，生成多个不同的回答。人类标注者对这些结果综合考虑给出排名顺序。（有点像互联网企业的审图师）接下来，使用这个排序结果数据来训练GPT-4的奖励模型。对多个排序结果，两两组合，形成多个训练数据对。RM模型接受一个输入，给出评价回答质量的分数。这样，对于一对训练数据，调节参数使得高质量回答的打分比低质量的打分要高。这一过程类似于教练或老师辅导。▲PPO算法示意（来源：澳大利亚Monash大学）第三阶段：采用PPO（Proximal Policy Optimization，近端策略优化）强化学习来优化策略PPO的核心思路在于将Policy Gradient中On-policy的训练过程转化为Off-policy，即将在线学习转化为离线学习，这个转化过程被称之为Importance Sampling。这一阶段利用第二阶段训练好的奖励模型和基于规则的奖励模型，靠奖励打分来更新预训练模型参数。在GPT-4数据集中抽取问题，使用PPO模型生成回答，并用上一阶段训练好的RM+RBRM模型给出质量分数。把回报分数依次传递，由此产生策略梯度，通过强化学习的方式以更新PPO模型参数。如果不断重复第二和第三阶段，通过迭代，会训练出更高质量的GPT-4模型。整个训练的核心在于高质量数据集/Prompt的构建、思维链的训练技术、大算力工程能力、早期训练空间的预测和优选。数据量反而不那么重要，并不是富裕人家的孩子看的电视剧越多，就越能够考上好大学。3.3模型训练参数空间的早期筛选这一部分讲的是降低训练量的技术，或者说是节约大量购买服务器的经费的技术。据论文Scaling Laws for Neural Language Models (2020)，可以通过三个要素计算通过交叉熵损失预估GPT-4/GPT-3.5预训练模型性能。这三个要素是模型参数数量、训练期间计算量以及训练数据大小。这三个因素与损失之间存在幂律关系。这意味须将计算量、数据和参数量增加10倍才能将损失减少一个单位，增加100倍才能将损失减少两个单位，依此类推。为了提升GPT模型性能（主要是精度），比较经济的做法是在相对少量的数据上训练较大的模型，并在更多的训练步骤或数据上投入更多的计算，以最大限度地减少精度损失。因此“大型预训练模型+少量高质量标注”成为当前语言模型领域的主流。▲缩放定律图示（来源：OpenAI）对于计算量每增加10倍，该论文大约建议将参数数量增加5倍，将训练令牌的数量增加2倍，将串行训练步骤的数量增加1.2倍。这也可以说明，GPT-3等模型越来越大，是从综合训练成本考虑的。缩放定律不仅影响到模型的设计要素，也影响到基于算力的训练策略。OpenAI开发了针对性的算力基础设施和优化方法，在多个尺度预测GPT-4模型的训练效果，从而能在只使用大概千分之一的算力消耗时提前探索GPT-4训练的解空间，而无需完全遍历所有可能的训练参数设置。换句话说，投资人也可以提前预知自己投的创企大概能训练出多高精度的模型。缩放定律可以量化为公式L(C) = aCb+ c，从而只要在每个参数模型的早期使用小一些的算力就可以拟合该曲线，提前估算在该训练参数集下能达到的模型精度。这一方法大大加速了GPT-4模型训练调优的进程。▲较小算力模型准确的预测了GPT-4的训练精度（来源：OpenAI）4 GPT-4的算力基座由GPT-4/ChatGPT及其下游需求，带动了大量的模型设计与产业应用需求，带动了从服务器集群到大算力芯片的海量计算需求。这一需求业导致了A100 GPU的价格在近几个月内暴涨。那么，在多模态大模型逐渐火热之后，GPU会是唯一的算力选择吗？GPU会不会导致模型企业同质化竞争的加剧？根据IDC预计，到2026年AI推理的负载比例将进一步提升至62.2%，特别是大模型将成为AI开发的新范式。对于GPT-4等大模型设计或应用企业，算力的需求体现在如下三个细分阶段。1）GPT-4预训练与应用微调阶段。这一阶段从无到有建立预训练模型，通过大量通用数据训练和验证预训练模型。（形成模型的“通识”）然后针对具体的商用或应用场景的特定数据，对预训练进行针对性的微调，加强对场景的应答准确度。在这一阶段，一般需要超算级别或数十台服务器来进行一个大模型的训练计算，计算以大量矩阵计算和求解为主。这一阶段的算力可通过上述的缩放定律来进行预测和缩减不必要的训练参数集合所需的算力（非必要参数集分支大概可缩减到千分之一）。2）GPT-4推理与部署阶段。根据场景微调后的大模型，就可部署到实际生产环境中应用。相对训练来说，部署要求的算力较低，但是基数很大。对于大量在线交互来说，部署阶段的服务器/芯片成本要远远超过训练阶段。在这一阶段，每台AI服务器可以部署一个GPT-4模型，集群上会有大量服务器进行并行的网络服务，计算以大量矩阵计算和存储调度为主。同时，在这些场景下，特别是端侧应用场景，也会有硬件性价比和反应延迟的特定要求，目前的GPU就不一定适合。3）GPT-4模型迭代的微调阶段。每使用一段时间，就会根据使用者或者客户反馈，对模型进行调整，以提高客户满意度，特别是提升模型的安全度以确保合规。这个过程就是模型迭代的过程，一般相当于小规模的训练，训练所用的数据规模不大，计算以大量矩阵计算和求解为主。04.GPT-4的算力基座4.1 GPT-4计算服务器架构▲计算服务器架构对比针对GPT-4这类大模型的计算架构，按照计算芯片的组合方式，一般可以分为：“CPU+GPGPU”，“CPU+DSA”，和“CPU+DSA+GPGPU”三种类型。这三种类型目前都已在云计算场景广泛应用和部署。DSA即领域专用加速器，是用于一些特定场景或算法族计算的芯片级加速。最早的GPU也属于DSA，也就是图形加速的DSA。随着GPU逐渐演化，将非常小的CPU核心加入GPU形成GPGPU架构后，才具备了通用化的计算能力。1）CPU+GPGPU是较早且部署众多的一种。由于这种架构的计算灵活度高，也可用于模型训练和非AI类计算。适合任务种类繁多且差异化大的云计算场景。2）CPU+DSA是目前Google云计算（GCP）应用较多的方式。例如Google去年发布的Pathways计算系统（包含6144块TPU）就是这类架构的典型代表。这类架构计算灵活性稍低一点，但是计算性能和成本都非常明显优于CPU+GPGPU模式，非常用于GPT-4或其他算法部署场景。例如早些年的AlphaGo的性能突破很大程度上来自于Google自研的TPU。当时如果用GPU，估计超过人类棋手的集群成本恐是当年的Google也难以承受的。▲Google Pathways “CPU+DSA”训练集群基础架构（来源：Google）3）CPU+DSA+GPGPU介于前两者之间，充分提高了灵活性又明显降低了计算成本。这类架构需要算法设计/部署人员有丰富的异构架构部署经验。计算卡间的高速互连对GPT-4计算的影响排在单卡算力之后。对于多数GPU来说，由于一般需要多卡才能放下一个模型，因此整体的计算效率受限于互连带宽和单卡有效算力密度。（算力密度大可以减少互连交互的总数据量）▲AI服务器内部的卡间互连以英伟达为例，HGX平台上有六个NVSwitch。每个NVSwitch连接2块GPU，总共有12个最短数据通道直连，形成接近立方体式的数据交互网。这里之所以说接近，是因为0123和4567平面之间的连接还不是两两互连，这一结构会限制GPU所能运行的模型的规模效率。计算服务器之间的高速互连通过Infiniband实现。每个NVIDIA A100/H100可以访问服务器Infiniband结构，通过Infiniband提升服务器间的数据交互带宽，而大带宽的Infiniband一般通过光纤连接（使用SerDes技术）来减少损耗。Infiniband网络基于“以应用程序为中心”的观点，目标是让应用程序访问其他应用程序以及存储尽可能的简单、高效和直接，避免网络分层带来的延迟。这种结构大大提高了GPT-4训练的速度。4.2 GPT-4计算相关芯片对于GPT-4这类大模型来说，其部署需要大量的大算力计算、存储和数据交互芯片，包括：AI计算：算力>100TFLOPS的GPGPU或大算力AI芯片CPU：核数>8的CPU存储：内存/GDDR/HBM/NVMe数据交互：Infiniband卡在Transformer技术兴起之后，传统CUDA核心的算力支持能力已表现出劣势，英伟达便在其GPU中添加Tensor Core这类DSA单元，以适应算力需求的变化。考虑到GPU这类通用架构芯片在计算效率和算力上已经暂时落后于GPT-4/ChatGPT这类大模型的发展速度，从芯片/半导体的历史发展规律看，预计近几年可能会有新的专用架构大发展来填补这一需求。这类新架构也许是针对大模型的DSA，或者是更接近于DSA的GPGPU。▲CPU、GPU和存算一体芯片的架构对比从目前GPT-4的部署需求来看，GPT-4大模型具有数据量大、数据带宽要求高、算力要求高的计算特点，且算法相对单一。如果要提高计算效率和性价比，就应该像超算那样选择更高计算密度的算力芯片。从这个角度上看，具备存算一体结构的DSA可以很好的满足这些要求，并且具备比GPGPU更高的计算性能，未来很有可能与CPU或GPU组合，形成GPT-4这类算法的主要部署芯片。在GPT-4这类大模型训练中，一般需要使用Infiniband进行大算力芯片间的协同工作，整合海量芯片的算力。Infiniband摒弃了传统网络和应用程序之间消息传递的复杂结构，使应用程序之间直接进行通信，绕过了操作系统，大大提高了效率。05.GPT-4的局限与未来改进方向5.1 GPT-4局限尽管GPT-4表现出出色的上下文对话能力甚至编程能力，以及能看懂图梗和分析数据图。我们也要看到，GPT-4技术仍然有一些局限性，还在不断的进步。1）GPT-4在其未经大量语料训练的某些领域缺乏“人类常识”和引申能力”。GPT-4在很多领域可以“创造答案”，但当用户寻求正确答案时，GPT-4也有可能给出有误导的回答。大预言模型的安全性问题仍是横亘在其大规模商用上的拉路虎。如果遇到关系重大利益的抉择时，我们是该相信GPT-4的结果还是不相信呢？▲大语言模型的SWOT分析（来源：互联网）2）GPT-4需要非常大量的算力（芯片）来支持其训练和部署。抛开需要大量语料数据训练模型不说，在目前，GPT-4在应用时仍然需要大算力的服务器支持，而这些服务器的成本是普通企业在大流量服务时无法承受。对于私有化部署来说，还需等待更轻量型的模型或更高性价比的算力平台。3）GPT-4还没法在线的把新知识纳入其中，而出现一些新知识就去重新预训练GPT模型也是不现实的，无论是训练时间或训练成本，都是普通训练者难以接受的。如果对于新知识采取在线训练的模式，看上去可行且语料成本相对较低，但是很容易由于新数据的引入而导致对原有知识的灾难性遗忘的问题。另一方面，缺乏检查的新的不良知识也可能导致模型本身的安全性问题。特别是GPT-4可能会继承从新数据中学到的偏见和不平等性。如果数据集中存在种族、性别、地理位置等方面的偏见，GPT-4也可能会在生成文本时出现类似的偏见。4）GPT-4仍然是黑盒模型。目前还未能对GPT-4的内在算法逻辑进行分解，因此并不能保证GPT-4不会产生攻击甚至伤害用户的表述。OpenAI提供的文档报告称，GPT-4-launch的错误行为率为0.02%，远低于GPT-3.5的0.07%和GPT-3的0.11%1。这意味着GPT-4-launch生成的文本在10000次完成中只有2次违反OpenAI的内容政策或用户偏好。但即便这2次违反依然可能导致OpenAI受到严重的法律诉讼。5）GPT-4仍存在社会和道德风险。由于GPT-4这类大型语言模型仍然存在根本性的黑盒特征。GPT-4仍然可以生成有偏见的、虚假的和仇恨的文本；仍然可以被黑客攻击（例如Prompt越狱）以绕过它的安全防护墙。尽管OpenAI提出了多项措施来减轻其模型的风险和挑战，但GPT-4仍然可能被滥用于创建假新闻、宣传、垃圾邮件或有害内容，还可能产生可能误导或伤害用户的事实错误或偏见。OpenAI的研究表明，GPT-4可以在许多领域与人类宣传员相媲美。6）GPT-4仍存在幻觉和推理错误。尽管OpenAI声称GPT-4相对于以前的模型可以显着减少幻觉（在其内部评估中得分比GPT-3.5高40%）。GPT-4仍然可能会产生有害的建议（尽管GPT-4更有可能拒绝回答）、错误代码或不准确的信息，因此，GPT-4暂不应该用于错误成本高的区域（比如医学手术？）。7）GPT-4存在泄露隐私可能。GPT-4可从各种内部许可和公开可用的数据源中学习，其中可能包括大量公开有效的个人信息。包括互联网上学习到有重要影响力人物（例如名人和公众人物）的大量隐私信息。GPT-4还可以聚合不同信息，将大量隐含信息关联挖掘出来形成有效的隐私信息。5.2未来改进方向5.2.1减少人类反馈的RLAIF2020年底，OpenAI前研究副总裁Dario Amodei带着10名员工创办了一个人工智能公司Anthropic。Anthropic的创始团队成员，大多为OpenAI的早期及核心员工，并在2022年12月，发表论文《Constitutional AI: Harmlessness from AI Feedback》介绍人工智能模型Claude。▲CAI模型训练过程（来源：Anthropic）Claude和GPT-4都依赖于强化学习（RL）来训练偏好（Preference）模型。CAI（Constitutional AI）也是建立在RLHF的基础之上，不同之处在于，Claude的CAI的排序过程使用模型（而非人类）对所有生成的输出结果提供一个初始排序结果。这种模式的好处是节约了大量人工标注的时间和资源，可以加速大模型的训练进程，并降低成本。CAI用人工智能反馈来代替人类对表达无害性的偏好，即RLAIF，人工智能根据一套法规（Constitution）原则来评价回复内容。▲CAI与RLHF技术对比（来源：Anthropic）5.2.2数理能力的增强GPT-4虽然已经具备解物理题的能力，但毕竟不是专门的解题算法，一些复杂的数理问题对话中仍会出现一本正经胡说八道的情况。计算机学家Stephen Wolfram为这一问题提出了解决方案。Stephen Wolfram创造了的Wolfram语言和计算知识搜索引擎Wolfram|Alpha，其后台通过Mathematica实现。▲ChatGPT与Wolfram|Alpha结合处理梳理问题（来源：Wolfram）目前Woflframe已经可以通过ChatGPT调用（通过插件），未来也会实现GPT-4的集成。在这一结合体系中，GPT-4可以像人类使用Wolfram|Alpha一样，与Wolfram|Alpha“对话”，Wolfram|Alpha则会用其符号翻译能力将从GPT-4获得的自然语言表达“翻译”为对应的符号化计算语言。在过去，学术界在GPT-4使用的这类“统计方法”和Wolfram|Alpha的“符号方法”上一直存在路线分歧。但如今GPT-4和Wolfram|Alpha的互补，给NLP领域提供了更上一层楼的可能。▲ChatGPT调用Wolfram（来源：Wolfram）GPT-4不必生成这样的计算代码，只需生成常规自然语言，然后使用Wolfram|Alpha翻译成精确的Wolfram Language，再由底层的Mathematica进行计算。5.2.3 GPT-4的本地化与小型化虽然GPT-4很强大，但其模型大小和使用成本也让很多人望而却步。有三类模型压缩（model compression）技术可以降低模型的大小和成本。第一种方法是量化（quantization），即降低单个权重的数值表示的精度。比如Transformer从FP32降到INT8对其精度影响不大，但是会显著提升计算效率。笔者团队已研发出INT4量级的Transformer高精度量化算法，无需再次训练或更改模型，即可部署到GPT-4算力平台上，大大提升计算效率并降低成本。第二种模型压缩方法是剪枝（pruning），即删除GPT-4的网络元素，包括从单个权重（非结构化剪枝）到更高粒度的组件如权重矩阵的通道。这种方法在视觉和较小规模的语言模型中有效，也是很多框架（Framework）上自带的功能。第三种模型压缩方法是稀疏化。例如奥地利科学技术研究所（ISTA）提出的SparseGPT可以将GPT系列模型单次剪枝到50%的稀疏性，而无需任何重新训练。当然这种稀疏结构目前还仅仅是基于GPU架构实现的，在其他硬件平台上并不兼容，而且GPT-4的稀疏化是否在综合成本上优于压缩还有待观察。▲SparseGPT压缩流程（来源：ISTA）06.GPT-4的产业未来与投资机会6.1大模型的技术栈GPT-4这类大模型的用户量巨大，算力需求巨大，连接的设备和软件众多。其技术栈具有更多组件，可包括用于容器化、性能监控、商业智能、事件处理、云服务、微服务和分析的工具。GPT-4/ChatGPT等大模型的技术栈可以分为5层：1）应用层：将生成的AI模型（可通过接口）集成到面向用户的应用程序，运行私有模型或通过第三方接口运行模型。这一层的应用企业最多。大量企业无需研发自有的大模型，即可使用GPT-4带来的人工智能协作能力和生成能力，形成各类应用。2）接口层：包括各种调用API和数据中心调用工具，同时提供对应的提示工程接口和模型精调接口。接口层将应用层和模型层衔接，方便应用层调用，使得开发者和用户能够以编程方式与模型进行交互。这可以简化GPT-4在实际应用中的部署和调用，从而降低使用门槛。3）模型层：包括各类开源或非开源模型，以及各种模型的共享平台。这一层提供了不同的模型数据和功能，通过接口层为应用层提供大模型的功能支持。4）框架层：提供训练或云部署的深度学习框架和中间件等，包括PyTorch、TensorFlow等知名深度学习框架和中间件。5）计算层：为模型层提供模型计算和调度的各种算力支持，为训练AI模型运行训练和运行推理任务提供基础设施。计算层包括了各种云计算平台和计算芯片。在这一层，AI芯片会是核心瓶颈。▲GPT-4等大模型的技术栈目前GPT-4的几乎所有内容都通过云计算GPU或TPU来运行，使用者包括运行训练工作的模型提供商/研究实验室、进行模型部署或精调的应用企业。在GPU替代CPU成为主要的AI算力芯片之后，AI界10多年来再一次受到大规模计算能力的限制。截至目前，GPT-4这个领域目前还未看到非常明确的技术或产品护城河。由于使用相似的模型，应用层企业在早期可能会缺乏很强的产品差异化；由于大部分云服务提供方目前只能使用同一FAB生产的GPU作为主力算力芯片，普通云提供商实质上也难以提供成本或性价比的差异化。GPT-4等大模型目前以同质化的模式进行构建，具备统一的“图像+自然语言”接口，因此短期内，除了模型参数本身不易训练好的壁垒外，暂时还未明确通过软件生态或数据管道建立自家独有竞争壁垒的路线。就目前来说，我们还无法判断GPT-4这类多模态大模型领域是否会像互联网那样出现少数几家独大的情况。也许大模型的时代会是一个无中心的状态，每个团队都有可能成为英雄。6.2 GPT-4的产业应用AIGC即利用人工智能技术来生成内容。与此前Web1.0、Web2.0时代的UGC（用户生产内容）和PGC（专业生产内容）相比，代表人工智能构思内容的AIGC，是新一轮内容生产方式变革，而且AIGC内容在Web3.0时代也将出现指数级增长。GPT-4模型的出现对于图像/文字/语音多模态的AIGC应用具有重要意义，会对AI产业上下游产生重大影响。▲GPT-4的应用领域（修改自OpenAI）GPT-4对依赖人类智能处理和生成的各个领域和行业具有许多潜在的应用和影响。与其考虑哪些细分领域可以使用GPT-4这类技术辅助人工，不如考虑哪些领域还不能用GPT-4辅助，后者的名单或许更短一些。可以快速使用GPT-4的一些行业包括（可视为会快速变革的行业的预测）：1）教育行业：GPT-4可以作为想要学习新技能或学科的学生的辅导员或指导者。GPT-4还可以根据学生的学习目标和进度，为他们提供个性化的反馈和指导。2）文娱行业：GPT-4作为讲故事的人或作曲家，为观众和平台输出原创且引人入胜的内容。GPT-4还可以用作游戏设计师或角色，为游戏玩家创造身临其境的互动体验。3）商业：GPT-4可用作营销人员或销售人员，为顾客和客户创建有效且有说服力的话术。GPT-4还可以用作客户服务代理或聊天机器人，以快速准确地响应查询和投诉。4）新闻：GPT-4可用作记者或编辑，以生成有关各种主题和事件的高质量和真实的新闻文章。GPT-4还可以用作新闻检查器或验证器，用于检测和纠正错误信息和假新闻。5）医疗大健康：医生或护士可以使用GPT-4作为助手诊断和治疗患有各种疾病的患者。GPT-4也可以用作治疗师或健康顾问，提供心理健康支持和建议。6）法律：GPT-4可以作为律师或法官助理起草和审查法律文件和合同。7）生命科学：GPT-4及其模型的生物分支可用于从用于临床试验的合成数据创建到基于蛋白质折叠模型的生成式蛋白质设计以加速药物发现，再到学术论文的研究总结。虽然采用还处于早期阶段，但加速药物发现和批准、改善患者疗效和节省医疗成本的潜力是巨大的。8）供应链和物流：借助GPT-4的思维链能力来进行自动化产品开发，包括设计和组件替换，从而以更低的成本生产出具有更高性能和可持续性的新产品。GPT-4还可支持文档自动化和合同生成，以更好的简化工作流程。随着算法技术和算力技术的不断进步，GPT-4也会进一步走向更先进功能更强的版本，在越来越多的领域进行应用，为人类生成更多更美好的对话和内容。6.3 GPT-4对我们和未来的影响GPT-4这类多模态大模型技术会对我们每个人的生活和工作产生一系列的影响。例如：1）GPT-4会极大的影响宣传和社交。以后GPT-4这类技术会在互联网上横行，我们会很难分辨到底是“大众的声音”还是“中心服务器的声音”，大量没有主见的人可能会盲从于GPT-4这类技术生成的观点，人类会变成机器的复读机。同时GPT-4工具会大量渗透入普通人的社交，“唯有套路得人心”的场景会遍地开花。2）AI大量替代低端重复性沟通和多模态工作。GPT-4会与机器人技术结合，从云渗透到端，进入每个人的日常生活。操作系统和办公软件的交互UI会大量被大模型主宰化。也许开始会有很多人因为AI技术的替代而失业，逐渐更多的人借助GPT-4这类技术获得更高的效率并成为自然语言程序员，人类开始剥削机器，创造力和自然情感成为人类能坚守的宝贵特质。3）各种考核将从知识型考核转向综合能力考核。知道多少或者会什么外语已经不重要，工作经验或技术经验也只是看是否拥有更先进的GPT模型或算力。一些曾经的热门专业可能会逐渐凋落。人类下一代从人类“内卷”过渡到“人机互卷”，高层次能力竞争会更加激烈。GPT-4这类多模态大模型到底会给我们每个人带来什么样的具体影响，也许是现在的我们还不能完全想象的。但这影响一定是巨大和深远的。毕竟“圣杯”的语义不仅仅代表贵重，也代表了神奇和不可思议，甚至是独一无二。（本文系网易新闻•网易号特色内容激励计划签约账号【智东西】原创内容，未经账号授权，禁止随意转载。）
683254342,c4af492e353c019c23b9090207e95d19,toyama nao,https://api.zhihu.com/people/c4af492e353c019c23b9090207e95d19,people,独立音游开发者/业余AI评测员,2月国产AI大模型横评（豆包，文心3.5，Kimi）,article,https://api.zhihu.com/articles/683254342,本评测是个人性质，结合自己需求和对<em>大模型</em>的理解，使用私有题库进行长期跟踪评测。不够权威，不够全面。但可以从一个侧面观察各个<em>大模型</em>的长期进化趋势。 任何评测都无法给出无死角的权威排行榜,236,60,171,1708514991,1708514991,#0 本期看点1）豆包忙活了3个月，近况怎样？2）Kimi这家神秘低调的公司1月更新了什么？3）文心3.5 春节偷偷更新了啥？#1 前情提要本评测是个人性质，结合自己需求和对大模型的理解，使用私有题库进行长期跟踪评测。不够权威，不够全面。但可以从一个侧面观察各个大模型的长期进化趋势。任何评测都无法给出无死角的权威排行榜，笔者写这个系列也是分享一种评测思路，以及个人见解。每个人应该根据自己所需，对大模型进行考察。由于所有规则没有变化，老读者请直接跳至各段的解析段落。蓝色表示本轮新测。12月的评测见：https://zhuanlan.zhihu.com/p/673886532 GLM-4，讯飞3.5，百川3之前已分别写文评测，此处不重复，仅列入总榜。#2 参赛选手截止2月期间有更新的包括：豆包（2月测试）Kimi（1月26静默更新）百度文心3.5（0205静默更新）Gemini Pro（正式版）其余模型由于没有更新，继续沿用前次的成绩，本次没有重测。#3.1 基础题基础题采用负向打分，即出现相应负向情况，扣1分。最后按扣分从少到多排序。基础题考察模型基本功：指令理解，阅读理解，逻辑推理，数学计算，科学常识，翻译，机器幻觉等共9个分类。基础题都有明确且固定的答案，并且对答案也有明确约束，如字数，格式，知识范围等。解题需要的信息大部分在题干中直接给出，但需要模型掌握必要科学计算公式，理科常识，社会学常识等。不涉及生僻问题。基础题的扣分维度包括：答案错误：计算类，知识类答案不正确，翻译类理解错误。混乱输出：输出内容人类不可读，重复，错误解释，翻译用词不够准确等。指令误解：不能完全按照指令输出回答。拒答：因触发关键词或人类价值观对齐（基于错误理解）导致不回答。同时因为输出不符合预期，答案错误和指令误解也各扣1分。#3.2 高阶逻辑目前只给GPT-4和号称对标GPT-4的模型做高阶测试题。同时增加了ChatGPT 3.5的成绩作为对照。高阶题库目前有17道题，侧重复杂任务，复杂逻辑和长链推理，属于大模型可用性中最核心的部分。后面会在不泄漏原题的情况下，简介每道题的考察点。每道题都有唯一正确答案，根据问题类型，答案的给分点也不同。每道题总分10分，按给分点折算成绩，比如有10个给分点，答对2点，得2分。部分题目需要输出长字符串，这类题目按最长连续答对的字符数除以总字符数给分。最终成绩是成绩相加，未来会继续完善题库，增加考察角度，总分也会随之提升。目前的题目：#4 成绩解析1）豆包：字节的豆包真的是个很可惜的模型，早在去年9月，就以总扣分20的好成绩遥遥领先一众国产模型，但之后就一直在20-15附近徘徊，本次测试也仅仅稍微改善2分到13，可以算在波动范围。字节明明就有国内最大的算力池，没有道理AI进化如此缓慢。不过豆包的应用端动作比较多，新出的Coze平台也颇有口碑，此处按下不表。具体到错误情况上，指令理解有非常轻微提升，比12月多对了一道数学题，由于此题是24年新题，排除背题作弊。其余变化基本乏善可陈，大部分还是和12月测试相当。2）Kimi：Kimi是最近被关注较多的大模型，1月靠“劝女友模拟器”的火爆连带出圈，2月又有新闻拿到国内最大单笔AI投资。月之暗面团队总体比较低调，本次更新也只在公众号上有推文。实测下来，回答速度确如changelog所述，有显著变快，几乎是国内第一梯队，但笔者没有进行量化测试，有条件读者可以自行对比。其次理解能力改善显著，在阅读理解和翻译类问题上，几乎全对，对故意诱导错误信息也能准确识别。逻辑能力也有较大提升，但受限于指令跟随能力偏弱，整体发挥不好。其他方面，如数学计算，学科知识则几乎和之前相同。要特别说明，Kimi的内容安全判定还有问题，出现2次拒答，导致额外丢分。如果这方面加以改善，则整体会超过通义千问。有小道消息称Kimi在2月或3月还会有一次大更新，追平GPT4，值得关注。3）文心3.5：文心3.5是一次面向开发者的更新，网页版没有变，但开发接口变更到了0205版本，也就是百度在春节前训练完成，节后面向开发者全量。这次更新还是比较有意思，整体提升超过20%，接近自家4.0。部分题目甚至答的比4.0还要好。从回答中大概能推测百度的优化思路。其中，计算类问题，在输出数字前有短暂停顿，推测是内部执行了function call，调用计算器子程序。其次，学科类问题出现了非常格式化的解答过程，推测是百度使用了庞大的外包团队进行学科答题，作为训练材料。翻译类也有相似迹象，古文翻译质量显著高于4.0，推测也是大量输入了人工翻译语料。一些逻辑问题有着与4.0相似的回复。因此，这个版本很大概率是在4.0训练完成后，复用了4.0的算法优化，结合后面3个月的语料积累训练出的4.0低配版。合理推测，新的4.0也在路上，改善幅度也会较大。也许就在3月，文心发布一周年之际。4）Gemini Pro：注意，这次测的是Pro 1.0版，并非内测中的1.5。正式版相比12月的初版，出现了部分优化，部分劣化，整体改善的局面，尤其数学问题，Beta版还能对几处，到正式版就几乎全错了。阅读理解能力有小幅改善，之前出现不可理解的拒答，在正式版中也没有了。总体上Gemini Pro水平还是偏低，但这只是暂时的，并不需要为Google感到难过，新的1.5按纸面数据推算，应该达到目前文心4.0水平，后续会安排测试。
695717926,c4af492e353c019c23b9090207e95d19,toyama nao,https://api.zhihu.com/people/c4af492e353c019c23b9090207e95d19,people,独立音游开发者/业余AI评测员,AI大模型横评（v3题库，全模型重测）,article,https://api.zhihu.com/articles/695717926,本评测是个人性质，结合自己需求和对<em>大模型</em>的理解，使用私有题库进行长期跟踪评测。不够权威，不够全面。但可以从一个侧面观察各个<em>大模型</em>的长期进化趋势。 任何评测都无法给出无死角的权威排行榜,87,44,35,1714649285,1714649285,先祝大家五一国际劳动节快乐！#0 本期看点全新题库，排行榜全面更新。#1 前情提要本评测是个人性质，结合自己需求和对大模型的理解，使用私有题库进行长期跟踪评测。不够权威，不够全面。但可以从一个侧面观察各个大模型的长期进化趋势。任何评测都无法给出无死角的权威排行榜，笔者写这个系列也是分享一种评测思路，以及个人见解。每个人应该根据自己所需，对大模型进行考察。在4月Update（toyama nao：AI大模型横评-4月Update.2（商量Pro，Step1V））中提到，旧版题库使用近一年，题目已经略显陈旧，不能再反映模型真实水平了。尤其GPT-4已无限接近满分。为了应对今年各家大模型持续升级，从本月开始，启用以逻辑能力为主的新题库。现就二者区别陈述如下：旧题库新题库规模40题20题类型指令理解，阅读理解，逻辑推理，数学计算，科学常识，翻译，机器幻觉逻辑推理，计算推理，代码推理，角色扮演，规划计分方式采用负向打分，即出现相应负向情况，扣1分。最后按扣分从少到多排序。每道题有设计得分点，模型答对一点得1分，每道题得分归一化处理后，所有得分相加。为什么新题库反而缩减了问题类型和规模？原因其一是时间进入24年以来，各家头部模型纷纷支持了搜索增强，function call，all tools等能力。大模型传统弱项数学计算，在all tools调用计算器的帮助下也能以高准确率攻克。而知识类更是可以在搜索下获得大幅修正。而支撑大模型进行问题分析，工具调用，编写正确代码的基础正是其硬逻辑能力。在相同搜索能力下，硬逻辑能力更强的模型，文本的总结归纳能力也更强。类似的，写作，翻译能力也极大受逻辑能力限制。因此新题库聚焦最能反映硬逻辑能力的题型，尽可能逼近模型能力极限。当然如此一来，弊端就是测试成绩会和用户实际体验脱节。举个不恰当的例子，如果高考题目特别难，优等生依然可以考700分，而差生可能连200都没有。但如果都让他们同时进厂打螺丝，差生不一定输给优等生。读者需要意识到，你所体验到的大模型能力，是综合了各种工程优化之后的结果，不单是逻辑能力。#2 参赛选手所有非开源模型都参与重测。#3 题目和打分此处列出所有题目大纲和核心考点：1、长文本理解和角色扮演：长文本理解，指令遵守2、动态规划的推导求解：计算能力3、多步规则下的文本替换处理：多步遍历处理文本，指令遵守4、程序改错：代码理解5、基于多重规则判断单据合法性：规则理解，日期计算6、旅游路径规划：长文本理解，工具调用能力7、【Medium】公元纪年推算天干纪年：计算能力，规则理解8、【Medium】阅读代码输出结果：代码理解，代码推导9、【Medium】计算有时间重叠下最少会议室数量：计算能力10、【Medium】从代码中推测json结构：代码理解，短期记忆，数据结构理解11、【Medium】故事推理：长链逻辑能力12、【Medium】提供上下文的代码补全：代码理解，指令遵守13、【Medium】密文解码：模拟计算14、【Medium】棋盘上的图形：平面空间想象能力15、【Medium】不提供规则，阅读某中文编程代码，并推导输出：符号理解，迁移学习能力16、【Hard】按规则拧魔方后求魔方颜色：立体空间记忆能力17、【Hard】符号重定义后求表达式：规则理解，逻辑陷阱18、【Hard】模拟自动化测试工具点击界面：数据源理解，页面迷宫寻路，界面跳转关系记忆19、【Hard】压缩算法模拟：规则理解，文本计算20、【Hard】按提示猜单词：利用规则推导，排除干扰打分规则：1、每道题有至少1个得分点，回答每正确一点即得1分。最终得分是得分除以得分点总数，再乘以10。（即每道题满分10分）2、要求推导过程必须正确，猜对的答案不得分。3、要求回答必须完全符合题目要求，如果明确要求不写代码，而回答通过编程求解，即使正确，也记0分。#4 成绩解析1）这份成绩要怎么阅读？原始分数来自每道题得分相加，现在是20道题，每题满分10分。总分就是200分。而百分值是将总分转换为100满分。考虑到未来为了应对模型能力提升，题库还会持续增加，总分会超过200，而百分值将保持跨版本的可比较性。因此建议以百分制成绩为主要参考。2）为什么成绩和体验有差异？正如前文所述，模型的实际体验取决于大量工程优化工作，不但是模型本身素质。一个逻辑能力较差的模型，如果支持联网搜索，也能进行较好的内容归纳，用户如果不深究细节，大致可以满足需求。并且一些模型对特定题型有针对性训练，在命中的前提下，也能产出较为完美的回答。比如讯飞有大量初高中学科题目训练，豆包有大量古文训练，等。但在条件允许的情况下，优先选择逻辑能力高的模型来使用。3）为什么不公布题目细节？许多读者经常评论和私信问这个问题，笔者看法是，任何测评都只代表某种倾向下的成绩，不存在一个完全无偏且普适的题库。笔者最初是有感于早期自媒体使用各种无脑问题进行粗暴评测，甚至借此嘲笑某些国产模型，然后逐渐按自己对大模型的理解，增删优化题目。但也做不到完全客观。因此题目本身并没有意义，笔者希望提供一种评测和打分思路，给到读者参考。如果读者能活学活用，通过合理测试找到适合自己的模型，则是幸甚。
626223328,024c49f6dbc114a7a80ba94ba7e996ad,AIGC LLM从业者,https://api.zhihu.com/people/024c49f6dbc114a7a80ba94ba7e996ad,people,,大模型，真的有未来吗？,article,https://api.zhihu.com/articles/626223328,基于已有的<em>大模型</em>参数，用自己的物料做一些训练，可以弄出来一个比较不错的文本模型或者其他类型模型，这个对个人（有点技术）来说是比较有优势的 三、未来发展 <em>大模型</em>火之前是比特币、VR、web3、元宇宙,3,1,7,1682965416,1683037628,业界最近比较热门的应该就是大模型这个事情了，从openai发布chatgpt为开始，国内外众多公司~名人都有相应的反映。这真的是一件很有价值的事情吗，今天博主就来聊聊自己的看法文章从这样几个点出发看下，一是业界公司的一些消息，二是大模型对普通公司、个人来说可以干一些啥，三是谈谈未来情况一、业界消息最开始是openai开源gpt3.5 4，事例中最亮点的就是它可以喝人类聊天，还可以做很多事情，比如文章摘要、问题回答等。接下来是微软把这个应用到浏览器中，用来方便人类进行检索，但是这个只能实际覆盖一部分人群，因为大家在进行检索的时候很多情况需要真实、完整，这个是大模型做不到百分百的，所以替代应该是不太可能的，但是可以作为一个功能上线事情从去年年末开始发展到现在，国外有些人提出，应该禁止大模型再继续研究，因为担心人类不能控制住这些人工智能模型，这个点就考虑的有些早了，因为现在研究深度还很低，可以在研究的同时考虑下这个问题，不用因为这个，所以那个再来看看国内的一些动向，首先是百度的文言一心，在开发布会的时候是录播，这个就有点对标不上国内技术型公司的态度了，不过应该也是为了发布会直播不翻车，可以理解。接下来是阿里的通义千问，也宣布阿里的所有业务都将接入大模型。国内的这些跟风实际上完全没有必要，这个技能虽然在业界看起来很先进，可是不是这样用的，真的用来解决些实际问题才好，不要跟风做这个做那个。还有最近的大模型可以运行在手机端，电脑又不是多稀罕的东西，好好的做研究就是了，不用玩这些花里胡哨的才是。任何技术都应该建立~第一性原则，也就是最直接、解决问题才对博主这个专栏记录”沉浸式推荐算法“工作经验，博主先后做过推荐算法、搜索广告算法，现在从事”沉浸式推荐算法“ 也为咱们程序员做了一个专属的小程序：猿媛-程序员一站式服务，还没正式上线（也可以试着搜索，后面哪天 说不定就突然上线了），在开发内测中 主要有技术博客、程序员行业短视频、行业热点、你的朋友圈、it公众圈、每个细分行业的论坛、 解决我们的老大难相亲问题、各大互联网公司的招聘、我们it人的专属好物...这些功能， 想要一起来做的加微信（微信名：ranksearch，或者扫下面的码），最好是加微信群， 大家一起商量来做（加微信拉你，或者扫下面的码），开发、产品设计、运营推广都可以。 博主这些自己都会，所以先开发上线+运营着，但是每个方向要是能有更专业的人来做肯定更好， 希望真的爱咱们这个行业的同事们能加入我们，一起做一些一个人做不到的事情！二、实际应用百度的文言一心刚出来的时候，宣布和很多企业进行合作，目的也就是这个。阿里首先在钉钉里接入了大模型，现在还有一些淘宝里面的业务也在接入这个，做成用户说出自己的想法，然后淘宝给出来合适的商品，这个算是个好功能吧，但是只能适应小部分的用户，因为购物时一般都是要左比右比才能最终决定，这个要是放在主打低价格的商品可能合适一些对个人而言，一些做技术的，但是对文本处理不熟悉的，可以用这个比如文本关键词提取之类。还可以用这个做一些攻略，作为一些参考。还可以用来做文章中心思想提取基于已有的大模型参数，用自己的物料做一些训练，可以弄出来一个比较不错的文本模型或者其他类型模型，这个对个人（有点技术）来说是比较有优势的三、未来发展大模型火之前是比特币、VR、web3、元宇宙，每个都是昙花一现，他们有一个共同的点，就是很虚。看看过去成功的例子：电脑、互联网、电商、短视频，每一个都是实实在在能有产出的东西，如果大模型不能像这些这样，那么必然会被下一个热点所代替，从现在来看，大半年了，没有，所以未来应该没有啥发展，热度也就慢慢过去
642846310,f5911fddc7fa5fd74a80d5ce2c12e1a2,Naiyan Wang,https://api.zhihu.com/people/f5911fddc7fa5fd74a80d5ce2c12e1a2,people,"CS PhD, 机器学习与计算机视觉",关于大模型和自动驾驶的几个迷思,article,https://api.zhihu.com/articles/642846310,就会发现其实现在很多所谓的<em>大模型</em>，都不能叫做大模型。去讨论这样的模型到底能做什么，边界在哪里自然就是一个没有意义的事情。如果能理解这点，很多所谓的行业<em>大模型</em>本身就会成为一个伪命题,899,95,772,1689076663,1689076663,23年的AI绕不开的问题就是大模型，最近私下以及在几个workshop上反复和各种同学讨论相关的问题。借这个机会也整理一下几点个人的看法和大家分享讨论~Q1: 究竟什么是大模型？参数量或计算量大就叫大模型吗？Large Model or Foundation Model？A1: 其实在讨论所有关于大模型的问题之前，都需要明确这个问题。大模型（Large Model）这个名词其实是一个非常误解的名字，其实更准确的应该是基石模型（Foundation Model）。经常发现和很多人聊了很久，观点针锋相对，最后发现双方根本讨论的不是一个东西。。。就目前而言，我认为的大模型至少要有两个要素：1) 跨任务的通用性 2) 跨域的通用性。参数量和计算量的scale up只是大模型的一个必要而不充分条件。以GPT为代表的NLP大模型，通过prompt来统一了不同任务，也通过利用了基本上所有可用的文本数据将所有NLP问题变成了域内问题。如果我们以这个标准来讨论，就会发现其实现在很多所谓的大模型，都不能叫做大模型。去讨论这样的模型到底能做什么，边界在哪里自然就是一个没有意义的事情。如果能理解这点，很多所谓的行业大模型本身就会成为一个伪命题，只能叫做通用大模型在垂直行业的一个应用（当然这件事情本身很多时候也是有商业价值的）。而反过来讲，想只靠某些垂直领域的数据去训练一个所谓的行业大模型不如叫做小模型或者专业模型变大，这和大家在讨论的Foundation Model就是完全不同的两个东西。Q2: 目前视觉领域存在这样的大模型吗？如果没有可能的原因是什么？A2: 很遗憾，虽然这方面已经有了很多如CLIP或DINO之类的尝试和进展，但是目前对于CV而言，还远未到达ChatGPT时刻。究其原因，本质还是视觉信息与语言信息这两者在信息密度上的巨大差异。语言只需要考虑一维上（前后文）的context，然而视觉需要考虑在三维（2D图像的x y和时间）的context。这会使得context信息密度的急剧下降，对于需要的数据量和计算量有指数级的上升。说到底，语言毕竟还是人的逻辑思考的产物，而图像或者视频则完全是自然的展现。具体到技术层面上来说，也有若干没有确定答案的open问题：什么是最合适的监督形式？年初SAM带火了一波视觉大模型，但其实本质上仍然是监督学习的范式。做的事情也是在工程上推到极致，通过这样的方式继续scale up的难度是非常大的。而现在视觉自监督最常见的两种范式Contrastive Learning和Masked Autoencoder也遇到了继续scale的难题：似乎视觉模型随着参数量和计算量的增加并没有出现语言模型的Grokking现象，甚至随着规模的scale up性能出现了一定程度的饱和。这和在NLP里面的大力出奇迹的表现非常不同，究竟是规模还不够大，还是因为这样的监督方式就有问题，目前还不得而知。多任务的表示形式怎么统一？这一点其实是在一次的panel上 @代季峰 老师提到的。在NLP中我们可以通过prompt的形式把基本上所有的NLP任务统一成text to text的形式，但是CV中呢？naive地类比成image to image translation显然会存在很多的问题。没有统一的表示使得多任务很难共享同样的网络，仍然需要单独训练不同任务的head，使得模型的泛化性大大降低。不敢妄言视觉大模型正确的思路是什么，但是可以分享一些我认为的视觉大模型该考虑到的一些点或者说具有的一些性质：以自监督为范式，时序和3D为基础，能预测物理世界。第一点是scale up的基础，无需多言。第二三点其实我认为是目前所有视觉预训练模型都没有解决的一个核心问题，也是打通low level vision与high level vision的一个关键的桥梁。现有的视觉预训练数据基本都来自于单帧的web data，这和在自然物理世界中会遇到的数据分布其实是有巨大的差异的。而CV和NLP我认为最大的区别就在于，CV的大模型最终极的目标是能和这个物理世界产生互动，能够理解这个物理世界中的常识。为了达到这个目标，只使用单帧的web data是显然不行的。在这个方向上，SFMLearner是一个很好的尝试，解决了low level vision自监督中的一些问题，但是仍然没有向上去解决high level的语义问题。希望在这个角度上，能看到更多有意义的尝试。Q3: 现在有很多大语言模型在机器人领域的尝试，这是否意味着传统的规划控制会被全数据驱动的方法取代？A3: Of course no! 以有限的对LLM for robotics的了解，现在的工作大致可以分为两类：1) 用LLM作为一个更好的环境理解与人机交互的接口。 2) 用LLM + Neural Symbolic的方法去做task planning。在这两个方向上确实LLM取得了很好的进展，但是这就是规划和控制的全部吗？分享一些我能看到的问题：1) 更底层的运动控制与规划是一个被well solved的问题吗？目前还没看到有什么证据表明LLM可以直接去控制一些复杂的被控目标。和执行器打交道的更底层控制，仍然需要对被控对象进行精细的建模。我也不相信Boston Dynamic这样的公司真的会去用大模型去解决这些复杂的底层控制问题。2) 现在看到的应用中基本上都是对于静态环境与静态的agent交互。对于更复杂的POMDP，目前并没看到有什么相关的尝试。这恰恰算是在规划控制问题中一类常见的问题，例如无人车的规划，竞技类的棋牌游戏亦或是星际争霸。这样的一些任务中，状态空间巨大，也需要对不确定性进行很好的建模。目前也没看到LLM能够通过一些prompt的形式实现这样的功能。所以总结一下，LLM for robotics确实有很多有价值的应用，尤其是在任务理解和规划层面，但是想完全取代也是不现实的。Q4: 端到端自动驾驶算不算大模型？都拿了CVPR best paper了，你们L4为什么不跟进？A4: Well，如果以Q1中的定义的话确实不算。除了实际的一些工程问题之外，其实在L4中不去这样做的一个核心问题还是在于可靠性。在很多场合我都表达过追求全系统的端到端对于L2来说是可行的，但是对于L4是不适合的。L2系统很多时候可以在效率与安全之间tradeoff，但是L4系统的设计目标则是在保证worst case安全的前提下来优化效率。显然，任何一个纯数据驱动的feedforward系统是不会有这样保证的。为了达到这样的安全目标，我们必然要去在系统设计层面设计足够多的算法冗余，这就会打破端到端的模式。更详细的阐述见我之前写的这篇文章：Naiyan Wang：海量数据就是高级别自动驾驶的银弹吗？最后，为了防杠，想补充的一点是，上下游协同设计和优化的这个思想我认为是非常正确和有意义的，只不过在部署端一定会存在模块化的冗余。在这一点上，还是有很多有意义的方向可以尝试，除了常见一些模块之间的表示与接口有很大的优化空间，就算在端到端方向也有一些有意义的问题：第一，怎样把一些行为有保障的planner（一般会是一个非凸的优化问题） formulate成对输入可导的形式，从而指导上游模块的学习？这样系统就不是一个纯feedforward了，从而可以加入各种约束和保证。我们之前在多目标追踪方向上做过这样的一些尝试，但是这个问题中的优化形式相比于planner还是过于toy了。 第二，如何评测一个端到端训练的系统输出以便可以比较？在端到端的系统中，传统的感知评测的metric都已经不再适用。期待在这些问题上能有一些突破性的进展。Q5: 那大模型在自动驾驶中可能有哪些应用？有没有实际落地的场景呢？A5: 在车端而言，我确实没看到有什么去做部署的可能。一方面是实际算力的问题，一方面是并没看到一个特别清晰的杀手锏应用，也就是说没看到什么现在车端小模型一定解决不了但是大模型可以解决的问题。换句话说，如果给一台8卡A100在车上跑，L4自动驾驶就做出来了吗？答案显然不是的。但是在离线场景中，确实大有可为。这面最典型的应用可能就是数据集和corner case的检索与扩充。这个在 @王井东 老师在Apollo Day上有详细的介绍，我们做过一些类似的尝试，也取得了很不错的效果。除此之外，在预标注和自动标注方向也有很多的应用，也可以用更直接的model distill的方法来辅助在线模型的训练。总结下来，核心思想都是通过无论是数据还是特征的方式，将大模型更强的表征能力赋予在线的模型。也期待后续能有一些更新的范式出现，解决在线场景中的一些棘手的问题。但所有这些的前提都是大模型真的可以可靠解决小模型解决不了的问题。在2023.7这个时间点来mark一下现在的认知，可能随着技术的发展有很多东西都要被打脸，也欢迎大家评论区理性讨论，观点碰撞~
634608422,d77294bc17efbda6dad15cb2ebf64cf7,晴天,https://api.zhihu.com/people/d77294bc17efbda6dad15cb2ebf64cf7,people,深度学习，NLP算法，产品经理，文艺青年,CLiB中文大模型能力评测榜单（持续更新）,article,https://api.zhihu.com/articles/634608422,特别是对于不具备<em>大模型</em>研发能力的公司，熟悉大模型的技术边界、高效有针对性地做大模型技术选型，在现如今显得尤为重要。 而一个公开、公正、公平的<em>大模型</em>评测系统，恰好能够提供应有的助力,361,111,687,1685882165,1713192775,"目前已囊括77个大模型，覆盖chatgpt、gpt4、谷歌bard、百度文心一言、阿里通义千问、讯飞星火、360智脑、商汤senseChat、微软new-bing、minimax等商用模型， 以及百川、qwen、belle、chatglm6b、tigerbot、ziya、openbuddy、Phoenix、linly、MOSS、AquilaChat、vicuna、wizardLM、书生internLM、llama2-chat等开源大模型。模型来源涉及国内外大厂、大模型创业公司、高校研究机构。支持多维度能力评测，包括分类能力、信息抽取能力、阅读理解能力、数据分析能力、中文编码效率。不仅提供能力评分排行榜，也提供所有模型的原始输出结果！有兴趣的朋友可以自己打分、自己排行！ 最近更新[2024/4/13] 发布v1.16版本评测榜单新增中文编码效率排行榜，同等尺寸大模型，编码效率越高推理速度越快，几乎成正比。模型更新：minimax更新至minimax-abab6-chat新增3个大模型：Qwen1.5-32B-Chat、minimax-abab5.5-chat、minimax-abab5.5s-chat[2024/3/20] 发布v1.15版本评测榜单模型更新：gpt3.5更新至最新版本新增8个大模型：gpt-4-turbo、讯飞星火v3.5、MiniCPM-2B-dpo、miniCPM-2B-sft、AquilaChat2-70B-Expr、月之暗面kimichat、谷歌gemma-7b-it、谷歌gemma-2b-it排行榜删除陈旧的模型（比如Baichuan2-53B、chatglm-130b-v1、tulu-30b、belle-llama-13b-2m、belle-llama-13b-ext、openbuddy-llama-30b-v7.1、vicuna-33b等）[2024/2/28] 发布v1.14版本评测榜单新增11个大模型：deepseek-llm-67b-chat、baichuan3、internlm2-chat-20b、internlm2-chat-7b、openbuddy-mixtral-7bx8-v17.1以及qwen1.5系列的6个模型排行榜删除陈旧的模型（比如chatglm2-6b、AquilaChat-7B等）[2024/1/29] 发布v1.13版本评测榜单模型更新：微软new-bing、文心4.0更新至24年1月版本新增6个大模型：qwen-max、GLM4、BlueLM-7B-Chat、openbuddy-zephyr-7b-v14.1、openbuddy-deepseek-67b-v15.2、XVERSE-65B-Chat排行榜删除陈旧的模型（比如phoenix-inst-chat-7b、BELLE-on-Open-Datasets等）[2023/12/10] 发布v1.12版本评测榜单新增7个大模型：Yi-34B-Chat、tigerbot-13b-chat-v4、openbuddy-openllama-3b-v10、Qwen-1_8B-Chat、Yi-6B-Chat、Qwen-72B-Chat、chatglm-turbo新增开源模型细分排行榜：10B以下模型排行榜、10B~20B模型排行榜、20B以上模型排行榜[2023/11/22] 发布v1.11版本评测榜单新增4个大模型：openbuddy-mistral-7b-v13.1、Qwen-7B-Chat、Baichuan2-7B-Chat、tigerbot-70b-chat-v3将数据分析能力计入综合得分[2023/11/5] 发布v1.10版本评测榜单新增6个大模型：3个商用模型：文心4.0、谷歌bard、讯飞星火v33个开源模型：aquilachat2-34b、ziya2-13b-chat、chatglm3-6b排行榜删除陈旧的模型（比如第一代chatglm-6b、MOSS等）[2023/10/11] 发布v1.9版本评测榜单新增7个大模型：3个商用模型：阿里通义千问v1.0.7、字节跳动豆包、Baichuan2-53B4个开源模型：Baichuan2-13B-Chat、internlm-chat-20b、qwen-14b-chat、tigerbot-70b-chat-v2[2023/9/13] 发布v1.8版本评测榜单新增7个大模型：2个商用模型：chatglm-std、chatglm-pro5个开源模型：openbuddy-llama-30b-v7.1、openbuddy-llama-65b-v8、openbuddy-llama2-70b-v10.1、xverse-13b-chat、Baichuan-13B-Chat-v2[2023/8/29] 发布v1.7版本评测榜单新增2个商用大模型：讯飞星火v2.0、Baichuan-53B表格问答（数据分析）能力排行榜：新增21个模型参与排行。[2023/8/13] 发布v1.6版本评测榜单新增4个大模型：2个商用模型：商汤senseChat、微软new-bing；2个基于LLaMA2的开源中文模型：BELLE-Llama2-13B-chat-0.4M、Linly-Chinese-LLaMA2-13B[2023/7/26] 发布v1.5版本评测榜单新增7个大模型：gpt4、文心一言v2.2、vicuna-33b、wizardlm-13b、Ziya-LLaMA-13B-v1.1、InternLM-Chat-7B、Llama-2-70b-chat[2023/7/18] 发布v1.4版本评测榜单新增3个大模型：tulu-30b、chatglm2-6b、Baichuan-13B-Chat[2023/7/2] 发布v1.3版本评测榜单新增3个大模型：360智脑、MOSS-003-SFT、AquilaChat-7B讯飞星火更新为最新的v1.5模型[2023/6/17] 发布v1.2版本评测榜单新增2个大模型：tigetbot-7b官网、linly-chatflow-13b说明做评测榜单的初衷[2023/6/10] 发布v1.1版本评测榜单新增3个大模型：minimax、guanaco、Phoenix-7b新增表格问答评测维度，作为阅读理解能力的细分项[2023/6/4] 发布v1版本评测榜单⚓TODO将更多大模型加入评测：Claude、falcon等等增加开源大模型的授权协议，注明能否商用引入更多维度的评测：数学能力、代码能力、开放域问答、多轮对话、头脑风暴、翻译……评测维度更细分，比如信息抽取可以细分时间实体抽取能力、地址实体抽取能力……海纳百川，整合各类评测榜单，扩充细分领域榜单（比如教育领域、医疗领域）加入更多评测数据，使得评测得分越来越有说服力 大模型基本信息由于大模型较多，下表只展示部分大模型的信息，更多更详细的信息，见https://github.com/jeinlee1991/chinese-llm-benchmark大模型机构类别链接chatgpt-3.5openai商用https://chat.openai.com文心一言百度商用https://yiyan.baidu.comchatglm官方智谱AI商用https://chatglm.cn讯飞星火科大讯飞商用https://xinghuo.xfyun.cn/desk阿里通义千问阿里巴巴商用https://tongyi.aliyun.com…………………… 排行榜1 综合能力排行榜综合能力得分为分类能力、信息抽取能力、阅读理解能力、数据分析能力四者得分的平均值。具体数据见https://github.com/jeinlee1991/chinese-llm-benchmark(1) 10B以下大模型排行榜(2) 10B~20B大模型排行榜(3) 30B以上大模型排行榜2 分类能力排行榜具体数据见https://github.com/jeinlee1991/chinese-llm-benchmark3 信息抽取能力排行榜具体数据见https://github.com/jeinlee1991/chinese-llm-benchmark4 阅读理解能力排行榜具体数据见https://github.com/jeinlee1991/chinese-llm-benchmark5 表格问答排行榜（数据分析）专门考查大模型对表格的理解分析能力，常用于数据分析。具体数据见https://github.com/jeinlee1991/chinese-llm-benchmark6、中文编码效率排行榜暂不计入综合能力评分。 专门考查大模型编码中文字符的效率，同等尺寸大模型，编码效率越高推理速度越快，几乎成正比。 中文编码效率相当于大模型生成的每个token解码后对应的中文平均字数 （大模型每次生成一个token，然后解码成真正可见的字符，比如中文、英文、标点符号等）。 各项能力评分评分方法：从各个维度给大模型打分，每个维度都对应一个评测数据集，包含若干道题。 每道题依据大模型回复质量给1~5分，将评测集内所有题的得分累加并归一化为100分制，即作为最终得分。具体数据见https://github.com/jeinlee1991/chinese-llm-benchmark⚖️原始评测数据见本项目github：GitHub - jeinlee1991/chinese-llm-benchmark: 中文大模型能力评测榜单：覆盖百度文心一言、chatgpt、阿里通义千问、讯飞星火、belle / chatglm6b 等开源大模型，多维度能力评测。不仅提供能力评分排行榜，也提供所有模型的原始输出结果！评测样本示例分类评测样本举例请分类以下5种水果：香蕉、西瓜、苹果、草莓、葡萄。将下列单词按词性分类。狗，追，跑，大人，高兴，树将下列五个词分为两个组别，每个组别都有一个共同点：狗、猫、鸟、鱼、蛇。给定一组文本，将文本分成正面和负面情感。举例文本:这部电影非常出色，值得推荐。我觉得导演做得很好。这场音乐会真是个灾难，我非常失望。将以下10个单词分类为动物或植物。树木、狮子、玫瑰、草地、松鼠、猴子、蘑菇、兔子、山羊、香蕉……信息抽取评测样本举例HR: 你好，我是XYZ公司的招聘主管。我很高兴地通知你，你已经通过了我们的初步筛选，并且我们希望邀请你来参加面试。候选人：非常感谢，我很高兴收到你们的邀请。请问面试的时间和地点是什么时候和哪里呢？HR: 面试的时间是下周二上午10点，地点是我们公司位于市中心的办公室。你会在面试前收到一封详细的面试通知邮件，里面会包含面试官的名字、面试时间和地址等信息。候选人：好的，我会准时出席面试的。请问需要我做哪些准备工作呢？HR: 在面试前，请确保你已经仔细研究了我们公司的业务和文化，并准备好了相关的问题和回答。另外，请务必提前到达面试现场，以便有足够的时间了解我们的公司和环境。候选人：明白了，我会尽最大努力准备好的。非常感谢你的邀请，期待能有机会加入贵公司。HR: 很高兴能和你通话，我们也期待着能和你见面。祝你好运，并期待下周能见到你。基于以上对话，抽取出其中的时间、地点和事件。给定以下文本段落，提取其中的关键信息。今天早上，纽约市长在新闻发布会上宣布了新的计划，旨在减少治安问题。该计划包括增加派遣警察的人数，以及启动社区倡议，以提高居民对警察工作的支持度。在给定的短文中找出三个关键词。西方的哲学历史可上溯至古希腊时期，最重要的哲学流派包括柏拉图学派、亚里士多德学派和斯多葛学派。从以下诗句中提取人物名称：两个黄鹂鸣翠柳，一行白鹭上青天。明天天气怎么样？广州明天最冷多少度？广东大后天最暖多少度？北京冷不冷？提取出上述句子中的地理位置实体……阅读理解评测样本举例牙医：好的，让我们看看你的牙齿。从你的描述和我们的检查结果来看，你可能有一些牙齦疾病，导致牙齿的神经受到刺激，引起了敏感。此外，这些黑色斑点可能是蛀牙。病人：哦，真的吗？那我该怎么办？牙医：别担心，我们可以为你制定一个治疗计划。我们需要首先治疗牙龈疾病，然后清除蛀牙并填充牙洞。在此过程中，我们将确保您感到舒适，并使用先进的技术和材料来实现最佳效果。病人：好的，谢谢您，医生。那么我什么时候可以开始治疗？牙医：让我们为您安排一个约会。您的治疗将在两天后开始。在此期间，请继续刷牙，使用牙线，并避免吃过于甜腻和酸性的食物和饮料。病人：好的，我会的。再次感谢您，医生。牙医：不用谢，我们会尽最大的努力帮助您恢复健康的牙齿。基于以上对话回答：病人在检查中发现的牙齿问题有哪些？文化艺术报讯 国务院办公厅发布关于2023年部分节假日安排的通知，具体内容如下：元旦：2022年12月31日至2023年1月2日放假调休，共3天。春节：1月21日至27日放假调休，共7天。1月28日（星期六）、1月29日（星期日）上班。清明节：4月5日放假，共1天。劳动节：4月29日至5月3日放假调休，共5天。4月23日（星期日）、5月6日（星期六）上班。端午节：6月22日至24日放假调休，共3天。6月25日（星期日）上班。中秋节、国庆节：9月29日至10月6日放假调休，共8天。10月7日（星期六）、10月8日（星期日）上班。基于以上信息回答：2023年五一假期怎么放假。基于以下表格，请问张三的考勤情况员工姓名,日期,上班时间,下班时间,是否迟到,是否早退,是否请假张三,1月1日,8:30,17:30,否,否,否李四,1月1日,9:00,18:00,是,否,否王五,1月1日,8:00,16:30,否,是,否赵六,1月1日,8:30,17:00,否,否,是张三,1月2日,8:00,17:00,否,否,否李四,1月2日,8:30,17:30,否,否,否王五,1月2日,9:00,18:00,是,否,否赵六,1月2日,8:30,17:00,否,否,是对于给定的问答对，判断问题是否被正确回答问题：地球是第几颗行星？答案：地球是第三颗行星。根据以下文本回答问题：什么是能量守恒定律？物理学是自然科学的一个重要分支，研究自然界的物质、能量、空间和时间等基本概念。在物理学的研究中，科学家们探索了宇宙的奥秘，从微观粒子到宏观天体的各个方面都进行了深入研究。在物理学中，一个重要的概念是能量守恒定律。能量守恒定律指出，一个孤立系统的总能量在任何时间都是不变的。也就是说，能量不能被创造或摧毁，只能被转化成其他形式。例如，当一个物体被提起并释放时，它的势能被转化为动能，使得物体开始运动。……大模型输出结果示例prompt输入chatgpt3.5输出将下列单词按词性分类。狗，追，跑，大人，高兴，树好的，以下是这些单词按照词性分类后的结果：名词：狗、大人、树动词：追、跑形容词：高兴从以下诗句中提取人物名称：两个黄鹂鸣翠柳，一行白鹭上青天。这句诗出自唐代诗人杜甫的《绝句》，并没有明确提到人物名称。这句诗主要是在描绘自然景色，通过形象生动的语言来表现春天的美丽景象。诗中提到“两个黄鹂”，“一行白鹭”，他们是动物而非人物名称。黄鹂是一种鸟类，它们身体鲜黄色，嘴巴呈钩形，叫声清脆悦耳；白鹭是另一种水鸟，身体白色，嘴巴细长，善于捕捉鱼类等。杨婉这句诗意境深远，很好地表现了大自然的美丽和神奇。…………为什么做榜单？大模型百花齐放，也参差不齐。不少媒体的宣传往往夸大其词，避重就轻，容易混淆视听；而某些公司为了PR，也过分标榜自己大模型的能力，动不动就“达到chatgpt水平”，动不动就“国内第一”。 所谓“外行看热闹，内行看门道”，业界急需一股气流，摒弃浮躁，静下心来打磨前沿技术，真真正正用技术实力说话。这就少不了一个公开、公正、公平的大模型评测系统，把各类大模型的优点、不足一一展示出来。 如此，大家既能把握当下的发展水平、与国外顶尖技术的差距，也能更加清晰地看明白未来的努力方向，而不被资本热潮、舆论热潮所裹挟。对于产业界来说，特别是对于不具备大模型研发能力的公司，熟悉大模型的技术边界、高效有针对性地做大模型技术选型，在现如今显得尤为重要。 而一个公开、公正、公平的大模型评测系统，恰好能够提供应有的助力，避免重复造轮子，避免因技术栈不同而导致不必要的争论，避免“鸡同鸭讲”。对于大模型研发人员，包括对大模型技术感兴趣的人、学术界看中实践的人，各类大模型的效果对比，反应出了背后不同技术路线、技术方法的有效性，这就提供了非常好的参考意义。 不同大模型的相互参考、借鉴，帮忙大家躲过不必要的坑、避免重复实验带来的资源浪费，有助于整个大模型生态圈的良性高效发展。"
629087587,6ab5d9817bdcadd5eeaff0ccba56c61e,绝密伏击,https://api.zhihu.com/people/6ab5d9817bdcadd5eeaff0ccba56c61e,people,《推荐系统技术原理与实践》作者，欢迎知友京东购买。,大模型思维链（Chain-of-Thought）技术原理,article,https://api.zhihu.com/articles/629087587,CoT）很自然地被提出了。 大模型“涌现”的思维链 最近 AI 大厂的开发人员和高校的 NLP 研究人员，都在琢磨，怎么让<em>大模型</em>“涌现”。 所谓“涌现”，在大模型领域指的是当模型突破某个规模时,976,40,2252,1683983671,1704975891,"背景2021年，提示学习（prompt learning）浪潮兴起，而早在2020年，OpenAI 就在论文 Language Models are Few-Shot Learners 中提出了如何使用 prompt learning 提升大模型的推理能力。论文中提出了 Zero-shot、One-shot、Few-shot 三种不同的 prompt 方法，如下图所示。图1: zero-shot、one-shot、few-shot和微调方法的对比Few-Shot（FS）是指模型在推理时给予少量样本，但不允许进行权重更新。对于一个典型数据集，Few-shot 有上下文和样例（例如英语句子和它的法语翻译）。Few-shot 的工作方式是提供 K 个样本，然后期望模型生成对应的结果。通常将 K 设置在 10 到 100 的范围内，因为这是可以适应模型上下文窗口的示例数量（nctx = 2048）。Few-shot 的主要优点是大幅度降低了对特定任务数据的需求，并减少了从微调数据集中学习过度狭窄分布。主要缺点是该方法的结果迄今为止远不如最先进的微调模型。此外，仍需要一小部分特定任务的数据。One-Shot（1S）与 Few-Shot 类似，只允许一个样本（除了任务的自然语言描述外）。将 One-Shot 与 Few-Shot、Zero-Shot 区分开的原因是它最接近某些任务与人类沟通的方式。相比之下，如果没有示例，有时很难传达任务的内容或格式。Zero-Shot（0S）和 One-shot 类似，但不允许提供样本，只给出描述任务的自然语言指令。该方法提供了最大的方便性、稳健性以及避免虚假相关的可能性，但也是最具挑战性的设置。在某些情况下，即使是人类，在没有例子的情况下，也可能难以理解任务的格式。例如，如果要求某人“制作一张关于200米冲刺世界纪录的表格”，这个请求可能是模棱两可的，因为可能不清楚表格应该具有什么格式或包含什么内容。然而，至少在某些情况下，Zero-shot 是最接近人类执行任务的方法，例如图 1 中的翻译示例，人类可能仅凭文本指令就知道该做什么。但是，即使是 Few-Shot，这种方法还是有比较大的缺陷的。如果你的问题相对简单，不需要什么逻辑推理，可能靠大模型背答案就能做得不错，但是对于一些需要推理的问题，都不用太难，就一些简单的算术应用题，大模型就大概率不太 work。于是，思维链（Chain-of-Thought，CoT）很自然地被提出了。大模型“涌现”的思维链最近 AI 大厂的开发人员和高校的 NLP 研究人员，都在琢磨，怎么让大模型“涌现”。所谓“涌现”，在大模型领域指的是当模型突破某个规模时，性能显著提升，表现出让人惊艳、意想不到的能力。比如语言理解能力、生成能力、逻辑推理能力等。一般来说，模型在 100亿（10B） 到 1000亿（100B） 参数区间，可能产生能力涌现。但老话说得好“氪不救非，玄不改命”。靠砸钱和运气，只一味把模型做的大大大，也未必能让AI“显灵”。强大的逻辑推理是大语言模型“智能涌现”出的核心能力之一，好像AI有了人的意识一样。而推理能力的关键，在于一个技术——思维链（Chain of Thought，CoT）。大家如果看过类 GPT 应用的翻车问题，会发现大多都是数学算术题、逻辑思考题等，这类需要精确推理的问题，而这正是思维链能够重点解决的。现在训练大语言模型的企业和机构很多，但能够训练出思维链并应用的很少。换句话说，只有解锁了思维链技术，大语言模型才有可能“涌现”，才能在“大炼模型”的竞争中具备能力优势。而本篇文章就是为了介绍 CoT 这一重要技术，如何才能让大模型具备真正的智慧。1. 思维链概念的开山之作——Chain-of-Thought Prompting1.1 神奇男子——Jason Wei思维链的故事，我们从一个奇男子说起。一个神奇的男子思维链，在人工智能领域，是一个非常非常新的概念。2022 年 1 月，它的相关论文才被放到 arxiv 上，成果也特别惊艳，谷歌在当年五月的年度开发者大会 Google I/O 2022，也对思维链这一研究成果进行了宣传。当时同台宣传的还有大模型 PaLM 和 Pixel 系列手机等。你可能发现了华点，怎么让思维链闻名世界的，却变成了 OpenAI 的 ChatGPT 呢？这就要提到一个奇男子——思维链的提出者 Jason Wei。之所以神奇，一是本人能力卓绝。这位华人科学家，2020 年本科毕业成为谷歌大脑的高级研究员，在任职期间，提出了思维链的概念，发现思维链可以在大语言模型中增强推理能力。图1.1：Jason Wei的个人博客www.jasonwei.net二是他的个人际遇，对 AI 影响很大，2022 年 2 月他离开谷歌，加入了 OpenAI，进入 ChatGPT 团队，这也是思维链在 OpenAI 发扬光大，让 ChatGPT 拔得头筹的原因之一。那这位奇男子和同事的工作，究竟干了什么呢？Chain-of-Thought Prompting Elicits Reasoning in Large Language Models没错，就是这篇思维链的开山之作。谷歌之前在大模型下了很大功夫，GPT 生成式预训练模型中的“T”，也就是 Transformer，就是谷歌大脑搞出来的。但是，预训练 + 精调的大模型搞了几年，仍然没办法很好地完成多步骤推理任务，比如数学问题和常识推理。所以 Jason Wei 等人提出了思维链提示的方法，真的一下子就让大模型的逻辑推理能力不一样了。具体来说，有三个不一样：常识推理能力赶超人类。以前的语言模型，在很多挑战性任务上都达不到人类水平，而采用思维链提示的大语言模型，在 Bench Hard(BBH) 评测基准的 23 个任务中，有 17 个任务的表现都优于人类基线。比如常识推理中会包括对身体和互动的理解，而在运动理解 sports understanding 方面，思维链的表现就超过了运动爱好者（95% vs 84%）。数学逻辑推理大幅提升。一般来说，语言模型在算术推理任务上的表现不太好，而应用了思维链之后，大语言模型的逻辑推理能力突飞猛进。MultiArith 和 GSM8K 这两个数据集，测试的是语言模型解决数学问题的能力，而通过思维链提示，PaLM 这个大语言模型比传统提示学习的性能提高了 300%！在 MultiArith 和 GSM8K 上的表现提升巨大，甚至超过了有监督学习的最优表现。这意味着，大语言模型也可以解决那些需要精确的、分步骤计算的复杂数学问题了。图1.2：数学逻辑推理大幅提升大语言模型更具可解释性，更加可信。我们知道超大规模的无监督深度学习，打造出来的大模型是一个黑盒，推理决策链不可知，这就会让模型结果变得不够可信。而思维链将一个逻辑推理问题，分解成了多个步骤，来一步步进行，这样生成的结果就有着更加清晰的逻辑链路，提供了一定的可解释性，让人知道答案是怎么来的。Jason Wei 这位奇男子提出的思维链，可以说是大语言模型惊艳世界的必要条件。思维链效果如此拉满，那么 Jason Wei 提出的 CoT 到底是一项什么样的技术，接下来介绍下细节。1.2 CoT技术细节在解释何为 CoT 前，不妨来看个论文展示的结果：图1.3：CoT实验效果不难看出，在解数学问题上，同样使用 PaLM 这个 540B 的超级 LLM，CoT 的表现是传统 prompting 的300%以上，甚至超过了此前有监督的最优表现。这看起来很不可思议，然而 CoT 方法却极其简单。CoT 提示过程是一种最近开发的提示方法，它鼓励大语言模型解释其推理过程。思维链的主要思想是通过向大语言模型展示一些少量的 exapmles，在样例中解释推理过程，大语言模型在回答提示时也会显示推理过程。这种推理的解释往往会引导出更准确的结果。以一个数学题为例：图1.4 标准 Prompting可以看到模型无法做出正确的回答。但如果说，我们给模型一些关于解题的思路，就像我们数学考试，都会把解题过程写出来再最终得出答案，不然无法得分。CoT 做的就是这件事，示例如下：图1.5：CoT提示可以看到，类似的算术题，思维链提示会在给出答案之前，还会自动给出推理步骤：“罗杰先有5个球，2盒3个网球等于6个，5 + 6 = 11”“食堂原来有23个苹果，用了20个，23-20=3；又买了6个苹果，3+6=9”可以看出，CoT 在实现上修改了 demonstration 每个 example 的 target，source 保留原样，但 target 从原先的 answer(a) 换成了 rationale(r) + a。因此可以看到右侧，所有内容均由模型生成，模型不是生成 a，而是生成r+a。简单来说，语言模型很难将所有的语义直接转化为一个方程，因为这是一个更加复杂的思考过程，但可以通过中间步骤，来更好地推理问题的每个部分。思维链提示，就是把一个多步骤推理问题，分解成很多个中间步骤，分配给更多的计算量，生成更多的 token，再把这些答案拼接在一起进行求解。论文里面作者提到了很多 CoT 的优势，其中包括它把一个多步推理问题分解出多个中间步骤，并且让 LLM 更加可解释。它能解决的问题很多，除了上述的数学应用题，还有常识推理、以及 symbolic manipulation （符号操作）这类任务（就是一些手造的考验大模型的问题，比如最典型的 Last Letter Concatenation（最后一个字母串联） 和 coin flip（抛硬币）），下面补充几个例子方便理解：图1.6：CoT应用于 Symbolic manipulation（符号操作）的示例2. Zero-shot-CoT零样本思维链（Zero Shot Chain of Thought，Zero-shot-CoT）提示过程是对 CoT prompting 的后续研究，引入了一种非常简单的零样本提示。他们发现，通过在问题的结尾附加“Let's think step by step”这几个词，大语言模型能够生成一个回答问题的思维链。从这个思维链中，他们能够提取更准确的答案。图2.1：Zero-shot-CoT其实 Zero-shot-CoT 是一个 pipeline。也就是说“Let's think step by step”这句话，只是通过这个 prompt 让LLM 尽可能生成一些思考过程，然后再将生成的 rationale（理由） 和 question 拼在一起，重新配合一个answer 指向的 prompt 如“The answer is ”来激励模型生成答案。从技术上讲，完整的零样本思维链（Zero-shot-CoT）过程涉及两个单独的提示/补全结果。在下图中，左侧生成一个思维链，而右侧接收来自第一个提示（包括第一个提示本身）的输出，并从思维链中提取答案。这个第二个提示是一个自我增强的提示。图2.2：Full pipeline of Zero-shot-CoT作者还做了解释，说明这句“Let's think step by step”是经过验证的，比如对比下面的其它的 instruction，尤其那些不相关的和误导的，效果就非常差，说明大模型真的是在理解这句 instruction 的意思。图2.3：其它instruction的效果我们看一下 Zero-shot-CoT 的实验效果：图2.4：Zero-shot-CoT实验效果再看一下在 GPT-3 上的实验效果：图2.5：Zero-shot-CoT在GPT-3上的效果效果拉满，Zero-Shot-CoT 能让 GPT-3 从 17 提升到 78，换到 PaLM 上稍微小点，25 到 66。3. 多数投票提高CoT性能——自洽性（Self-consistency）Self-Consistency Improves Chain of Thought Reasoning in Language Models.这篇文章是 CoT 后很快的一个跟进工作，是 CoT 系列改进的重要一步，在 2022 年 3 月在arxiv上被放出来。这篇文章几乎用的和 CoT 完全一样的数据集和设置，主要改进是对答案进行了多数投票（majority vote），并且发现其可以显著地提高思维链方法的性能。文章提出的方法叫自洽性（Self-consistency），是对 CoT 的一个补充，它不仅仅生成一个思路链，而是生成多个思路链，然后取多数答案作为最终答案。在下面的图中，左侧的提示是使用少样本思维链范例编写的。使用这个提示，独立生成多个思维链，从每个思维链中提取答案，通过“边缘化推理路径”来计算最终答案。实际上，这意味着取多数答案。图3.1：自洽性 (self-consistency)我们看一下实验效果：图3.2：自洽性 (self-consistency)实验效果可以看到相比之前的单路径 CoT（图中的Single-path），Self-Consistency 显著提示了效果，而且随着采样个数的提升（图中的 Sampled Reasoning Paths对应图3.1的“采样不同的推理路径”），效果也不断变好。我们知道 LaMDA-137B 模型中可以通过热度（temperature）和 Top-k 参数控制采样方法，具体的效果如下图所示：图3.3：不同采样方法的效果（LaMDA-137B模型）备注：热度（temperature）：可以控制语言模型输出的随机度。高热度生成更难预料及富有创造性的结果，低热度则更保守。例如热度为 0.5 时模型生成内容将比 1.0 更容易预测且创造性更少。Top-k：即核心采样（nucleus sampling），是另一个控制语言模型输出随机性的超参数配置。它设定了一个概率阈值，并选择累积概率超过该阈值的最佳词汇，然后模型从这组词汇中随机抽取以生成输出。与传统方法（在整个词汇表中随机抽样）相比，这种方法可以产生更丰富多样且有趣的输出。例如 top-k 为 0.9 时模型将仅考虑概率阈值 90% 以上的词汇。4. LtM （Least to Most prompting）提示Least-to-Most Prompting Enables Complex Reasoning in Large Language Models最少到最多提示过程 (Least to Most prompting, LtM) 将思维链提示过程 (CoT prompting) 进一步发展，首先将问题分解为子问题，然后逐个解决。它是受到针对儿童的现实教育策略的启发而发展出的一种技术。与思维链提示过程类似，需要解决的问题被分解成一组建立在彼此之上的子问题。在第二步中，这些子问题被逐个解决。与思维链不同的是，先前子问题的解决方案被输入到提示中，以尝试解决下一个问题。简单来说就是一步一步来，step by step。单纯的 CoT 不足以解决复杂问题，但是我们可以把它分解成一个个小问题，然后再使用 CoT，这样模型就能把问题求解出来。所以从这个角度看，Least-to-Most 和 CoT 不是选择关系，而是可以互相打配合的。具体怎么做呢？其实就是两步走，分别是：分解问题解决问题这么说可能还有些许抽象，下面给一个原文的例子，先看第一阶段：图4.1：LtM 阶段1大多问题都可以定义成“输入-输出”的格式，同样两步走的两个任务也不例外。Stage 1 做的事情就是要求大模型根据输入和提示“To solve {problem}, we need to ”，生成红色的部分，即子问题。换句话说，这个任务的目标就是生成子问题，利用 LLM 和 prompt 去生成。而 Stage 2 做的事情则是利用 Stage 1 得到的子问题和其解题结果拼进 Stage 2 的输入当中，让模型来生成最终答案，如下图所示：图4.2：LtM阶段2可以看到图的上半部分即把问题替换成 Stage 1 的子问题，模型就根据 context 和新问题生成子问题的答案。当然你可能会产生疑问，问题虽然变简单了，但是它可能还是会做错不少，配合后面Stage 2，是不是就错上加错，最后也提升不了多少呢？这时候就可以看到，这一步也是可以用 CoT 来做的，可以看到右上角的框其实把解子问题的解题过程也列出来了。这也是为什么前文提到 Least-to-Most 是可以配合使用的。下一步做法就是合并：context + 子问题 + 子问题解题过程 + 子问题答案 + 最终问题，然后让模型去生成解题过程和正确答案，那么这一步也是用 CoT 来做的。显然这是一个两步走的 pipeline，具体不同任务怎么做这个 pipeline 是有些具体细节需要考虑的。在 SCAN 这个数据集上，Least-to-Most 的表现可以说是让人惊讶，如果使用 GPT-3 的 code-davinci-002，准确率能从 16 提到接近 100%。你甚至可以直接认为这个数据集已经被它直接解决了。方法标准PromptingCoTLeast-to-Mostcode-davinci-00216.716.299.7text-davinci-0026.00.076.0code-davinci-0010.40.060.7另外还有数学题方面也是在原版 CoT 上很有进一步明显提升。方法Non-football (DROP)Football (DROP)GSM8KZero-Shot43.8651.7716.38标准Prompting58.7862.7317.06CoT74.7759.5660.87Least-to-Most82.4573.4262.395. Flan-PaLM/T5：CoT + FinetuningScaling Instruction-Finetuned Language ModelsFlan-T5 是 Google 最新的一篇工作，通过在超大规模的任务上进行微调，让语言模型具备了极强的泛化性能，做到单个模型就可以在 1800 多个 NLP 任务上都能有很好的表现。这意味着模型一旦训练完毕，可以直接在几乎全部的 NLP 任务上直接使用。Flan-T5 是在吸收 Flan 的精华的基础上，加入了 CoT 的数据来做 finetune。PaLM/T5 是 2019 年 Google 发布的一个语言模型。论文的核心贡献是提出一套多任务的微调方案，来极大提升语言模型的泛化性。图5.1：Flan-PaLM/T5示意图这么 Finetune 过后的模型，其实不论在 CoT 任务和非 CoT 任务上其实都表现得很好，而且在 BBH 上做zero-shot 优势更是巨大。这也进一步证明了 CoT 是可以和当前流行的 instruction tuning 无缝衔接的。下面看一下Flan-PaLM/T5的具体做法。图5.2：微调任务(1) 任务收集：工作的第一步是收集一系列有标签的数据，这里一个任务可以被定义成<数据集，任务类型>的形式，比如“基于 SQuAD 数据集的问题生成任务”。需要注意的是这里有 9 个任务是需要进行推理的任务，即Chain-of-thought （CoT）任务。(2) 形式改写：因为需要用单个语言模型来完成超过 1800+ 种不同的任务，所以需要将任务都转换成相同的“输入格式”喂给模型训练，同时这些任务的输出也需要是统一的“输出格式”。图5.3：Flan 输入输出格式如上图所示，根据 “是否需要进行推理 （CoT）” 以及 “是否需要提供示例（Few-shot）” 可将输入输出划分成四种类型：CoT（chain-of-thought） : ❎，few-shot: ❎ （图中左上）输入：指令 + 问题输出：答案CoT（chain-of-thought） : ✅，few-shot: ❎ （图中右上）输入：指令 + CoT 引导（by reasoning step by step） + 问题输出：推理 + 答案CoT（chain-of-thought）: ❎，few-shot: ✅ （图中左下）输入：指令 + 示例问题 + 示例问题答案 + 指令 + 问题输出：答案CoT（chain-of-thought）: ✅，few-shot: ✅（图中右下）输入：指令 + CoT 引导 + 示例问题 + 示例问题推理 + 示例问题答案 + 指令 + CoT 引导 + 问题输出：推理 + 答案(3) 训练过程：采用恒定的学习率以及 Adafactor 优化器进行训练；同时会将多个训练样本“打包”成一个训练样本，这些训练样本直接会通过一个特殊的“结束 token”进行分割。训练时候在每个指定的步数会在“保留任务”上进行模型评估，保存最佳的 checkpoint。图5.4：微调保留任务尽管微调的任务数量很多，但是相比于语言模型本身的预训练过程，计算量小了非常多，只有 0.2%。所以通过这个方案，大公司训练好的语言模型可以被再次有效的利用，我们只需要做好“微调”即可，不用重复耗费大量计算资源再去训一个语言模型。图5.5：微调过程与预训练本身的计算量对比一些结论汇总：微调效果更好。与不微调相比，通过基于指令的微调可以大幅度提高语言模型的效果模型越大效果越好。伴随模型体积的增加， 尤其是指数级的增加，比如从 8B->62B，再从 62B->540B，不论是否微调，效果都有非常显著的提升，而且还没有看到收敛的信号。任务越多效果越好。伴随任务数量的增加，模型的性能也会跟着增加，但是当任务数量超过 282 个之后，提升就不是很明显了。因为继续增加新的任务，尤其任务形式跟之前一样，不会给模型带来新的知识；多任务微调的本质是模型能够更好的把从预训练学到的知识进行表达，超过一定任务之后，继续新增相似的任务，知识的表达能力不会继续有很大的收益。进一步统计全部微调数据集的 token 数，发现只占到了预训练数据 token 数的0.2%，这表明还是有很多的知识没有在微调阶段重新被激发。混杂CoT相关的任务很重要。尽管在 1800 多个任务中只有 9 个CoT任务，但是混杂了这 9 个任务之后对整个模型的提升很大。在针对 CoT 相关任务的预测上，如果在微调中混淆 CoT 任务能带来明显的提升；在针对非CoT 相关任务的预测上，如果在微调中混淆了 CoT 任务也不会对模型带来伤害。图5.6：引入 CoT 任务能显著提示模型效果总结一下，这篇工作提出了 Flan 的微调框架，核心有四点：统一的输入输出格式（4种类型）引入 CoT （chain-of-thought）大幅提高任务数量大幅提高模型体积实现了用一个模型来解决超过 1800 种几乎全部的 NLP 任务，通过较低的成本，极大发掘了现有语言模型的泛化性能，让大家看到了通用模型的希望。6. 提升小模型的推理能力：Fine-tune-CoTLarge Language Models Are Reasoning Teachers前面已经介绍过，通过包含少数思维链（CoT）推理的样本或通过 promp 来让模型逐步思考的方法可以在大型语言模型中促成复杂的推理能力。基于 CoT 方法的主要缺点是它需要依赖于拥有数百亿参数的巨大语言模型。由于计算要求和推理成本过于庞大，这些模型难以大规模部署。因此，来自韩国科学技术院的研究者努力使小型模型能够进行复杂的推理，以用于实际应用。有鉴于此，论文提出了一种名为 Fine-tune-CoT 的方法，该方法旨在利用非常大的语言模型的思维链推理能力来指导小模型解决复杂任务。为了详细说明，下面应用前面的 Zero-shot-Cot从非常大的教师模型中生成推理，并使用它们来微调较小的学生模型。图6.1：Fine-tune-CoT研究者注意到，与标准的 prompting 类似，对于训练语言模型来解决复杂推理的任务来说，纯微调往往是不够的。虽然已经有人尝试用规定好的推理步骤对小模型进行微调来解决这个问题，但这些方法需要巨量的推理注释，而且往往还需要与特定任务匹配的训练设置。论文中提出的方法，由于基于语言模型的教师具有显著的零样本推理能力，无需手工制作推理注释及特定任务设置，可以很容易地应用于新的下游任务。从本质上讲，论文的方法保留了基于 CoT 的多功能性，同时模型规模还不是很大。Fine-tune-CoT 的核心思想是采用 Zero-Shot-CoT 生成我们的问答数据，然后使用温度 T 采样（也可以用 Top-k 采样），以此生成尽可能多的数据，然后再进行 Fine-tune。其实就是使用不同的温度参数 T 采样，用 ChatGPT 这样的大模型生成 CoT 数据，然后再用小模型进行 Fine-tune。在样本研究中，研究者确认多样化推理样本包含各种推理路径以及语言模板，这一点也可以在细化的学生模型中观察到。多样化推理路径被生成并被边缘化以找到最优答案。7. CoT的局限性前面说了这么多，是不是有了思维链，大语言模型就所向披靡了呢？照这么发展下去，真能媲美人类的能力了？大可不必担心，思维链本身还是有很多局限的，而它的局限也是大语言模型的局限。首先，思维链必须在模型规模足够大时才能涌现。在 Jason Wei 等的研究中，PaLM 在扩展到 540B 参数时，与思维链提示结合，才表现出了先进的性能。一些小规模模型，思维链并没有太大的影响，能力提升也不会很大。谷歌大脑的研究人员认为，策略问题需要大量的世界知识，而小型模型没有足够的参数来记忆这些世界知识，所以也不太可能产生正确的推理步骤。但问题是，能落地到产业的模型，规模必然不会太大，思维链拆解了更多的步骤、用到更多的计算资源，相当于更加耗费脑力，很多研究机构和企业是负担不起 175B 参数以上的大模型。所以思维链必须要探索，如何在较小的模型中进行推理，降低实际应用的成本。图7.1：62B 比 540B 的语言模型更容易出错其次，思维链的应用领域是有限的。目前，思维链只是在一些有限的领域，比如数学问题，五个常识推理基准（CommonsenseQA，StrategyQA，Date Understanding 和 Sports Understanding 以及 SayCan）上显现出作用，其他类型的任务，像是机器翻译，性能提升效果还有待评估。而且，相关研究用到的模型（GPT-3 API）或数据集，都是半公开或不公开的，这就使其难以被复现和验证。严谨来看，思维链的效果还需要被进一步探索，才能下定论。图7.2：即使有思维链提示，大语言模型依然不能解决小学水平的数学问题此外，即使有思维链提示，大语言模型依然不能解决小学水平的数学问题。没有思维链，数学推理是指定不行。但有了思维链，大语言模型也可能出现错误推理，尤其是非常简单的计算错误。Jason Wei 等的论文中，曾展示过在 GSM8K 的一个子集中，大语言模型出现了 8% 的计算错误，比如6 * 13 = 68（正确答案是78）。这说明，即使有了思维链，大语言模型还是没有真正理解数学逻辑，不知道加减乘除的真实意义，只是通过更精细的叠加来“照葫芦画瓢”，所以，对于有精确要求的任务，还要进一步探索新的技术。思维链确实增强了大语言模型的能力，但逻辑推理仍然是大语言模型的弱项，等待着更多突破。One more thing通过思维链，我们可以看到大语言模型为什么强，也为什么弱。它强在，模型规模的提高，让语义理解、符号映射、连贯文本生成等能力跃升，从而让多步骤推理的思维链成为可能，带来“智能涌现”。它弱在，即使大语言模型表现出了前所未有的能力，但思维链暴露了它，依然是鹦鹉学舌，而非真的产生了意识。认知心理学教授斯坦尼斯拉斯·迪昂（Stanislas Dehaene）在《精准学习》中提出，缓慢地、理智地、符号化地运作，是人脑的特权。它可以在任何可能的时候，提取具有普遍性、逻辑性的、明确的原则。五六岁的儿童学会了较小数字的加法，就可以理解其含义，用到更大的数字的加法中，而目前最强大的大语言模型，还连“加法”这个简单的抽象定律都理解不了。大语言模型，正如科幻作家特德·姜所说，是网上所有文本的模糊图像，一张有损压缩的 JPEG，但它可以用远超人脑的算力和数据，极其高产地做好文本生成、图像生成这样的模糊任务。而人脑更擅长精确的、逻辑性的任务，就像特德·姜说的：“当你还有原始图片的时候，一张模糊的 JPEG 到底有多大用处呢？”智能时代的生存策略，就是不要以己之短，硬碰 AI 之长。而是用 AI 之长，让自己的长板变得更长；用人脑的精确，让 AI 生成的模糊答案变得更高质量；用好思维链提示，让 LLM 生成时事半功倍。总结本篇文章主要是介绍了 CoT 以及后续的改进，目前从推特上观察，CoT已经被广泛应用，甚至很多人认为就是标准的做法。但国内来看，似乎缺乏对它的重视，觉得不过是个简单的 trick。其实不只是 CoT，对整体 LLM 的认知和谷歌、OpenAI 那边确实有些差距。至于为何，Jason Wei 那条推特一定程度说明问题，知乎上也有人把他删掉的推特截图放出来，大致意思是20年后入门 NLP 的人比之前的幸福，他们对 LM 的认知来自于强大的 LLM，而过去的人往往还停留在 BERT 的范式。参考Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.大模型“涌现”的思维链，究竟是一种什么能力？_澎湃号·湃客_澎湃新闻-The PaperTimothyxxx：思维链（Chain-of-thoughts）作为提示霜清老人：有了Chain of Thought Prompting，大模型能做逻辑推理吗？nghuyong：Flan-T5: One Model for ALL Taskshttps://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.htmlKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., & Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.书籍推荐"
686664720,7c894b915042fe363aed838b276951eb,王焱,https://api.zhihu.com/people/7c894b915042fe363aed838b276951eb,people,废啥话，干丫的,如何从零开始训练大模型（minicpm分享&讨论）,article,https://api.zhihu.com/articles/686664720,像代码，数学。这种就是<em>模型</em>越<em>大</em>，混合的比例反而可以越高。 跟SFT是类似的，越<em>大</em>的<em>模型</em>，越聪明的模型，需要的SFT数据就越少。同理，越<em>大</em>的<em>模型</em>，越聪明，复杂样本混合比例就可以越高,530,26,866,1710250391,1710818122,"1 背景根据scaling law，模型越大，高质量数据越多，效果越好。但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，往往效果能轻松反超参数量两倍于它的模型。例如，最新出的minicpm，微信内部评测效果也是非常棒的。跟规模相对接近的2b、7b模型比，得分比qwen2b高，和qwen7b比有的高有的低。这个是minicpm的详细技术文档。https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a这说明，现有参数量情况下，哪怕是2B尺度，也并没有得到充分训练。近期，邀请minicpm作者内部做了一个小范围分享，参与人员有国内好几个大模型团队的成员，基本上都是预训练为主。很多人都签了保密协议，所以都是基于公开信息沟通。提前约法三章，对面拒绝回答，就不要做一些比较不知趣的追问操作。限制很多，但也得到了一些有趣的结论。因为都是基于公开信息的讨论，放出来也无伤大雅，也欢迎大家的反馈。参会人， 鼎叮， @swtheking， @孙培钦 ，殷超，晓楠， @孔某人。还有一些不方便挂名的，这里就不写了。2 样本2.1 样本构成大家已经达成一些基础的共识。如中英混合比例大家都大差不差。逻辑推理比较强的样本，像代码，数学。这种就是模型越大，混合的比例反而可以越高。跟SFT是类似的，越大的模型，越聪明的模型，需要的SFT数据就越少。同理，越大的模型，越聪明，复杂样本混合比例就可以越高。2.2 样本质量2.1.1 基本清洗导致ppl崩掉的，都要清洗掉，政治敏感数据清洗，去重等，肯定是一个很长的pipeline。大家比较一致的结论是，天工开源的那份预训练数据，是一个比较好的满足基础清洗要求的数据。2.1.2 进阶清洗大家都不太方便展开，但可以透露的信息。跟SFT一样，产出各种各样的label来刻画数据，有的公司实习生就优化几个label。不过随着优化的往后拓展，这些label的投入产出比越来越难以评估。2.1.3 PHI式的生成(synthetic)数据预训练清洗的pipeline搭建，对于开源团队，小公司来讲，成本其实还是蛮高的。所以，基于开源数据，做一些聚类的topic。然后基于这些topic，丢到更大的模型，来构建一批高质量的数据，是一个反而比较低成本的方案。2.1.4 买数据嗯，这次大模型，除了李一舟。卖数据的公司，也是真的赚到钱了。2.3 不同训练阶段的训练样本经过讨论，发现有三种方案。2.3.1 末期高质量样本（minicpm)快速收敛阶段和平稳阶段，都采用普通样本。退火阶段，混入高质量样本来做教科书式的学习。2.3.2 初期高质量样本快速收敛阶段，以高质量样本为主，让模型快速收敛。平稳阶段，逐步调整比例，增加更多的普通样本。退火阶段，跟平稳阶段一致2.3.3全程高质量样本（PHIL方式）全程都是高质量样本这里大家讨论的蛮激烈的，有这么几点。第一，初期就加入高质量样本，初期收敛的更快。但高质量样本少，不断的重复学习高质量样本，会不会导致过拟合？但反方认为，人类的本质上就是复读机，特别对于小模型，不断的重复学习，好像也没太大问题。第二，初期学习高质量样本，会不会导致初期模型的初始化，局限在特定的区域，后面的普通样本学了之后，也不一定能很好的跳出来，会不会影响泛化？但反方认为，末期加入高质量样本，反而会不会因为最后加入高质量样本，导致泛化能力受损，集中在高质量样本的领域？第三，PHIL方式，大家很快达成一致，PHIL就是探索小模型能不能在特定领域达到SOTA。好处，特定榜单/领域效果会特别好。坏处，模型泛化能力会很差（但PHIL从来没说要做世界模型。2.4 小模型样本的极限在哪？到底喂了多少tokens，小模型参数才算是充分得到训练？当天讨论，并没有一个很好的结论。最近YI-9B的公开技术文档，做了一个有趣的尝试。把每层的输入和输出算cos，来评估模型是否训练的非常充分。但内部讨论后，发现这种尝试有一个巨大的遗漏点。前段时间，我们做long context调研，也是把每层也都单独做了一个分析。结论是，如果模型深度足够的话，有些层其实区分度是在降低的，相当于几层做了一层做的事情。以及，另外一个可能，小模型每一层cos都小，有可能每一层在干不同的事，或者每一层都会注意到新的东西。大模型某些层cos大，有可能是因为句子太简单，大模型对结果更加肯定，靠后的层的功能没有被激活。感觉这种评估方式，仍旧有一定的优化空间，也期待业内能公开更多好用的评估方式。3 训练3.1 tokenizer小模型过大的tokenizer的确是一种浪费。很多小模型有大tokenizer，一个潜在的可能性，作者人力不足，直接是把大模型的tokenizer拿来复用了。3.2 阶段现在大家预训练分成三个阶段。快速收敛阶段，稳定阶段，退火阶段(minicpm比较显式的提出这个阶段）3.2.1 为什么要分阶段这个阶段来自于大家对loss曲线的观察，发现loss曲线的收敛就是这么一个特点。然后，大家发现不同的loss曲线阶段，做一些针对性样本和参数的调整，能带来更好的效果，于是就这么分了。3.2.2 不同阶段学的是什么东西？首先，我们现在的评估手段还是比较粗糙的，假如有了更细的评估手段，可能能观测到更多有趣的东西。例如之前俊林做过关于涌现的分享，涌现从指标观测来看，就是突然出现的。但当把指标细化后，可以发现这个涌现好像也没那么突然，这个可以参考https://arxiv.org/abs/2310.03262把原本离散的准确率在1e-5级别的时候的值估计出来了。但反方这里又有不同的观点，我们用物理学的一个理论来解释涌现。我们可以把涌现替换成相变来聊一聊它和指标突变的辩证关系：当我们谈论相变时，我们指的是物质从一种状态转变为另一种状态的过程，比如水从液态变成固态的过程（冰冻）或者从液态变成气态的过程（蒸发）。而指标突变通常指的是某种性质或者物理量在某个条件下突然发生明显的变化，比如在某个温度或者压力下，某种物质的导电性、磁性或者其他性质突然发生变化。相变与指标突变之间存在着密切的关系，因为相变往往伴随着物质性质的突变。当物质经历相变时，它的一些性质会突然改变，这些性质就是我们所说的指标。举个例子，当水温降到0摄氏度以下时，水会由液态变成固态，这就是相变，同时水的密度也会突然增加，导致它的体积变小，这就是指标突变。虽然相变往往伴随着物质性质的指标突破，但是不意味着不突变就不是相变，指标的突变不是相变的重点，相变的重点在于从一个状态/性质，变成另外一个状态/性质，这两种状态有着很不一样的差别。尽管可以使用一些技巧方法来构造一些看起来特别平滑的指标来反对大模型涌现这个词汇，但是不可否认的事实是，在不同的尺寸变化或者数据量、计算量变化之后，人们可以非常明显地感知到大模型表现的巨大差异，这就是一个相变的结果，就像是炼制一门18连环刃的法器，从第一把的炼制到第18把，从个数的指标上来说是非常平滑的，但是从威力上来说，18把可以构建一个法阵，极大地增加了武器的威力，与之前不可同日而语。3.3 batch size老调重弹的问题。2020年，transformer出来后，当时大家就碰到这么一个问题。模型太大了，用尽可能能塞进内存的batch size去train模型，来提升速度。很快，大家发现batch size有个trade off。batch size过小，波动会比较大，不太容易收敛。但这种波峰，也有助于跳出局部最优，模型更容易有更好的泛化能力。batch size变大，步数整体变少，训练的步数更少，本来就波动就小，步数也少，同样本的情况下，你收敛的会更慢。2020年其实有人就研究，如何用大batch size，更快的训练的同时，也能收敛的更好。一个解决思路是优化了优化器，例如谷歌当年出的LAMB，就把batch size从512扩充到了6w，收敛效果也不错。3.4 LR scheduler机器学习的目标，都是为了收敛loss，让学习的target和我们预测的target的loss尽可能低。学习的过程，就是基于样本，分批（batch size）丢进去。根据过去，现在学习的效果，来决定参数更新的方向和大小。batch size这里是很清晰的。比较纠结的点是，优化器和LR scheduler这俩好像边界不是很清晰。3.4.1 LR scheduler是什么假如我们要下山，山脚就是我们的目标，learning rate就是我们每一步要走多远。如果速度太快，可能开到山脚后，发现刹不住车，还会往山上多开一会，于是这样反复在目标处来回震荡。如果太小的话，到山脚的速度又太慢了。现在主流的就是cosine，初期warmup learning rate线性增长，然后learning rate就是以余弦函数的周期方式周期性变换。3.4.2 优化器做什么？优化器核心要解决的问题，初期怎么更好的学，那些地方要加速学，那些地方容易陷入局部最优，要如何跳出来。现在的主流做法都是基于历史的反馈。类似于爬山，某个地方你发现爬的很慢，那么就加下油门。有的地方你发现是下坡路，跑的贼快，那就就要松下油门，免得油门太快，直接从主路跑出去了。从momentum，到adagrad，再到adam，这两年还有人在各种折腾。3.4.3 优化器和LR scheduler如何协同工作？问题就来了，LR scheduler决定了learning rate的大小。优化器也会根据历史情况来自动调整。这俩会不会冲突？优化器的优点刚刚说了，但它的缺点就是无论优化器怎么说的高大上，它本质上还规则，是大家基于调参经验，或者一些假设，定的规则。规则就很难完美适配所有任务，事实上2020年左右，大家就发现不同的任务上不同的优化器效果是不同的。例如当年的炼丹经验，计算机视觉优先使用SGD(with Momentum)，NLP（特别是用Transformer）优先使用Adam，现在CV都上transformer了，那么就又切到了AdamW。除此之外，还有一个learning rate decay的问题，但这个偏经验，并不一定完全solid！用CIFAR或者ImageNet跑一跑常见的模型，就会发现，最后如果不把learning rate降低下去，loss很难收敛到一个很小的数字。SGD和Adam的收敛性证明也都是要求learning rate最后会降到足够低的。但自适应优化器的学习率不会在训练中自动降到很低。现在大模型预训练，大家其实最关注的就是这个loss的收敛效果。这个时候，LR schedule的出现就是一个比较好的补充，能够补足优化器的一些问题。所以，你可以理解为，现在我们没有一个完美的油门，所以搞了俩油门，互相辅助。优化器是个老司机的油门，好用，但人类的经验是有局限性的，很容易陷入局部最优跑不出来。LR schedule像是一个全局的油门，定期更新，帮助老司机跳出局部最优。3.4.4 W-S-D的讨论和优化方案minicpm提出了W-S-D LR scheduler，但stable阶段高learning rate，相当于把调节油门的压力全给到优化器了。但S-D的确也有很多好处，例如我想train到什么时候就train到什么时候。这里提出了一个解决思路，W-S-D 是不是可以改成，warm-cosine-stable-decay，cosine占据训练阶段大头，甚至多个余弦波段，余弦波段多了，如上文所说，是不是能更好的跳出局部最优？快要结束训练的时候，把cosine的learning rate给升上去，走一段stable+decay。3.5 退火加sft &“和面”前段时间，业界流行一个说法，你发现某块效果差，在预训练最后阶段补充一些类似的数据，效果就会蹭蹭的往上涨。简称，和面——水多了加面，面多了加水。刚开始，大家都很鄙视这种行为，觉得这种行为不就是刷榜么？但现在我们来探讨这块的合理性，minicpm可以算是更进一步的“作弊”了，如果按照之前的观点。他都直接把sft数据混入了预训练数据里面，更加明目张胆的“刷榜”。我个人觉得这里可以用两个角度去理解:角度一，模型学习的训练动态角度，在退火的时候loss下降较stable和正常的cosine都要快，证明这里的学习效率在提升(最聪明的时候?)，而此时在这一刻使用高质量数据来喂给模型, 可以尽可能发挥高质量数据的作用;角度二， SFT数据较正常的文本数据， 我猜测会更偏向于benchmark一些，因为SFT很多都是""QA型""结构的数据, 对于模型认识bechmark有一定的改善。之前预训练完毕后，直接上SFT数据，语料分布差距很大，其实天然是不太好的。这种作弊的行为，只要控制样本比例得当，反而能保证后面通用对其的一致性。4 再看scaling law随着一些common sense的建立，scaling law的指导意义的确是在不断下降的。举个例子，假如我有300张卡，我要train多大的模型？计算方式，往往就变成，我大致知道训练1T-2T tokens效果往往就不错了，这个模型两个月后我想赶一个发布会。那么就来反推，1T的tokens训练2个月，300张卡能train多大的。但我们回到2020年，当大部分人都在基于bert做各种魔改的时候。OpenAI发现了这么一个规律。数据，训练，参数一直增长下去，好像loss的确是在不断的下降哎？于是，他们拿着这个paper去问微软的CTO，你想不想看看这个loss下降到一定程度会发生什么？会发生什么？chatgpt就出来了"
668469373,530acbe1f01916f2f2214014582c822d,老宋聊AI,https://api.zhihu.com/people/530acbe1f01916f2f2214014582c822d,people,归来仍是少年|公众号：老宋聊AI,2024 大模型面试指南：兄弟们，冲啊,article,https://api.zhihu.com/articles/668469373,整个的面试思路主要分为三个方向： 方向1：先面试<em>大模型</em>应用方向，看看各个公司在应用端的思考 方向2：面试<em>大模型</em>垂域基座方向，看看垂域的基座落地情况 方向3：面试<em>大模型</em>基座方向，看看最顶级的基座是怎么做的,215,44,572,1700743931,1700745463,前言老宋这俩月又跳槽了，自从去年从百度出来来到新公司，躺了一年，最近因为大模型技术发展，重新有了奋斗的方向和动力。大模型的诞生必然会重塑整个 NLP 方向，因此，必须参与到这波浪潮中，果然，这行就是得奋发向上，不然躺的时间久了真滴容易被时代抛弃。话不多说，来聊聊这次的跳槽过程。萌生跳槽想法其实早在4月份左右就想跳槽，那时候其实想回大厂，但是当时题也没有刷，且在这家公司还没有呆够一年，于是迟迟没有下定决心跳槽，毕竟躺着是真舒服。到了5月份，又发生了一些事情，整个人丧了下来，到了8月份才开始重整旗鼓，决定跳槽。当时跳槽的想法其实很简单：大模型这波浪潮必将引发生成式AI 接下来的发展，要是不懂大模型一整套东西，很容易被时代抛下。面试过程到了8月10号左右开始投简历，到10月初初步定了offer范围，这次战线拉的还是太长了，不过最后的结果符合我的预期。整个的面试思路主要分为三个方向：方向1：先面试大模型应用方向，看看各个公司在应用端的思考方向2：面试大模型垂域基座方向，看看垂域的基座落地情况方向3：面试大模型基座方向，看看最顶级的基座是怎么做的本次面试一大缺陷是，题是边面边刷的，前期很多面试都挂在题上了，后期倒是一面一个过。第一波：8-9月第一波主要投了一些中小厂公司，如知乎，360数科，新浪，伊对，面壁，以及阿里的几个NLP不强的部门如高德等。第一波刚开始没有刷题，八股文也没怎么看，直接裸面，感觉很酸爽。这一波主要还是积累面试经验，找找感觉，其中知乎和360数科很快发了offer，其余挂的原因如下：新浪：给不起钱挂了智谱：挂，题没做出来，不该这么早面的，这家水平蛮高的，值得重视。伊对：跟面试官气场不和，面试官有点傲气凌然的感觉，于是自己主动结束了面试面壁智能：题没做出来，挂了，这个不应该放在前期面，这家要求蛮高的，该好好准备的。猎头老催面试，真不能听猎头瞎说。阿里高德：三面挂，应该是我说不想做大模型在ner，分类方向的应用阿里优酷：题没做出来。天眼查：二面主动结束面试了。阿里 LAZADA： 说不想做大模型在ner，分类的应用。作业帮：题没做出来。百度-语音语义一体化： 跟大模型没关系，一面主动结束面试。百度电商：新部门，感觉风险大，二面主动结束面试。高途：二面主动结束面试面了十多天，基本上俩offer，360数科和知乎，阿里一个没过也是出乎意料，打击了我的自信。跟知乎和360数科仔细聊了聊工作内容，果断拒掉了。第二波：9-10月此时，我注意到了刷题的重要性，开始迅速补齐短板，大概刷了200+道左右。接下来主要面了几家公司：阿里-夸克： 挂，面了两次，题都出的忒偏了，出几何题，不知道是故意难为我还是就是这种风格，无缘。阿里淘天：三面主动结束面试，说是只有知识图谱方向了，我不想去。美团外卖：三面主动结束面试，非大模型基座方向。好未来：offer，大模型垂类基座，好未来有很强的转型需求，我蛮看好在教育场景落地的，没双减我就去了。字节-剪映：一面主动结束面试，去了做搜推，不太感兴趣。字节加班太多，因此我主要是以感受为主。中科创达：二面主动结束面试，车上的大模型，感觉风险有点大。金山云：offer。美团-语音交互部：二面主动结束面试。我有兄弟在里面，其实挺想去的，但是纠结了很久还是想做 50B 以上的model。baichuan：一面主动结束面试，这家其实我很想去，因为我蛮看好的。但是跟面试官聊，面试官很着急，也不知道为啥，问的深度也不深，总是浅尝截止，还judge了我，我不太喜欢这种面试，就主动结束了。百度-文心一言 ernie 组：offer，本来非常想去。拿完百度文心后，我就开始躺了，毕竟在我心中，百度文心是国内最顶级团队。但是跟经理聊了聊，发现做的方向不确定性很大，并且加班强度非常大，忍痛拒绝了。后面有其他非常好的团队约面我都拒绝了，面不动了。总的来看，这次面试通过率前期比较低，主要还是挂在题上，后期基本都能过，主要是题刷够了。还有一些非常牛逼的团队拒了我简历，因为我躺了一年，简历还是有所影响的，比如腾讯混元。这次面试也有几个筛选标准，比如说太卷的不去如字节，太小的不去，钱少的不去等。到此，面试就初步告一段落了，实在是面不动了，最后还是选了相对work-banlance的一家。为啥不开卷？这里有必要聊一聊我为啥不开卷。首先就是奋斗的意义问题，众所周知，我国的税很高，五险也是税，那么涨工资的意义在哪呢？ 你累死累活都交税了，卷个屁啊。其次就是生活问题，人这一辈子就活这么一次，总不能你回首往事时，发现自己的人生一大半都在学习和加班吧？接下来打算学学摄影，健健身，找找女朋友，人生不止加班呀。最后就是身体，懂得都懂。本次面试教训总结要先刷够300道再面，不能直接裸面八股文先总结一遍，做到非常熟悉练手三家即可，圈定好核心面试的公司，别面太多家，太累了别去不知名的小厂，不然影响简历太严重面试常见题接下来我来讲讲面试中经常遇到的题，以及这些题的简单回答，主要包括：文本生成的重复问题介绍下 ChatGLM介绍下baichuan介绍下 CRF介绍下Deepspeed的zero介绍下 Adam的原理和思想FP16，BP16区别介绍下 GLU 和 SwiGLU介绍下 Lora介绍下 Lamma 系列介绍下 Transformer介绍下 BART，以及 BART与T5的区别BN 和 LN介绍下交叉熵几个位置编码的区别详细介绍下旋转位置编码简单介绍下分词方式用torch 实现欧氏距离三大并行方式大小王概率题大家想要获得上述面试题的详细回答，可以关注我的微信公众号，然后回复：面试，即可领取。老宋的微信公众号搜索：老宋聊AI。最后我认为 AI 再次进入了快速迭代期，躺了一年多，这次出来打算好好拼一把，看看这两年能不能搏一搏，单车变摩托。如果兄弟们还在搞传统 NLP，我的建议是赶紧转大模型方向，无论是应用还是基座。诸位加油。
692869923,6ed048bd3f2a45ef8dc7b6e7e4135b4f,海鸣,https://api.zhihu.com/people/6ed048bd3f2a45ef8dc7b6e7e4135b4f,people,公众号《AIGC小白入门记》,面试阿里大模型算法工程师，太离谱了。。。,article,https://api.zhihu.com/articles/692869923,个人本次面试总结 很多题目非常强调实践，没有做过<em>大模型</em>的项目且没有针对性准备过，很难回答上。 大模型微调是很多公司的考察重点。 几种模型的注意力机制、位置编码要熟悉。 4.,77,5,243,1713277590,1713277590,"阿里大模型算法工程师面试题应聘岗位：阿里大模型算法工程师 面试轮数：第二轮 整体面试感觉：偏难面试过程回顾1. 自我介绍在自我介绍环节，我清晰地阐述了个人基本信息、教育背景、工作经历和技能特长，展示了自信和沟通能力。2. Leetcode 题具体题意记不清了，但是类似 【23. 合并 K 个升序链表】题目内容给你一个链表数组，每个链表都已经按升序排列。请你将所有链表合并到一个升序链表中，返回合并后的链表。示例 1：输入：lists = [[1,4,5],[1,3,4],[2,6]] 输出：[1,1,2,3,4,4,5,6] 解释：链表数组如下： [ 1->4->5, 1->3->4, 2->6 ] 将它们合并到一个有序链表中得到。 1->1->2->3->4->4->5->6 示例 2：输入：lists = [] 输出：[] 示例 3：输入：lists = [[]] 输出：[] 提示：k == lists.length0 <= k <= 10^40 <= lists[i].length <= 500-10^4 <= lists[i][j] <= 10^4lists[i] 按 升序 排列lists[i].length 的总和不超过 10^4题目解答# Definition for singly-linked list. # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: def mergeKLists(self, lists: List[ListNode]) -> ListNode: lists_len = len(lists) if lists_len == 0: return return self.merge(lists,0,lists_len-1) def merge(self,lists,left,right): if left == right: return lists[left] mid = left+(right-left)//2 l1 = self.merge(lists,left,mid) l2 = self.merge(lists,mid+1,right) return self.mergeTwoList(l1,l2) def mergeTwoList(self,l1,l2): head = ListNode(0) h = head while l1 and l2: if l1.val >= l2.val: h.next = l2 l2 = l2.next else: h.next = l1 l1 = l1.next h = h.next if l1: h.next = l1 if l2: h.next = l2 return head.next 3. 技术问题回答llama2中使用的注意力机制是什么?手写实现下分组注意力。了解langchain吗?讲讲其结构。对位置编码熟悉吗?讲讲几种位置编码的异同RLHF的具体工程是什么?包含了哪几个模型?分别讲讲 encoder-only、decoder-only、encoder-decoder 几种大模型的代表作。具体讲讲 p-tuning、lora 等微调方法，并指出它们与传统fine-tuning微调有何不同。显存不够一般怎么解决的?几种主流大模型的 loss 了解过吗? 有哪些异同?了解半精度训练吗?展开讲讲。deepspeed 用过吗? 展开讲讲。注：我回答了大部分问题，但仍有部分问题回答不够准确。后期会总结一下，然后重新写一下上面的面试题，并贴到 留言栏！！！个人本次面试总结很多题目非常强调实践，没有做过大模型的项目且没有针对性准备过，很难回答上。大模型微调是很多公司的考察重点。几种模型的注意力机制、位置编码要熟悉。4.RLHF的几步多熟悉熟悉"
663556778,15245012909c348b3cc4c8900881639a,maijun,https://api.zhihu.com/people/15245012909c348b3cc4c8900881639a,people,关注程序分析、SCA、代码大模型应用、DevSecOps等,大模型学习路线记录,article,https://api.zhihu.com/articles/663556778,作为一个<em>大模型</em>领域的新人，记录一下个人学习大模型相关的知识路线。 当然，因为我长期从事程序分析领域工作，这里<em>大模型</em>或者训练相关的知识，会偏自然语言<em>大模型</em>或者代码大模型的学习路径,50,4,194,1698324639,1698324856,作为一个大模型领域的新人，记录一下个人学习大模型相关的知识路线。当然，因为我长期从事程序分析领域工作，这里大模型或者训练相关的知识，会偏自然语言大模型或者代码大模型的学习路径。1. 深度学习相关的基础知识编程语言：python学习框架：numpy、pytorch（我自始至终没有看过TensorFlow），各种向量相关的操作数据处理：数据的处理，数据 embedding 等基础的知识：感知机，神经网络等相关的基本概念（比如激活函数、损失函数、梯度、学习率、训练等）稍微进阶一点儿的知识：循环神经网络（RNN、LSTM等）我倒没有手撕过细节代码，了解基本概念之后就是调包的（我一直记得我是搞程序分析的，不是搞AI的），这些在 pytorch 里面都有。从一个非专业的AI从业人员来看，这一步能知道基本的概念、能调包、知道怎么训练一个模型就够了。2. Transformer这一步，了解了 Translormer 的基本架构，了解了相关的概念，比如自注意力机制、多头注意力、残差网络、位置编码、encoder、decoder等，但是我学 Transformer 的时候，没有敲过一行代码，罪过。3. 各种大模型的结构在这一步，我主要就是学习了各种大模型（个人碰的最多的：GPT 和 StarCoder，及从 StarCoder 训练出来的 StarChat 等）。我也没有真正训练过一个大模型，主要有两点：① 毕竟不是真正的搞AI大模型的，是做程序分析的，搞大模型也是为了给我们业务服务的；② 有点儿穷。这一步，最重要的就是熟悉各种大模型，熟悉大模型的输入和输出格式等，方便我们做下游任务。4. 使用大模型做下游任务这就是我最主要的活儿了，其实做大模型，我估计绝大部分的工作都在这一部分。4.1 使用的框架这里，我要感谢 huggingface 的 transformers 和 peft 两个库，我在做大模型训练时，几乎没有碰过其他的库了。4.2 任务和训练因为每一种大模型，都有自己的预训练好的模型，和自己的 PretrainedTokenizer，所以我们自己只需要准备好自己的数据，知道自己要做什么事情就行了。我举几个例子：(1) 误报识别这是一个很典型的二分类问题，实际上就是个二分类任务。我们知道大模型最大的贡献，就是对输入的分词，给了一个更优的向量表示（动态词向量），所以把一个句子给大模型，输出就是针对这个句子的词向量，我们在大模型的后面，跟一个全连接 + softmax任务，就实现了分类。输入就是我们的有缺陷的代码、缺陷类型、缺陷描述，输出就是二分类的是误报和非误报。这里只提两点：① 因为我们输入的信息有代码，有描述，所以我们使用的是一个支持自然语言和不同代码的多模态大模型；② 我们在这类任务中，通常不对大模型做微调，而是主要训练后面的这个全连接 + softmax 步骤。(2) 缺陷自动修复我们需要给出问题代码，问题代码的问题，给出来一个修复后的代码，这是个典型的text-generation的任务。训练可以参考 text-generation 的训练方式来执行。可以参考 starcoder 是如何训练出来 starchat 的步骤来做这类训练。参考地址：starcoder/chat/train.py at main · bigcode-project/starcoder (github.com)大模型后面还可以接大量的其他的下游任务，如 huggingface 上面给的：huggingface上自然语言处理的下游任务每种类型的下游任务，也都有相应的训练方式，感谢 huggingface。4.3 微调方式其实这部分，我也只是个调包的，huggingface 的 peft 拯救了我。peft 支持的微调方式有：huggingface peft支持的微调方式而且 huggingface peft 已经把微调的接口都规整起来了，直接调接口就行了。而我就不行了，我知道 LoRA，其他的都没有用过。5. 总结其实，我做的最多的工作在第4点上，但是还是需要一些基础知识，主要在于怎么训练，所以前面还是需要懂一点儿的。对于我这种面向具体业务的来说：最重要的是知道要做什么事情，基于一个已有的大模型，构造对应的训练数据，训练完模型后构造合适的 Prompt去完成应用。
696588130,ae9fbe0842d7c11ccb04a6c89ef32cbe,hzwer,https://api.zhihu.com/people/ae9fbe0842d7c11ccb04a6c89ef32cbe,people,github.com/hzwer 阶跃星辰搞算法的,[大模型06] 大模型强化学习面试题学习笔记,article,https://api.zhihu.com/articles/696588130,我曾经调研过<em>大模型</em>的涌现，小模型的指令跟随能力是比较差的，那么对于复杂问题，小模型连问题都无法理解，应该是很难评判回答质量的。2. <em>大模型</em>的知识储量大，判断回答中的幻觉会更准确,41,3,41,1715162468,1715176110,接上一篇文章：hzwer：[大模型05] 7B小模型苦练小学数学的一年通过 swtheking 的专栏学习一些面试题现阶段LLM的对齐阶段分为sft和rlhf阶段，我们可以跳过sft阶段直接进行rlhf么？DPO的第0步loss是多少？DPO是一个on-policy还是off-policy的算法，以及这样的算法有什么优劣？在PPO过程中，reward model的效果上会有什么问题？如何看待各种ppo rlhf的平替算法dpo/kto/rrhf/slic/orpo/samug/remax等算法号称性能等能超过ppo？如何处理reward model中的噪声数据？现有的 reward model 泛化能力怎么样？我们需要用大 reward model 吗？sft 模型出现各种复读问题如何解决？DPO训练时，为什么chosen和rejected的reward一起下降的猜想？chosen reward 和模型最终性能有关吗？我近一年在阶跃星辰做大模型对齐，打个广告：跃问和冒泡鸭。后文是我的回答：----------现阶段LLM的对齐阶段分为sft和rlhf阶段，我们可以跳过sft阶段直接进行rlhf么？有这样的算法，比如 ORPO。但如果有 sft 数据，不用不亏吗？主流的 DPO / PPO 算法，都是要求以 sft 后的模型作为起点的。DPO的第0步loss是多少？DPO 的 reward 是根据 policy 网络和 reference 概率比确定的，在初始的时候，chosen_reward = rejected_reward = 0，所以 loss 是 -logsigmoid(0)DPO是一个on-policy还是off-policy的算法，以及这样的算法有什么优劣？off-policy，即训练数据不来自当前优化的 policy 模型，优势是数据可以复用，节约采集标注成本，劣势是可能因为分布漂移降低性能在PPO过程中，reward model的效果上会有什么问题？reward model 的数据本身是有限标注的，那么在 PPO 训练过程中，模型产生的新样本可能是分布外的，那么 reward model 的准确率可能会降低；更极端的情况下，policy 模型可能找到一些 hacking 解，虽然可能毫无意义，但是获得了很高的 reward；这里可能可以用 reward model ensemble / merge weight 的方式来增强鲁棒性如何看待各种ppo rlhf的平替算法dpo/kto/rrhf/slic/orpo/samug/remax等算法号称性能等能超过ppo？swtheking 的回答 相当全面。我注意到 Reward Model 有个很有意思的点，当使用大模型来搭建 reward model 时，我们获得的可能不仅仅是人类标注中蕴含的那部分知识，同时还激发了模型在预训练过程中学到的一部分能力来做判别，这是我认为 PPO 和 DPO 一个比较重要的区别，甚至我们还可以对 reward model 做一些提示词工程。举一个具体例子，可能我们给的所有标注都是没有长度偏好的，但是 reward model 在预训练过程中就理解到详细的回答可能是更专业，更受人喜欢的，这样的知识就会通过 PPO 传导给 policy 模型，而 DPO 没有这样的效果。最近还有一篇论文 https://arxiv.org/pdf/2404.10719v2，他们的实验说在一些任务，特别是编程上，DPO 几乎没有效果，而 PPO 效果很好。如何处理reward model中的噪声数据？实践中可以通过让多个 reward model 进行投票，把一致性低的拿出来检查或者丢掉；也可以寄希望于闭源的大模型，比如 GPT4，替代人完成清洗工作。现有的 reward model 泛化能力怎么样？我们需要用大 reward model 吗？我和 swtheking 的回答 观点不太一样。首先原始的 InstructGPT 是用 7B 模型指导 175B 模型对齐，但是后续大部分人用的 reward model 至少不小于 policy 模型。我想到几个理由：1. 我曾经调研过大模型的涌现，小模型的指令跟随能力是比较差的，那么对于复杂问题，小模型连问题都无法理解，应该是很难评判回答质量的。2. 大模型的知识储量大，判断回答中的幻觉会更准确。3. 大模型通常具有更好的 OOD 能力。sft 模型出现各种复读问题如何解决？一种说法是把 pretrained 模型的能力和复读现象联系起来，即模型在 overfit 的过程中，过多修改了原来的 attention 模式和模型参数分布。实践中，我发现的严重的模型复读现象，出现在 sft 时喂了大量的 coding / math 数据的情况，可能是这类数据中天然有很多重复模式，而且某些 checkpoint 就是会特别严重，过了一段时间又抖回来了。改动 pretrain 的工作量是很大的，我觉得比较好的解决方案还是上 RLHF，针对性地把这种负样本干掉，DPO 一点点数据就能有非常好的缓解效果。数据我用过两种方式收集，一种是从模型 in the wild 的回答中，通过匹配和简单脚本找到那些带重复的数据，另一种是人工造出重复数据，比如把最后一句话或者一个词复制粘贴。DPO训练时，为什么chosen和rejected的reward一起下降的猜想？这里是一篇参考文章。我说两个民科见解，一是在构建训练集的时候，一个样本的 chosen 可能是另一个样本的 rejected，那么这时候看总体 chosen reward，当然不一定会大于 0。从另一个方面，在实验中，感觉把模型向一个样本拉近没有推远要容易，所以为了拉开 chosen 和 rejected，容易的方向是让 rejected 出现概率大幅下降。chosen reward 和模型最终性能有关吗？最近有一张出名的图片来自论文 https://arxiv.org/pdf/2404.02078v1，他们发现 chosen reward 和最终结果似乎有相关性。实话说我不信这个说法，可以做一个简单的实验，把 sft loss 加到 DPO 的学习中，可以发现 chosen reward 会被拉高很多，但是模型最终的表现却没有明显的提升。所以我觉得这里得出结论的过程不够扎实。如果结论反过来说，chosen reward 大幅降低可能会把模型毁掉，感觉可信度会更高。
649241113,0acfb7a3d48c75e13d7f34859ff3e5c0,养生的控制人,https://api.zhihu.com/people/0acfb7a3d48c75e13d7f34859ff3e5c0,people,互联网打工人|大模型｜数据科学｜推荐搜索,大模型面试基础｜秋招,article,https://api.zhihu.com/articles/649241113,最近参与了一些秋招的面试工作，其中包括<em>大模型</em>的相关职位。从面试官的角度分享一些关于<em>大模型</em>面试的基础考察点，希望能助各位求职者顺利获得理想的offer。,66,0,133,1691663410,1691663410,最近参与了一些秋招的面试工作，其中包括大模型的相关职位。从面试官的角度分享一些关于大模型面试的基础考察点，希望能助各位求职者顺利获得理想的offer。
651023365,a4b98a38d2a6de30dff3f545521a02d4,IT图书馆,https://api.zhihu.com/people/a4b98a38d2a6de30dff3f545521a02d4,people,AIGC初学者，分享大模型学习笔记，争取日更，欢迎督促！！！,【LLM】大模型面试准备-1（题库整理篇）,article,https://api.zhihu.com/articles/651023365,这部分的答案整理在连接中：IT图书馆：【LLM】<em>大模型</em>面试准备-1（答案篇-6） 参考链接： <em>大模型</em>面试八股 QianLing：和<em>大模型</em>相关的一些术语（持续完善） https,634,5,2788,1692429834,1692625080,"这个系列主要是准备大模型面试的一些准备，参考网络上各种大模型面经的学习笔记，大家如果有比较好的可以参考的文章链接和内容都可以在评论区提出来，一起进步呀~填坑中ing.......一、大模型基本概念这部分的答案整理在连接中：IT图书馆：【LLM】大模型面试准备-1（答案篇-1）二、基础知识1.transformer 八股文2.attention计算方式以及参数量，attention layer手写，必考。3.NLU以及NLG各种任务的差异。4.tokenizer的细节,tokenizer的计算方式，各种tokenizer的优缺点。5.各种norm方式的优缺点。6.bert参数量的推演，任何一个transformer结构参数量的推演，和显存占用关系的推演。这部分的答案整理在连接中：IT图书馆：【LLM】大模型面试准备-1（答案篇-2）三、大模型算法1.在指令微调中，如何设置、选择和优化不同的超参数，以及其对模型效果的影响？2.在指令微调中，如何选择最佳的指令策略，以及其对模型效果的影响？3.llama, glm，bloom等现有大模型的数据处理，训练细节，以及不足之处模型架构的优化点，包括但不限于attention, norm, embedding4.解决显存不够的方法有哪些？5.请解释P-tuning 的工作原理，并说明它与传统的 fine-tuning方法的不同之处。6.介绍一下Prefix-tuning的思想和应用场景，以及它如何解决一些NLP任务中的挑战7.Lora的原理和存在的问题讲一下？8.bf16，fp16半精度训练的优缺点9.如何增加context length 模型训练中节约显存的技巧。10.RLHF完整训练过程是什么？RL过程中涉及到几个模型？显存占用关系和SFT有什么区别？11.RLHF过程中RM随着训练过程得分越来越高，效果就一定好吗？有没有极端情况？12.encoder only，decoder only，encoder-decoder 划分的具体标注是什么？典型代表模型有哪些？这部分的答案整理在连接中：IT图书馆：【LLM】大模型面试准备-1（答案篇-3）四、训练框架1.Megatron以及deepspeed实现原理，各种参数以及优化策略的作用2.模型训练以及推理中的显存占用各种混合精度训练的优劣3.deepspeed的特点是什么？各个zero stage都有什么用？这部分的答案整理在连接中：IT图书馆：【LLM】大模型面试准备-1（答案篇-4）五、评测1.除了loss之外，如何在训练过程中监控模型能力？2.如果想全面的评测模型能力，有哪些维度以及数据集？评测指标等评测中比较重要的部分要了解.3.如何评测生成，改写等开放性任务?4.zeroshot和Fewshot具体做法的区别？这部分的答案整理在连接中：IT图书馆：【LLM】大模型面试准备-1（答案篇-5）六、数据1.bloom,llama， glm等开源模型的数据来源，配比，以及不足之处2.cot以及ic能力是如何涌现的？与预训练数据有何关系？3.数据处理的重要步骤，如何保证预训练以及sft时候的数据多样性，数据质量，数据数量等,包括但不限于去重，质量筛选，敏感及有害信息过滤，各种来源数据配比对于模型能力的影响。这部分的答案整理在连接中：IT图书馆：【LLM】大模型面试准备-1（答案篇-6）参考链接：大模型面试八股QianLing：和大模型相关的一些术语（持续完善）https://mp.weixin.qq.com/s/9ctZKgDwiIDkRdHbeBftBA花甘者浅狐：大模型面试八股"
654733518,5147499fe26deae04b15a3ea9cdf36c2,Aigc百晓生,https://api.zhihu.com/people/5147499fe26deae04b15a3ea9cdf36c2,people,GZH:Ai自然说,一篇文章让你了解大模型项目的整个研发流程,article,https://api.zhihu.com/articles/654733518,"深度学习模型尤其引人注目，而“<em>大模型</em>”更是近年来的热门话题。但是，什么是“<em>大模型</em>”呢？为何它如此重要？本文将带您探索<em>大模型</em>的神奇世界。 大模型是什么？ <em>大模型</em>,全名大语言模型.简单来说",18,3,79,1694059039,1706440634,"随着科技的发展，人工智能（AI）逐渐成为我们日常生活中不可或缺的一部分。其中，深度学习模型尤其引人注目，而“大模型”更是近年来的热门话题。但是，什么是“大模型”呢？为何它如此重要？本文将带您探索大模型的神奇世界。大模型是什么？大模型,全名大语言模型.简单来说，模型是AI系统的核心，用于处理数据和执行任务。而“大”模型则是指拥有大量参数的模型。这些参数可以看作是模型的“知识”。例如，OpenAI的GPT-4就是一个具有千亿级参数的大模型。为什么大模型如此重要？高准确性：随着模型参数的增加，模型通常能更好地学习和适应各种数据，从而提高其预测和生成的准确性。多功能性：大模型通常更为“通用”，能够处理更多种类的任务，而不仅仅局限于特定领域。持续学习：大模型的巨大容量使其更适合从持续的数据流中学习和适应新知识。大模型的研发流程大模型的研发流程涵盖了从数据采集到模型训练的多个步骤，以下是详细的过程：一、数据采集：这是大模型项目的起点，根据大模型训练的需求收集大量的数据。这些数据可以来自多种来源，如公开的数据集、公司内部的数据库、用户生成的数据、传感器数据等。数据的类型可以多样，包括图像、文本、声音、视频等。以下是详细的数据采集流程：定义数据需求：确定你需要收集什么样的数据。这应该基于你的问题陈述和项目目标。你需要理解你的问题是什么，然后决定哪种类型的数据（例如，数字、类别、文本、图像等）和哪些特定的特征可能对解决问题有帮助。找到数据源：确定数据来源。这可能包括公开的数据库、在线资源，或者你可以从公司内部的数据库或系统中收集数据。在某些情况下，你可能需要收集新的数据，例如通过调查或实验。数据收集：从选择的数据源中收集数据。这可能涉及到从数据库中导出数据，使用API来收集在线数据，或者使用特殊的数据采集设备。数据存储：将收集到的数据存储在合适的地方，以便进一步处理和分析。这可能涉及到设置数据库或使用文件系统。检查数据质量：查看收集的数据，确保其质量满足需求。你需要检查数据是否完整，是否有错误，是否有重复的数据等。数据整理：如果数据来自多个来源，或者在一个大的数据集中，你可能需要整理数据，使其在一定的上下文中有意义。这可能包括对数据进行排序，或者将数据分组，或者将数据从多个源合并在一起。数据采集可能是一个持续的过程，特别是对于需要实时更新或处理新信息的项目。在整个数据采集过程中，需要关注数据的质量和一致性，同时也要注意遵守数据隐私和安全的相关规定。二、数据清洗和预处理：收集的原始数据可能含有噪声、缺失值、错误数据等，所以首先要对数据进行清洗。清洗后的数据要进行一系列预处理操作，如归一化、编码转换等，使其适合输入到模型中。数据清洗和预处理是数据科学项目的重要步骤，它们有助于提高模型的性能并减少可能的错误。以下是数据清洗和预处理的详细流程：数据质量检查：这是数据清洗的第一步，其中涉及识别和处理数据集中的错误、重复值、缺失值和异常值。你需要验证数据的完整性、一致性和准确性，确保所有的记录都是准确的，与实际情况相符。处理缺失值：有多种方法可以处理数据集中的缺失值。这些方法包括：删除包含缺失值的记录；用特定值（如列的平均值、中位数或众数）填充缺失值；使用预测模型（如 KNN 或回归）预测缺失值；或者使用一种标记值来表示缺失值。处理重复值：如果数据集中存在重复的记录，那么可能需要删除这些重复的记录。在一些情况下，重复的记录可能是数据收集过程中的错误，但在其他情况下，重复的记录可能是有意义的，所以这需要根据具体情况来判断。处理异常值：异常值是那些远离其他观察值的值，这些值可能由测量错误或其他原因产生。处理异常值的方法包括：删除这些异常值；使用统计方法（如四分位数间距法）将它们替换为更合理的值；或者使用机器学习算法对其进行预测。数据转换：将数据转换为适合进行分析或建模的形式。这可能包括一下几种形式：规范化或标准化：将数值特征缩放到同一范围内，如 0 到 1，或者转换为具有零均值和单位方差的值。分类变量编码：例如，将分类变量转换为独热编码或标签编码。特征工程：创建新的特征，这些特征可能更好地表达数据的某些方面或者提高模型的性能。这个流程根据具体的项目和数据集可能会有所不同。在进行数据清洗和预处理时，你需要对数据有深入的理解，以便做出最好的决策。三、数据标注：数据标注，也叫数据标记，是一项为原始数据添加元信息的工作，以帮助大模型更好地理解和学习数据。对于监督学习任务，模型需要有标签的数据进行训练，数据标注的目标就是为数据提供这些标签,这个过程可能需要专门的标注团队.对于非监督学习任务，如聚类或生成模型，这一步则不需要。以下是数据标注详细流程：理解任务需求：首先需要理解你要解决的问题以及数据标注应该如何进行。例如，如果你在进行图像分类任务，你可能需要给每个图像一个分类标签；如果你在进行物体检测任务，你可能需要在图像中的每个目标物体周围画一个边界框，并给出这个物体的分类标签。制定标注规范：这是一个详细解释如何进行数据标注的指南，它应该解释哪些数据应该被标记，应该如何标记，以及如何处理可能出现的问题或歧义。清晰、详细的标注规范可以帮助保持标注的一致性，并提高标注的质量。选择或开发标注工具：有许多可用的数据标注工具，可以用于各种类型的数据标注任务。你应该选择或开发一个适合你的任务的标注工具。标注工具应该方便使用，提高标注效率，并尽可能减少错误。进行数据标注：按照标注规范，使用标注工具进行数据标注。这可能是一个时间和人力密集型的过程，尤其是当你有大量数据需要标注时。质量检查：检查标注的数据，确保标注的质量。这可能涉及到随机抽查一部分数据，并检查它们是否被正确和一致地标注。反馈和修正：根据质量检查的结果，如果发现任何问题或不一致，需要反馈给标注团队，并修正错误的标注。数据标注是一个重要但往往被忽视的步骤。高质量的标注数据对于训练出高性能的机器学习模型至关重要。因此，尽管这是一个复杂和耗时的过程，但投入在这个过程中的努力会得到回报。四、数据集划分：数据通常被划分为训练集、验证集和测试集。训练集用于模型训练，验证集用于超参数调整和模型选择，测试集用于最后的模型性能评估。数据集划分是大模型项目中的一个重要步骤，它可以帮助我们更好地理解模型在未见过的数据上的性能。以下是数据集划分的详细流程：确定划分策略：确定数据集划分的策略，这主要取决于你的数据集的大小和特性。一般的策略是将数据集划分为训练集、验证集和测试集。在大多数情况下，数据被划分为80%的训练集，10%的验证集和10%的测试集，但这并不是硬性规定，具体的划分比例需要根据实际情况来确定。随机划分：为了确保每个划分的数据分布与原始数据集相似，通常需要对数据进行随机划分。这可以通过洗牌数据索引来实现。分层抽样：在某些情况下，你可能需要确保每个划分中各类别的数据比例与整个数据集相同。这称为分层抽样。例如，如果你的数据集是一个二分类问题，你可能希望训练集、验证集和测试集中正负样本的比例都与整个数据集中的比例相同。时间序列数据的划分：对于时间序列数据，数据划分的策略可能会不同。通常，我们不能随机划分数据，而是基于时间来划分数据。例如，我们可能会使用前80%的数据作为训练集，然后使用接下来10%的数据作为验证集，最后10%的数据作为测试集。分割数据：按照你选择的策略，使用编程语言或者数据处理工具来划分数据。保存数据：保存划分后的数据集，以便于后续的训练和测试。确保训练数据、验证数据和测试数据被正确地保存，并且可以方便地加载。这个流程可能根据数据的类型和任务的需求有所不同。无论如何，正确的数据划分策略对于避免过拟合，以及准确评估模型的性能至关重要。五、模型设计：模型设计是大模型项目的关键环节，需要结合项目目标、数据特性以及算法理论选择或设计适合任务的模型架构。大模型可能会使用复杂的深度学习架构，如Transformer、BERT、ResNet等。以下是一般的模型设计流程：理解问题：首先，你需要理解你要解决的问题，并根据问题类型（例如，分类、回归、聚类、生成模型等）决定采用何种类型的模型。选择算法：根据你要解决的问题，选择合适的机器学习算法。这可能包括决策树、线性回归、逻辑回归、支持向量机、神经网络、集成学习等。在选择算法时，你需要考虑各种因素，如问题的复杂性、数据的大小和维度、计算资源等。设计模型架构：这主要涉及到深度学习模型，你需要设计模型的架构，例如神经网络的层数、每层的节点数、激活函数的选择等。此步骤可能需要根据经验和实验结果进行调整。设置超参数：超参数是在开始学习过程之前设置的参数，而不是通过训练得到的参数。例如，学习率、批量大小、迭代次数等。超参数的选择可能需要通过经验或者系统的搜索（例如，网格搜索、随机搜索或贝叶斯优化）来确定。正则化和优化策略：为了防止过拟合并提高模型的泛化能力，你可能需要使用一些正则化策略，如L1/L2正则化、dropout、early stopping等。同时，你还需要选择合适的优化算法（例如，SGD、Adam、RMSprop等）以及可能的学习率调整策略。定义评估指标：你需要定义合适的评估指标来衡量模型的性能。选择的评估指标应与你的业务目标和模型目标相一致。常见的评估指标包括精度、召回率、F1分数、AUC、均方误差等。这个流程可能需要根据具体的项目和需求进行迭代和调整。模型设计是一个需要技术知识、经验以及实验验证的过程。在设计模型时，你需要保持对模型复杂性和泛化能力之间平衡的认识，并始终以实现业务目标为导向。六、模型初始化：模型初始化是大模型项目中的一个重要步骤。在训练开始前，需要初始化模型的参数。这通常通过随机的方式进行。正确的初始化策略可以帮助模型更快地收敛，并减少训练过程中可能出现的问题。以下是模型初始化的详细流程：选择初始化策略：有许多不同的初始化策略可以选择，例如零初始化、随机初始化、He初始化、Xavier初始化等。你需要根据你的模型和激活函数来选择合适的初始化策略。例如，如果你的模型使用ReLU激活函数，He初始化可能是一个好的选择；如果你的模型使用tanh或sigmoid激活函数，Xavier初始化可能是一个好的选择。初始化权重：使用选择的初始化策略来初始化模型的权重。对于每一层，你都需要初始化它的权重。在大多数情况下，权重应该被初始化为小的随机数，以打破对称性并保证不同的神经元学到不同的特征。初始化偏置：初始化模型的偏置。在许多情况下，偏置可以被初始化为零。但是，对于某些类型的层（如Batch Normalization层），偏置的初始化可能需要更复杂的策略。设置初始化参数：某些初始化策略可能需要额外的参数。例如，随机初始化可能需要一个范围或者一个标准差，你需要设置这些参数。执行初始化：在你的模型代码中，执行初始化操作。大多数深度学习框架（如TensorFlow和PyTorch）都提供了内置的方法来执行初始化。模型初始化是一个比较技术性的主题，正确的初始化策略可能对模型的训练速度和性能有很大的影响。你应该了解不同的初始化策略，以便根据你的模型选择最适合的策略。七、模型训练：模型训练是大模型项目中的关键步骤，其中包含了多个环节。以下是大模型训练的详细流程：设置训练参数：首先，你需要设置训练参数，如学习率、训练迭代次数（epochs）、批次大小（batch size）等。准备训练数据：你需要将数据集划分为训练集、验证集和测试集。通常，大部分数据用于训练，一部分用于验证模型性能和调整超参数，剩余的一部分用于测试。前向传播：在前向传播阶段，模型接收输入数据，并通过网络层传递，直到输出层。这个过程中会生成一个预测输出。计算损失：根据预测输出和实际标签，使用损失函数（如均方误差、交叉熵等）计算损失。损失反映了模型预测的准确程度。反向传播：在反向传播阶段，算法计算损失函数关于模型参数的梯度，并根据这些梯度来更新模型参数。这个过程通常使用优化算法（如梯度下降、随机梯度下降、Adam等）来进行。验证和调整：在每个epoch结束后，使用验证集评估模型性能。如果模型在验证集上的性能没有提高，或者开始下降，这可能意味着模型过拟合了。你可能需要调整模型的超参数，或者使用一些正则化技术（如dropout、L1/L2正则化、早停法等）。重复上述步骤：重复前向传播、计算损失、反向传播和验证的步骤，直到模型性能达到满意，或者达到预设的训练迭代次数。模型测试：当模型训练完成后，使用测试集进行最终的性能评估。这能够提供模型在未见过的数据上的性能表现。以上就是模型训练的基本流程。但需要注意的是，实际操作中可能需要根据特定任务或特定模型进行相应的调整。八、模型验证：模型验证是大模型项目中非常关键的一步，目的是在训练过程中，评估模型的性能，定期在验证集上测试模型的性能，监控过拟合,根据测试和监控结果调整模型的超参数。以下是模型验证的详细流程：准备验证集：在数据集划分阶段，你应该保留一部分数据作为验证集。这部分数据不参与模型训练，仅用于模型验证。进行模型预测：使用训练好的模型对验证集进行预测。通常，在每一轮（epoch）训练结束后进行一次验证。计算评估指标：根据模型在验证集上的预测结果和真实标签，计算相应的评估指标。评估指标的选择取决于你的任务类型，例如，对于分类任务，常见的评估指标有准确率（accuracy）、精确率（precision）、召回率（recall）、F1分数（F1-score）等；对于回归任务，常见的评估指标有均方误差（MSE）、平均绝对误差（MAE）等。比较性能：将这一轮的验证性能与前一轮进行比较。如果性能提高，则可以继续进行下一轮训练；如果性能下降，则可能需要调整学习率、增加正则化等措施。早停法：如果在连续多轮训练后，验证性能没有显著提高，你可以使用早停法（early stopping）来提前结束训练，以避免过拟合。调整超参数：如果模型在验证集上的性能不佳，你可能需要调整模型的超参数，如学习率、批次大小、正则化参数等。一种常用的方法是使用网格搜索（grid search）或随机搜索（random search）等方式来自动搜索最优的超参数组合。以上就是模型验证的基本流程。需要注意的是，验证集应保持独立，不能用于训练模型，否则就可能导致模型的性能评估不准确，无法真实反映模型在未见过的数据上的性能。九、模型保存：模型保存是大模型项目的重要一步，能够让我们将训练好的模型存储起来，以便于后续的测试、部署或进一步训练或分享。以下是模型保存的详细流程：选择保存格式：你需要选择一个合适的模型保存格式。常用的模型保存格式包括：Python的pickle文件、joblib文件，或者某些深度学习框架的专有格式，如TensorFlow的SavedModel格式和PyTorch的pth格式。你的选择可能会受到你使用的工具和框架、模型的大小和复杂性、以及你的具体需求等因素的影响。保存模型参数：对于神经网络模型，你通常会保存模型的参数（即权重和偏置）。这些参数是通过训练学习到的，可以用于在相同的模型架构上进行预测。保存模型架构：除了模型参数，你也可能需要保存模型的架构。这包括模型的层数、每层的类型（例如，卷积层、全连接层等）、每层的参数（例如，卷积核的大小和数量、步长、填充等）、激活函数的类型等。保存训练配置：此外，你也可能需要保存一些训练的配置信息，如优化器类型、学习率、损失函数类型等。执行保存操作：使用所选工具或框架的保存函数，将模型保存到文件中。通常，这会创建一个可以在其他计算机或在其他时间加载的文件。验证保存的模型：加载保存的模型，并在一些测试数据上运行，以确保模型被正确保存并可以再次使用。以上就是模型保存的基本流程。需要注意的是，这个流程可能会根据你的具体需求和所使用的工具或框架进行一些调整。十、模型测试：模型测试是大模型部署前的最后一步，目的是在测试集上评估模型的最终性能。以下是模型测试的一般流程：准备测试集：在数据集划分阶段，你应该保留一部分数据作为测试集。这部分数据既不参与训练，也不参与验证，仅用于最后的模型测试。进行模型预测：使用训练并经过验证的模型对测试集进行预测。在此步骤中，你应当使用已保存的模型，而不是在训练过程中任何阶段的模型。计算评估指标：根据模型在测试集上的预测结果和真实标签，计算相应的评估指标。这些指标应当与你在训练和验证阶段使用的指标一致，以便于进行比较。分析结果：除了计算总体的评估指标，你也可以分析模型在特定类型的任务或数据上的性能。例如，你可以查看模型在某个特定类别上的精确率和召回率，或者分析模型在不同难度级别的任务上的表现。记录和报告：记录模型在测试集上的性能，并编写报告。报告应当包含模型的详细信息（例如，架构、训练参数等），以及模型在测试集上的性能结果。以上就是模型测试的基本流程。需要注意的是，测试集应当保持独立和未知，不能用于训练或验证模型，以确保测试结果能够真实反映模型在实际环境中的表现。十一、模型部署：模型部署是将训练好的大模型应用于实际生产环境中，使模型能够对新的数据进行预测。以下是大模型部署的详细流程：模型选择：在多个模型中选择一个适合部署的模型。这个模型应该是在验证和测试阶段表现最优秀的模型。模型转换：如果需要，将模型转换为适用于特定生产环境的格式。例如，如果你计划在移动设备上运行模型，你可能需要将模型转换为TensorFlow Lite或Core ML格式。部署策略：确定你的模型部署策略。你可能会选择将模型部署在本地服务器上，也可能选择将模型部署在云服务器上。此外，你还需要决定是否使用API、微服务或其他形式来提供模型服务。环境配置：配置你的生产环境。这可能包括安装必要的软件库，设置服务器参数，配置网络等。模型加载和测试：在生产环境中加载你的模型，并对其进行测试，以确保模型在生产环境中能够正确运行。模型监控：设置监控系统，以实时监测模型的性能。如果模型性能下降或出现其他问题，你应该能够及时得到通知。模型更新：根据模型在生产环境中的表现和新的数据，定期更新模型。这可能涉及到收集新的训练数据，重新训练模型，测试新模型，然后将新模型部署到生产环境中。以上就是模型部署的基本流程。需要注意的是，这个流程可能会根据你的具体需求和所使用的技术进行一些调整。部署机器学习模型是一个复杂的过程，需要考虑的因素很多，如模型性能、可扩展性、安全性、成本等。大模型的挑战然而，大模型在设计、研发、运行的过程中面临很多挑战：计算资源：训练和运行大模型需要大量的计算资源，这可能限制了许多机构和研究者使用它的能力。环境影响：大规模模型的训练对能源的需求是巨大的，可能会对环境造成负面影响。偏见和公正性：由于大模型通常从大量的互联网文本中学习，它们可能会吸收并再现存在于这些数据中的偏见。4. 未来的展望尽管大模型存在上述挑战，但研究者仍在积极寻找解决方法。例如，通过更高效的训练方法、结构优化等技术来降低能源消耗；或者通过更公正的数据收集和处理方法来减少模型偏见。总的来说，大模型为AI带来了无限的可能性和机会，但同时也伴随着一些挑战。只有不断地探索和创新，我们才能充分利用大模型的潜力，为人类创造更加美好的未来。作者简介:有过多年的ai产品和算法经验,目前ai创业中.微信公众号:微信公众号"
685915213,56eb8c4b9867b91db106129a5993305e,Suprit,https://api.zhihu.com/people/56eb8c4b9867b91db106129a5993305e,people,NLP方向硕士在读 退役ACMer,大模型快速入门学习路径【持续更新】,article,https://api.zhihu.com/articles/685915213,Prompt设计 Step3：<em>模型</em>微调（Fine-tuning） Step4：RAG（外挂数据库） 其它知识 学习目标 了解<em>大模型</em>以及对应 NLP 知识的基础原理,48,1,157,1709865413,1709865501,LLM 快速学习路径目录学习目标前置知识：Python 基础、Linux 基础Step1：NLP 相关基础知识Step2：GPT API 调用及 Prompt设计Step3：模型微调（Fine-tuning）Step4：RAG（外挂数据库）其它知识学习目标了解大模型以及对应 NLP 知识的基础原理，能够熟练调用GPT API，编写 Prompt 完成各种任务。能够实现简单的微调模型，构造 QA 对，上传并外挂数据库等常见主流技术方案。 注：大模型作为目前最火的技术热点之一，几乎所有的学习资料和教程都可以公开查到，不推荐报名参加任何的付费课程。前置知识：Python 基础、Linux 基础 学习目的： 熟练掌握 Python 语言，熟悉常用的 Python 库和工具，如 NumPy、Pandas、Matplotlib、Scikit-learn、TensorFlow、PyTorch 等。 具备 NLP 相关的基础知识，包括文本预处理、分词、词性标注、命名实体识别、词向量表示等。 对大模型有一定了解，包括 Transformer 模型的结构和原理、基于注意力机制的自然语言处理技术等。 - 学习内容 廖雪峰 python 教程：https://www.liaoxuefeng.com/ - 学习要求 熟练掌握并能够编写基础的 python 函数、语法等，能够熟练使用 Linux 系统 Step1：NLP 相关基础知识 学习目的： 了解文本预处理、分词、词性标注、命名实体识别、词向量表示等基础知识。 掌握自然语言处理（NLP）相关技术，如分词、词性标注、命名实体识别、句法分析等。 掌握机器学习中的数据预处理、特征提取、分类、回归等基础算法，并了解其在 NLP 领域的应用。 了解大规模 NLP 任务中的常用技术和方法，如深度学习中的 Transformer 模型、BERT、GPT 等。 - 学习内容 李沐 动手学深度学习：https://zh.d2l.ai/ 视频：https://space.bilibili.com/1567748478/ 大模型理论基础：https://github.com/datawhalechina/so-large-lm - 学习要求 理解并掌握机器学习、深度学习、自然语言处理的基础概念，最好能够阅读并吸收课程中提到的经典论文，能够独立实现在 colab 上训练模型（小模型） Step2：GPT API 调用及 Prompt设计 学习目的： 了解 GPT API 的调用方式和基本操作，熟悉 Prompt 设计技巧和要点，能够结合自己的任务调用 API 实现对应的任务代码。 - 学习内容： 推荐 API 购买地址：https://peiqishop.me/ 调用教程：https://zwxvec6g91g.feishu.cn/docx/KS7AddreTouQg6xs44Oc0ZgHnOg 常用 Prompt：https://zhuanlan.zhihu.com/p/626024467 - 学习要求 了解大模型以及对应 NLP 知识的基础原理，能够熟练调用GPT API，编写 Prompt 完成各种任务。 Step3：模型微调（Fine-tuning） 学习目的 了解常见的微调模型的基本流程和原理，熟悉数据集的构造、训练、评估等过程，能够独立构建 QA 对，在服务器上对模型进行微调。 - 学习内容 QA 问答对微调数据构建：https://blog.csdn.net/u012960155/article/details/132658756 LLM 全流程框架（包含教程）：https://github.com/hiyouga/LLaMA-Factory/tree/main 手把手微调教程：https://zhuanlan.zhihu.com/p/645010851 服务器租用（最便宜，但可能要等）：https://gpushare.com/、https://www.autodl.com/login?url=/market/list - 学习要求 能够独立完成大模型的微调数据构建、训练以及部署工作。 Step4：RAG（外挂数据库） 学习目的 RAG 作为目前最火的一个 LLM 落地方向，可以结合私有数据（表格、word、txt、pdf、数据库 皆可）实现本地问答，且训练成本较低，可以快速实现效果。 - 学习内容 Langchain：https://www.langchain.asia/ FastGPT：https://github.com/labring/FastGPT LangChain-Chatchat（快速部署框架）：https://github.com/chatchat-space/Langchain-Chatchat DB-GPT：https://github.com/eosphoros-ai/DB-GPT RAG进阶优化：https://zhuanlan.zhihu.com/p/675560679 - 学习要求 能够在本地实现基于大模型的外挂部署方案 其它知识作为补充，目前有一些公开的教程或者资料库可以作为补充：LLM 资料库：https://github.com/morsoli/llm-booksLLM 应用开发实践笔记：https://aitutor.liduos.com/ref/ref.htmlLLM 九层妖塔：https://github.com/km1994/LLMsNineStoryDemonTower?tab=readme-ov-file
668455345,c4af492e353c019c23b9090207e95d19,toyama nao,https://api.zhihu.com/people/c4af492e353c019c23b9090207e95d19,people,独立音游开发者/业余AI评测员,11月国产AI大模型横评（GPT4 Turbo，文心4.0，通义2.0，vivo蓝心）,article,https://api.zhihu.com/articles/668455345,3）SuperCLUE榜单空降国产AI第一名的vivo蓝心<em>大模型</em>，实际表现如何？ 4）文心一言4.0 够不够强？值不值得付费？ #1 前情提要 本评测是个人性质，结合自己需求和对<em>大模型</em>的理解，使用私有题库进行长期跟踪评测,193,108,204,1700736699,1700736699,#0 本期看点1）GPT4 Turbo相比GPT4进化了多少？2）阿里通义从1.0.8跃升到2.0.0，除了版本号，还有什么变化？3）SuperCLUE榜单空降国产AI第一名的vivo蓝心大模型，实际表现如何？4）文心一言4.0 够不够强？值不值得付费？#1 前情提要本评测是个人性质，结合自己需求和对大模型的理解，使用私有题库进行长期跟踪评测。不够权威，不够全面。但可以从一个侧面观察各个大模型的长期进化趋势。任何评测都无法给出无死角的权威排行榜，笔者写这个系列也是分享一种评测思路，以及个人见解。每个人应该根据自己所需，对大模型进行考察。10月的评测见：toyama nao：10月国产AI大模型横评（新写作榜单，讯飞3.0，腾讯混元）#2 参赛选手11月期间有更新的包括：腾讯混元 1.5.1阿里通义 2.0.0百度文心一言 4.0 （网页显示2.5.0）ChatGPT（GPT-4 Turbo）Kimi Chat（月之暗面）Vivo 蓝心（通过蓝心小V App）其余模型由于没有更新，继续沿用前次的成绩，本次没有重测：字节豆包（10月末版本）讯飞星火 3.0紫东太初（应@烄防御天赋北国风光 要求加入）360智脑 （9月末版本）昆仑天工（v3.5.20230830.a）商汤商量（8月末版本）ChatGPT（Lagacy）Claude2（7月末版本）ChatGLM3（官网版本，非开源6b）#3.1 基础题基础题采用负向打分，即出现相应负向情况，扣1分。最后按扣分从少到多排序。基础题考察模型基本功：指令理解，阅读理解，逻辑推理，数学计算，科学常识，翻译，机器幻觉等共9个分类。基础题都有明确且固定的答案，并且对答案也有明确约束，如字数，格式，知识范围等。解题需要的信息大部分在题干中直接给出，但需要模型掌握必要科学计算公式，理科常识，社会学常识等。不涉及生僻问题。基础题的扣分维度包括：答案错误：计算类，知识类答案不正确，翻译类理解错误。混乱输出：输出内容人类不可读，重复，错误解释，翻译用词不够准确等。指令误解：不能完全按照指令输出回答。拒答：因触发关键词或人类价值观对齐（基于错误理解）导致不回答。同时因为输出不符合预期，答案错误和指令误解也各扣1分。趋势汇总：#3.2 写作题写作题考察诗词创作，中学作文，应用文，角色扮演，风格化改写，共5个分类。写作题只提要求，不提供任何额外信息。比如要求创作五言绝句时，不会提示押韵规则，平仄规则等。写作题并不能体现文章写得有多好，只反映文章有多“不可用”。写作题的扣分维度包括：字数问题：超过要求的字数10%以上。格式问题：诗词类，不满足格式。不押韵：诗词类韵律不正确。病句：不通顺，不合逻辑。主题偏离：不符合创作要求的中心思想。文体问题：行文不符合指定的角色，不符合要求的用词风格。口语化表达：在所有没有明确要求口语表达的题目中，用词造句过于口语，平铺直叙，流水账等。标点问题：中英文标点混用，断句错误。指令误解：特殊问题，完全不按要求创作，比如原样输出题目，用英文答题。由于不按要求创作就无法按以上8项扣分，所以此项额外扣1分。其中字数问题和标点问题和其他扣分点相比，不算太严重，用户在真实使用场景很容易自行纠正，所以也额外提供一个不包含这2项的榜单。但排名仍然按总负分排。#3.3 高阶逻辑目前只给GPT-4和号称对标GPT-4的模型做高阶测试题。同时增加了ChatGPT 3.5的成绩作为对照。高阶题库目前有14道题，侧重复杂任务，复杂逻辑和长链推理，属于大模型可用性中最核心的部分。后面会在不泄漏原题的情况下，简介每道题的考察点。每道题都有唯一正确答案，根据问题类型，答案的给分点也不同。每道题总分10分，按给分点折算成绩，比如有10个给分点，答对2点，得2分。部分题目需要输出长字符串，这类题目按最长连续答对的字符数除以总字符数给分。最终成绩是成绩相加，未来会继续完善题库，增加考察角度，总分也会随之提升。目前的题目：#4. 成绩解析1）炸裂全场的GPT-4 Turbo，上下文更长，能力更强，调用价格更低。基准测试里也确实如此，基础题已经都全对了，扣了4分扣在古文解析和高中三角函数题这2个大模型传统弱项上。这意味着，如果你的工作不涉及古诗文，不用来辅导数学题，那GPT4 Turbo现阶段就能完全满足你。但价格便宜不是没有代价，代价就是相比老GPT-4，长链逻辑能力有轻微劣化。2）同样炸裂的还有文心4.0，发布时号称和GPT4 “不相上下”。但测试下来，依然发现大量和老文心3.5类似的回答错误，并不惊艳，基础题分数提升不小，但还不如老GPT4，更比不过新Turbo。但单看文心4.0的复杂逻辑能力，是毫无疑问接近GPT4了，只落后不到10%。并且领先ChatGPT是没有争议的。要知道，高阶逻辑题目前14题，满分140，经受高等教育的普通人做一遍也需要接近30分钟，并且拿不到满分。笔者也测过号称能打ChatGPT的几个不透露名字模型，分数只有20不到。可见文心4.0的含金量。但作为国内第一个付费的模型，还得多说两句，目前文心4.0的定价偏高了。文心相比老GPT4的定价是有优势的，但奈何新Turbo只有原来的1/3定价，这让文心显得很尴尬。考虑国内用户使用成本，笔者认为文心定价在30包月比较合适，或者推出按问答次数付费。3）阿里通义2.0.0一如既往的让人失望，各方面改进极其微弱，只有6%提升，让人一度怀疑模型是不是没有更新。但确实又能答对一些之前的错题，并且在之前正确的题上犯错。4）腾讯混元在10月榜单空降倒数，11月连发2个大迭代，终于有点支棱起来了。1.5相比1.1版本提升高达20%，目前仅次于通义。不过分数虽然很差，但从回答思路来看，混元模型的底子应该还可以，再提升20%，接近ChatGPT问题不大。5）来自月之暗面的Kimi Chat让人有些意外，空降榜单前列，能力和商汤相当，但目前对安全把控过严，拒答情况很多。Kimi的特点是上下文很长，但由于基础题和写作对上下文要求都不高，所以体现不出来。按ChatGPT以下皆为不可用来评判，这一款大模型不太能日常使用。6）月初Vivo的发布会重点宣传了蓝心大模型，接着买了一堆Kol一顿吹，也在SuperCLUE榜上空降第一名，压文心4.0一大截。但实测成绩十分不乐观，错误方式非常离奇，有服务器不响应，有花样拒答，有梦游答题。推测是上游算力并没有准备好，不具备一下子开放给全体用户使用的基础。并且测了这么多大模型来看，从初始成绩基本能推测能力上限，蓝心模型极限大概就在开源的ChatGLM3附近。#5. 下期看点1）还没拿到Claude2.1 的测试途径，这次更新号称提升30%能力，增加一倍上下文。还是很有看头。2）字节豆包已经沉寂3个月没有大动作了，不知12月会不会有大更新。3）腾讯迭代速度较快，12月应该还会有大更新。
682893729,d522413871e0705b89072284ecdd5a94,林夕 AIGC,https://api.zhihu.com/people/d522413871e0705b89072284ecdd5a94,people,专注多模态大模型研究,多模态大模型入门指南-长文慎入【持续更新】,article,https://api.zhihu.com/articles/682893729, 部分模型 总结如下： 1.引言： 1.1 多模态LLMs 的现状： 最近，多模态<em>大模型</em>取得重大进展。随着数据集和模型的规模不断扩大，传统的 MM 模型带来了巨大的计算量,161,4,359,1708350052,1712396667,"内容总结，本篇综述主要介绍和分析了以下几个方面：概述了MM-LLMs的设计形式，将模型架构分为5个部分：模态编码器、输入投影器、语言模型骨干、输出投影器和模态生成器。阐述了每一部分的实现选择。描述了MM-LLMs的训练流程，主要包括多模态预训练和多模态指令微调两个阶段。总结分析了26种主流的MM-LLMs模型，从模型架构、训练数据集规模等多个维度进行了对比。综合回顾了主要MM-LLMs在18个广泛使用的视觉语言评测集上的表现，并总结提炼出提升模型效果的重要训练方法。探讨了MM-LLMs未来发展的5大方向：构建更强大的模型、设计更具挑战性的评估集、移动端/轻量级部署、具备实体性的智能和持续性指令调整。综上，该论文系统梳理了MM-LLMs的框架、模型、评估指标和未来研究方向，对其现状和发展趋势进行了全面而深入的总结，为相关领域的研究与进一步发展奠定了基础。部分模型 总结如下：1.引言：1.1 多模态LLMs 的现状：最近，多模态大模型取得重大进展。随着数据集和模型的规模不断扩大，传统的 MM 模型带来了巨大的计算量，尤其是从头开始训练的话。研究人员意识到 MM 的研究重点工作在各个模态的连接上，所以一个合理的方法是利用好现成的训练好的单模态基础模型，尤其是 LLM。这样可以减少多模态训练的费用，提升训练效率。MM-LLM 利用 LLM为各种 MM 任务提供认知能力。LLM 具有良好的语言生成，zero-shot 和 ICL 的能力。其他模态的基础模型则提供了高质量的表征。考虑到不同模态的模型是分开训练的，如何将不同模态连接起来，实现协同推理，是核心挑战。这里面的主要工作便是通过多模态预训练和多模态的指令微调，来实现模态之间的对齐，以及模型输出与人类意图的对齐。1.2 多模态的发展历程：关于多模态的发展主要有：最初的发展集中在多模态的内容理解和文本的生成：Flamingo,BLIP-2, Kosmos-1,LLaVA/LLaVA-1.5/LLaVA-1.6，MiniGPT-4，MultiModal-GPT，Video-Chat，VIdeo-LLaMA，IDEFICS，Fuyu-8B，Qwen-Audio同时实现多模态的输入和输出工作 MM-LMM，探索特定模态的生成，例如 Kosmos-2，Mini-GPT5，以及语音生成的 SpeechGPT将 LLM 和外部工具继承进来，实现“any-to-any”的 多模态理解和生成。visual-chatgpt，ViperGPT，MM-React，HuggingGPT，AudioGPT同样，有为了减少级联过程中传播误差的工作，有 NExT-GPT 和 CoDI-2，来开发任意模式的多模态模型1.3 内容结构：本文接下来的机构如下所示。第二节讲模型架构，第三节讲训练流程。第二节模型架构分为五部分，不同模态的编码器 Encoder，输入 Projector，LLM 骨干，输出 Projector，不同模态的生成器。训练流程包含连个部分，预训练流程和指令微调部分。同时提供了主流的数据集的介绍。第四节，讨论 26 个最先进的多模态模型。2.模型结构：如图 2 所示，这里包含了通用多模态模型结构的五个组件部分，以及每个组件部分的常用选择。多模态理解主要是前三个部分。（模态对齐）训练期间，encoder，LLM Backbone 和 generator 一般保持冻结。主要优化输出和输出的 projector。由于Projector 是轻量级的模块，MM-LLMs 中可以训练的参数比例和总参数相比非常小（2% 左右），模型的总体参数规模取决于LLM 部分。这里对论文的内容进行更正，目前有的多模态大模型，也会对 Encoder，甚至 LLM 进行训练，来提升整体的模型的能力。2.1 Modality Encoder：模态编码器主要是对来自不同模态的输入进行编码，来获得相应的特征：F_X = ME_X(I_X)存在各种预训练的编码器来处理不同的模态，模态可以是图像，视频，音频，3D 等。视觉模态：对于图像，一般有四个可选的编码器，NFNet-F6，ViT，CLIP VIT，EVA-CLIP ViT。NFNet-F6：是一个无归一化的 ResNet 网络，可以在增强过的数据集上获得 SOTA 的图像识别的性能。VIT：采用 transformer 模型，将 image 变成 patch，然后对图像进行处理。然后经过线性投影flatten，然后经过多个 transformer 模块。CLIP-VIT：利用大量的文本-图像快，通过对比学习来优化 ViT，将成对的文本图像视为正样本，其他的文本和图像视为负样本。EVA-CLIP：对大规模的 CLIP 训练稳定了训练过程和优化过程。对于视频，可以统一采样 5 帧，进行与图像同样的处理。音频模态：通常使用 C-Former，HuBERT，BEATs 和 Whisper 等进行编码。C-Former：使用了 CIF 对齐机制来实现序列的转换，并且使用一个 Transformer 来提取音频特征HuBERT：是一个自监督的语音表征徐诶框架，基于 BERT。通过离散hidden units 的mask 预测来实现BEAT 是：是一个迭代的音频预训练框架，使用音频 Transformer 来学习双向编码表示2.2 输入 Projector：输出projector 的任务是将其他模态的编码特征F_X与文本特征空间的特征T进行对齐。对齐后的特征作为prompts P_x联通文本特征F_T输入到 LLM Backbone 内。给定 X 模态-text数据集\{I_X,t\},目标是最小化生成损失。输入 Projecor 可以通MLP 或者多层 MLP 来实现。也有复杂的实现，比如 Cross-Attention，Q-Former，P-Former 等。Cross-Attention 使用一系列的可训练的 query 和编码特征 F_X作为 key 来压缩特征序列到固定的长度。将压缩的表示特征输给 LLM。2.3 LLM Backbone：LLM作为核心智能体，MM-LLMs 可以继承一些显着的属性，如零样本泛化（zero-shot）、少样本 ICL、思想链 (CoT) 和指令遵循。 LLM 主干处理来自各种模态的表示，参与有关输入的语义理解、推理和决策。它产生 (1) 直接文本输出 t，以及 (2) 来自其他模式（如果有）的信号token S_x。这些信号token充当指导生成器是否生成 MM 内容的指令，如果是，则指定要生成的内容：t,S_X = LLM(P_X,F_T)上式中，其他模态P_X的对齐后的表征，可以认为是软 prompt-tuning，输给 LLM Backbone。发而且一些研究工作引入了 PEFT 的方法，例如 Prefix-tuning，Adapter 和 LoRA。这些 case 里面，希望更少的参数可以被训练，甚至少于 0.1% 的 LLM 的参数参与训练。通常用到的 LLM 模型有 Flan-T5，ChatGLM，UL2，Qwen，Chinchilla，OPT，PaLM，LLaMA ，LLaMA2 ，Vicuna 等。2.4 Output Projector：输出Projector将 LLM 的输出的 token 表征S_X转变成特征H_X，然后输给生成器MG_X。给定数据X-text数据集\{I_X, t\}，首先将文本t输给 LLM，生成对应的S_X，然后映射得到H_X。模型优化的目标是最小化H_X与MG_X的条件文本之间的距离。2.5 模态生成器：模态生成器MG_X一般用于生成不同的模态来输出。当前的工作一般使用现成的扩大模型（Latent diffusion model），例如 Stable Diffusion用于图像生成，Zeroscope用于视频生成，AudioLDM-2 用于音频生成。输出 Projector 输出的特征H_x作为条件输入，在去噪的过程中，用于生成 MM 的内容。训练过程中， gt content 首先转换为 latent feature z_0，由预训练好的 VQA 模型。然后噪声\epsilon加到z_0上，获得 noise latent feature z_t,预训练好的 UNet 用于计算条件损失，通过最小化 loss 来优化参数。3.训练过程：3.1 预训练阶段：在预训练阶段，通常利用 X-Text 的数据集，来训练输入，输出的 Projector。通过优化损失函数来实现不同模态的对齐。PEFT 有时候用于 LLM Backbone。X-文本数据集包含图像-文本、视频-文本和音频-文本，其中图像-文本有两种类型：图像-文本对（即<img1><txt1>）和交错图像-文本语料库（即，txt1><img1><txt2><txt3><img2><txt4>）。这些 X-Text 数据集的详细统计数据如附录 F 的表 3 所示。3.2 多模态微调：多模态微调是对满足指令微调格式的一系列数据集对预训练好的多模态大模型进行微调。通过这种微调，MM-LLM 是可以遵循新的指令泛化到没有见过的任务，增强 zero-shot 的能力。这个简单而有影响力的概念促进了 NLP 领域后续努力的成功，例如，InstructGPT、OPT-IML、InstructBLIP。MM IT 包括监督微调（SFT）和 RLHF 两部分，目的是为了使得模型符合人类的意图或者偏好，并且增强 MMLLMs 的交互能力。SFT 将 PT 阶段的额数据转换为指令-aware 的格式，使用 QA 任务作为例子。可以采用各种模板，例如：(1) <Image>{Question} A short answer to the question is; (2) <Image>Examine the image and respond to the following question with a brief answer: {Question}. Answer:优化目标和预训练是相同的，SFT 数据可以构造为单论的 QA 或者多伦的 QA。常用的 SFT 和 RLHF 的数据集见表 4。4.多模态大模型总结：4.1 26 个多模态大模型全面比较：如表1所示，对26 SOTA MM-LLMs的架构和训练数据集规模进行了全面比较。随后，简要介绍这些模型的核心贡献并总结了它们的发展趋势。(1) Flamingo。 代表了一系列视觉语言 (VL) 模型，旨在处理交错的视觉数据和文本，生成自由格式的文本作为输出。(2) BLIP-2 引入了一个资源效率更高的框架，包括用于弥补模态差距的轻量级 Q-Former ，实现对冻结 LLMs 的充分利用。利用 LLMs，BLIP-2 可以使用自然语言提示进行零样本图像到文本的生成。(3) LLaVA 率先将 IT 技术应用到 MM 领域。为了解决数据稀缺问题，LLaVA 引入了使用 ChatGPT/GPT-4 创建的新型开源 MM 指令跟踪数据集以及 MM 指令跟踪基准 LLaVA-Bench。(4) MiniGPT-4 提出了一种简化的方法，仅训练一个线性层即可将预训练的视觉编码器与 LLM 对齐。这种有效的方法能够复制 GPT-4 所展示的功能。(5) mPLUG-Owl提出了一种新颖的 MM-LLMs 模块化训练框架，结合了视觉上下文。为了评估不同模型在 MM 任务中的表现，该框架包含一个名为 OwlEval 的教学评估数据集。(6) X-LLM 陈等人 扩展到包括音频在内的各种模式，并表现出强大的可扩展性。利用Q-Former的语言可迁移性，X-LLM成功应用于汉藏语境。(7) VideoChat 开创了一种高效的以聊天为中心的 MM-LLM 用于视频理解对话，为该领域的未来研究制定标准，并为学术界和工业界提供协议。 (8)InstructBLIP 基于预训练的BLIP-2模型进行训练，在MM IT期间仅更新Q-Former。通过引入指令感知的视觉特征提取和相应的指令，该模型使得能够提取灵活多样的特征。(9) PandaGPT 是一种开创性的通用模型，能够理解 6 不同模式的指令并根据指令采取行动：文本、图像/视频、音频、热、深度和惯性测量单位。(10) PaLI-X 使用混合 VL 目标和单峰目标进行训练，包括前缀完成和屏蔽令牌完成。事实证明，这种方法对于下游任务结果和在微调设置中实现帕累托前沿都是有效的。(11) Video-LLaMA 张引入了多分支跨模式PT框架，使LLMs能够在与人类对话的同时同时处理给定视频的视觉和音频内容。该框架使视觉与语言以及音频与语言保持一致。(12) 视频聊天GPT Maaz 等人。 （2023）是专门为视频对话设计的模型，能够通过集成时空视觉表示来生成有关视频的讨论。(13) Shikra Chen 等人。 (2023d) 介绍了一种简单且统一的预训练 MM-LLM，专为参考对话（涉及图像中区域和对象的讨论的任务）而定制。该模型展示了值得称赞的泛化能力，可以有效处理看不见的设置。 (14) DLP提出 P-Former 来预测理想提示，并在单模态句子数据集上进行训练。这展示了单模态训练增强 MM 学习的可行性。(15) BuboGPT 是通过学习共享语义空间构建的模型，用于全面理解MM内容。它探索图像、文本和音频等不同模式之间的细粒度关系。 (16)ChatSpot 引入了一种简单而有效的方法来微调 MM-LLM 的精确引用指令，促进细粒度的交互。由图像级和区域级指令组成的精确引用指令的结合增强了多粒度 VL 任务描述的集成。 (17) Qwen-VL 是一个多语言MM-LLM，支持英文和中文。 Qwen-VL 还允许在训练阶段输入多个图像，提高其理解视觉上下文的能力。 (18) NExT-GPT 是一款端到端、通用的any-to-any MM-LLM，支持图像、视频、音频、文本的自由输入输出。它采用轻量级对齐策略，在编码阶段利用以LLM为中心的对齐方式，在解码阶段利用指令跟随对齐方式。(19) MiniGPT-5 郑等人。是一个 MM-LLM，集成了生成 voken 的反演以及与稳定扩散的集成。它擅长为 MM 生成执行交错 VL 输出。在训练阶段加入无分类器指导可以提高生成质量。4.2 论文详细解读：林夕 AIGC：多模态LLM论文分享（一）： MiniGPT V2版本林夕 AIGC：多模态LLM论文分享（二）： 智谱开源CogVLM林夕 AIGC：多模态LLM论文分享（三）：Sphinx更全面的视觉表征林夕 AIGC：多模态LLM论文分享（四）：Google Gemini 的雏形 PaLI-X参考资料：综述：[1] Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey[2] MM-LLMs: Recent Advances in MultiModal Large Language Models"
660759033,94d294e72b5058a06aa8f95f797f8fad,akaihaoshuai,https://api.zhihu.com/people/94d294e72b5058a06aa8f95f797f8fad,people,喜欢就去做，不喜欢的才需要理由,从0开始实现LLM：1、大模型训练踩坑,article,https://api.zhihu.com/articles/660759033,之前一直使用开源的模型进行简单微调，并没有亲自训练过<em>大模型</em>。因此参考了一些github代码和文章，踩一踩训练大模型的坑。 虽说是训练大模型，但也就几百兆的参数量,338,19,1127,1697785433,1711375539,"之前一直使用开源的模型进行简单微调，并没有亲自训练过大模型。因此参考了一些github代码和文章，踩一踩训练大模型的坑。虽说是训练大模型，但也就几百兆的参数量，主打一个流程体验。。。1、LLM训练代码参考（直接拉下来用）baby-llama2-chinese的训练方法。相关文章：limzero：从头训练一个迷你中文版Llama2--一个小项目踏上LLM之旅首先拉取数据进行数据整理进行预训练CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --use_env pretrain.py微调CUDA_VISIBLE_DEVICES=0 python sft.py2、模型分析本人服务器使用的4090，内存24G。使用代码中默认的模型参数内存会爆，因此修改了模型参数。dim: 512 n_layers: 8 n_heads: 8此时模型结构为模型结构模型参数量—预训练GPU内存分析模型参数量（其中tok_embeddings和output的权重参数相同，只算一个即可） 模型内存为 模型训练使用了AdamW优化器，训练总共所需内存为模型状态模型权重：fp32精度：58,470,912*4 bytes(float) = 223.11M模型权重更新梯度：fp32精度：58,470,912*4 bytes = 223.11MAdamW优化器内存（存储两部分的优化器状态：time averaged momentum(动量估计)和variance of the gradients(梯度方差)）：58,470,912*4 bytes(float)*2=446.22M中间激活值：attention激活值FFn激活值Batch_size为1时，占用内存大小为 参考Connolly：Self Attention 固定激活值显存分析与优化及PyTorch实现参考：宋鸣：LLM模型训练的内存消耗GPU初始化会占用内存800M（任意变量to('cuda')，GPU显示内存为变量内存+800M左右，这个不同显卡占用大小并不相同）3090服务器上GPU初始化内存占用1400M左右因此当batch_size为1时，GPU内存消耗如下所示。 实际运行消耗如下图所示（大概符合）因此对于24G内存的显卡，使用 ，是比较合适的选择（batchsize太小效果不好，太大内存不够）。layers/head_nums数量对内存影响使用 参数，测试layer数目对参数的影响每个layer影响3M的模型内存，影响500M的预训练内存。head_nums对模型权重基本没有影响。（因为attention本身权重在整个模型权重所占的比例并不高，大头是tok_embeddings权重）hidden_dims数量对内存影响使用 参数，测试layer数目对参数的影响hidden_dims对模型参数量影响极大。对预训练内存也有较大影响。max_seq_len对内存影响max_seq_len参数不影响模型内存，影响中间激活值内存。当前的max_seq_len默认为512，若修改为256，则训练内存几乎减半。注意：模型预训练和微调时加载的数据大小会有差异，比如baby-llama2-chinese的SFTDataset比PretainDataset多了loss_mask参数（可以去除），同样大小的模型，pretain阶段内存刚好够用，但sft阶段内存就不够了。建议先减少训练数据，快速将整个流程跑通，测试参数的合理性。耗时估计确定模型参数后，便可读取数据进行预训练。当前使用了3.8547B个token，数据内存7.7G。若seq_len=512, batch_size为32，则需要迭代 根据每一次迭代的耗时，即可推算出整个预训练耗时。使用4090单卡预训练1个epoch大概需要600min。4卡同时训练大概150min≈2.5h微调模型数据为25091条，每个epoch耗时30min左右。注意：大语言模型由于训练时间较长，因此建议每个epoch都保存一下权重，且可以通过nohup进行后台挂起，以防止ssh远程时，主机重启导致ssh断开程序中止的问题。tokenizer模型tokenizer模型采用了ChatGLM2的分词模型，也可以自己根据词表重新训练。参考：https://github.com/yanqiangmiffy/how-to-train-tokenizerGitHub - bojone/bytepiece: 更纯粹、更高压缩率的TokenizerBytePiece：更纯粹、更高压缩率的Tokenizer - 科学空间|Scientific Spaces 3、模型训练+微调+评估预训练pretrain log微调模型，评测模型的效果。当前工程中有eval.py，运行可以针对目标数据集进行推理，将推理结果和数据集答案计算BLEU score，根据得分评估质量。也可以参考lm-evaluation-harness项目评测模型质量参考文档：呵呵哒：LLM模型评测代码实践Loss判断首先当batch_size=32，gradient_accumulation_steps=1、2、4、8时可以看出模型在迭代30000次左右，还没有训练完成1个epoch(120000)时，loss便已经趋于稳定。对上面预训练的模型进行微调，可得到如下结果用高质量数据微调后，loss有一定程度下降。评估指标判断可以看出模型的medical数据评估得分较低。ceval和mmlu虽然都在25左右，但由于这两个测试集是选择题，即使随便蒙平均也有25%的正确率，因此25分左右可认为没有什么回答能力。估计模型的参数量太低也占据部分原因，考虑采用更大的参数量重新训练。4、模型参数扩增由于模型的参数量太低，导致LLM训练无法出现涌现现象，考虑增大参数量。经过测试后，模型参数确定为max_seq_len: 1024 dim: 1024 n_layers: 24 n_heads: 32参数量为369,910,784。模型大小为1.38G。由于参数量增加，在显卡内存不变的情况下，batch_size只能减小，设置为4，此时预训练占用显卡内存20G。（24G内存无法支持更大的batch_size）参考GPT3和LLaMA的介绍，batch_size通常设置在0.5M~4M。因此gradient_accumulation_steps增大为64，这样总batch_size等同于4x4x64=1024。四卡同时训练10个epoch，单个epoch耗时在10个小时左右。质量评估从loss上看出，最终loss基本稳定在3上下波动。评估结果可以看到评分更低了。。。Ceval_eval_scores: 21.9298 MMLU_eval_scores: 22.9454针对每个spoch结果进行测评得多个epoch的模型相比，基本上没有什么区别，且都效果很差。（/(ㄒoㄒ)/~~）不过基模型不善于问题也是正常。 针对不同epoch的模型进行微调，可以得到评测结果如下所示微调结果也不好。5、flash-attention优化首先统计当前模型训练时每推理一次的平均耗时为0.133s左右。从github链接下载flash-attention代码，并进行安装。GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention官方使用教程如下图所示在要替换的项目代码中查找attention计算部分attention计算过程修改如下（---需要设置causal=True---）或者对比计算结果，修改前后的output结果值完全相同，表明修改正确。几种计算方式的耗时对比如下表所示torch.nn.functional.scaled_dot_product_attention()确实比一步一步手动计算要快，且省内存。但和flash_attn_func()、flash_attn_qkvpacked_func()相比，耗时和内存基本相差无几。。。/(ㄒoㄒ)/~~考虑到可能是由于batchsize太小，没有明显区别。因此将模型参数量减少，增大batch size=64/128。 dim: 256 n_layers: 8 n_heads: 8 max_seq_len: 256torch.nn.functional.scaled_dot_product_attention()和flash_attn_func()耗时也几乎完全相同。在batch size=1时，几乎没有差别。在batchsize=4时，flash_attn_func大概快了1.5%。估计是batch size和seq_lens不够大，越大应该越能表现出flash-attention的优势。6、MQA、GQA优化一张图简单理解，就是多个heads之间共用KV权重的程度代码中就是在创建kv线性层的时候将输出层的数量设置为对应的head_num*dims其中模型参数减少 对于heads=32，layer=24，dims=1024的模型来说，参数量从369,950,720减少至321,191,936，降低了13.2%耗时从0.128s减少至0.116s，降低了9.37%很多地方会说使用了MQA和GQA时，kv结果的维度减小，需要repeat_kv到和q一样的维度其中 self.n_rep = self.n_heads // self.n_kv_heads 其实从结果上看没有必要。测试也能看出结果完全一摸一样（模型参数不变，内存消耗不变，运行时间也影响极小）。但是如果不加repeat_kv()，训练时会报错。待补充一个性能对比测试。。。总结1、模型参数量足够大。根据GPU显存确定合适的模型参数量（在训练充分的情况下，参数量越大肯定质量越好，batch size可以设置的比较小，通过设置较大的gradient_accumulation_steps，达到类似的效果。但是gradient_accumulation_steps不能太大，不然loss很容易飞）。2、数据集数量足够多。至少要有足够量的数据才能保证模型训练效果（1B的模型至少10B的训练数据吧，训练数据越多越好）3、数据集质量足够好。数据质量决定了模型的质量，尤其是微调数据集的质量，一定要比较高。考虑用现成的或者自己洗数据。4、足够大的batch size。一般设置为1M以上。5、只需要训练1个epoch。LLaMA2和PaLM都只训练了1个epoch，多epoch对模型效果并没有多大作用？！！从loss曲线也可以看出在数据量足够大的时候，1个epoch还没有跑到一半，loss就趋向于稳定了。参考： tokens危机到来怎么办？新国立最新研究：为什么当前的大语言模型的训练都是1次epoch？多epochs是否会降低大模型性能？6、小批量跑通全流程。可以先用小批量数据训练模型，跑通整个流程，以防后面由于内存原因不得不修改模型架构，需要完全重新训练的问题。（微调时加载的数据参数比预训练多，同样的模型，消耗内存也会更多）7、谨防训练中断。训练过程中及时保存权重文件。为防电脑重启或者网络波动影响ssh远程，也可以使用nohup后台挂起运行程序。8、MQA可以适当减少模型参数量，并对模型效果产生中性影响（LLaMA），且可以提升模型推理速度。上面模型训练到还是有些问题，loss维持在3左右，比较好的大模型好像一般能降到2以下。基模型本身不善于问答，微调模型在问答上效果会更好一些。由于训练经验不够多，因此需要各种尝试（不同激活函数、norm函数、位置编码等），但由于LLM训练时间较长，因此下一步先在当前训练框架上进行加速，主要考虑flash-attention和deepspeed等。等训练时间短了，才方便做更多更充足的测试。本人项目链接：https://github.com/akaihaoshuai/baby-llama2-chinese_fix （持续更新）在此链接基础上修改而来：https://github.com/DLLXW/baby-llama2-chinese系列文章：呵呵哒：LLM从0开始预训练系列：2、大模型技术报告总结（GPT/PaLM/GLM/LLaMA/Skywork）主要链接limzero：从头训练一个迷你中文版Llama2--一个小项目踏上LLM之旅何枝：如何更好地继续预训练（Continue PreTraining）JMXGODLZ：LLM训练指南:Token及模型参数准备其他知识warmup策略：神经网络中 warmup 策略为什么有效；有什么理论解释么？warmup的一种可能性解释tokenizer训练\合并代码https://github.com/yanqiangmiffy/how-to-train-tokenizer"
542596233,86c56aa624d94ae542c2cee9ac73ef53,YY.TT,https://api.zhihu.com/people/86c56aa624d94ae542c2cee9ac73ef53,people,AI软件搬砖工,如何训练千亿/万亿大模型,article,https://api.zhihu.com/articles/542596233,以及国内的盘古、紫极太初、M6、悟道等等。<em>模型</em>规模的提升，带来了模型精度的提升，但因为<em>模型</em>规模<em>过大</em>，也对模型的训练带来的极大的挑战，包括内存需求大、网络通信量大、训练/推理性能低等问题,106,2,277,1658029423,1680683484,"AI模型的规模最近几年快速提升，尤其是在Transformer结构出现以后，不断有新的模型刷新记录，当前各大土豪厂商已经将模型规模提升到了千亿&万亿级别，比如OpenAI的GPT-3、Microsoft&NVIDIA的Megatron-Turing、Google的T5/Switch-XXL/Switch-C，以及国内的盘古、紫极太初、M6、悟道等等。模型规模的提升，带来了模型精度的提升，但因为模型规模过大，也对模型的训练带来的极大的挑战，包括内存需求大、网络通信量大、训练/推理性能低等问题，需要通过分布式部署、内存优化、自动部署等技术来解决这些问题。分布式技术数据并行：Data Parallel在一个存在有N个计算节点的系统，每个节点上都有完成的计算程序（模型参数、模型结构）。将每一步要训练的数据（global-batch）拆分成N份（mini-batch），每个节点负责完成1/N数据的计算。这种计算方式就是数据并行。数据并行是一种非常常见的并行模式。如果将一个GPU看作一个系统，一个CUDA Core看作一个计算节点，那么CUDA Kernel就是以数据并行的方式在进行计算。在分布式训练中，我们通常将一个加速器（比如GPU）看作是一个计算节点。因为训练的目的是完成对模型参数的更新，因此在采用数据并行时，最后需要在每个节点完成梯度计算后，共同进行模型参数的更新，因此数据并行的通信也就发生在此位置。数据并行的参数更新支持两种模式同步更新：所有节点都计算完成后，将梯度进行平均，然后以统一的梯度对参数进行更新；然后再同步启动下一个迭代的训练在采用Parameter-Server模式进行训练时，需要在参数服务器对每个Node的梯度进行累积，并在所有（或一定个数）的节点的梯度累积完成后，计算平均梯度，再使用平均梯度更新参数。在采用All-Reduce模式进行训练时，所有的节点在准备后各自的梯度后，通过All-Reduce算法求得平均梯度，然后在每个节点上各自更新自己本地的参数。因为All-Reduce模式具有更好的扩展性和性能，当前在同步更新时主要采用All-Reduce模式。异步更新：每个节点计算完成后，独立进行参数更新，并立即启动下一个迭代的训练异步更新仅在Parameter-Server模式下存在。在采用Parameter-Server模式进行训练时，参数服务器在每一个Node的梯度时，直接使用该梯度进行更新参数。在一次训练迭代中参数会进行多次更新。每个节点获取到的上参数服务器接收到请求时的参数值。同步更新的好处时所有节点使用的参数值相同，模型训练过程中更容易收敛；但缺点时引入了全局同步点，训练时间受限于系统中执行最慢的节点；同时同步更新要求所有节点都可以正常工作，一量出现节点故障，就需要从checkpoint重训。异步更新的好处则刚好相反，每个节点的执行性能不受其它节点影响，且可通过N+M的模式容忍少量节点的故障；但有时会因为参数不一致引入收敛问题。数据并行的通信仅发生成参数更新阶段，且通信时间通常可以和计算overlap，因此数据并行具有非常好的扩展性，可以获得非常好的加速比。当前基本所有的AI框架，都具有原生的数据并行支持，比如通过TensorFlow的ParameterStrategy/MirroredStrategy/MultiWorkerMirroredStrategy，Pytorch的nn.torch.DataParallel/DistributedDataParallel，可以比较简单的进行数据并行的实现。另外也可以借助第三方库如horovod等非常容易的实现数据并行。层内模型并行：Tensor Parallel数据并行要求每个节点上都部署完整的模型，但对于千亿/万亿级别的模型，完整模型所需的内存将远超加速器节点的内存容量。比如一个完整的GPT-3模型需要3TB以上的内存，此时就必须对模型进行拆分。层内模型并行是每一个算子的计算，切分到多个不同的Device上，从而降低每个Device的内存需求。下图是Megatron-LM在Transformer结构的网络中使用的一种层内模型并行方式，对MLP的两个GEMM计算的右矩阵A和B，分别按K轴和N轴进行切分，并在Dropout前插入All-Reduce算子。Transformer模型并行示例层内模型并行需要插入通信，并且因为后面的计算依赖于前面的输出，所以通信和计算必须串行执行，而不能像数据并行一样进行计算和通信的overlap。因此，层内模型并行通常要求采用高带宽的网络，比如NVIDIA的Megatron-LM在训练GPT-3大模型时，仅在部署了NVLink的Node内使用模型并行；即使如此，层内模型并行的通信时长仍然远远高于数据并行以及流水并行的时长。参考《DIVE DEEP INTO THE PERFORMANCE MODEL OF GPT-3 TRAINING ON MEGATRON LM: STORAGE, COMPUTATION,AND COMMUNICATION》。Sequence并行：Sequence Parallel对于Transformer结构的大模型，对Tensor Parallel可以进一步扩展。在Reducing Activation Recomputation in Large Transformer Models中提出了Sequence Parallel技术，可以进一步降低内存消耗和计算量。Sequence Parallel采用Sequence Parallel时，对Dropout、LayerNorm按Sequence轴进行切分，如上图所示。在切分后，原Tensor Parallel中的all-reduce算子，被拆分为了reduce-scatter、all-gather算子，实现了在不增加通信量的情况下，降低了每卡上的计算量和activation内存需求。流水并行：Pipeline Parallel流水并行是一种符合人类直觉的并行模式。此并行模式是将模型拆分为多个stage，将每一个stage（或多个）部署在不同的Device上串接起来，前一个stage的输出作为后一个stage的输入的一种方式。在训练场景下，每个stage的前向和后向需要部署在相同的Device上，以避免跨Device的weights传输。如果采用了参数同步更新的方式训练，一个Device在执行完正向的stage后，需要等待该stage的反向也执行完成，才能开始下一个迭代的训练。因此每个Device会存在等待时间，即“Bubble”（参考GPipe）。Bubble Time为了减小Bubble以提升效率，流水并行需要将一个mini-batch切分为多个micro-batch，每个Device在执行完一个micro-batch的计算后，可以立即执行下一个micro-batch的计算。采用了micro-batch后，Bubble的时长为(K - 1) / (M + K - 1)，K是stage数，M是micro-batch数。要减少Bubble就需要尽可能增加micro-batch数。比如Megatron-LM中的M = 192。mini-batch拆分micro-batch在切分了多个micro-batch后，每个Device上需要缓存多个micro-batch的activation，因此需要占用大量的内存。而完全的优先调度反向又会导致只有反向完成后才能调度前向，使得Device出现空闲，因此PipeDream采用了一种1F1B的流水方案，每个Device以交替的方式执行每一个micro-batch的正向和反向计算，如下图：1F1B1F1B使得每个stage的反向计算可以尽早得到调度执行，因此可以及时释放对应的内存，减少内存占用。但此方式存在两个问题：同一个mini-batch的前向和后向weight不一致（如上图的Machine 1的mini-batch 5的正反向）要解决此问题，需要为每个mini-batch引入weight副本，同一个mini-batch的前向和反向使用相同的weight副本。此方式解决了weight不一致的问题，但引入了多份副本，又增加了内存占用。同一个mini-batch的不同stage的参数不一致（mini-batch 5的正向在Machine 1和2上）；要解决此问题，需要引入Vertical Sync机制，每个mini-batch在进入Pipeline开始使用最新的weight，并且将信息传递给后续stage，所有stage使用相同的weight副本。《Memory-Efficient Pipeline-Parallel DNN Training》则采用了PipeDream-2BW的方案来降低PipeDream方案的内存需求。PipeDream-2BW采用double-buffer的方式，在每一个Device上保存2个weights副本，当所有stage都完成了新版本的weights的更新后，新的micro-batch就可以采用新的weights进行正向和反向的计算。PipeDream-2BW方案在该文中还提出了一个PipeDream-Flush的方案，该方案和GPipe一样只保留一份weight，但又采用了1F1B的调度方式，通过这种方式以执行性能为代价降低了GPipe的峰值内存。PipeDream-FlushMegatron-LM采用了PipeDream-Flush的方案，并进一步增加了interleaved Pipeline优化来优化Bubble Time。此方案中，每一个Device上可以处理v（下图中v=2）个不相邻的stage。以下图为例，原来的4个stage被拆分为了8个stage，Device 1负责stage 0和4，在micro-batch 1经过Device 1/2/3/4处理完成后，又回到了Device 1进行stage 4的计算。在warm-up完成后采用1F1B的方式调度。使用此方案，要求micro-batch数必须是stage数的整数倍。Megatron-LM的pipeline方案interleaved pipeline，深/浅蓝表示正向的2个stage，深/浅绿表示反向的2个stage。interleaved Pipeline将Bubble Time降低了v倍，但代价时引入了v倍的通信量。流水并行模式下，通信仅发生在stage之间，通信量相对较小。但因为存在Bubble，会导致设备利用率受限。并且stage的切分要同时兼顾内存、通信与计算的均衡。专家并行：Expert Parallel专家并行是Google在GShard中提出的一种并行部署技术，它在MoE结构的模型中被广泛应用，本质也是一种模型并行技术。在MoE结构的模型中引入了Gating和Expert，通过将每个Expert部署在不同的Device上，Expert的个数可以随着集群规模的扩展，可以提供非常好的扩展性。但在MoE的前后需要通过All2All算子进行全局通信，且这部分通信必须和计算串行，因此MoE模型的训练受All2All性能的影响非常大。而All2All的性能随集群规模变大而降低，这极大的限制了MoE模型的扩展性。内存优化技术重计算重计算是一种以时间换空间的技术，它的原理可以参考Rematerialization，陈天奇在Training Deep Nets with Sublinear Memory Cost中将其应用在了NN训练中。NN训练反向计算中，需要依赖于正向计算过程中生成的中间结果，因此这部分中间结果需要保存在内存中。重计算的思路是在正向计算中不再保存这些中间结果，而是在反向计算时进行重新计算，从而减少NN训练中的内存需求。重计算的关键是在尽可能小的影响计算性能的情况下，降低内存需求，因此在网络中哪些算子的输出结果需要重新计算就至关重要。应该优先选择输出内存大、计算耗时小的算子进行重计算，而对于计算耗时大、输出内存小的，则不做重计算。当前的大模型主要是Transformer结构的网络，最简单的方式是每一个Transformer Layer进行一次recompute。但在Transformer Layer中，Multi-head attention、FFN的计算量比较大，重计算会导致反向的计算过长，因此在Reducing Activation Recomputation in Large Transformer Models中，提出了SAR技术，只对计算量比如小的部分计算做recompute，使得反向计算中仅增加少量的计算时长：Selective Activation Recomputation参数Sharding模型的参数，包括权重参数和优化器参数，占用了大量的内存。如果可以采用分布式存储的方式将参数存储在多个Device上，则可以降低每个Device的内存需求。Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training提出了一种对模型参数进行切分部署的方案，Zero-DP也采用了相同的方案。weight&opt parameter sharding采用Sharding后，每个Device上仅保存了1/N的参数，包括权重和优化器；权重和优化器可以分别做Sharding。如果权重进行了切分，在正向计算使用权重进行计算之前，通过AllGather获取到完整的权重进行计算。因为每个Device上仅保存了1/N的权重，因此在更新参数前，只需要使用1/N的梯度进行本Device上的参数更新，因此需要对梯度进行ReduceScatter。如果优化器参数进行了切分，也只需要使用1/N的梯度进行优化器参数的计算和更新。如果权重没有切分，要在优化器计算后增加AllGather用于更新完整的权重。引入参数Sharding会大幅降低参数的内存需求，但因为要获取到其它Device上的参数进行计算，会影响训练性能。因此需要引入参数的预取，在参数使用之前提前将其获取到本Device上，通过参数预取可以掩盖一定的通信时间。预取的时机受Device内存容量、计算算子的耗时、网络带宽等因素影响。SwappingSwapping方案是将Device内存中的数据交换到容量更大的Host内存中，以释放掉Device内存。Swap的数据可以是权重参数、优化器参数，也可以是activations、gradients等Tensor。TFLMS提出了一种Tensor的Swapping方案，在计算过程中将算子产生的输出数据swap-out到Host侧，释放Device侧的内存；在数据要被使用之前，再通过增加swap-in操作将数据从Host拉回到Device。TFLMS通过对swap操作的融合，以及增加控制边的方式触发swap-in的提前释放来优化Swapping的性能。tensor swappingZero-Offload则将权重、优化器和梯度进行Swapping。此方案将权重和优化器保存在Host侧。对于权重参数，在使用时传输给Device；如果使用了混合精度，在传输前进行FP32->FP16的转换以降低带宽需求。Device在梯度计算完成后，将FP16的梯度传输到Host侧进行优化器计算和参数更新。ZeRO-offloadZero-Offload的一个关键问题是优化器计算放在了Host侧计算。优化器计算主要是Memory Bound类型的矩阵计算，因为Host侧DDR内存的带宽远低于Device上的HBM带宽，同时很多CPU没有SVE矩阵计算的加速能力，因此Zero-Offload方案的Host性能将成为模型训练的性能瓶颈。结合应用以上的各种分布式并行和内存优化技术，单一应用时都无法满足千亿/万亿级别的模型训练需求，需要在部署时同时采用多种不同的技术。在选择这些技术时，需要同时考虑模型结构、计算资源（Host/Device的算力和内存容量）、网络拓扑和带宽（Host-Device、Device-Device、Node-Node）等因素。下面举几个常见的部署方式：3D并行（DP + TP + PP） + 重计算NVIDIA的Megatron-LM，以及Microsoft&NVIDIA在DeepSpeed&Megatron，通过3D并行的方式，实现了Transformer类模型的部署。在每一个数据并行的Device集合内，模型被拆分为多个stage，在每个stage内有若干个Transformer Layer，Tranformer Layer内的Attention和MLP通过模型并行方式进行训练。同时，对每一个（或多个）Transformer Layer开启重计算。Megatron-LM GPT-3 3D并行在一个1K GPU的集群上，Megatron-LM采用如下参数完成了GPT-3 175B模型的部署：DP + TP + EPGoogle的Switch-Transformer中，对MoE结构的模型，采用DP + TP + EP的方式进行部署。MoE结构的模型通常深度不深，因此在进行了专家并行的拆分后，模型不再需要采用Pipeline的方式进行部署，以避免引入Bubble Time。DP + TP + EP设计原则在模型可以部署下的前提下，优先选择数据并行；模型并行因为带宽需求高且无法流水，需要将其控制在高带宽的网络内；流水并行要减小stage数，增加macro-batch数，以减小Bubble Time；流水并行的Stage切分，要将切分点放在通信需求少的位置，并尽可能使各节点的计算均衡；重计算会降低计算性能，优先选Memory Bound的算子做重计算，避免选Compute Bound算子做重计算；如果Host-Device带宽较高，可以考虑采用Swapping方案代替重计算方案来提升性能；CPU的内存带宽如果比较低且不支持向量加速，要避免将计算放在CPU侧；自动部署方案要将一个千亿/万亿级别的模型部署在集群中，从前面的技术分析可以看出，要考虑非常多的因素。要想要实现一个最优的部署是非常困难的，最好能有一些辅助工具自动或半自动的帮助我们完成部署，当前学术界和业界已经有了一些方案；但因为搜索空间过大，目前仍然没有看到一个成熟的解决方案，能够综合以上所有的技术来完成模型的最优部署。大多的方案都是在一定限定条件下，或是仅考虑部分技术的部署方案。下面是一些已有的方案简略介绍：NVIDIA Megatron-LM针对Transformer类的模型提供半自动的分布式部署。用户只需要提供DP、TP、PP的并行度，以及重计算的参数，Megatron-LM可以自动完成网络的分布式部署。下面是在Megatron-LM上启动GPT-3训练的脚本参数示例：options="" \ --tensor-model-parallel-size 8 \ --pipeline-model-parallel-size 16 \ ...... \ --micro-batch-size 1 \ --global-batch-size 1536 \ --activations-checkpoint-method uniform "" MindSpore AutoParallelMindSpore支持自动并行部署，支持数据并行、模型并行与混合并行。使用方式参考：https://www.mindspore.cn/tutorial/en/0.3.0-alpha/advanced_use/distributed_training.html#FlexFlow提出了一种基于仿真 + CostModel的自动并行搜索方案，以尽可能的找到最优的模型切分策略。该方案可以实现模型并行的自动搜索，但无法实现带Pipeline的混合并行搜索。GSPMD提出了一种模型定义和分布式策略分离的分布式训练的表达和切分方式。算法工程师在定义网络时将整个分布式环境当成一个逻辑设备定义，然后再通过单独的device_mesh标识网络拓扑（通常使用2维，以表示server内的高速互联和设备间的互联网络），通过mesh_split在Tensor上增加annotation表示切分策略。GSPMD可以通过少量的用户annotation推导出整个网络的annotation，然后再根据annotation进行自动切分。AlpaAlpa支持将JAX表达的计算图，lower为XLA HLO IR，通过inter-op pass和intra-op pass完成分布式部署的编译优化，并实现了runtime实现流水并行的Stage调度，然后通过GSPMD完成每个stage内的分布式部署。在通过JAX开发模型时，只需要指定并行策略，并在模型前通过@alph.parallelize的decorator装饰，就可以完成自动部署，示例如下：method = alpa.PipeshardParallel(num_micro_batches=16, layer_option=alpa.AutoLayerOption(layer_num=2), stage_option=""auto"") # Define the training step. The function body is the same as the above one. @alpa.parallelize(method=method) def auto_pipeline_train_step(state, batch): # ......"
675509396,17636fdafe0e68df8736c2e242edce1a,产品经理大群,https://api.zhihu.com/people/17636fdafe0e68df8736c2e242edce1a,people,点赞就好，十年AI策略产品经验 持续分享 公众号同名,一文读懂：大模型RAG（检索增强生成）,article,https://api.zhihu.com/articles/675509396,然后合并成为提示模板，给<em>大模型</em>生成漂亮的回答。 经历23年年初那一波<em>大模型</em>潮，想必大家对大模型的能力有了一定的了解，但是当我们将<em>大模型</em>应用于实际业务场景时会发现,280,13,610,1704089694,1710915793,"RAG检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。理解不难，就是通过自有垂域数据库检索相关信息，然后合并成为提示模板，给大模型生成漂亮的回答。经历23年年初那一波大模型潮，想必大家对大模型的能力有了一定的了解，但是当我们将大模型应用于实际业务场景时会发现，通用的基础大模型基本无法满足我们的实际业务需求，主要有以下几方面原因：知识的局限性：模型自身的知识完全源于它的训练数据，而现有的主流大模型（ChatGPT、文心一言、通义千问…）的训练集基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线的数据是无法获取到的，这部分知识也就无从具备。幻觉问题：所有的AI模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。而这种幻觉问题的区分是比较困难的，因为它要求使用者自身具备相应领域的知识。数据安全性：对于企业来说，数据安全至关重要，没有企业愿意承担数据泄露的风险，将自身的私域数据上传第三方平台进行训练。这也导致完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍。而RAG是解决上述问题的一套有效方案。一句话总结：RAG（中文为检索增强生成） = 检索技术 + LLM 提示。例如，我们向 LLM 提问一个问题（answer），RAG 从各种数据源检索相关的信息，并将检索到的信息和问题（answer）注入到 LLM 提示中，LLM 最后给出答案。RAG 是2023年基于 LLM 的系统中最受欢迎的架构。许多产品基于 RAG 构建，从基于 web 搜索引擎和 LLM 的问答服务到使用私有数据的chat应用程序。尽管在2019年，Faiss 就实现了基于嵌入的向量搜索技术，但是 RAG 推动了向量搜索领域的发展。比如 chroma、weaviate.io 和 pinecone 这些基于开源搜索索引引擎（主要是 faiss 和 nmslib）向量数据库初创公司，最近增加了输入文本的额外存储和其他工具。在这个过程中，有两个主要步骤：语义搜索和生成输出。在语义搜索步骤中，我们希望从我们的知识库中找到与我们要回答的查询最相关的部分内容。然后，在生成步骤中，我们将使用这些内容来生成响应。有两个最著名的基于 LLM 的管道和应用程序的开源库——LangChain 和 LlamaIndex，受 ChatGPT 发布的启发，它们在 2022 年 10 月和 11 月创立，并在 2023 年获得大量采用。本文的目的是参考 LlamaIndex实现，来系统讲解关键的高级 RAG 技术，以方便大家深入研究。问题在于，大多数教程只会针对个别技术进行详细讲解，而不是整体全面地系统化归纳总结。另一件事是，LlamaIndex 和 LangChian 都是了不起的开源项目，他们的开发速度非常快，以至于他们的文档已经比2016年的机器学习教科书还要厚。RAG实现过程目前我们已经知道RAG融合是一种用于（可能）提升RAG应用检索阶段的技术。在这个部分里，我会简单阐述我的观点，不过如果你想知道更多详细信息，你可以查阅这篇文章。下面这张图片展示了大概的工作流程。基本上，主要思路就是利用LLM来生成多个查询，期望能够通过这些查询让问题的各个方面在上下文中显现出来。之后你可以使用生成的查询进行向量搜索（如本系列之前的部分所述），并且基于其在结果集中的显示方式来对内容进行重新排序。可以用下面提示词生成额外问题：You are a helpful assistant that generates multiple search queries based on a single input query. Generate multiple search queries related to: {USER_INPUT} OUTPUT (4 queries):如上所述，LLM能够生成覆盖原问题多个方面的查询。这样可以帮助我们在数据库中找到包含各个相关方面的信息，从而潜在地提高我们从RAG应用得到的结果。RAG架构RAG的架构如图中所示，简单来讲，RAG就是通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答。因此，可以将RAG的核心理解为“检索+生成”，前者主要是利用向量数据库的高效存储和检索能力，召回目标知识；后者则是利用大模型和Prompt工程，将召回的知识合理利用，生成目标答案。RAG架构完整的RAG应用流程主要包含两个阶段：数据准备阶段：数据提取——>文本分割——>向量化（embedding）——>数据入库应用阶段：用户提问——>数据检索（召回）——>注入Prompt——>LLM生成答案下面我们详细介绍一下各环节的技术细节和注意事项：数据准备阶段：数据准备一般是一个离线的过程，主要是将私域数据向量化后构建索引并存入数据库的过程。主要包括：数据提取、文本分割、向量化、数据入库等环节。数据准备数据提取数据加载：包括多格式数据加载、不同数据源获取等，根据数据自身情况，将数据处理为同一个范式。数据处理：包括数据过滤、压缩、格式化等。元数据获取：提取数据中关键信息，例如文件名、Title、时间等 。文本分割：文本分割主要考虑两个因素：1）embedding模型的Tokens限制情况；2）语义完整性对整体的检索效果的影响。一些常见的文本分割方式如下：句分割：以”句”的粒度进行切分，保留一个句子的完整语义。常见切分符包括：句号、感叹号、问号、换行符等。固定长度分割：根据embedding模型的token长度限制，将文本分割为固定长度（例如256/512个tokens），这种切分方式会损失很多语义信息，一般通过在头尾增加一定冗余量来缓解。向量化（embedding）：向量化是一个将文本数据转化为向量矩阵的过程，该过程会直接影响到后续检索的效果。目前常见的embedding模型如表中所示，这些embedding模型基本能满足大部分需求，但对于特殊场景（例如涉及一些罕见专有词或字等）或者想进一步优化效果，则可以选择开源Embedding模型微调或直接训练适合自己场景的Embedding模型。模型名称描述获取地址ChatGPT-EmbeddingChatGPT-Embedding由OpenAI公司提供，以接口形式调用。https://platform.openai.com/docs/guides/embeddings/what-are-embeddingsERNIE-Embedding V1ERNIE-Embedding V1由百度公司提供，依赖于文心大模型能力，以接口形式调用。https://cloud.baidu.com/doc/WENXINWORKSHOP/s/alj562vvuM3EM3E是一款功能强大的开源Embedding模型，包含m3e-small、m3e-base、m3e-large等多个版本，支持微调和本地部署。https://huggingface.co/moka-ai/m3e-baseBGEBGE由北京智源人工智能研究院发布，同样是一款功能强大的开源Embedding模型，包含了支持中文和英文的多个版本，同样支持微调和本地部署。https://huggingface.co/BAAI/bge-base-en-v1.5数据入库：数据向量化后构建索引，并写入数据库的过程可以概述为数据入库过程，适用于RAG场景的数据库包括：FAISS、Chromadb、ES、milvus等。一般可以根据业务场景、硬件、性能需求等多因素综合考虑，选择合适的数据库。应用阶段：在应用阶段，我们根据用户的提问，通过高效的检索方法，召回与提问最相关的知识，并融入Prompt；大模型参考当前提问和相关知识，生成相应的答案。关键环节包括：数据检索、注入Prompt等。数据检索数据检索常见的数据检索方法包括：相似性检索、全文检索等，根据检索效果，一般可以选择多种检索方式融合，提升召回率。相似性检索：即计算查询向量与所有存储向量的相似性得分，返回得分高的记录。常见的相似性计算方法包括：余弦相似性、欧氏距离、曼哈顿距离等。全文检索：全文检索是一种比较经典的检索方式，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，找到对应的记录。注入PromptLLM生成Prompt作为大模型的直接输入，是影响模型输出准确率的关键因素之一。在RAG场景中，Prompt一般包括任务描述、背景知识（检索得到）、任务指令（一般是用户提问）等，根据任务场景和大模型性能，也可以在Prompt中适当加入其他指令优化大模型的输出。一个简单知识问答场景的Prompt如下所示：【任务描述】 假如你是一个专业的客服机器人，请参考【背景知识】，回 【背景知识】 {content} // 数据检索得到的相关文本 【问题】 石头扫地机器人P10的续航时间是多久？Prompt的设计只有方法、没有语法，比较依赖于个人经验，在实际应用过程中，往往需要根据大模型的实际输出进行针对性的Prompt调优。原始 RAG本文 RAG 管道从一个文本文档语料库开始，直接跳过如何通过数据加载器从Youtube等数据源获取步骤。标准的 RAG 流程简介：将文本分块，然后使用一些 Transformer Encoder 模型将这些块嵌入到向量中，将所有向量放入索引中，最后创建一个 LLM 提示，告诉模型根据我们在搜索步骤中找到的上下文回答用户的查询。在运行时，我们使用同一编码器模型对用户的查询进行向量化，然后搜索该查询向量的索引，找到 top-k 个结果，从我们的数据库中检索相应的文本块，并将它们作为上下文输入到 LLM 提示中。提示与下边内容类似：def question_answering(context, query): prompt = f"""""" Give the answer to the user query delimited by triple backticks ```{query}```\ using the information given in context delimited by triple backticks ```{context}```.\ If there is no relevant information in the provided context, try to answer yourself, but tell user that you did not have any relevant context to base your answer on. Be concise and output the answer of size less than 80 tokens. """""" response = get_completion(instruction, prompt, model=""gpt-3.5-turbo"") answer = response.choices[0].message[""content""] return answer提示工程是提升 RAG 流程性能的一种简便有效的方法。可以查阅 OpenAI 提供的详尽的提示工程指南。虽然 OpenAI 是 LLM 提供商的领头羊，但还有其他不少选择，例如 Anthropic 的 Claude，Mistral 的小型但功能强大的模型Mixtral，Microsoft 的Phi-2，以及如Llama2，OpenLLaMA，Falcon等众多开源模型，都可以供你选择最合适的，作为 RAG 管道大脑。高级 RAG现在我们深入讲解高级 RAG 技术。包括所涉及的核心步骤和算法的方案，但是省略了一些逻辑循环和复杂的多步代理行为，以保持方案的可读性。上图中绿色部分是我们接下来详细探讨的核心 RAG 技术。一张图并不能全部展示所有的高级 RAG 技术，比如我们这里省略了上文扩展技术。1：分块 (Chunking) & 向量化 (Vectorisation)首先我们需要为文档内容创建向量索引，然后在运行时搜索与查询向量余弦距离最近的向量索引，这样就可以找到与查询内容最接近语义的文档。1.1 分块 (Chunking)Transformer 模型具有固定的输入序列长度，即使输入上下文窗口很大，一个句子或几个句子的向量也比几页文本的向量更能代表其语义含义，因此对数据进行分块—— 将初始文档拆分为一定大小的块，而不会失去其含义。有许多文本拆分器实现能够完成此任务。块的大小是一个需要重点考虑的问题。块的大小取决于所使用的嵌入模型以及模型需要使用 token 的容量。如基于 BERT 的句子转换器，最多需要 512 个 token，OpenAI ada-002 能够处理更长的序列，如 8191 个 token，但这里的折衷是 LLM 有足够的上下文来推理，而不是足够具体的文本嵌入，以便有效地执行搜索。有一项关于块大小选择的研究。在 LlamaIndex 中，NodeParser 类很好支持解决这个问题，其中包含一些高级选项，例如定义自己的文本拆分器、元数据、节点/块关系等。1.2 向量化 (Vectorisation)下一步是选择一个搜索优化的模型来嵌入我们的块。有很多选项，比如 bge-large 或 E5 嵌入系列。只需查看 MTEB 排行榜以获取最新更新即可。有关分块和向量化步骤的 end2end 实现，请查看 LlamaIndex 中完整数据摄取管道的示例。2. 搜索索引2.1 向量存储索引RAG 管道的关键部分是搜索索引，它存储了我们在上一步中获得的向量化内容。最原始的实现是使用平面索引 — 查询向量和所有块向量之间的暴力计算距离。为了实现1w+元素规模的高效检索，搜索索引应该采用向量索引，比如 faiss、nmslib 以及 annoy。这些工具基于近似最近邻居算法，如聚类、树结构或HNSW算法。此外，还有一些托管解决方案，如 OpenSearch、ElasticSearch 以及向量数据库，它们自动处理上面提到的数据摄取流程，例如Pinecone、Weaviate和Chroma。取决于你的索引选择、数据和搜索需求，还可以存储元数据，并使用元数据过滤器来按照日期或来源等条件进行信息检索。LlamaIndex 支持多种向量存储索引，同时也兼容其他简单的索引类型，如列表索引、树索引和关键词表索引。关于这些索引，我们会在后续的融合检索部分详细介绍。2.2 分层索引在大型数据库的情况下，一个有效的方法是创建两个索引——一个由摘要组成，另一个由文档块组成，然后分两步进行搜索，首先通过摘要过滤掉相关文档，然后只在这个相关组内搜索。2.3 假设性问题和 HyDE另一种方法是让 LLM 为每个块生成一个问题，并将这些问题嵌入到向量中，在运行时对这个问题向量的索引执行查询搜索（将块向量替换为索引中的问题向量），然后在检索后路由到原始文本块并将它们作为 LLM 获取答案的上下文发送。这种方法提高了搜索质量，因为与实际块相比，查询和假设问题之间的语义相似性更高。还有一种叫做 HyDE 的反向逻辑方法——你要求 LLM 在给定查询的情况下生成一个假设的响应，然后将其向量与查询向量一起使用来提高搜索质量。2.4 内容增强这里的内容是将相关的上下文组合起来供 LLM 推理，以检索较小的块以获得更好的搜索质量。有两种选择：一种是围绕较小的检索块的句子扩展上下文，另一种是递归地将文档拆分为多个较大的父块，其中包含较小的子块。2.4.1 语句窗口检索器在此方案中，文档中的每个句子都是单独嵌入的，这为上下文余弦距离搜索提供了极大的查询准确性。为了在获取最相关的单个句子后更好地推理找到的上下文，我们将上下文窗口扩展为检索到的句子前后的 k 个句子，然后将这个扩展的上下文发送到 LLM。绿色部分是在索引中搜索时发现的句子嵌入，整个黑色 + 绿色段落被送到 LLM 以扩大其上下文，同时根据提供的查询进行推理。2.4.2 自动合并检索器（或父文档检索器)这里的思路与语句窗口检索器非常相似——搜索更精细的信息片段，然后在在LLM 进行推理之前扩展上下文窗口。文档被拆分为较小的子块，这些子块和较大的父块有引用关系。首先在检索过程中获取较小的块，然后如果前 k 个检索到的块中有超过 n 个块链接到同一个父节点（较大的块），我们将这个父节点替换成给 LLM 的上下文——工作原理类似于自动将一些检索到的块合并到一个更大的父块中，因此得名。请注意，搜索仅在子节点索引中执行。查看 LlamaIndex 教程 递归检索器 + 节点引用 以更深入地了解。2.5 融合检索或混合搜索这是一个很早以前的思路：结合传统的基于关键字的搜索（稀疏检索算法，如 tf-idf 或搜索行业标准 BM25）和现代语义或向量搜索，并将其结果组合在一个检索结果中。这里唯一的关键是如何组合不同相似度分数的检索结果。这个问题通常通过 Reciprocal Rank Fusion 算法来解决，该算法能有效地对检索结果进行重新排序，以得到最终的输出结果。在 LangChain 中，这种方法是通过 Ensemble Retriever 来实现的，该类将你定义的多个检索器结合起来，比如一个基于 faiss 的向量索引和一个基于 BM25 的检索器，并利用 RRF 算法进行结果的重排。在 LlamaIndex 中，这一过程也是以类似的方式 实现 的。混合或融合搜索通常能提供更优秀的检索结果，因为它结合了两种互补的搜索算法——既考虑了查询和存储文档之间的语义相似性，也考虑了关键词匹配。3. 重排（reranking）和过滤（filtering）我们使用上述任何算法获得了检索结果，现在是时候通过过滤、重排或一些转换来完善它们了。在 LlamaIndex 中，有各种可用的后处理器，根据相似性分数、关键字、元数据过滤掉结果，或使用其他模型（如 LLM）、sentence-transformer 交叉编码器，Cohere 重新排名接口或者基于元数据重排它们。这是将检索到的上下文提供给 LLM 以获得结果答案之前的最后一步。现在，我们将探索更高级的 RAG 技术，比如查询转换和路由。这些技术涉及到大语言模型的使用，代表了一种更复杂的逻辑思维——在 RAG 流程中融合了 LLM 的推理能力。4. 查询转换查询转换是一系列技术，使用 LLM 作为推理引擎来修改用户输入以提高检索质量。有很多技术实现可供选择。对于复杂的查询，大语言模型能够将其拆分为多个子查询。比如，当你问：“在 Github 上，Langchain 和 LlamaIndex 这两个框架哪个更受欢迎？”，我们不太可能直接在语料库找到它们的比较，所以将这个问题分解为两个更简单、具体的合理的子查询：“Langchain 在 Github 上有多少星？”“Llamaindex 在 Github 上有多少星？”这些子查询会并行执行，检索到的信息随后被汇总到一个 LLM 提示词中。这两个功能分别在 Langchain 中以多查询检索器的形式和在 Llamaindex 中以子问题查询引擎的形式实现。Step-back prompting 使用 LLM 生成一个更通用的查询，以此检索到更通用或高层次的上下文，用于为我们的原始查询提供答案。同时执行原始查询的检索，并在最终答案生成步骤中将两个上下文发送到 LLM。这是 LangChain 的一个示例实现。查询重写使用 LLM 来重新表述初始查询，以改进检索。LangChain 和 LlamaIndex 都有实现，个人感觉LlamaIndex 解决方案在这里更强大。5. 聊天引擎关于构建一个可以多次用于单个查询的完美 RAG 系统的下一件工作是聊天逻辑，就像在 LLM 之前时代的经典聊天机器人中一样考虑到对话上下文。这是支持后续问题、代词指代或与上一个对话上下文相关的任意用户命令所必需的。它是通过查询压缩技术解决的，将聊天上下文与用户查询一起考虑在内。与往常一样，有几种方法可以进行上述上下文压缩——一个流行且相对简单的 ContextChatEngine，首先检索与用户查询相关的上下文，然后将其与内存缓冲区中的聊天记录一起发送到 LLM，以便 LLM 在生成下一个答案时了解上一个上下文。更复杂的情况是 CondensePlusContextMode——在每次交互中，聊天记录和最后一条消息被压缩到一个新的查询中，然后这个查询进入索引，检索到的上下文与原始用户消息一起传递给 LLM 以生成答案。需要注意的是，LlamaIndex 中还支持基于 OpenAI 智能体的聊天引擎，提供更灵活的聊天模式，Langchain 还支持 OpenAI 功能 API。还有像 ReAct 智能体 这样的其他聊天引擎类型，但我们接下来将直接跳转到第 7 节，讨论智能体本身。6. 查询路由查询路由是 LLM 驱动的决策步骤，决定在给定用户查询的情况下下一步该做什么——选项通常是总结、对某些数据索引执行搜索或尝试许多不同的路由，然后将它们的输出综合到一个答案中。查询路由器还用于选择数据存储位置来处理用户查询。这些数据存储位置可能是多样的，比如传统的向量存储、图形数据库或关系型数据库，或者是不同层级的索引系统。在处理多文档存储时，通常会用到摘要索引和文档块向量索引这两种不同的索引。定义查询路由器包括设置它可以做出的选择。选择特定路由的过程是通过大语言模型调用来实现的，其结果按照预定义的格式返回，以路由查询指定的索引。如果是涉及到关联操作，这些查询还可能被发送到子链或其他智能体，如下面的多文档智能体方案所展示的那样。LlamaIndex 和 LangChain 都提供了对查询路由器的支持。7. 智能体（Agent）智能体（ Langchain 和 LlamaIndex 均支持）几乎从第一个 LLM API 发布开始就已经存在——这个思路是为一个具备推理能力的 LLM 提供一套工具和一个要完成的任务。这些工具可能包括一些确定性功能，如任何代码函数或外部 API，甚至是其他智能体——这种 LLM 链接思想是 LangChain 得名的地方。智能体本身就是一个复杂的技术，不可能在 RAG 概述中深入探讨该主题，所以我将继续基于 agent 的多文档检索案例，并简要提及 OpenAI 助手，因为它是一个相对较新的概念，在最近的 OpenAI 开发者大会上作为 GPTs 呈现，并在下文将要介绍的 RAG 系统中发挥作用。OpenAI 助手基本上整合了开源 LLM 周边工具——聊天记录、知识存储、文档上传界面。最重要的是函数调用 API， 其提供了将自然语言转换为对外部工具或数据库查询的 API 调用的功能。在 LlamaIndex 中，有一个 OpenAIAgent 类将这种高级逻辑与 ChatEngine 和 QueryEngine 类结合在一起，提供基于知识和上下文感知的聊天，以及在一个对话轮次中调用多个 OpenAI 函数的能力，这真正实现了智能代理行为。让我们来看一下多文档智能体的方案—— 这是一个非常复杂的配置，涉及到在每个文档上初始化一个Agent（OpenAIAgent），该智能体能进行文档摘要制作和传统问答机制的操作，还有一个顶层智能体，负责将查询分配到各个文档智能体，并综合形成最终的答案。每个文档智能体都有两个工具：向量存储索引和摘要索引，它根据路由查询决定使用哪一个。对于顶级智能体来说，所有文档智能体都是其工具。该方案展示了一种高级 RAG 架构，其中每个智能体都做路由许多决策。这种方法的好处是能够比较不同的解决方案或实体在不同的文档及其摘要中描述，以及经典的单个文档摘要和 QA 机制——这基本上涵盖了最常见的与文档集合聊天的用例。这种复杂配置的缺点可以通过图片发现 —— 由于需要在智能体内部的大语言模型之间进行多次往返迭代，其运行速度较慢。顺便一提，LLM 调用通常是 RAG 管道中耗时最长的操作，而搜索则是出于设计考虑而优化了速度。因此，对于大型的多文档存储，我建议考虑对此方案进行简化，以便实现扩展。8. 响应合成这是任何 RAG 管道的最后一步——根据我们检索的所有上下文和初始用户查询生成答案。最简单的方法是将所有获取的上下文（高于某个相关性阈值）与查询一起连接并提供给 LLM。但是，与往常一样，还有其他更复杂的选项，涉及多个 LLM 调用，以优化检索到的上下文并生成更好的答案。响应合成的主要方法有：通过将检索到的上下文逐块发送到 LLM 来优化答案概括检索到的上下文，以适应提示根据不同的上下文块生成多个答案，然后将它们连接或概括起来。有关更多详细信息，请查看响应合成器模块文档。 RAG 融合和其他软件世界的架构决策一样，RAG融合也有权衡取舍，你需要清楚它们，以便为你的具体情境做出最好的决定。不过首先，我们先列一下RAG融合的优缺点。优点：提供多样化的上下文：因为你可以从不同的角度查看用户的查询，所以顶级结果里会出现更多样化的内容片段。与专注于单一视角的内容段落相比，你更有可能看到能够涵盖话题多个方面的内容作为顶级结果出现。额外的控制层面：像其他机器学习解决方案一样，RAG融合提供了额外的控制手柄，让你可以微调你的应用，并让它更加符合你的期望目标。自动校正：通过使用LLM作为用户在文本框中输入内容与实际在数据库中搜索内容之间的中间人，你可以纠正拼写错误，添加与用户查询相关的上下文信息（关于用户的信息、时间、他们的账户状态等），以及/或潜在地筛选特定类型的内容。成本：这是一个有些争议的问题，因为成本既是RAG融合的优点也是缺点，让我来解释一下。你大概知道，向量搜索比LLMs要便宜得多，那么用于RAG融合的额外LLM调用是不是应该会增加应用的整体成本呢？不过……让我们再来看看LLM的成本。你大概也知道，你可能使用的大多数主流LLMs都采用基于令牌的计费模式。即使是自己托管LLM，你的成本也会与处理的令牌数量大致成正比。我们基本上在这儿向LLM发送两次请求，一次大概有100个令牌用来生成相似的查询，另一次则会有数千个令牌，我们提供相关文本块并希望从LLM那里得到适当的回应。所以基本上我们第一次对LLM的调用要比第二次便宜10倍到100倍。所以即使RAG融合每10次查询节省一次后续问题，你在成本上还是能领先的。缺点：延迟:正如你现在可能知道的，LLMs需要大量的计算资源，因此它们的速度相对较慢(相对于我们应用程序中的其他部分)。根据你的应用程序，向LLM发送一次额外的请求可能会让用户的体验变得糟糕，因为他们可能需要等待几百毫秒的时间。自动纠错:是的，这是一个优点，但是当它不起作用时，也可能是一个缺点。这通常发生在你的内容包含内部术语或行话，而这些术语或行话没有出现在LLM学习过的数据中。在这种情况下，LLM可能会出现困惑并生成完全无关的查询，从而影响到最后的结果。成本:正如我们之前讨论的，如果RAG融合对你应用程序的整体效益贡献不大，你最终可能会花费更多的费用，但收益却很有限。有了以上的介绍，让我们来讨论一下在什么情况下最有可能通过实现RAG融合得到好的效果。如果你的应用程序的内容主要基于常见概念，那么你更有可能从使用RAG融合中获益良多。RAG融合n不适用场景如果您拥有的内容包含大量内部行话或与知名品牌重复的词语，则您可能需要调整RAG融合提示以获得良好的效果，或者最好避免使用。以下是一个例子来说明这一点。如今，所有的知名LLMs都是基于“注意力就是你所需要”的论文中首次引入的transformer架构，这是一种根据语境中其他单词对生成下一个token的重要性的度量方法。现在，如果我在该论文的基础上构建一个RAG应用并添加RAG融合功能，其工作方式可能是这样的：(绿色文字表示对RAG融合有贡献的LLM生成的查询)由于在这个语境下理解“注意力”的含义依赖于上下文，LLM对这个含义产生了误解，并生成了一些与之完全无关的搜索查询。这可能导致您的应用得出糟糕的结果。现在，我们来看一下通过更改系统提示，将其改为“您是一个有用的助手，可以根据单一输入查询为试图解释transformer架构概念的应用生成多个搜索查询”，是如何在这个特定例子中解决这个问题的。根据您的具体情况，微调提示可能有用也可能没用。您也可以试试以下技巧，然后才放弃：使用语义搜索找到相关的查询：这个选项适合较为成熟的使用场景，但是如果你有很多用户基础，你可以尝试搜索一个相似的查询数据库，利用它们来寻找相关的内容。通过少量示例学习实现上下文理解：有时候，在给出提示前先给LLM展示几个例子也许能帮助提升效果。微调小型LLM：如果上述方法对你特定的使用场景都没有用（而测试这些方法相对较快），那么你可以考虑微调自己的LLM。这样可能会有效果，因为即使是很小的LLM（即使只有几百万个参数），也有可能足够好到能在特定模板下生成几个相似的查询。请注意，这种方法相比于前面提到的方法，会花费更多的成本和时间，但可能会给你带来最好的效果（代价是增加了复杂性）。正如您所见，在这种情况下(像HyDe、TF-IDF、BM25或混合搜索等许多其他方法一样)，不清楚这种方法是否会胜过针对您具体用例的基本语义搜索功能。但是，就像人们常说的那样，“如果你不衡量，你就无法改进”。因此，我的建议是：一旦构建了基本的RAG应用程序，就要立即创建一个评估过程。在这一过程中，你会有大量的想法去优化你的提示或搜索功能，而每一种改变所带来的附带影响是不清楚的。有时候，提升某一组查询的效果会以牺牲另一组查询为代价。在这种情况下，最好的办法就是把它当作另一种机器学习问题，看看数据告诉你什么。编码器和 LLM 微调这种方法主要是对 Transformer 编码器 和 LLM 进行微调。其中，编码器影响嵌入质量，从而影响上下文检索质量。LLM 负责最好地使用提供的上下文来回答用户查询。如今的一大优势是可以使用像 GPT-4 这样的高端 LLM 来生成高质量的数据集。但是必须清楚，使用小型合成数据集进微调基础模型，可能会降低基础模型的通用能力。编码器微调作者进行了一项测试，对 bge-large-en-v1.5 编码器进行微调，发现对于检索效果提升影响有限。因为针对搜索优化的最新 Transformer 编码器已经非常高效。排序器微调如果不完全信任基础编码器，可以使用交叉编码器对检索到的结果进行重排。这个过程是这样的：你把查询和每个前 k 个检索到的文本块一起送入交叉编码器，中间用 SEP (分隔符) Token 分隔，并对它进行微调，使其对相关的文本块输出 1，对不相关的输出 0。一个这种微调过程的成功案例可以在这里找到，结果显示通过交叉编码器微调，成对比较得分提高了 4%。LLM 微调最近，OpenAI 开始提供 LLM 微调 API，LlamaIndex 有一个关于在 RAG 设置中微调 GPT-3.5-turbo 的教程。RAG 管道评估的 ragas 框架显示，忠实度指标增加了 5%，这意味着微调后的 GPT 3.5-turbo 模型比原始模型更好地利用了提供的上下文来生成答案。Meta AI Research 最近的论文 RA-DIT： Retrieval Augmented Dual Instruction Tuning 展示了一种更复杂的方法，提出了一种同时调整 LLM 和 Retriever 的技术（原始论文中的双编码器）关于查询、上下文和答案的三元组。该技术被用于通过微调 API 微调 OpenAI LLM。也被用于微调了Llama2 开源模型（在原始论文中），结果与带有 RAG 的 Llama2 65B 相比，知识密集型任务指标增加 ~5%和常识推理任务增加几个百分点。评估RAG 系统性能评估的多个框架，都包含了几项独立的指标，例如总体答案相关性、答案基础性、忠实度和检索到的上下文相关性。在之前章节提到的 Ragas，使用真实性和答案相关性来评价生成答案的质量，并使用经典的上下文精准度和召回率来评估 RAG 方案的检索性能。最近推出的课程构建和评估高级 RAG中，以及 LlamaIndex 和评估框架Truelens，他们提出了RAG 三元组评估模式 — 分别是对问题的检索内容相关性、答案的基于性（即大语言模型的答案在多大程度上被提供的上下文的支持）和答案对问题的相关性。最关键且可控的指标是检索内容的相关性 — 实际上是上述高级 RAG 管道的前 1-7 部分加上编码器和排名器的微调部分，这些都是为了提高这个指标。而第 8 部分和大语言模型的微调则专注于提高答案的相关性和基于性。一个简单有效的检索器评估管道的例子可以在这里找到，它已被应用于编码器的微调部分。一个更高级的方法不仅考虑命中率，还包括了常用的搜索引擎评估指标平均倒数排名 (Mean Reciprocal Rank)，以及生成答案的质量指标，如真实性和相关性，这在 OpenAI 的实用指南中有所展示。LangChain 提供了一个颇为先进的评估框架 LangSmith。在这个框架中，你不仅可以实现自定义的评估器，还能监控 RAG 管道内的运行，进而增强系统的透明度。如果你正在使用 LlamaIndex 进行构建，可以尝试 rag_evaluator llama pack。总结本文概述 RAG 的核心算法，并举例说明其中的一些方法。RAG融合是一个强大的功能，能够提高RAG应用的语义搜索效率。通过使用语言模型生成多个查询并对搜索结果进行重新排序，RAG融合可以呈现更丰富多样的内容，并提供了一个额外的层次，用于调整应用。此外，RAG融合还可以实现自动纠正、节省成本以及增加内容多样性。但是，需要注意一些权衡，比如潜在的延迟问题、自动纠正的挑战以及成本影响。对于依赖常见概念但可能出现内部行话或重叠词汇的应用来说，RAG融合尤其有用。重要的是要评估RAG融合的表现并测量其影响，以确定它是否适合特定应用场景。还有很多其他的事情需要考虑，比如基于网络搜索的 RAG（LlamaIndex 的 RAG、webLangChain 等），更深入地研究智能体架构以及关于 LLM 长期记忆的一些想法。除了答案相关性和忠实度之外，RAG 系统的主要生产挑战是速度。ChatGPT 和大多数其他助手使用的这种流式特性不是随机的赛博朋克风格，而只是一种缩短感知答案生成时间的方法。这就是为什么我认为小参数规模的 LLM 有一个非常光明的未来，最近发布的 Mixtral 和 Phi-2 正在引领我们朝着这个方向前进。参考资料：爱吃牛油果的璐璐：借助知识图谱和Llama-Index实现基于大模型的RAGhttps://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6?gi=77e3202eaa34果壳PAI：一文搞懂大模型RAG应用（附实践案例）iyacontrol：图解高级RAG技术ketchum：构建基于RAG的聊天机器人（四）：RAG融合(RAG Fusion)"
684472814,6fef64729ed5fb3a3388cb14c12b8515,菜人卷,https://api.zhihu.com/people/6fef64729ed5fb3a3388cb14c12b8515,people,厚积薄发,一文看完多模态：从视觉表征到多模态大模型,article,https://api.zhihu.com/articles/684472814,距离上次知乎写作就快过去一年，上一次的计划主题还是“开源<em>大模型</em>”（参见《ChatGPT的朋友们：大语言模型经典论文一次读到吐》），无奈这个方向变化太快，而且也不乏优质总结文章,400,21,824,1709171128,1712147638,"转眼2024，距离上次知乎写作就快过去一年，上一次的计划主题还是“开源大模型”（参见《ChatGPT的朋友们：大语言模型经典论文一次读到吐》），无奈这个方向变化太快，而且也不乏优质总结文章，也就一直没有动笔。正好最近做图文多模态大模型相关的工作，在查阅资料的过程中没遇到比较完整的脉络梳理文章，往往需要综合参考；反观这个方向的综述型论文又过于追求“完美”，个人感觉详略把控不尽人意。因此，借此机会结合自己的学习过程，对多模态和多模态大模型做一个系统的梳理，尝试以一个亲历者的视角谈谈这部分技术的发展思路，希望能给读者一些不一样的收获，如有偏颇，欢迎指正。为了表述简单，我们不严谨的将“图文多模态”表述为“多模态”（标题有些夸张），如果后面有机会可以讨论更多模态的相关工作。此外，本文假设读者已经对视觉表征和多模态融合有一定入门背景，希望通过一篇文章回顾将过去几年的经典工作。一、总览由于是讲“图文多模态”，还是要从“图”和“文”的表征方法讲起，然后讲清楚图文表征的融合方法。对于文本模态的表征发展，我们在《闲话NLP：文本表征的半世今生》一文中有过一轮的梳理，因此本文只要讲两件事情：视觉表征：分为两个部分问题，一是如何合理建模视觉输入特征，二是如何通过预训练手段进行充分学习表征，这两点是基于视觉完成具体算法任务的基础；视觉与自然语言的对齐（Visul Language Alignment）或融合：目的是将视觉和自然语言建模到同一表征空间并进行融合，实现自然语言和视觉语义的互通，这点同样离不开预训练这一过程。模态对齐是处理多模态问题的基础，也是现在流行的多模态大模型技术前提。对于视觉表征，从发展上可以分为卷积神经网络（CNN）和Vision Transformer（VIT）两大脉络，二者分别都有各自的表征、预训练以及多模态对齐的发展过程。而对于VIT线，另有多模态大模型如火如荼的发展，可谓日新月异。因此，本文的行文思路也就非常简单，如图1所示。第一部分介绍以CNN为基础的视觉表征和预训练手段，以及在此基础上的多模态对齐的方法。由于预训练已经成为AI技术取得效果的标配，多模态对齐部分的内容也是以多模态预训练技术承载；第二部分从VIT技术出发，分别介绍VIT视觉表征的预训练探索工作、多模态对齐的预训练工作以及近两年火热的研究方向多模态大模型。图1由于多年间的优秀工作太多，不胜枚举，本文仅挑选笔者从业过程中印象较深，且有标志性特点的工作为代表。优秀的工作不止于本文，不过还是期望通过有限的工作，将近几年的图文多模态相关技术串连起来，方便读者按图索骥进行更深入的学习。下面开始正式的内容。二、CNN：视觉理解的一代先驱2.1 卷积视觉表征模型和预训练对视觉信息的表征，简单来说是将图像信息转化成深度学习输入所需的特征向量或向量序列，如图2。深度学习时代，卷积神经网络（CNN）凭借其局部区域连接、权重共享以及位移不变性等特点，天然的符合了图像信息的建模归纳假设，成为早期最适合视觉表征的模型。具体的，卷积神经网络应用视觉表征的模型很多，我们简单从LeNet-5、AlexNet、VGG和ResNet等模型的演进一窥其在关键要素。图22.1.1 卷积视觉表征：从LeNet到ResNetLeNet-5早期在数字识别中取得了成功的应用，网络结构是 [CONV-POOL-CONV-POOL-FC-FC]。卷积层使用 5*5的卷积核，步长为1；池化层使用 2*2 的区域，步长为2；后面是全连接层；AlexNet相比LeNet-5做了更多层数的堆叠，网络参数进行了相应的调整，并在ImageNet大赛2012夺得冠军；相应VGG网络使用更小的卷积核，同时相比AlexNet进一步提升了网络层数。随着研究的深入，神经网络的层数也出现了爆发式地增长，由此也不可避免的带来梯度消失和梯度爆炸的问题，使得模型训练的困难度也随之提升。一种解决方法是将神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为Residual Network(ResNet)残差网络，网络结构的原理是将卷积层的堆叠，替换成跨层连接的模块，如图3所示。图3有了合理的建模模型，可以使用具体任务的训练数据学习视觉表征，进而完成不同的任务（如分类、分割、目标检测等）。而更加有效的方式通常是先用“海量”的数据让模型学到通用的视觉表征，再进行下游具体任务数据的学习，也就是预训练+微调的范式。2.1.2 卷积视觉预训练在CNN视觉表征体系下，早期的视觉预训练有另一个叫法是迁移学习，在BERT的预训练+微调范式流行之前就已经被广泛应用。迁移学习中，传统CNN视觉模型在做具体任务训练之前，先在大量的图像任务数据集上进行预先训练（如ImageNet分类任务数据集等）。然后使用预训练的CNN权重初始化Backbone，并增加一些任务定制网络模块，完成在下游任务上的微调（如Backbone+全连接层做分类任务）。卷积神经网络视觉表征和预训练的优化升级工作还有很多，介绍相关内容的资料也很多，篇幅原因我们对此不进行详细展开和概述，而是把更多的笔墨放在近几年更热门的研究方向上。2.2 早期多模态融合与预训练接着是CNN体系下的多模态融合和预训练，视觉和自然语言的跨模态对齐和融合有两种表现形式：一种是双塔结构，多模态分别表征，通过对比学习机制实现视觉和文本在同一空间中的距离度量；另一种是视觉表征和文本表征通过交互型网络结构融合成多模态表征，进而完成下游任务应用。由于前者可以看作后者的特例，我们用后一种表现形式为例，将二者统一，进而讲述以CNN为基础的早期多模态融合与预训练技术。如图4，展示了上述的多模态融合框架，包括视觉特征提取模块、文本特征提取模块和模态融合模块。文本模块是常见的Token Embedding方式；视觉表征方面，由于CNN已经验证了有效性，因此大多数的工作在都考虑使用CNN做视觉特征抽取，得到高级语义特征，然后将高级语义表征作为输入，和文本Token Embedding序列一起输入到下游融合模块。不同工作的差异主要集中在视觉特征提取CNN Backbone以及Modality Interaction两个模块。图4我们以2019年作为粗略分界点，在此之后BERT的训练范式开始流行，多模态方向上的研究热点则是借鉴BERT的成功，使用Transformer网络（特指Transformer Encoder）作为Modality Interaction模块把视觉和自然语言进行特征融合，并通过大规模预训练来学习得到多模态表征；而在此之前的方案通常是简单的多层全连接网络实现，我们不多赘述。顺着这个思路，确定了使用Transformer作为模型融合模块这个大方向后，第二个问题是如何对视觉特征进行有效编码，得到和文本一样的Token Embedding序列作为模型输入？这一问题的解法在CNN为主的时期有两种主要方式，如图5：Region Feature Base：先通过基于CNN的目标检测模型（Fast R-CNN等），识别图像中的关键物体区域集合（ROI，Region Of Interest），并提取区域的表征向量，作为Transformer模型的视觉输入Embedding序列。这么做的动机是，每个ROI区域，都有明确的语义表达（人、建筑、物品等），方便后续和文本特征的对齐。比较有代表性的工作如LXMERT、VL-BERT和UNITER等；Grid Feature Base：区域特征方法虽然看上去合理，但是依赖前置的目标检测模型，整体链路较重。因此也有工作探索，不经过区域检测，直接使用CNN网络提取深层的像素特征作为交互模型输入，同样取得了一些成果。比较有代表性的工作如Pixel-Bert等。图5下面我们分别介绍这一时期的经典工作，了解其中基本思路和方法。2.2.1 LXMERTLXMERT是早期进行多模态特征融合的工作之一，如图6，模型采用经典的两路深层表征输入结构。在视觉侧关注单图，图像经过目标检测模型得到区域块的特征序列，又经过Transformer做进一步编码区域块之间的关系（Object-Relationship Encoder）；文本侧通过BERT结构得到文本的特征序列（Language Encoder），最后两者使用深层Transformer结构做交叉Attention，最后进行多任务的预训练。LXMERT的预训练任务相比BERT较多，包括Masked图像特征的预测、图像Label的预测（猫、狗等）、VQA、图文是否匹配以及纯文本侧的Masked语言模型（MLM）。图6预训练模型经过特定任务微调后，LXMERT在两个视觉问答数据集（VQA和GQA）上达到了当时最先进的结果。作者还展示了LXMERT可以很好地泛化到一个具有挑战性的视觉推理任务（NLVR2），并将之前的最佳结果提高了22%（从54%到76%），是一个比较优秀的工作。2.2.2 VL-BERT另一个Region Feature Base的经典工作是VL-BERT。如图7，与LXMERT不同的是，VL-BERT属于单路输入模式，视觉特征在经过目标检测模型进行Region特征提取后，直接和文本Embedding一起拼接输入到Transformer网络中进行多模态的交叉Attention。图7VL-BERT设计了两个预训练任务：带视觉特征的掩码语言模型学习（Masked Language Modeling with Visual Clues）、带文本特征的视觉Region分类（Masked RoI Classification with Linguistic Clues）。经过预训练和微调流程，模型可以适用于多种视觉和语言任务，并在视觉问答、图像-文本检索、视觉常识推理等任务上都取得了非常不错的性能。VL-BERT印证了，多模态语义特征不需要各自的单独深度编码，直接做交互也可以取得有效结果。2.2.3 UNITER如图8，UNITER使用和VL-BERT类似的架构，同样的单路架构，同样是目标检测模型做视觉的语义特征抽取，并进一步使用更多的训练数据、更多的预训练任务，希望得到一个更加通用的图文多模态表征模型。UNITER通过在四个图像和文本数据集（COCO, Visual Genome, Conceptual Captions, and SBU Captions）上进行大规模的预训练，可以支持多种视觉和语言任务的联合多模态表征。同时设计了四种预训练任务：遮蔽语言建模（MLM），遮蔽区域建模（MRM，有三种变体），图像-文本匹配（ITM），和词-区域对齐（WRA）。图8相比于之前方案，UNITER提出了通过最优传输（OT，Optimal Transport）的方法来进行WRA，在预训练过程中显式地加强词和图像区域之间的细粒度对齐。相比其他工作仅使用图像-文本匹配（ITM）的全局对齐方式，WRA更加精准。经过大量的消融实验，UNITER还探索了预训练任务的最佳组合方式，并最终在视觉问答，图像-文本检索，指代表达理解，视觉常识推理，视觉蕴含，和NLVR2等任务上都达到了新的最先进的水平。UNITER称得上是Region Feature Based多模态预训练的集大成者，同时期的大多数工作也多是类似结构上的修补或增强。但也不乏另辟蹊径的工作，其中以Grid Feature Based相关工作最具影响力。2.2.4 Pixel-BERTPixel-BERT是Grid Feature Based多模态融合代表工作之一。如图9，与Region Feature Based方法不同的是，Pixel-BERT不需要使用目标检测模型进行ROI区域的特征抽取，而是直接通过卷积网络提取图片的像素级别特征，直觉和文本特征一起输入到下游的Transformer网络进行特征融合。这种方式减少了目标检测区域框标注的成本，同时缓解了视觉语义label与文本语义的不均衡问题（区域框的物体类别往往上千规模，而文本可以表达的语义远不止于此）。图9详细来说，当时主流的Region Feature Based方法提取视觉特是使用如Fast R-CNN的目标检测模型，通常在Visual Genome数据集上训练得到。这种目标检测模型，通常先提取可能存在物体的区域，然后根据区域特征进行物体类别的分类。相应的，这些区域的特征往往局限在固定的类目集合范围内，语义范围较为有限，这是也使用区域语义特征的固有缺陷。Pixel-BERT的思路是直接学习像素级别的表征来代替物体框为主的区域特征，具体的，像素特征通过ResNet之类的卷积神经网络提取得到。对于给定的图片I，先使用CNN backbone提取特征（如从224*224*3经过多层卷积网络到5*5*channel_dim）。然后按空间顺序，铺平网格特征序列，计作 ，k为像素的数量。最终的视觉语义embedding特征记作 ， ，其中 是可学习的语义表征向量，用于区分文本表征向量。由于所有的像素共用s_v，索引s_v也可以看作是CNN backbone的一个bias项。此外，Pixel-BERT使用随机像素采样机制来增强视觉表示的鲁棒性，并使用MLM和ITM作为预训练任务进行预训练。最后通过对下游任务进行广泛的实验，在包括视觉问答（VQA）、图像文本检索和视觉推理等下游任务中取得了SOTA效果。三、VIT：拥抱TransformerPixel-BERT之类的网络，减少了对与目标检测模型的依赖，仅使用深层卷积神经网络提取像素级别的特征作为下游多模态融合模块，极大简化了图文多模态表征模型的网络结构。那么，我们能不能进一步简化视觉表征模块，直接把图像特征简单加工后就直接输入到Transformer网络和文本特征一起做模态的融合？要做到这一点，我们需要先回答另一个问题，Transformer网络能不能替换CNN作为视觉表征的Backnone？虽然现在来看，答案是肯定的，但在开始阶段，这个过程并不是那么顺利。我们知道，CNN应用于视觉表征有着很强的归纳偏置或者说先验，在 CNN 中，局部性、二维邻域结构和平移不变性是在整个模型的每一层中都有体现，和视觉图像的特点极其类似：局部感知性：卷积层通过卷积操作和参数共享，能够高效地提取输入图像的局部特征。这种局部感知性使得CNN能够捕捉图像中的局部结构，例如边缘、纹理等，从而更好地表征图像。 层级结构：CNN的层级结构包括卷积层、激活函数、池化层和全连接层。这种层级结构使得CNN能够逐层提取和组合特征，从低级到高级，形成更复杂的视觉表征。参数共享：卷积层中的参数共享使得CNN的训练更加高效。相同的卷积核在不同位置对图像进行卷积操作，共享参数减少了模型的复杂度，同时也增强了模型的泛化能力。 空间不变性：卷积操作具有平移不变性，即无论图像中的物体在图像中的位置如何变化，卷积核都能检测到相应的特征，这对于图像分类、目标检测和图像分割等计算机视觉任务非常重要。而在 Transformer 中的Self-Attention层则是全局的，对于视觉输入的局部关系建模、图像的2D位置关系的建模，以及图像元素的平移不变性的把握上，都需要从头学习。然而，即便是困难重重，因为有BERT的巨大成功，仍然有许多的研究者前赴后继投入到这个方向，并最终取得成功，其中Vision Transformer (ViT) 是最为经典的案例之一。3.1 VIT范式视觉表征和预训练3.1.1 VIT：Transformer视觉表征如图10，VIT将输入图片平铺成2D的Patch序列（16x16），并通过线性投影层将Patch转化成固定长度的特征向量序列，对应自然语言处理中的词向量输入。同时，每个Patch可以有自己的位置序号，同样通过一个Embedding层对应到位置向量。最终Patch向量序列和视觉位置向量相加作为Transfomer Encoder的模型输入，这点与BERT模型类似。图10同样，VIT通过一个可训练的CLS token得到整个图片的表征，并接入全链接层服务于下游的分类任务。当经过大量的数据上预训练，迁移到多个中等或小规模的图像识别基准（ImageNet, CIFAR-100, VTAB 等）时，ViT取得了比CNN系的模型更好的结果，同时在训练时需要的计算资源大大减少。按说，ViT的思路并不复杂，甚至一般人也不难想到，但是为什么真正有效的工作确没有很快出现？不卖关子，VIT成功的秘诀在于大量的数据做预训练，如果没有这个过程，在开源任务上直接训练，VIT网络仍会逊色于具有更强归纳偏置的CNN网络。因此，在此之后的一大研究方向就是如何更加有效的对VIT结构的网络进行预训练。下面我们通过MAE和BEIT两个优秀的工作，来讨论这个方向上的两类主流方案。3.1.2 MAE：激进的Mask自监督预训练与自然语言理解类似，VIT模型能取得成功得益于预训练+微调的训练范式。前文提到，传统CNN视觉模型的预训练，仅仅是在大量的图像任务数据集上进行预先训练（如ImageNet分类任务等），然后使用训练后的权重进行初始化Backbone，在下游任务上继续微调完成相应任务。早期的VIT的预训练和CNN预训练一样，都是通过大规模的有监督分类任务数据集进行训练，和BERT的自监督预训练仍有区别。而自监督预训练有着数据获取成本低、不需要标注、任务难度大模型学习充分等诸多好处，因此很多研究工作探索自监督视觉预训练，比较有代表性的实践工作如Masked AutoEncoder（MAE）。图11如图11，所示，MAE以VIT为基础模型，先对完整图片进行Patch掩码，接着使用一个Transformer Encoder对未Mask的Patch进行编码，然后通过相对小的Transformer Decoder模型还原被Masked Patch，从而实现模型的自监督预训练。MAE取得成功的另一个核心原因是通过75%的高掩码率来对图像添加噪音，这样图像便很难通过周围的像素来对被掩码的像素进行重建，从而使编码器去学习图像中的语义信息。预训练之后，解码器被丢弃，编码器可以应用于未掩码的图像来进行识别任务。相对于自然语言的自监督训练，MAE使用了更大的掩码比例。后人进一步分析，这么做动机是考虑自然语言和视觉特征的信息密度不同，简单来说：文本数据是经过人类高度抽象之后的一种信号，信息是密集的，可以仅仅预测文本中的少量被掩码掉的单词就能很好的捕捉文本的语义特征。而图像数据是一个信息密度非常小的矩阵，包含着大量的冗余信息，像素和它周围的像素存在较大的相似性，恢复被掩码的像素并不需要太多的语义信息。3.1.3 BEIT：视觉“分词”表征预训练另一类Transformer视觉模型预训练的代表范式是BEIT（BERT Pre-Training of Image Transformers）模型。为了与BERT的预训练框架对齐，BEIT通过辅助网络模块先对视觉Patch进行Tokenizer，得到整张图各部分的视觉Token ID。然后将视觉Patch视为自然语言中的单词进行掩码预测，完成预训练流程。图12具体的如图12，在预训练之前，BEIT先通过一个离散自回归编码器（ discrete Variational AutoEncoder，dVAE）学习了一个“图像分词”器，最终可以将图像编码成离散的视觉Token集合。而在预训练阶段，输入的图片存在两个视角，一是图像Patch，另一个是视觉Token。BEIT随机对Patch进行掩码，并将掩码部分替换为特殊的Mask Embedding（[M]，图中的灰色部分），随后将掩码后的Patch序列输入到VIT结构的模型中。预训练的目标则是基于被掩码的图像输入向量序列，预测源图像对应的视觉Token ID。BEIT需要单独的dVAE网络辅助，相对MAE更为复杂。在效果上，MAE验证了使用normalized pixels进行目标像素重建，也可以实现类似效果，因此视觉tokenization过程并非必须。但即便如此，BEIT为视觉预训练提供了一个不错的范式，同样是一次十分有价值的探索。3.2 VIT为基础的多模态对齐与预训练以VIT为基础的视觉预训练可以通过Transformers对视觉进行有效表征，这种方法也逐渐成为目前视觉信息编码的主流手段。以此为延伸，基于此的多模态预训练工作也层出不穷，也为如今的多模态大模型的顺理成章打下了坚实基础。如图13，梳理了以VIT为延伸的多模态对齐和预训练工作，各工作之间都或多或少的有所关联，可谓是一脉相承。下面我们分别介绍这个技术方向的经典工作，读完本小结下面的内容再来看图中的模型关系，可能会更有感觉。图133.2.2 CLIPCLIP模型是OpenAI 2021发布的多模态对齐方法。与OpenAI的许多工作类似，CLIP强调强大的通用性和Zero-Shot能力，也因此至今仍有很强的生命力，相关技术被广泛应用。CLIP的核心思路是通过对比学习的方法进行视觉和自然语言表征的对齐。如图xx（1），CLIP首先分别对文本和图像进行特征抽取，文本的Encoder为预训练BERT，视觉侧的Encoder可以使用传统的CNN模型，也可是VIT系列模型。得到图文表征向量后，在对特征进行标准化（Normalize）后计算Batch内图文Pair对之间的余弦距离，通过Triple Loss或InfoNCELoss等目标函数拉近正样本对之间的距离，同时使负样本对的距离拉远。图14经过大量的图文Pair对进行预训练后，我们可以得到在同一表征空间下的文本Encoder和图像Encoder。下游应用通常也是两种方式，一是在下游任务上对模型进行微调，适应定制的图文匹配任务，或者仅使用文本或图像Encoder做单模态任务；另一种使用方式是直接使用预训练的图文表征Zero-Shot方式完成下游任务。CLIP进行Zero-Shot的一种使用方式如图14（2）和（3），对于一个图像分类任务，可以首先将所有的候选类别分别填充“A photo of a {object}”的模板，其中object为候选类别，对于一张待预测类别的图像，通过图像Encoder的到视觉表征后，与所有类别的模板文本Encoder表征进行相似度计算，最后选择相似度最高的类别即可作为预测结果。CLIP凭借其简洁的架构和出众的效果，被后来很多工作引用，并使用CLIP预训练的Backbone作为视觉表征模块的初始化参数。3.2.3 VILTCLIP方法简单有效，双塔的网络结构对于下游应用也十分友好。但是如同表示型语义匹配类似，双塔结构同样也有交互不足的问题，内积或余弦距离的模态融合方式匹配能力上限较低，对于一些需要细粒度跨模态匹配的任务（VQA等）有时力不从心。因此，交互式的多模态对齐和融合仍然极具价值，典型的如VILT模型。VILT是VIT在图文多模态方向上的工作延续。我们了解了基于Transformer的自然语言模型和视觉模型的预训练范式后，进阶到多模态融合十分容易理解。如图15所示，与BERT文本对的输入方式类似，VILT将文本和视觉Patch的Embedding直接拼接作为Transformer编码器的输入，两种模态有各自可学习的位置编码和模态类型编码。图15通过深层的Transformer编码，文本与视觉的模态得到了充分的融合。ViLT使用常用的ITM（Image Text Matching）和MLM（Masked Language Modeling)作为预训练目标。ITM（Image Text Matching）：图文是否匹配的二分类目标，正样本为常用数据集中提供的语义一致的图文Pair对，负样本对以0.5的概率随机地用替换正图文对中的图片为其他图片；此外借鉴前人工作，匹配目标还增加了图文子区域的匹配目标Word Patch Alignment (WPA)，该目标并不常用，我们也不作过多展开。MLM（Masked Language Modeling)：以0.15的概率对文本的Token进行掩码，并通过图文的整体上下文信息对预测被掩码的Token。如图16，可以对比以CNN为基础的多模态预训练和以VIT为基础的预训练，在模型架构上的区别。图16而在ViLT之后，多模态预训练的一个较为明显的趋势，是进一步提升模态对齐与融合的效果以及模型结构的通用性，使用统一模型视角进行跨模态对齐和融合。在这个过程中，ALBEF（Align before Fuse）、BLIP（Bootstrapping Language-Image Pre-training）与BEIT-3系列等工作极具参考价值，下面我们简单对比其设计思路。3.2.5 ALBEF与BLIPALBEF通过多任务联合训练将类似CLIP的对比学习和类ViLT的交互融合范式统一到一个训练框架中。如图17所示，模型结构包括一个图像Encoder（12层）、一个文本Encoder（6层）和一个多模态融合的Encoder（6层），各Encoder均沿用Transformer网络。ALBEF的训练任务包括图文对比ITC（Image-Text Contrastive Learning）、ITM（Image-Text Matching）、MLM（Masked Language Modeling）。图17ITC：在图文模态深层融合之前，在对图文的表征序列Pooling后，通过对比学习Loss对图文单模态表征进行对齐。这部分和CLIP模型的训练设置类似，不同的是文本的Encoder相对视觉Encoder层数更浅。ITM：图文Encoder输出的表征序列深层交互后，判断输入图文对是否匹配，与VILT一样是二分类任务。不同的是负样本对的构造，使用对比学习模块进行了Batch内的难负样本挖掘。主要思路是，对比学习模块中一个Batch中，模型认为最为相似的负样本对可以作为难负样本。MLM：与VILT类似，随机对输入文本token进行掩码，通过图文上下文的输入信息预测被掩码的Token。最后，由于ALBEF的预训练数据多数为互联网中挖掘的图文对，天然存在较大的噪声数据。为了缓解这个问题，ALBEF在训练过程中通过一个动量自蒸馏的模块（一个移动平均版本的ALBEF模型），生成训练数据集的伪标签，用来辅助模型的训练。ALBEF通过多任务训练机制将模态对比匹配和深度模态融合结合在一起，下游任务可以根据具体需求使用不同的模块进行微调。与之遥相呼应的的是BLIP模型，在ALBEF基础上，将MLM替换为LM（ Language Modeling）Loss，的使得训练得到的模型同时可以支持图像描述文本的生成能力，如图18所示。使得多模态融合预训练有了多模态大模型即视感。图18经过大规模多模态数据的预训练，ALBEF和BLIP在下游任务微调中均取得了十分亮眼的效果，在工业界也被广泛应用。3.2.8 VL-BEIT、VLMO与BEIT-3ALBEF和BLIP之类的工作虽然能够同时兼顾对比和深度融合两种训练模式，但视觉和自然语言仍然需要单独的Encoder分别编码，这显然还不是我们所期望的真正的多模态统一模型框架。我们可以从Microsoft Research的VL-BEIT、VLMO与BEIT-3这一系列工作一窥这个方向的探索过程。VL-BEIT可以看作是前文提到的BEIT在多模态对齐预训练工作的延续，同时借鉴了ViLT的网络结构。如图19，与ViLT的区别在于，VL-BEIT期望将单模态和多模态统一到一个模型中，在预训练任务设计上，同时考虑了纯文本、纯视觉以及图文多模态任务。纯文本任务为MLM（a）；纯视觉特征的MIM，其中MIM的目标是BEIT工作中的Visual Token ID（b）；图文多模态任务包括考虑文本特征的视觉Token预测，以及考虑视觉特征的文本Token预测（c）。图19VLMO是VL-BEIT的同期工作，如图20。VLMO相较于VL-BEIT的不同之处在于：1、舍弃了视觉侧的Visual Token ID预测，简化了整体的网络结构；2、增加了类似CLIP的图文对比学习任务，以及交互型的图文匹配任务。虽然VLMO相对于VL-BEIT在效果上并不出彩，但为后续BEIT-3的工作奠定了基础。网络结构上，VLMO是VL-BEIT都使用MoME Transformer结构，对不同的模态使用不同的Expert头，以区分不同模态的表征。图20与VLMO网络结构类似，BEIT-3将图像、文本和图文多模态输入统一到一个单独的Multiway Transformer网络。不同于经典的Transformer，BEIT-3使用一个多类型输入共享的多头自注意力模块（Multi-Head Self-Attention），不同类型的模态输入各有一个全连接专家模块单独学习。如图21，视觉模态使用V-FFN、文本模态对应L-FFN，图文多模态输入对应VL-FFN，模型会根据不同类型的模态输入选择不同的模块生效。图21在预训练任务上，如图22，BEIT-3相比之下也更加全面，不仅包括常用的图文对比学习、MLM和图像文本描述生成任务，还引进了文本和图像的单模态任务。这样的训练方式，使得BEIT-3真正统一了多模态的不同输入类型，同时更加全面和灵活的支持不同模态的下游任务。为了能够实现这样的能力，BEIT-3使用了更多的预训练数据，模型容量相对于之前的工作也有了显著的提高（达到1.9B），相应地最终也取得了在当时更好的效果。如图22BEIT-3将多模态对齐和预训练的研究推到了一个新的高度，验证了更多的数据+更大的模型取得更好的效果，在这个研究方向仍不失准。虽然开始饱受争议，但随着ChatGPT的问世，这个发展思路的正确性被进一步加深，也催生了后面多模态大模型的一众研究工作。四、多模态与大模型写到这里，经过大规模篇幅的铺垫，终于到了大家都关心的多模态大模型章节。打开前两天的笔记，原来的计划是这部分内容参考综述论文的梳理，选择一些有代表性的工作进行问题的串连。当我重新下载这篇综述论文，想截一张示意图时，却是目瞪狗呆。对比一下，图23是年前的截图，图24是年后的。图23图24这个速度，属实有点跟不上了。调整情绪后，我告诉自己很多工作万变不离其宗，可以延续原来的思路继续写，不增加加新的内容。这样，应该也挺合理的吧。下面我们通过各时期的优秀工作，来系统看一下类似GPT-4的多模态大模型的主流思路。4.1 Flamingo如今GPT-4代表着多模态大模型的顶尖水平，但在此之前，甚至在ChatGPT之前就已有相关探索工作，其中谷歌的Flamingo最具当前主流技术雏形。事实上，Flamingo更像是图文多模态领域的GPT-3，不同的是它支持图文上下文的输入，通过In-Context Few-Shot方式完成任务。Flamingo同样支持视频帧序列作为输入，通过Prompt指令完成Video理解任务。图25做到这种功能，在模型侧和GPT-3类似，不同的是Flamingo在文本Transfomer网络中增加视觉输入特征，模型结构如图26，包括三个部分。视觉侧特征抽取使用预训练的ResNet和采样模块（Perceiver Resampler，将变长的视觉特征输入转成少量的视觉特征）模型；文本侧模型使用LLM（基座使用Chinchilla，同样是谷歌发布的对标GPT-3的大语言模型，并提供了1.4B、7B、和70B等版本，分别对应Flamingo-3B、Flamingo-9B和Flamingo-80B）；GATED XATTN-DENSE层，用于连接LLM 层与视觉特征，允许 LM 在处理文本时考虑视觉信息。通过交叉注意力，LM 可以关注与视觉特征相关的部分。预训练LLM和视觉ResNet参数训练过程中是冻结状态。图26相应的，在数据层面Flamingo也是使用了多样形式的训练语聊，包括：图文穿插形式：MultiModal MassiveWeb (M3W)，43 Million；图文Pair对形式：LTIP(Long Text & Image Pairs)，312 Million；带文本描述的短视频：VTP (Video & Text Pairs) ，27 Million 。最后Flamingo在各种多模态任务上的效果也非常优秀，甚至在有些数据集上通过few-shot方式可以超过经典模型的SOTA。图27Flamingo凭借其出色的效果，吸引了许多研究者对于多模态大模型的注意，但当时这种规模的模型训练不是谁都能玩的起，因此并没有引起特别火热的跟风潮。直到ChatGPT的出现，让人逐渐接受了大模型这条道路的正确性，以前觉得自己玩不起的机构，砸锅卖铁拉投资也愿意投入，自此相关的开源研究开始如火如荼。在众多开源工作中，BLIP-2以及与之一脉相承的InstructBLIP算是早期的探路者之一，我们可以从这两个工作开始讲起。4.2 BLIP-2和InstructBLIPBLIP-2的论文标题是Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models，核心思路是通过利用预训练好的视觉模型和语言模型来提升多模态效果和降低训练成本。BLIP-2的网络结构如图28所示，从架构上来说，和Flamingo十分类似。包括视觉编码层、视觉与文本的Adapter（Q-Former）以及大语言模型层。视觉编码层：使用ViT模型，权重初始化通过CLIP预训练完成，并剔除最后一次提升输出特征的丰富性；训练过程中冻结权重，不更新；文本侧的大语言模型层：早期的BLIP-2使用OPT/FlanT5来实验Decoder based和Encoder-Decoder based LLM的效果；这部分同样在训练过程中冻结权重，不更新；图文Adapter层：Q-Former结构，类似BLIP网络（同样先进行了图文多模态预训练模块），通过Queries向量，提取视觉侧的关键信息输入到LLM；这部分是多模态大模型训练过程中的主要参数。图28和Flamingo相比，BLIP-2简化了视觉特征和大模型的交互，直接仅仅将视觉特征和文本特征一起作为大模型的输入，没有深层的交互模块（如GATED XATTN-DENSE层）；另一方面在视觉和LLM的Adapter层做了更多的设计，即Q-Former结构，如图29。从Q-Former结构图，我们可以看到BLIP的影子，最大的不同在于一个Learned Queries模块，用于对ViT输出的视觉特征进行采样（Pooling），得到固定长度的视觉特征序列。图29上面提到，为了避免灾难遗忘，BLIP-2冻结了ViT和LLM的参数，只训练Q-Former模块。为了训练更加稳定，Q-Former模块的训练包括两个阶段。Stage1: 将Q-Former与冻结的ViT拼接，借鉴BLIP，使用 ITC（图文对比学习）、ITG（图生成文本）和ITM（图文匹配）任务进行学习，对参数进行初始化，学习图文相关性特征。Stage2：如图30，将Stage1得到的模型再拼接LLM，即Q-Former的输出可通过线性投影输入到LLM（冻结参数），进行视觉到自然语言的生成学习，目标是训练Q-Former使其输出的视觉特征和LLM的输入分布对齐。图30BLIP-2通过视觉和LLM的特征对齐，使得LLM具备了多模态理解能力，但其训练数据主要沿用BLIP（图文Pair对形式），和当下的多模态模型的主流技术方案仍存在一定GAP，是早期代表性探索之一。不过，随着指令微调成为大模型必备流程，后续BLIP-2也自然升级为InstructBLIP。如图31，InstructBLIP的网络结构与BLIP-2几乎一致，同样也是2阶段训练，不同的是采样了指令微调范式，将文本模态的Instruction也作为输入同时给到Q-former和LLM进行学习。图31对应的，InstructBLIP的另一个不同是训练数据也使用指令形式，将各种类型任务的开源学术数据，使用模板构造成指令多模态数据。数据模板如图31。图31通过指令数据和指令微调，是的InstructBLIP可以像GPT-4一样通过指令提示词的方式完成任务，虽然效果上仍有差距。即使不是InstructBLIP的训练范式并不是开创性的，但是我们依然可以用InstructBLIP作为参考，来看对比后面要介绍的其他工作。4.3 Qwen-VL阿里巴巴的Qwen-VL是另一个比较经典的模型，十分值得作为案例介绍多模态大模型的训练要点。Qwen-VL使用Qwen-7B LLM作为语言模型基座，Openclip预训练的ViT-bigG作为视觉特征Encoder，随机初始化的单层Cross-Attention模块作为视觉和自然语言的的Adapter，总参数大小约9.6B。图32如图33，Qwen-VL的训练过程分为三个阶段：Stage1 为预训练，目标是使用大量的图文Pair对数据对齐视觉模块和LLM的特征，这个阶段冻结LLM模块的参数；Stage2 为多任务预训练，使用更高质量的图文多任务数据（主要来源自开源VL任务，部分自建数据集），更高的图片像素输入，全参数训练；Stage3 为指令微调阶段，这个阶段冻结视觉Encoder模块，使用的数据主要来自大模型Self-Instruction方式自动生成，目标是提升模型的指令遵循和多轮对话能力。 图33Qwen-VL的另一个启发是在Stage2和Stage3的训练过程中，不止使用VL数据，还使用了纯文本的训练数据，避免遗忘LLM的能力，这个策略的效果在其他的工作中也有所印证。此外，相比InstructBLIP，Qwen-VL模型视觉和LLM的Adapter模块简化很多，仅仅是一个浅层的Attention Pooling模块，通过更加细节的训练流程和更加丰富的训练数据，仍取得了比InstructBLIP更优的效果。4.4 LLaVA1.5同样，微软的LLaVA也是一个持续更新的系列工作，这里主要总结LLaVA和LLaVA1.5的核心思路。图34为LLaVA1.5的数据和模型概况。可以看到，和Qwen-VL相比，LLaVA1.5在预训练和指令微调数据上使用了更少的数据（将Qwen-VL的Stage2和Stage3都视作指令微调）；在模型结构上，除了视觉Encoder和LLM均使用了不同的基座模型，视觉和自然语言的Adapter使用更简单的MLP层。图34LLaVA1.5模型的效果在一些评测数据集上相比Qwen-VL有更好的效果，说明通过一些优化工作，使用更少的数据，更简单的Adapter结构，也能使LLM具备不错的多模态理解能力。在数据层面，如图35，对比LLaVA1.5和LLaVA工作，通过增加高质量细粒度的VL数据、丰富指令、纯文本指令微调数据、提升图片输入像素、提升LLM参数规模等手段，可以有效提升模型效果。 图354.5 VILA另一个与LLaVA比较类似，但有所补充的工作是英伟达的VILA（不是显卡）。VILA模型的网络结构和LLaVA十分类似，我们不做过多赘述。不同的是VILA通过实验，总结了多模态预训练的一些经验，其中有些经验在相关工作中也有所体现，主要为以下三点：LLM参与训练更好：在预训练阶段冻结LLM参数，能做到不错的zero-shot的能力，但会损失in-context学习的能力，而LLM参数参与训练的话可以有效缓解；预训练数据使用图文交替数据更好：图文Pair对并不是最优的选择，图文交错的数据效果更好；图36SFT时纯文本数据图文数据混合更好：在图文指令微调训练数据中混入纯文本的指令数据，不仅可以缓解纯文本能力的遗忘，还能提升VL任务的能力。具体的，如图37，VILA的训练分为3个阶段，视觉编码模块ViT参数均是冻结状态。Step 0 使用图文Pair数据对初始化Projector（图文Adapter）参数，LLM模块参数冻结；Step 1使用图文交替数据全参数预训练；Step 2使用指令微调数据进行全参数微调，其中微调数据混合了图文指令和纯文本指令；图37VILA是较新的工作，因此有更丰富的模型效果对比，如图38，相对各时期的SoTA，VILA在公开评测指标上有不错的效果。图384.6 Gemini 1.0和Gemini 1.5目光来到闭源世界，与VILA同阶段，谷歌公司发布了Gemini系列，又在近期发布了性能更强的Gemini 1.5，可惜被另一个热爱闭源的OpenAI的Sora抢了风头，属实悲催。由于Gemini系列并没有开源，我们只能通过技术报告中的简单介绍来了解其方法。Gemini 1.0是一个多模态模型，这里模态除了图图像和文还包括音频、视频，符合谷歌多模态大模型一贯的ALL IN ONE的风格，这也是依赖积累丰富的数据资源和算力资源。Gemini 1.0提供Ultra、Pro和Nano版本，分别适应不同能力、参数大小和推理速度要求，最小的Nano甚至可以端上运行。方法上，Gemini 1.0的网络结构同样是Transformer Decoders，支持32K上下文长度，使用了Multi-Query Attention等优化机制。如图39，模型输入可以是文本、音频、视觉输入，输入视觉可以是图片、图表、截图、PDFs或视频等，输出可以是图片和文本（没错，可以生成图片）。视觉的Encoder模块借鉴了谷歌自己的Flamingo、CoCa和PaLI，结合这些模型，可以输入多模态的同时，也可以通过离散的视觉Tokens生成图片或视频等视觉模态。图39对于音频模态，Gemini可以直接输入Universal Speech Model (USM)的16kHz特征，具体可以参考USM工作。对于视频理解，Gemini通过将视频编码为长上下文窗口中的一系列帧来实现。视频帧或图像可以与文本或音频自然交织在一起，作为模型输入的一部分。Gemini同时支持不同像素输入的视觉以满足不同粒度的理解任务。在具体训练数据方面，技术报告同样并没有提过多细节，只是简单说了数据包括什么模态、经过了什么清洗步骤等，我们也不再深究。至于最近的Gemini 1.5，同样是技术报告的形式发布，没有特别多技术细节，主要介绍了模型是如何的强。区别要点包括：模型在Gemini 1.0基础上引入了sparse mixture-of-expert (MoE)，同时强化了上下文长度（32K->10M）同时几乎没有损失上下文感知能力。在训练过程中，Gemini 1.5强化了指令微调过程，使用了用户偏好数据。总体来说，虽然Gemini没有提供技术细节，但也体现了谷歌对于多模态大模型技术方向的判断，比如我们可以get到网络结构的MoE、一个模型更多模态、超长上下文、文本生成+多模态生成结合等。4.7 LWM最后，我们再介绍一篇和Gemini类似的开源工作《World Model on Million-Length Video And Language With RingAttention》，模型名LWM（Large World Model）。至于为什么叫World Model，意思可以通过视觉和视频的理解物理世界，，LWM是UC Berkeley最近发布的一篇工作，个人认为在开源方向上是一个优秀的工作，但好像也是由于Sora和Gemini 1.5的热度，没有引起太多关注。LWM希望完成的任务和Gemini十分相似，核心是超长上下文理解的多模态大模型。凭借支持1M的token输入，LWM可以对超过一小时的视频进行理解，在Gemini 1.5之前几乎是多模态大模型中最长的上下文输入（之一）。LWM的主要工作要点包括：支持超长上下文，可处理超长的文本、图片序列或视频等；一些技术难点方案：Masked Sequence Packing方法混合的输入长度；通过loss weighting 平衡视觉和文本模态；模型自动生成长序列问答数据集用于模型训练；实现了高性能的RingAttention，Masked Sequence Packing等优化项，完成了百万级别长度的多模态序列的训练；开源7B参数规模的大模型，包括长上下文的文本模态模型（LWM-Text，LWM-Text-Chat），和多模态模型（LWM，LWM-Chat）。具体方案上，LWM使用Transformer架构，在LLama2 7B基础上扩充上下文理解的长度上限，模型结构如图40： 图40与之前大多数方法不同的是，视觉的编码器使用VQGAN，可以将256 × 256输入图片编码成16 × 16 离散Token。这使得LWM不仅可以生成文本，也可以基于文本生成Image Token还原成视频。对于多图或视频帧，可以分别做视觉特征抽取，和文本模态一起输入到LLM中。在模型训练流程上，主要分为两个阶段的训练：阶段一，使用Books数据集，先扩充文本LLM上下文长度到1M；阶段二，长上下文的多模态训练，即混合图-文数据、视频-文本数据、以及纯文本的Books数据进行训练。上面两个过程有两个核心问题需要解决：1、长文档的可扩展训练；2、如何稳定地扩展LLM的上下文。前者关注训练的效率和开销，后者则关注长上下文拓展的有效性。针对问题1，LWM主要实现了高效的RingAttention，同时结合了FlashAttention；针对问题2，一方面，两个训练阶段都是多轮训练方式，逐步提升上下文长度的方式，如图41。另一方面通过简单的调整了RoPE的 参数，提升模型长文本的位置编码能力。图41总的来说，LWM是一篇不错的文章，最重要的是开源，技术方案基本没有保留，值得拉出来单独讨论。在效果上LWM和Gemini 1.0 Pro以及GPT4有一定的竞争力，更多的细节可以阅读原论文。五、总结写到这里，吐一口老血，但还是要总结一下。本文梳理了2019年之后视觉表征和多模态表征的一些变化，主要涉及视觉表征和视觉预训练、多模态表征对齐（或融合）和多模态预训练、多模态大模型技术的相关工作。各工作之间的简化关系如图42，脉络主要是结合笔者自己各阶段的实践经历和认识，会出现一些地方不严谨的地方，欢迎指正。图42关于未来畅想，从最近的工作上来看，多模态的呈现出以大模型为主线，逐步开始朝长上下文、混合模态、世界模型、多模态生成等方向发展。开始在自己工作的实践中得到的一个个人观点，是多模态大模型的惊艳能力主要来自于文本大模型中所蕴含的知识，以及超强的上下文理解能力，视觉特征只是从属的信息输入或感知源。但近期Gemini 1.5、LWM、甚至Sora等工作又开始尝试大模型理解物理世界（引出世界模型的概念），大模型好像开始从文本之外的模态强化输入信息的影响力。不管怎么说，持续的更新迭代让人耳目一新，相信也会不断刷新人们对人工智能边界的认知。最后，感觉知乎长文确实耗时耗力，如果有收获欢迎关注本账号：菜人卷。另外打算后面开一个小红书的坑，写一些日常学习和实践的短篇经验，提高一下更新频率，系统性内容也会周期性的搬到知乎上来，欢迎扫码关注支持。 图43六、论文打包下载图45PS：引用文章可以细读，论文打包下载链接: https://pan.baidu.com/s/18tkHDkDmGKvzEeGSEc52Jg 提取码: hvkf"
665227907,221279692b3a63a223e69a7cb27afd38,夜如何其夜乡晨,https://api.zhihu.com/people/221279692b3a63a223e69a7cb27afd38,people,图学习&amp;大模型,大模型遗忘：如何让Llama忘记哈利波特,article,https://api.zhihu.com/articles/665227907,将目标数据中的特殊表达替换为通用对应项，以及通过这些通用对应项对<em>模型</em>进行微调，从而在提示与上下文相关时有效擦除<em>模型</em>对原始文本的记忆。 图1 一些例子，说明<em>模型</em>已经遗忘相关内容 图2 在benchmark上仍然取得了相当的结果,312,17,447,1699230364,1699230364,"分享一篇ICLR2024 under review的论文：WHO’S HARRY POTTER? APPROXIMATE UNLEARNING IN LLMS链接：https://openreview.net/pdf?id=PDct7vrcvT大语言模型的合规性和隐私保护问题是不论政府还是用户都非常关心的问题。而Unlearning，即如何使模型遗忘一部分内容，是实现安全和隐私保护的重要技术。这篇论文作为第一个在大语言模型上做Unlearning的工作，研究了一个从Llama2 7B上遗忘哈利波特相关内容的案例，在抹去相关内容的同时在benchmark上与原模型取得了相当的效果。背景大语言模型（LLMs）是通过在大规模的互联网文本数据上进行训练而得到的，这些数据中通常包含受版权保护的内容。这给开发者、用户以及出版商等带来了法律和道德伦理方面的挑战。具体来说，这些挑战包括但不限于：版权问题：LLMs经常在其训练数据中包含受版权保护的文本，这可能引发版权侵权问题。恶意内容：LLMs可能在生成文本时复制或受到恶意、有害内容的影响，这可能导致伦理和法律问题。不准确或虚假内容：LLMs有时会生成不准确或虚假信息，这可能损害信息的可靠性。隐私问题：LLMs在生成文本时可能包含个人数据，这涉及隐私问题。为了应对这些问题，这篇论文提出了一种允许LLMs选择性地“遗忘”其训练数据的特定部分，而无需从头开始重新训练模型的方法。具体来说，他们关注了如何从Llama2 7B中删除与“哈利·波特”（Harry Potter）相关的内容，而不影响模型在其他任务上的性能。这篇论文的主要贡献包括：1、提出了一种新颖的技术，用于从大型语言模型（LLMs）中去除其训练数据的子集，而无需从头重新训练模型。2、作者在Meta的Llama2-7b上评估了他们的技术，实验结果表明在约1小时的GPU微调中，成功地抹去了模型生成的与哈利·波特相关的内容，同时在常见基准测试上的性能几乎没有受到影响。3、该技术由三个主要组成部分构成：通过加强模型识别与去除目标数据相关的token，将目标数据中的特殊表达替换为通用对应项，以及通过这些通用对应项对模型进行微调，从而在提示与上下文相关时有效擦除模型对原始文本的记忆。图1 一些例子，说明模型已经遗忘相关内容图2 在benchmark上仍然取得了相当的结果方法实现遗忘特定内容的一种最直观的方法是，当模型预测的下一个单词是要遗忘的文本时，对模型施加一个loss，该loss随着分配给该单词的概率增加而变得更大。然而，作者指出这一方法并没有取得令人满意的结果。作者举了一个例子，其中""Harry Potter went up to him and said, 'Hello. My name is'""，如果下一个单词是""Harry""，那么采用负损失的方法可能会导致模型忘记了""my name is""这个短语的含义，而不是遗忘与""Harry Potter""相关的内容。作者进一步解释说，预测token的能力与对""Harry Potter""小说的知识无关，而更多地与对语言的一般理解能力相关。随后，作者介绍了一种新的方法，旨在为文本中的每个标记提供通用预测（generic prediction），即模拟未经过""Harry Potter""书籍训练的模型在该句子中的下一个标记预测是什么。下文介绍了两种获得通用预测的方法，分别为“ 通过强化Bootstrapping获得通用预测”和 “通过锚定属于获得通用预测”。最后将这两种方法进行了结合。这些方法旨在让模型生成与""Harry Potter""无关但又合适的文本，从而减少模型对特定文本的记忆。通过强化Bootstrapping获得通用预测尽管如何遗忘文本的训练方法尚不清楚，但相反的操作相对容易，即训练一个对特定文本的记忆更深的模型。例如，提示""His best friends were""将生成""Ron Weasley和Hermione Granger""，提示""The scar on his""将生成""forehead""，尽管没有在上下文中提到这些书。为了说明强化模型对我们有用的原因，考虑一个例子：""Harry Potter went back to class where he saw .""尽管原模型都将最高概率分配给""Ron""和""Hermione""作为下一个标记，但强化模型将预测更高的概率。基于此，为了知道通用预测可能是什么，我们可以简单地查看那些在强化过程中概率没有增加的所有标记。具体来说，我们可以取两个模型vbaseline和vreinforced分配的两个对数概率向量，并定义一个新向量：其中，vgeneric是通用预测标签， 是原模型的预测标签， 是增强模型的预测标签，α是参数。一旦有了这个向量，我们可以简单地取它的arg-max作为通用预测（或者以它为基础使用soft label的交叉熵）。实际上，作者使用略微修改的公式采用ReLU函数的原因在于我们只关心那些在增强预测中概率值增加的token。例如，在根据上述公式微调模型后，对于句子""He had a scar on his forehead. His name was""，""Harry Potter""将不太可能出现。然而，这个想法在某些情况下无法取得好的效果，原因在于：1、首先，考虑这个句子： ""When Harry left Dumbledore’s office, he was so excited to tell his friends about his new discovery, that he didn’t realize how late it was. On his way to find"" 原模型可能会分配最高概率给完成""Ron""，次高的概率给""Hermione""，而由于强化模型对书籍的更精细知识，它分配给这两个标记的概率的顺序发生了变化。在这种情况下，使用第一个公式将进一步增加""Ron""的概率。2、在许多情况下，当模型在特定细微差异（如主要角色的名字）上进行调整时，与目标文本相关的特定完成结果已经有了很高的概率，似乎强化模型几乎没有影响，因此提出第二个技术。通过锚定术语获得通用预测首先考虑以下补全句子的任务： ""Harry Potter studies__."" 原模型可能会分配最高的概率给完成诸如”magic”, ”wizardry”, ”at the Hogwarts school”等等，而不知道哈利·波特是谁的模型可能会用""艺术""、""科学""或""当地小学""等来完成。因此，我们可以简单地用一个通用名称替换哈利·波特里的名字。一个简单的方法是在模型中用""Jon""这个词的嵌入替换""Harry""这个词的嵌入。但这不是最好的方法，因为我们可以简单地切换提示中的相同标记，然后生成翻译。事实上，与其忘记""Harry Potter""，我们的目标更应该是忘记实体""Harry Potter""与实体""魔法""（或""霍格沃茨""）之间的联系。为此，作者希望对模型进行训练，使其对原本建立了与""Harry Potter""世界相关的不同实体之间的联系的文本进行扰动，以便其中一些实体保持不变，而其他实体被通用版本替代。为此，作者用GPT-4来执行简单的实体提取，通过""遗忘目标""：作者用书籍的随机片段提示模型，要求提取与文本特定的表达、名称或实体的列表，称之为锚定术语（anchor terms.）。对于每个这样的表达，要求提供一个替代表达，对于文本中的每个段落都生成了一个小字典，如下例所示：图3将这些生成的词汇拼接在一起，得到了一个包含大约1,500个锚定术语的字典。一般的思路是遍历遗忘目标的每个文本块，将锚定术语替换为它们对应的的通用版本，然后使用原模型处理生成的文本，以获取下一个标记的预测，并将其用作通用预测。因此，一个做法是简单地采取模型在翻译文本上的预测，并微调模型以在原始文本上具有相同的预测。然而，这将会引发其他问题。假设文本包含句子""Harry went up to him and said, 'Hi, my name is Harry'""。简单地采用上述方法实际上会导致模型在第二个""Harry""被替换为""Jon""的相同句子上进行训练。在实验中，发现这确实会导致模型产生不一致的补全。为了缓解这个问题，需要 (i) 确保同一块中以前出现过的任何锚定术语不会从第二次出现开始集成到损失函数中，(ii) 减少与之前出现的锚定术语相对应的概率。微调数据集中如图4所示，其中输入标记为黑色，相应的目标标签为蓝色。图4值得注意的是在第二行中，原始标记""Ron""被替换为目标标记""her""（在这个上下文中，""her""是一个合适的补全，因为这个句子的宾语是Hermione）。在同一行中，原始标记""Harry""被替换为""Jack""。在第五行中，""Ravenclaw""一词的第一个标记被替换为""the""。在第六行中，""They directed their wands""中的""wands""一词被替换为""gaze""。整合上述两种方法总结一下，遗忘过程主要包括以下步骤：创建一个锚定术语翻译的字典。将文本分成块（使用512个标记的上下文长度），对于每个块，生成通过使用强化模型处理文本获得的强化预测，以及通过字典翻译文本并使用原模型获得的通用预测。根据公式1将预测的logits合并，取具有最大logits的标记以生成通用预测标签。使用原始文本作为输入标记，使用通用标签作为目标标记对基线模型进行微调（在设置中，大约需要150个steps）。"
649923916,3f277949b88e7274bd232a6239276732,小神弟弟,https://api.zhihu.com/people/3f277949b88e7274bd232a6239276732,people,宝可梦爱好者，ai从业者,大模型训练的一些记录（8月）,article,https://api.zhihu.com/articles/649923916,使用fastllm加速推理 这个fastllm用起来还挺方便的。也不挑显卡，主流的<em>模型大</em>多都支持。实测llama也能加速接近2倍，又支持自动组batch。而且可以自动做量化,183,19,452,1692089799,1692142802,"1.百川13b强化学习时遇到参数不更新的问题超郁闷，一模一样的代码chatglm2和llam都能训，百川的模型参数就一直不更新。首先，是如何在accelerate+deepspeed的环境下打印出梯度：具体是要修改accelerate/utils/deepspeed.py文件的DeepSpeedEngineWrapper类：class DeepSpeedEngineWrapper: """""" Internal wrapper for deepspeed.runtime.engine.DeepSpeedEngine. This is used to follow conventional training loop. Args: engine (deepspeed.runtime.engine.DeepSpeedEngine): deepspeed engine to wrap """""" def __init__(self, engine): self.engine = engine def backward(self, loss, **kwargs): # runs backpropagation and handles mixed precision self.engine.backward(loss, **kwargs) grads = [] #新增内容如下 import deepspeed for name, param in self.engine.module.named_parameters(): if param.requires_grad: grads.append((name,deepspeed.utils.safe_get_full_grad(param)))然后发现，百川的参数里，第0-27层的梯度都是nan，28-39都是正常的。很奇怪，最后发现是pad的原因。强化学习的实验制作阶段是以batch的形式去generate数据，所以就设置了padding_side=left。然后训练的时候，重新把他们组成batch的时候，也是没有改这个设定。在llama和chatglm2里面这样设定没有什么影响，但是百川就不行，在训练的时候改成padding_side=right就正常了。百川的模型在微调的时候还遇到挺多问题的：遇到超长的question时，answer很容易出现提前终止bos加上去效果会变差，看它原始的配置似乎时没有加bos的百川官方的prompt拼接方式，如果直接用原始文本<reserved_102>去拼接后再encode，会发现转出来的id会有一些出入：https://github.com/baichuan-inc/Baichuan-13B/issues/100 不知道对训练有什么影响2.使用lion优化器节省显存https://github.com/google/automl/blob/master/lion/lion_pytorch.py论文里面推荐是把学习率降低到原来的1/10~1/3，然后weight_decay放大10倍，同时betas改为(0.95, 0.98)。实测了一下，在微调13b的模型的时侯，我的学习率是2e-6，原来adam是1e-5。weight_decay改为1.0，betas和论文保持一致。训练效果最终是和adam基本差不多，甚至dev loss还低了一点点。训练70b时，用了lion后，可以在16张a100上用zero3全量微调4096长度的context，还是很强的。用adam的话是不行的，目测显存应该能至少节省1/3吧。3.使用fastllm加速推理这个fastllm用起来还挺方便的。也不挑显卡，主流的模型大多都支持。实测llama也能加速接近2倍，又支持自动组batch。而且可以自动做量化，很适合小显存的卡，像vllm就不支持。简单部署的话还是可以用用的，也比较简单。就是改动比较频繁，还有就是百川的性能下降比较多，chatglm和llama不明显。不知道是什么原因。如果是70b级别的模型，用huggingface推出的text-generation-inference效果会更好，支持多卡部署，也支持动态的batch解码，速度很快。小模型多卡就没有优势了。缺点就是不支持老型号的显卡，例如v100。https://github.com/huggingface/text-generation-inference4.使用lora训练的参数配置lora训练生成任务，最终的loss总是会比全量差一些。为了尽可能接近全量，参照qlora，可以将lora的各个参数都调到激进一些，比peft原始的配置要好很多：r=64, lora_alpha=32, target_modules=find_all_linear_names(model), lora_dropout=0.01, bias=""none"", task_type=""CAUSAL_LM""如果是做33b以上尺寸的强化学习，qlora也是个不错的选择，借助量化将大模型塞到一张卡上做推理，勉强能训。5.accelerate+deepspeed下配置学习率步数的最佳实践accelerate的那个学习率调节是真的复杂，乱七八糟的。最终找到了一个超简单的方法：lr_scheduler.step_with_optimizer = False只要在prepare后设置这个东西，你的学习率调节就是所见即所得，不用踩坑。6.accelerate+deepspeed下调整fp16的初始scale的方法这个scale默认是超级大的一个值，而且没有函数可以调整，如果硬要调整的话，可以在prepare后这样设置：optimizer.optimizer.loss_scaler.cur_scale = 655367.llama2风格的reward model loss实践llama2里面提到说他们在构造reward model的loss时，增加了一个映射的变量。实测还是有用的，可以提高dev的acc。如果你的标注数据是有等级的，比如两个偏好一个是5分，一个是1分，为了体现这个差异，可以在loss里面将这个差值4分，映射成一个数字。llama2给了一个简单的方式，就是在算两个reward的差的时候，多减去一个（差值-1）* 1/3。这个1/3可以自己设置一个合适的值。暂时就想到这么多了，有空再补充"
688873027,d67cdc60f22a01252f8f09e95ad9b9d6,elihe,https://api.zhihu.com/people/d67cdc60f22a01252f8f09e95ad9b9d6,people,整点薯条,从啥也不会到DeepSpeed————一篇大模型分布式训练的学习过程总结,article,https://api.zhihu.com/articles/688873027,从啥也不会到GPT-3和InstructGPT……一篇LLM的学习过程总结 为什么我要写这个？ 系统化的学习<em>大模型</em>，除了知道大模型是什么，也得知道<em>大模型</em>是如何训练的，对大模型的实际应用有更加定量的认知，该文章也算是一篇分布式训练的学习过程总结,523,4,1156,1711343975,1713148655,"导航Stack for AI专栏：Stack for AI算法学习文档：elihe：从啥也不会到GPT-3和InstructGPT————一篇LLM的学习过程总结为什么我要写这个？系统化的学习大模型，除了知道大模型是什么，也得知道大模型是如何训练的，对大模型的实际应用有更加定量的认知，该文章也算是一篇分布式训练的学习过程总结，作为循序渐进学习分布式训练的总结。类似之前写过的LLM文章，本文也建议读者先定性有个宏观认知，然后再细化到某个概念定量了解，遇到不太清楚的概念深度递归去学习为什么需要分布式训练？主要有两点： 对小模型而言训练速度更快 对大模型而言，其所需内存太大，单机装不下分布式训练的加速这个就很直观了，对于一些单卡可以装载的模型我们可以通过多个卡数据并行的方法训练，然后把一轮数据算出来的梯度求和更新参数进行下一轮的梯度下降这个范式比较经典的例子就是Parameter Server，后续的章节会定量的介绍大模型的内存开销我们来定量的算一算大模型需要的内存空间比如一个 参数的GPT-2模型，我们用 代表这个参数量的数量，那么在FP16的精度（单个参数大小 ，或者 ）下模型本身达到了 如果是推理，那么模型的加载确实只需要这些参数，推理过程中再算上输入embedding占用的额外参数便是需要的所有内存但如果是训练，那么就麻烦了，我们用DeepSpeed论文中Adam Optimizer+混合精度训练（如果不知道Adam和混合精度是什么建议自行了解下）作为例子。模型在训练过程中需要储存自身的参数和梯度（注意这里还不是Adam最后算出来的参数更新量，只是根据loss反向传播得到的原始梯度），这便需要 的内存，同时混合精度fp32训练时，Adam需要一份fp32大小的模型拷贝，momentum和variance去储存模型的优化器状态，这需要 ，最终我们需要 的内存用于训练，即对于一个3GB的GPT-2模型，我们训练时需要24GB的内存，对比一张V100的显存为32GB不难看出，一个GPT-2模型的训练就能吃光V100的显存了，对于175B的GPT-3的训练，哪怕是目前的H100（显存188GB）也无法单卡装载。因此对于这类大模型只能通过张量并行等方式训练，比如后面会介绍的Megatron和DeepSpeed。我会怎么展开接下来的内容我会先介绍一下分布式的一些必要前置知识，然后我会根据分布式训练随着大模型需求的演进路线展开后续的内容： Data Parallelism：模型一台设备装得下，所以同一个模型同时用多份数据分开来训练 Pipeline Parallelism：模型装不下了，模型的一层或多层一台设备装得下，所以同一个模型按层拆分开训练 Tensor Parallelism：模型的一层都装不下了，所以同一个模型层内拆分开训练 DeepSpeed：可以算作目前大参数大模型分布式训练的终极方法，因此单独提出来了分布式的一些必要前置Map Reduce这个是分布式架构的鼻祖级神文，我推荐每个想要了解分布式的人都定性的读一遍这个文章，MapReduce架构可以说是分布式架构的鼻祖，别说分布式训练了，哪怕大数据和k8s等分布式架构上都有它的影子，强烈推荐阅读原文论文和随便找了篇中文定性的介绍（如果英文阅读很吃力的话可以看这个）本节需要了解：定性掌握Map Reduce原理通信原语通信原语就是分布式训练用到的数据交换方式的原子操作定义和Pytorch代码实现可以直接看这个，还有其他一些NCCL的操作可以看这个，每个通信原语其实只是定义了一个数据的分发or合并方式，其CPU和GPU上的实现需要深入到MPI和NCCL，这个就不做展开了。定性了解每个通信原语是干什么，如何在Pytorch里使用这些原语即可后面的很多内容也会用到这些通信原语的定义，所以这个也需要好好定性的去了解本节需要了解： 定性了解各个通信原语的定义 最好能用Pytorch上手使用一下这几个通信原语训练流程先定量了解下模型训练时的大概流程fwd and bwd还是以混合精度的的Adam为图例，假设数据为fp16和fp32： 正向传播时（FWD），上一层fp16的激活值和fp16的参数参与了计算，得到这一层的fp16激活值 反向传播时（BWD），本层的激活值和参数参与计算，得到fp16的梯度 参数更新时，fp16的梯度以及fp32的参数副本，momentum和variance参与计算，最终算出更新后的fp32参数、momentum和variance，然后将fp32的参数转化为fp16进行本层的参数更新本节需要了解： 混合精度下Adam优化器的参数更新流程 定量了解混合精度下Adam优化器在各个流程的数据消耗Re-materialization论文和解读，建议直接看代码其实re-materialization这个叫法是在后文的GPipe论文内提到的，这里直接拿来用了。提到该技术的论文其实比较早了，文章提出了几种节省训练时内存的方法，但我们只需要了解gradient checkpoint是啥和这玩意是如何让梯度的内存实现平方根级别内存优化的即可本节需要了解： 定性了解gradient checkpoint是如何实现的 最好能定量推导gradient checkoutpoint为什么能把训练内存节省到平方根级别数据并行(Data Parallelism)数据并行是比较早期的模型训练方法，在CPU训练或单张GPU能装载下模型的场景下，我们可以用数据并行的思路加速模型的收敛，这是一种很直观的分布式模型加速方案其实数据并行主要就是Ring All Reduce（无master节点）和Parameter Server（有master节点），其他的方案（如Spark MLlib）原理上都大同小异，因此本章只介绍这俩Ring All-reduce原理建议直接看这个文章，代码可以参考这个，其实Ring All-reduce在Pytorch里的实现就是DistributedDataParallel，Ring All-reduce本身流程也是reduce-scatter+all-gather的组合Ring All-reduce其实是数据并行训练中一轮迭代完成后的参数同步流程，比如下图的例子，模型有 参数量，并且被四台机器分为了四块，每台机器的出入带宽都为 ：all reduce machines如果要实现一次All Reduce，最直观的就是把0、1、2节点的参数同步到3，等待3计算完成后再把参数反向同步回0、1、2，该方法的时间约为 。但如果是Ring All-reduce，时间可以为 ，时延有明显的削减不过，不管哪种其实单台机器的出入数据量是差不多的（少了大概 ），只是通过环状算法将出入带宽利用率拉到最大实现加速本节需要了解： Ring All-reduce的原理 定量的计算Ring All-reduce的传输时延 尝试用Pytorch写一个Ring All-reduce的代码Parameter Server论文，随便找了篇中文解读parameter server简单来说PS设计了一套分布式训练框架，将训练拆分为一个master节点和多个worker节点，其中worker节点每轮迭代执行：从master pull参数 -> 加载训练数据 -> 计算梯度训练 -> push梯度到master -> 等待master完成一次全worker同步但是这种简单的""同步阻断""训练会导致同步时间占比较大，反而降低了计算效率，因此基于上面的流程又有了""异步非阻断式""训练过程，即每个worker用本地模型参数完成了多次梯度的计算后再重新pull模型参数，虽然直观上感觉这样会造成梯度下降变慢，但是经过论文的测试发现梯度下降速度没想象的影响那么大，具体的测试数据在论文中都有提到同时如果模型数据过多，master节点就会有单点依赖，所以PS架构支持用哈希环的方法拆分master节点，使每个master节点只负责一部分模型参数的储存，降低单点网络问题带来的影响本节需要了解： 定性知道PS架构的原理 PS的同步阻断和异步非阻断式训练的流程区别，他们在计算和同步等待上的耗时占比分别是多少？ master节点是如何解决网络单点问题的？管线并行(Pipeline Parallelism)随着模型越来越大，一张卡变得装不下模型。为了训练单卡无法装载的模型，我们可以将模型按层切分到多个机器上,管线并行就是干这么个事这是最初级的模型拆分多卡的方案，该方案将模型根据层拆分到不同机器上，但该方法的训练效率也是大打折扣的，，每一张卡在大部分时间下都是空闲的。因此我们可以认为，管线并行主要是为了解决下图的问题：pipeline本章会选择PipeDream和GPipe两个最知名的Pipeline并行方式进行展开PipeDream论文和中文解读PipeDream也是将模型根据层拆分到多个机器上，训练时，允许机器在反向传播第一个批次数据的梯度前执行第二个甚至更多后续批次的数据计算，从而实现了pipeline并行。这个方法虽然会造成梯度收敛不稳定，但我们可以通过限制反向传播前最多执行多少批次来控制梯度的不确定性同时，PipeDream还将Data Parallelism结合了起来，模型的每一层都可以用数据并行的方式训练同时，梯度下降的时候，机器需要用保存好的参数备份来计算梯度（即不会用当前最新的参数来计算梯度)：假如我们始终用最新的参数来计算梯度，那么对于图中的机器1，3号任务计算梯度时其实使用的是2号任务梯度更新之后的参数，而实际上3号任务正向传播时用到的是原始参数，因此这会带来大量误差通过保存参数备份，我们可以实现近似于Data Parallelism的效果，但当然这也会倍率放大模型的内存占用，具体倍率和备份的参数量有关本节需要了解： PipeDream是如何提升机器利用率从而加速训练的？ PipeDream为什么会放大模型的内存占用？GPipe论文和中文解读GPipe其实和PipeDream类似，将模型拆分到不同的设备上，通过管线并行的方式，多批次数据为一组更新模型参数。GPipe是通过将一个batch的数据拆分为micro-batch实现，并且每一个batch完成后会等待全局同步梯度；同时，GPipe用到了前文提到的Re-materialization技术，用时间换来了显存的降低。因此总体而言GPipe速度比PipeDream慢，但是显存方面占用更少，且收敛更稳定本节需要了解：GPipe和PipeDream的区别是什么？张量并行(Tensor Parallelism)管线并行其实主要还集中在多层神经网络架构的训练上，但是对于Transformer架构的模型（如BERT，GPT等），MultiHead Attention Layer和MLP的计算量翻了几倍，如果继续按管线切分模型的话可能单层参数都无法被显存装载，因此我们也需要横着把同一层的模型切分开来，这便是张量并行其中最有名的也就是Megatron和Deepspeed了，本章也会详细介绍这两个方法的原理Megatron论文，中文解读可以看目前个人认为全网最详细的系列，官方代码在这里其实Megatron的原理就下面的公式： 横切map：，split操作 ，all-gather操作横切reduce： ，all-reduce操作 ，identity操作 纵切map： ，identity操作，all-reduce操作纵切reduce： ，all-gather操作，split操作不过要注意下，实际的横切和纵切是分为map和reduce两步操作的，所以前向和后向的传播对于横切和纵切也有两步。根据这些，我们可以将Transformer的MLP和Multi Head Attention通过上面两种矩阵切分形式组合为下面的计算流程：其中Multi Head Attention甚至不需要切分 三个矩阵，因为Transformer本身设计就是多个，每个head都可以放到一个GPU上计算Megatron多用于训练BERT、GPT和T5等几十B的大参数模型，算大模型分布式训练的一个早期张量并行方法，后来随着大模型进一步变大到了几百B，DeepSpeed的ZeRO逐渐变为了主流本节需要了解： Megatron是如何做到把矩阵相乘拆分到不同GPU上计算的？ 横切和纵切的正向传播和反向传播的公式？ MLP计算时是如何结合应用纵切和横切的？ Multi Head Attention计算时是如何结合应用纵切和横切的？DeepSpeedPS：这里我把DeepSpeed单独提了出来，因为融合了数据并行、管线并行和内存节省等方法，因此可以看做上面很多方法的整合随着GPT-3把LLM的参数量推到了175B，训练所需参数大小更是到达了万亿规模，Megatron面对如此大规模的参数也变得无能为力，而DeepSpeed的ZeRO方法问世则很好的解决了这个问题ZeRO-DP论文和中文解读ZeRO-DP简单来说就是想办法在数据并行的管线上把模型的参数分配到不同的显卡上，而不用所有显卡都装载所有参数该论文把训练期间的内存消耗归结为三类： OS：优化器状态（如Adam的momentum和variance） G：梯度 P：模型参数如前文”大模型内存开销“小节有提到，混合精度下的Adam三种消耗占比大约为 ， 和 。同样的，ZeRO-DP根据显存优化粒度也分为 ， ， 三种接下来我们介绍三种优化方案，其中 是机器数量， 是模型参数量， 、 、 分别代表优化器状态、梯度和模型参数， 为单台机器的出入带宽 会让每台设备上都有完整的 和 ，但是只保留 的 ，训练时也只更新这部分状态对应的参数，每轮训练完成后进行reduce-scatter将所有机器上所有参数的梯度合并到负责的机器上去，然后再用all-gather将每台机器计算出的参数分发给全局，因此参数同步时间为 其具体流程和数据并行类似，因此这里就不画图了 该方法的假设在于每台设备只能训练部分参数，所以单台设备上其他参数的梯度其实不需要保存。每台设备有完整的 ，但是只保留 的 、 。反向传播时每经过一层参数就开启一轮reduce-scatter将梯度整合到一个节点，计算出下一层梯度后删除前一层梯度。参数同步时间为 ，和数据并行一致 简单来说就是在 的基础上每一轮加上一次broadcast，把模型参数分发到各个设备上，因此时延相较前两个有显著提升，具体就不画图了，参数同步时间为因为 ，约为正常数据并行的1.5倍上述三个级别的内存优化效率如下图所示：该方法能有效地降低单卡的内存，可以看到120GB的单卡所需训练内存最大可以压缩到1.9GB本节需要了解： ZeRO-DP的三种方法分别是什么？是通过什么原理实现的内存节约？ ZeRO-DP的三种方法的通讯时间分别为多少？ZeRO-RZeRO-R和ZeRO-DP在同一个论文里出现（中文解读也是同一篇），我理解是一些单节点显存节约方法 该方法使用类似Re-materilization的方法，在正向传播时设置激活值的存档，反向传播时用存档重新计算激活值，不过该方法更进一步地把激活值拆分到了不同机器上，每次计算时先进行一轮all-gather操作合并激活值，甚至论文中还提到可以把激活值存档放到CPU上去，用额外的通信成本降低激活值的显存开销 这个就偏向显存管理了，对于矩阵计算申请的临时显存而言，由于显存管理策略申请的显存可能远大于实际使用到的显存（例如伙伴算法会申请2幂次方显存大小），该方法便是解决这种显存浪费导致的模型无法装载问题，它设置了一个静态的缓冲区大小，使更多的显存可以拿来装载模型，当然这也会带来一定的显存分配延迟（比如静态的缓冲区满了要等待释放） 该方法也偏向显存管理，其目的就是降低显存的碎片，简单来说就是预先分配好激活值和梯度的显存块，并及时化显存整理，降低显存碎片的问题来使大块的张量可以得到分配，当然，这也会带来时间成本（毕竟显存整理也需要时间）本节需要了解：定性了解ZeRO-R三个方法的原理ZeRO-Offload论文和中文解读ZeRO-Offload的核心思路就是让CPU和内存也参与到训练中去，回顾一下前文用到的训练流程的图，ZeRO-Offload就是把这个流程用上图的方式把fp32参数的更新和float2half操作拆分到了CPU和内存上计算，而前向和后向传播依然由GPU负责那么，在单卡场景下，流程大概如下图所示：GPU在计算时异步的把已经计算好的参数传递给CPU，同样的，CPU也会异步的传递计算好的参数给GPU。多卡场景其实也一样，每张卡把自己负责的参数传递给CPU计算即可，甚至由于每个CPU负责的数据变少了，CPU的时延也大大的降低了同时，文章进一步提出了One-Step Delayed，在模型后期得到了充分的收敛后，CPU的参数计算和更新可以放到下一轮迭代期间完成。当然，这会带来振荡的收敛，但是同时也大幅度的稀释了CPU计算的时间，论文中有证明在模型训练的后期开启该方案是可行的，虽然会带来一定的振荡，但总体不会对训练的收敛效果产生影响本节需要了解： 定性地知道单卡和多卡场景下ZeRO-Offload如何将训练流程拆分到CPU的 定量的了解ZeRO-Offload降低显存消耗的比例 定性地了解One-Step Delayed机制和它带来的影响ZeRO-Infinity论文和中文解读，代码就是DeepSpeed开源项目ZeRO-Infinity结合了ZeRO系列的论文，直接把分布式训练的方案推到一个目前来说接近终点的位置，改论文探讨的模型大小甚至直接到达了Trillion参数量级别（1 Trillion=1000 Billion），因此有很多之前不用考虑的场景在这里都涉及到了。强烈推荐自行阅读论文或者解读，下面本文简单介绍下该论文的思路显存评估作者先对显存进行了评估。对于超大的模型，它们的参数量如下图所示（注意模型参数量的单位为Trillions）：这种规模的模型很明显是没办法交给GPU去装载的，自然而然会用到CPU和NVME，其中(a)图内的Working Mem per GPU就是把所有的模型状态offload到CPU或者NVME后GPU所需要的工作显存，其中Model State就是计算FWD和BWD所需显存，Act.是激活层重新计算（就是激活层ckpt，或者本文提到的re-materilization）所需要的显存带宽评估然后作者又对带宽的需求进行了评估，作者推导了效率的表达式，并结合测试给出了不同带宽和超参数情况下的效率对比，这里直接给个论文结论，具体推导可以看论文或解读依赖这个结论，我们可以根据需要的效率推导所需的带宽约为多少设计ZeRO-Infinity的设计分为几个关键点，这里定性展开下，定量建议看论文或解读解决超大规模的设计（Design for Unprecedented Scale）Infinity offload engine for model states：用到了ZeRO-DP的 模式，把模型状态都进行了分区，并且所有模型参数存储都放到了CPU或者NVME上 CPU Offload for activations：激活值的ckpt也放到了CPU内存中 Memory-centric tiling for working memory：可以简单理解为把大张量的计算拆分成多个较小的线性算子序列，用时间换空间的方式防止显存不够用解决训练效率的设计（Design for Excellent Training Efficiency） Efficiency w.r.t Parameter and Gradients：提出一种基于带宽的划分策略来提高参数和梯度的传递效率，并允许通过PCIe的重叠通信 Efficiency w.r.t Optimizer States：其实就是基于ZeRO-Offload的优化器状态参数传递策略，CPU一边计算参数一边并行传递给GPU，不过在这里用NVME offload时需要经过一次NVME Efficiency w.r.t Activations：也是基于ZeRO-Offload的激活值传递策略，每个GPU通过PCIe并行写数据到CPU，可以超过80%的效率，很显然如果减少激活值ckpt的频率的话也能提升该效率（代价就是增加激活值重算的时间）解决易用性的设计（Design for Ease of Use）其实就是基于PyTorch在代码层封装好了各种算子操作（如reduce-scatter和all-gather等），不需要用户自行再写相关的代码了 automated data movement：自动在FWD和BWD后触发收集和分区的相关操作，把数据同步到CPU或者NVME automated model partitioning during initialization：初始化时自动分区模型和参数然后论文后续对上述提到的所有设计进行了原理展开，最后给出了效果和结论，具体的原理本文就不讲了，希望读者能结合梳理出来的脉络通读一下论文，然后再结合论文的原理阅读DeepSpeed的源码本节需要了解： 定性的了解ZeRO-Infinity做了哪些事 了解ZeRO-Infinity每个设计所需方法的原理（如Bandwidth-Centric Partitioning，Overlap Centric Design等论文第六章和第七章内容） 最好能结合了解到的原理读一读DeepSpeed的开源代码3D Parallelism可以参考这个该方法可以说是把模型并行、管线并行和数据并行结合到了一起，几乎可以用于训练目前所有规模的模型。该方法首先把模型按照层拆分为不同的Pipeline Stage，每个Pipeline Stage内的张量可以用多个设备装载，然后一个完整的Pipeline作为一个Data Parrallel，和其他的完整Pipeline用Zero进行数据并行训练本章需要了解：- 3D Parallelism是如何结合模型并行、管线并行和数据并行的写在结尾至此为止本文已经介绍完DeepSpeed的原理了，下一篇我将会介绍更底层的ai-infra相关技术。对于该文的错误和遗漏的地方，也希望各位读者指正~Other References[1] https://www.zhihu.com/question/53851014"
676729627,c750adcfb5d0e1e613edd5d82ddfdfd4,曹宇,https://api.zhihu.com/people/c750adcfb5d0e1e613edd5d82ddfdfd4,people,数据+ AI驱动的新变革,大模型团队招人,article,https://api.zhihu.com/articles/676729627,北京 职位责任：负责大语言模型的偏好对齐算法，价值观建模，提升<em>大模型</em>的安全能力、增强模型的指令遵从能力、加强自研<em>大模型</em>的回答事实准确率，提升大模型在文创，教育，法律,200,27,437,1704768754,1711523887,我们的团队：阿里巴巴--智能信息大模型团队正在招聘 Alignment 及 Agent 方向的人才，服务集团战略级创新业务之一的夸克，目前该APP有亿级的日活及流量。关于校招和实习的Update：这两个岗位当前是面向社招开放的，实习（春招）开放时间是3月份左右，对于特别优秀的同学也欢迎提前联系。最新2024-03-27 Update，2025 实习（春招）已经开放了，欢迎扫码投递。两个方向的 JD 见以下描述，每个岗位都招聘若干名同学：Alignment 大语言模型对齐算法工程师工作地点：杭州 / 北京职位责任：负责大语言模型的偏好对齐算法，价值观建模，提升大模型的安全能力、增强模型的指令遵从能力、加强自研大模型的回答事实准确率，提升大模型在文创，教育，法律，RAG等场景下的模型综合能力。技能要求：计算机、控制科学、软件工程、人工智能等相关学科硕士及以上学位熟悉大模型的相关基础知识，具备大语言模型大规模相关训练基础知识熟悉强化学习（RL）概念深入了解PPO相关算法知识，了解DPO，IPO等非RL对齐手段及方法扎实的Python或者C++编程功底，了解PyTorch，Deepspeed，Megatron，vLLM等大模型训练推理技术栈 良好的技术沟通及协作能力加分项： 参与过知名大模型项目的相关RLHF工作深入了解对齐算法工程实践，能够优化RLHF相应框架训推效率了解多模态大模型的对齐，了解步骤监督学习相关内容Agent 大模型智能体算法工程师工作地点：杭州 / 北京职位责任：负责大语言模型，多模态大模型的智能体算法构建，结合夸克已有大量高质量语料库，小工具，检索等能力构建复杂大模型智能体系统。提升大模型自主工具调用及工具创造能力，增强Agent在文创，教育，法律，RAG等场景下的综合能力。技能要求：计算机、控制科学、软件工程、人工智能等相关学科硕士及以上学位熟悉大模型智能体相关基础知识，具备大模型相关知识熟悉ReAct，ReST等大模型智能体推理及训练范式，了解大模型预训练，微调基础手段扎实的Python或者C++编程功底，了解大模型训练推理技术栈，对于 AutoGen，LangChain，XAgent，AutoGPT等常见agent框架有独立的实践和理解良好的技术沟通及协作能力加分项： 参与过知名大模型项目的相关Agent构建工作深入了解多智能体理论（Multi Agents）博弈论，最优控制，搜索型智能（MCTS，MuZero）等经典智能体理论及算法具有Ray，iPython，ZeroMQ 的扎实功底，可以构建大规模智能体框架/平台投递可以直接CV 投递到我个人邮箱 tsaoyu@gmail.com / 或者企业邮箱 molan.cy@alibaba-inc.com，我会持续跟踪面试流程和进展
675007238,eaa7416cb70d0a51c35f57cf9150f30a,前沿技术研究者,https://api.zhihu.com/people/eaa7416cb70d0a51c35f57cf9150f30a,people,技术情怀的小白,大模型 | 大模型关键能力的评估指标,article,https://api.zhihu.com/articles/675007238,<em>大模型</em>是指具有大规模参数和复杂计算结构的机器学习模型。这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。<em>大模型</em>的设计目的是为了提高模型的表达能力和预测性能,8,0,22,1703748582,1703748582,"大模型是指具有大规模参数和复杂计算结构的机器学习模型。这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。大模型关键能力知识图谱问答能力知识图谱问答（Knowledge-based Question Answering, KBQA）是指给定自然语言问题，通过对问题进行语义理解和解析，进而利用知识库进行查询、推理得出答案。大模型利用知识图谱（一种存储事实和信息的结构化数据库）来回答问题的能力。这包括理解和处理与特定事实、关系和概念相关的查询，并能准确地从图谱中检索和解释信息。推理能力推理是运用逻辑思维能力，从已有的知识出发，得出未知的、隐性的知识。具体到知识图谱中，推理是利用图谱中现有的知识（如三元组），得到一些新的实体间的关系或者实体的属性（三元组）。大模型进行逻辑推理和解决问题的能力。这涉及理解语境、推断隐含信息、处理复杂和抽象的概念以及应用通用或特定领域的知识来得出结论。数学能力数学能力通常指的是模型理解和解决数学问题的能力，包括算术、代数、几何、概率统计等方面的计算能力。这不仅仅是进行计算，还包括理解数学概念和公式，并能够应用这些概念来解决实际问题。代码生成能力代码生成能力是指模型能够根据给定的任务或问题，自动生成符合要求的计算机程序代码的能力。大模型编写和理解编程代码的能力。这包括理解编程语言的语法和结构、编写功能性代码、调试和优化现有代码以及理解和实现算法逻辑。机器翻译能力机器翻译能力是指模型能够将一种自然语言文本自动翻译成另一种自然语言文本的能力，涉及对源语言文本的理解和目标语言文本的生成。这涉及理解源语言文本的意义、语境和文化细节，并能够准确、流畅地用目标语言表达相同的意思。文本生成能力文本生成能力是指模型能够根据给定的输入或任务，自动生成符合语法和语义规则的自然语言文本的能力，生成连贯、逻辑和有吸引力的文本的能力。情感理解能力情感理解能力是指模型能够识别和理解文本中所表达的情感（如正面、负面或中性）以及情感强度，从而更好地理解和回应用户的情感需求，如同理心、幽默感和情感支持。多模态理解和生成能力多模态理解和生成能力是指模型能够同时处理和生成多种媒体数据（如文本、图像、音频和视频）的能力。评估大模型的关键能力是一个多方面的过程，除了提到的上述能力外，还包括以下方面：计算性能：大模型的计算性能表现通常需要考虑模型训练和推理的速度。评估计算性能时，可以考虑模型的训练时间、推理速度、批处理大小等因素。内存使用：大模型通常需要大量的内存来存储模型参数和中间计算结果。因此，评估大模型的内存使用情况是必要的，特别是在部署到嵌入式系统或移动设备上时。精度和泛化能力：大模型的精度和泛化能力是评估模型性能的重要指标。通过在各种数据集上进行测试，观察模型的准确性和对未见数据的适用性。可解释性：对于许多应用场景，模型的可解释性是一个关键因素。大模型可能更复杂，因此评估其是否易于理解以及是否能够提供有意义的解释是很重要的。鲁棒性：评估大模型在不同情况下的稳定性，包括数据变化、噪声和其他外部干扰，也是一项关键能力。扩展性：随着数据和模型规模的增加，评估模型是否能够有效地进行训练和推理是很重要的。这涉及到在更高计算能力的硬件上（例如更强大的GPU或TPU）进行评估的能力。实时性：对于许多应用，尤其是实时应用，评估大模型的实时性能（如响应时间）是关键。可迁移性：评估大模型在不同任务或领域之间的可迁移性，以及其适应新任务的能力。隐私和安全性：对于涉及敏感数据的场景，模型的隐私保护和安全性至关重要。这涉及到评估加密技术和隐私增强技术在模型中的应用。可维护性和更新：随着数据和模型的变化，评估大模型的维护和更新能力是很重要的。这包括对模型进行微调、重新训练和更新参数的能力。成本效益分析：从经济角度评估大模型的可行性，包括硬件成本、能源消耗、训练和推理时间等。大模型的规模巨大，模型大小可以达到数百GB甚至更大，参数量通常在十亿个以上。这种巨大的规模使大模型拥有强大的表达能力和学习能力。此外，大模型通常会一起学习多种不同的NLP任务，如机器翻译、文本摘要、问答系统等，这可以使模型学习到更广泛和泛化的语言理解能力。同时，大模型需要海量的数据来训练，通常在TB以上甚至PB级别的数据集。只有大量的数据才能发挥大模型的参数规模优势。然而，大模型也存在一些挑战。首先，训练大模型需要大量的计算资源和时间，通常需要数百甚至上千个GPU，以及数周到数月的时间。其次，大模型的训练通常采用知识蒸馏技术，即首先训练一个更大的教师模型，然后使用教师模型指导一个较小的学生模型训练。此外，由于大模型的复杂性，其可解释性相对较差，这使得理解模型的决策过程和预测结果变得更为困难。综上所述，大模型是人工智能领域的一个重要和活跃的研究方向，随着技术的发展，它们在社会和经济中的作用越来越重要。同时，它们也带来了一系列挑战和问题，需要业界、学术界和监管机构共同努力来解决。"
687028103,7e7c58f5c3238105aeb94fd466bc456a,MyComm联信志诚,https://api.zhihu.com/people/7e7c58f5c3238105aeb94fd466bc456a,people,依靠科技创新，提升企业沟通效率，打造值得信赖的客户服务平台！,大模型的发展历程,article,https://api.zhihu.com/articles/687028103,大模型逐渐成为了人工智能领域的重要分支。接下来想和大家浅谈一下将从<em>大模型</em>的起源、发展历程、应用领域以及未来展望等方面，全面介绍<em>大模型</em>的发展历程。 一、大模型的起源 <em>大模型</em>的起源可以追溯到20世纪90年代,0,0,2,1710404996,1710404996,大模型的发展历程可以追溯到人工智能领域的早期。随着计算机技术的不断发展和数据资源的日益丰富，大模型逐渐成为了人工智能领域的重要分支。接下来想和大家浅谈一下将从大模型的起源、发展历程、应用领域以及未来展望等方面，全面介绍大模型的发展历程。一、大模型的起源大模型的起源可以追溯到20世纪90年代，当时人工智能领域还处于初创期，研究人员主要关注的是基于规则的专家和知识表示系统。数据资源的不断增加和计算机性能的不断提升，人们开始意识到基于数据驱动的机器学习方法在人工智能领域具有更大的潜力。大模型的概念逐渐浮出水面。二、大模型的发展历程1. 早期大模型的探索早期的大模型主要是基于统计学习的方法，例如朴素贝叶斯分类器、决策树和逻辑回归等。这些模型通常需要在小规模数据集上进行训练，因此它们的性能受到了很大的限制。这些早期的大模型为后来的发展奠定了基础。2. 深度学习模型的崛起随着深度学习技术的不断发展，大模型的性能得到了极大的提升。深度学习模型通过构建深度神经网络，可以自动学习数据的特征表示，从而避免了手工设计特征的繁琐过程。在图像识别、语音识别、自然语言处理等领域，深度学习模型取得了显著的成果。3. 大规模预训练模型的兴起近年来，随着数据资源的不断增加和计算资源的不断提升，大规模预训练模型成为了大模型发展的重要方向。这些模型在大量的数据上进行预训练，可以学习到更多的知识和特征，从而在各种任务上取得更好的性能。例如，GPT、BERT等模型在自然语言处理领域取得了巨大的成功。三、大模型的应用领域大模型在各个领域都有着广泛的应用。在自然语言处理领域，大模型可以用于文本分类、情感分析、机器翻译、问答系统等任务。在图像识别领域，大模型可以用于目标检测、图像分类、人脸识别等任务。在语音识别领域，大模型可以用于语音转文字、语音合成等任务。此外，大模型还可以应用于推荐系统、金融风控、智能客服等领域。四、大模型的未来展望随着技术的不断发展和数据资源的日益丰富，大模型在未来仍然有着广阔的发展前景。未来，大模型将会更加注重模型的可解释性和鲁棒性，以提高模型的稳定性和可靠性。同时，随着分布式训练技术的不断发展，大模型的训练速度也将会得到极大的提升。另外，大模型也将会在更多的领域得到应用，如医疗、交通等。大模型作为人工智能领域的重要分支，其发展历程充满了曲折和变革。未来，随着技术的不断发展和数据资源的日益丰富，大模型将会迎来更加广阔的发展空间和应用前景。
689536889,c750adcfb5d0e1e613edd5d82ddfdfd4,曹宇,https://api.zhihu.com/people/c750adcfb5d0e1e613edd5d82ddfdfd4,people,数据+ AI驱动的新变革,大模型团队招人(校招),article,https://api.zhihu.com/articles/689536889,北京 职位责任：负责大语言模型的偏好对齐算法，价值观建模，提升<em>大模型</em>的安全能力、增强模型的指令遵从能力、加强自研<em>大模型</em>的回答事实准确率，提升大模型在文创，教育，法律,63,13,80,1711611343,1711618413,阿里巴巴智能信息，2025届春招开始啦，欢迎有意向的优秀同学扫码投递。实习的内容也是大语言模型的核心方向Alignment，在这里有丰富的实验资源、良好的数据支持、优秀的师兄师姐带领你进入大模型的全新领域。内推直达：https://talent.quark.cn/campus/qrcode/apply/positions?code=V3Txwo9_tpl17rs2C9ZtjA== (选择算法工程师--大模型)团队对于Alignment的相关理解可见：https://zhuanlan.zhihu.com/p/685064567快2024了， RL是通往真正的智能比较主流的赛道吗？谷歌团队提出用AI反馈强化学习 (RLAIF) ，替代人类进行偏好标注，这会对AI研究产生什么影响？及其他高赞回答，我们重视技术探索与业务实践相结合，通过数据，算力，算法的scaling law原则全面提升模型的Helpful，Honesty及Harmfulness 相关岗位的基础要求如下：Alignment 大语言模型对齐算法工程师工作地点：杭州 / 北京职位责任：负责大语言模型的偏好对齐算法，价值观建模，提升大模型的安全能力、增强模型的指令遵从能力、加强自研大模型的回答事实准确率，提升大模型在文创，教育，法律，RAG等场景下的模型综合能力。技能要求：计算机、控制科学、软件工程、人工智能等相关学科硕士及以上学位熟悉大模型的相关基础知识，具备大语言模型大规模相关训练基础知识熟悉强化学习（RL）概念深入了解PPO相关算法知识，了解DPO，IPO等非RL对齐手段及方法良好的技术沟通及协作能力加分项： 扎实的Python或者C++编程功底，了解PyTorch，Deepspeed，Megatron，vLLM等大模型训练推理技术栈 相关领域顶会、开源项目参与Agent 大模型智能体算法工程师工作地点：杭州 / 北京职位责任：负责大语言模型，多模态大模型的智能体算法构建，结合夸克已有大量高质量语料库，小工具，检索等能力构建复杂大模型智能体系统。提升大模型自主工具调用及工具创造能力，增强Agent在文创，教育，法律，RAG等场景下的综合能力。技能要求：计算机、控制科学、软件工程、人工智能等相关学科硕士及以上学位熟悉大模型智能体相关基础知识，具备大模型相关知识熟悉ReAct，ReST等大模型智能体推理及训练范式，了解大模型预训练，微调基础手段良好的技术沟通及协作能力加分项： 参与过知名大模型项目的相关Agent构建工作，相关领域顶会、开源项目参与如有相关问题可以联系个人邮箱 tsaoyu@gmail.com / 或者企业邮箱 molan.cy@alibaba-inc.com
670907685,17636fdafe0e68df8736c2e242edce1a,产品经理大群,https://api.zhihu.com/people/17636fdafe0e68df8736c2e242edce1a,people,点赞就好，十年AI策略产品经验 持续分享 公众号同名,一文读懂：大模型思维链 CoT（Chain of Thought）,article,https://api.zhihu.com/articles/670907685,增强了<em>大模型</em>的可解释性：对比向大模型输入一个问题<em>大模型</em>为我们仅仅输出一个答案，CoT 使得<em>大模型</em>通过向我们展示“做题过程”，使得我们可以更好的判断<em>大模型</em>在求解当前问题上究竟是如何工作的,118,0,215,1701966864,1711605719,"前言：思维链，在人工智能领域，是一个非常非常新的概念。强大的逻辑推理是大语言模型“智能涌现”出的核心能力之一，好像AI有了人的意识一样。而推理能力的关键在于——思维链（Chain of Thought，CoT）。一种prompt方法，对于复杂问题（尤其是复杂的数学题），大模型很难直接给出正确答案。COT通过要求模型在输出最终答案之前，显式输出中间逐步的推理步骤这一方法来增强大模型的算数、常识和推理能力。简单，但有效。 CoT 大幅度提高了 LLM 在复杂推理任务上的表现，并且输出的中间步骤方便使用者了解模型的思考过程，提高了大模型推理的可解释性。目前，思维链推理已经成为大模型处理复杂任务的一个常用手段本文整理了 爱吃牛油果的璐璐 、 夕小瑶和绝密伏击的文章，展开讨论 CoT的诸多概念的介绍：1. 什么是思维链 CoT ？关键概念介绍在介绍什么是思维链 CoT 之前，让我们先从两个更大的概念开始。首先，什么是“语言智能”？语言智能可以被理解为“使用基于自然语言的概念对经验事物进行‘理解’以及在概念之间进行‘推理’的能力”，无疑，人类是目前已知生物之中唯一具备这种高级的抽象与理解能力的，从另一个层面而言，语言智能能力也是将人类从动物之中区分出来作为一种“智慧物种”的标志能力之一。而随着参数量的飞升，以 Transformer 为基础架构的大规模语言模型以 “Chat” 的方式逐渐向人们展现出了它的概念理解与概念推理的能力。直观上，作为“语言模型”的大模型具备概念理解能力并不难理解，但是仅仅像 Word2vec 一样只能得到“国王”与“男人”的“距离”更近的结论对于语言智能而言必然远远不够。真正让大模型逼近“语言智能”，在于大模型展现出的概念推理能力。推理，一般指根据几个已知的前提推导得出新的结论的过程，区别于理解，推理一般是一个“多步骤”的过程，推理的过程可以形成非常必要的“中间概念”，这些中间概念将辅助复杂问题的求解。思维链2022 年，在 Google 发布的论文《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》中首次提出，通过让大模型逐步参与将一个复杂问题分解为一步一步的子问题并依次进行求解的过程可以显著提升大模型的性能。而这一系列推理的中间步骤就被称为思维链（Chain of Thought）。区别于传统的 Prompt 从输入直接到输出的映射 <input——>output> 的方式，CoT 完成了从输入到思维链再到输出的映射，即 <input——>reasoning chain——>output>。如果将使用 CoT 的 Prompt 进行分解，可以更加详细的观察到 CoT 的工作流程。如上图所示，一个完整的包含 CoT 的 Prompt 往往由指令（Instruction），逻辑依据（Rationale），示例（Exemplars）三部分组成。一般而言指令用于描述问题并且告知大模型的输出格式，逻辑依据即指 CoT 的中间推理过程，可以包含问题的解决方案、中间推理步骤以及与问题相关的任何外部知识，而示例则指以少样本的方式为大模型提供输入输出对的基本格式，每一个示例都包含：问题，推理过程与答案。以是否包含示例为区分，可以将 CoT 分为 Zero-Shot-CoT 与 Few-Shot-CoT，在上图中，Zero-Shot-CoT 不添加示例而仅仅在指令中添加一行经典的“Let's think step by step”，就可以“唤醒”大模型的推理能力。而 Few-Shot-Cot 则在示例中详细描述了“解题步骤”，让模型照猫画虎得到推理能力。2. CoT 的作用1、COT原则上允许模型把一个复杂问题拆解成多个步骤，也就是说需要更多推理步骤的问题可以多分点计算量2、COT提供了一个观察模型为何会犯错的窗口，因此也就提供了一个debug模型的机会3、COT能用在数学应用题、常识推理和符号操作上，也就有可能用在任何人类通过语言能解决的问题上4、COT非常好用，任何语言模型都可以用，加在 few-shot的样例中就能生效。省去了重新训练模型的功夫。大模型的逻辑推理能力提升谷歌之前在大模型下了很大功夫，GPT 生成式预训练模型中的“T”，也就是 Transformer，就是谷歌大脑搞出来的。但是，预训练 + 精调的大模型搞了几年，仍然没办法很好地完成多步骤推理任务，比如数学问题和常识推理。所以 Jason Wei 等人提出了思维链提示的方法，真的一下子就让大模型的逻辑推理能力不一样了。具体来说，有三个不一样：常识推理能力赶超人类。以前的语言模型，在很多挑战性任务上都达不到人类水平，而采用思维链提示的大语言模型，在 Bench Hard(BBH) 评测基准的 23 个任务中，有 17 个任务的表现都优于人类基线。比如常识推理中会包括对身体和互动的理解，而在运动理解 sports understanding 方面，思维链的表现就超过了运动爱好者（95% vs 84%）。数学逻辑推理大幅提升。一般来说，语言模型在算术推理任务上的表现不太好，而应用了思维链之后，大语言模型的逻辑推理能力突飞猛进。MultiArith 和 GSM8K 这两个数据集，测试的是语言模型解决数学问题的能力，而通过思维链提示，PaLM 这个大语言模型比传统提示学习的性能提高了 300%！在 MultiArith 和 GSM8K 上的表现提升巨大，甚至超过了有监督学习的最优表现。这意味着，大语言模型也可以解决那些需要精确的、分步骤计算的复杂数学问题了。图1.2：数学逻辑推理大幅提升大语言模型更具可解释性，更加可信。我们知道超大规模的无监督深度学习，打造出来的大模型是一个黑盒，推理决策链不可知，这就会让模型结果变得不够可信。而思维链将一个逻辑推理问题，分解成了多个步骤，来一步步进行，这样生成的结果就有着更加清晰的逻辑链路，提供了一定的可解释性，让人知道答案是怎么来的。Jason Wei 这位奇男子提出的思维链，可以说是大语言模型惊艳世界的必要条件。 CoT 的好处自从 CoT 问世以来，CoT 的能力已经被无数工作所验证，如果对使用 CoT 的好处做一个总结，那么可以归纳为以下四点：增强了大模型的推理能力：CoT 通过将复杂问题分解为多步骤的子问题，相当显著的增强了大模型的推理能力，也最大限度的降低了大模型忽视求解问题的“关键细节”的现象，使得计算资源总是被分配于求解问题的“核心步骤”；增强了大模型的可解释性：对比向大模型输入一个问题大模型为我们仅仅输出一个答案，CoT 使得大模型通过向我们展示“做题过程”，使得我们可以更好的判断大模型在求解当前问题上究竟是如何工作的，同时“做题步骤”的输出，也为我们定位其中错误步骤提供了依据；增强了大模型的可控性：通过让大模型一步一步输出步骤，我们通过这些步骤的呈现可以对大模型问题求解的过程施加更大的影响，避免大模型成为无法控制的“完全黑盒”；增强了大模型的灵活性：仅仅添加一句“Let's think step by step”，就可以在现有的各种不同的大模型中使用 CoT 方法，同时，CoT 赋予的大模型一步一步思考的能力不仅仅局限于“语言智能”，在科学应用，以及 AI Agent 的构建之中都有用武之地。为了更加直观的展现出 CoT 对大模型能力带来的提升，论文作者在七个不同的推理任务数据集中对 CoT 的效果进行了实验，如下图所示，可以看到，相较于直接 Prompt， CoT 对所有的推理任务都带来了显著的提升。思维链效果如此拉满，那么 Jason Wei 提出的 CoT 到底是一项什么样的技术，接下来介绍下细节3. CoT 生效的原理关于 CoT 为什么会生效，目前尚且没有一套被大家广泛接受的普遍理论。但是，有许多论文对 CoT 与大模型的互动进行了一系列实验，类似物理实验与物理理论的关系，在实验中一些有意思的现象或许可以帮助我们理解 CoT 的工作原理：模型规模小会导致 CoT 失效；简单的任务 CoT 不会对模型性能带来提升；训练数据内部彼此相互联结程度的增加可以提升 CoT 的性能；示例中的错误，或者无效的推理步骤不会导致 CoT 性能的下降；……如果我们对这些现象做一些总结与延申，或许可以认为：首先，CoT 需要大模型具备一些方面“最基础”的知识，如果模型过小则会导致大模型无法理解最基本的“原子知识”，从而也无从谈起进行推理；其次，使用 CoT 可以为一些它理解到的基础知识之间搭起一座桥梁，使得已知信息形成一条“链条”，从而使得大模型不会中途跑偏；最后，CoT 的作用，或许在于强迫模型进行推理，而非教会模型如何完成推理，大模型在完成预训练后就已经具备了推理能力，而 CoT 只是向模型指定了一种输出格式，规范模型让模型逐步生成答案。4. CoT技术细节COT成果在解释何为 CoT 前，不妨来看个论文展示的结果：图1.3：CoT实验效果不难看出，在解数学问题上，同样使用 PaLM 这个 540B 的超级 LLM，CoT 的表现是传统 prompting 的300%以上，甚至超过了此前有监督的最优表现。这看起来很不可思议，然而 CoT 方法却极其简单。CoT 提示过程是一种最近开发的提示方法，它鼓励大语言模型解释其推理过程。思维链的主要思想是通过向大语言模型展示一些少量的 exapmles，在样例中解释推理过程，大语言模型在回答提示时也会显示推理过程。这种推理的解释往往会引导出更准确的结果。以一个数学题为例：图1.4 标准 Prompting可以看到模型无法做出正确的回答。但如果说，我们给模型一些关于解题的思路，就像我们数学考试，都会把解题过程写出来再最终得出答案，不然无法得分。CoT 做的就是这件事，示例如下：图1.5：CoT提示可以看到，类似的算术题，思维链提示会在给出答案之前，还会自动给出推理步骤：“罗杰先有5个球，2盒3个网球等于6个，5 + 6 = 11” “食堂原来有23个苹果，用了20个，23-20=3；又买了6个苹果，3+6=9”可以看出，CoT 在实现上修改了 demonstration 每个 example 的 target，source 保留原样，但 target 从原先的 answer(a) 换成了 rationale(r) + a。因此可以看到右侧，所有内容均由模型生成，模型不是生成 a，而是生成r+a。简单来说，语言模型很难将所有的语义直接转化为一个方程，因为这是一个更加复杂的思考过程，但可以通过中间步骤，来更好地推理问题的每个部分。标准的prompt让大模型直接做数学题，果然大模型一问一个胡说八道，证明它确实没有推理能力。思维链则在one-shot（点这里看啥是one-shot、few-shot）当中加入了解题的中间过程，诱导大模型“按步骤解题”，不是直接给出计算结果，这一回大模型终于推导出了正确的答案。思维链提示思维链提示，就是把一个多步骤推理问题，分解成很多个中间步骤，分配给更多的计算量，生成更多的 token，再把这些答案拼接在一起进行求解。论文里面作者提到了很多 CoT 的优势，其中包括它把一个多步推理问题分解出多个中间步骤，并且让 LLM 更加可解释。它能解决的问题很多，除了上述的数学应用题，还有常识推理、以及 symbolic manipulation （符号操作）这类任务（就是一些手造的考验大模型的问题，比如最典型的 Last Letter Concatenation（最后一个字母串联） 和 coin flip（抛硬币）），下面补充几个例子方便理解：关于何时应该使用 CoT 事实上还是一个开放问题，但是这篇论文从“工程”与“理论”两个角度为我们带来了一些 CoT 适用场景的洞见。首先，从工程的角度而言，CoT 的适用场景抽象一下可以被归纳为三点，分别是使用大模型（1），任务需要复杂推理（2），参数量的增加无法使得模型性能显著提升（3）。此外，现有的论文实验也表明，CoT 更加适合复杂的推理任务，比如计算或编程，不太适用于简单的单项选择、序列标记等任务之中，并且 CoT 并不适用于那些参数量较小的模型（20B以下），在小模型中使用 CoT 非常有可能会造成机器幻觉等等问题。而从理论角度，一篇来自斯坦福的论文《Why think step-by-step? reasoning emerges from the locality of experience》揭示了当大模型的训练数据表现出了如上图中的变量的局部簇结构（Local Clusters of Variables）时，CoT 将会展现极好的效果。而变量的局部簇主要指训练数据中变量之间有着强的相互作用，相互影响的关系。此外，也有研究指出，当给予大模型的示例之间彼此之间互相区分并不相同时，也有助于提升 CoT 的性能。同时，逻辑依据是否与问题相关，逻辑推理步骤的顺序也会显著影响 CoT 的性能。另外一个有趣的发现是，使用代码数据训练大模型，或者使用符合 CoT 格式的数据训练模型也有助于提升 CoT 的性能。总结一下:CoT 应当被用于 20B 以上参数规模的模型之中，并且模型的训练数据应当于任务问题相关且彼此相互有较强的联结。COT构造1、人工构造：质量高，但人力成本大，不好优化、不好跨任务迁移2、自动构造：分为 Zero-shot CoT 和 Auto CoT 两种方式。前者通过特定的提示文本激发模型在没有示例的情况下生成推理链条；后者则是使用前者零样本生成的推理链条，并结合示例选择策略，通过少样本学习的方式生成推理链条。但自动的质量一般没有人工的好，导致大模型幻觉问题严重。5. CoT 朝着什么方向发展？在这 CoT 问世的一年多以来，CoT 也开始从最简单的“Let's think step by step”慢慢进化，作为一篇综述，这篇论文也相当全面的概括了 CoT 的发展方向与进化路径，如果我们需要按图索骥 CoT 的现有文献，可以从下面这张图出发：总的来说，CoT 的发展方向有三条主要的路径，如图从左到右分别是 “Prompt 模式”，“推理结构”以及“应用场景”。从这三个主要的发展方向出发，我们来概述一下主要的论文：Prompt 模式首先，是 Prompt 模式，在上图中的最左边，Prompt 模式主要研究“向大模型输入怎样的 Prompt 可以使得大模型获得更好的推理能力”，关于 Prompt 模式的研究也可以分为两类，分别是指令生成与范例生成。对于指令生成问题，又可以分为手动指令生成与自动指令生成，显然简单的“Let's think step by step”就属于手动指令生成模式，此外，另一类的手动指令生成模式是 Plan-and-Solve 方法，其主要思想在于让模型制定一个将任务分为更小子任务的计划，再让模型一步一步执行计划、解决问题，其 Prompt 为“Let’s first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan and solve the problem step by step”。显然，手动指令生成无法适应复杂的实际情况，因此自动指令生成应运而生，自动指令生成的代表作有两个，分别是自动 Prompt 工程（APE）以及提示优化（OPRO），如上图所示，APE 与 OPRO 的核心思想都在于设计了一套机制让大模型通过观察各个候选的 Prompt 的实际任务中的表现，通过最大化表现得分来自动选择最优的 Prompt 。类似的，范例生成也可以分为手动范例生成与自动范例生成，传统的 Few-Shot-CoT 就是一种典型的手动范例生成方法，在 Few-Shot-CoT 的基础上，一种让大模型使用手动生成的范例多次回答问题，再从其中依据如熵、方差等的不确定性度量选择“最不确定”的问题，通过手动注释来加强范例生成的 ActivePrompt 方法诞生，成为了一种介于手动范例生成与自动范例生成之间的范例生成方法。而为了将范例生成完全“自动化”，Auto-CoT 方法被提出，具体而言，Auto-CoT 分为两个阶段：（1）问题聚类，对任务数据集进行聚类（2）示例采样：从每个聚类中心中选择一个代表性问题使用 Zero-Shot-CoT 生成思维链作为示例。推理结构除了研究“什么样的 Prompt 会诱导出更好的 CoT 能力以外”，还有很大一部分研究者关注于 CoT 本身的结构问题，主要的研究思路包含 “CoT 构造”、“推理聚合”以及 “CoT 验证”。CoT 构造主要将传统线形，链式的 CoT 转化为如表格、树状、图状格式，代表工作有非常出名的 PoT，Tab-CoT，ToT 以及 GoT-Rationale，下面这张图非常清晰的展示了这四种方法的异同：首先是 PoT，其中 P 指 Programm 即程序，PoT 的思想也非常简单，对思维链中大模型有可能出错的一些计算问题，让大模型生成出编程语言在解释器中运行，以将复杂计算与模型的文本生成解耦。其次是 Tab-CoT，其中 Tab 指 Tabular 表格，在 ToT 中，研究者迫使大模型在每一步的推理中记录一个“∣步数∣子问题∣过程∣结果∣”的推理表格，并让大模型在推理时从生成的表格中提取答案，从而增强大模型的推理能力。此外，就是 ToT，其中 T 指 Tree 即思维树，简单理解就是将 CoT 的链式结构扩展为树形结构。ToT 让大模型在解决子问题时生成多个不同的答案选择，通过此建立的树形结构让大模型可以展望未来确定下一步的决策并且通过追溯来纠正历史决策。基于 ToT 的思想，将 Tree 拓展为 Graph，就形成了 GoT。GoT 系统的核心在于一个“控制器”，控制器处理对图的操作（GoO）以及图状态推理（GRS），其中 GoO 用于将一个给定的任务进行图分解，将一个任务分解为相互连接的节点-边关系，而 GRS 则负责维护大模型在 GoO 生成的图上的推理过程，记录当前步的状态，决策历史等等信息。除了各种 XoT 以外，对于推理过程的“解码”问题，也有一些工作进行了研究。其中，推理聚合的代表性工作是 Self-consistency CoT。Self-consistency CoT 使用手动设计的 Prompt 生成采样一组不同的推理路径，再通过“多数投票”找到推理步骤中“最一致”的路径，使用这条解码路径驱动原始的贪心解码方式来提示 CoT 性能。最后，在针对推理结构的研究，还有一类是 CoT 验证，CoT 验证开始侧重于通过多轮提问，让大模型进行“自我验证”，在前向后向的反复问答中让大模型可以验证自己的回答，而伴随着 CoT 验证的发展，也有工作开始引入“外部工具”对 CoT 中的信息进行验证，例如信息检索、计算器、计算机程序等等。CoT 验证最经典的工作即是自我验证（Self-Verification），自我验证有两个步骤，分别是（1）对多个候选的推理路径进行采样；（2）给定问题结论让大模型验证条件是否满足结论，并根据验证分数对候选结论进行排序。而引入外部工具的 CoT 验证的代表性工作譬如 CRITIC 框架，CRITIC 使得大模型可以交互式的引入外部工具来验证与修改自己的答案输出，经过大模型输出，外部工具验证，验证结果反馈，反馈修改四个循环的步骤加强 CoT 输出的可靠性。而将 CRITIC 的思想进一步推向机制，即出现了任务自适应与流程自动化的 AuRoRA，AuRoRA 从多个来源提取相关知识，将不同来源的知识进行组合、检查与提炼来修改初始 CoT，以提示 CoT 的准确性与逻辑性。比较有意思的一点在于，在论文《Can large language models really improve by selfcritiquing their own plans?》中，作者质疑了大模型是否可以真的进行可靠的 CoT 验证，在大模型的能力本身“无法解决验证结果反馈提出的问题”时，大模型有可能会过度纠正推理过程，直接跳过正确答案。应用场景除了对 CoT 本身的改变，还有许多工作将 CoT “部署”于不同的应用场景之下以提升各种场景下大模型的能力，譬如最简单的从单语言 CoT 扩展到多语言 CoT。这些应用场景包括从单模态到多模态以及从复杂推理任务到通用推理任务的扩展。其中，多模态 CoT 具有很大的应用前景，在 CoT 中，多模态可以分为两类：输入多模态与输出多模态。其中，MM-CoT 是输入多模态研究的第一篇工作，MM-CoT 侧重使用微调方法嵌入 CoT，通过将语言和图像合并在一个包含推理生成与答案推理的两阶段的框架中，使用微调大模型赋予输入多模态 CoT 的能力。基于 MM-CoT，GoT-Input 方法通过对 CoT 生成的思维图进行抽取构建三元组，并使用 GNN 将文本、图像与 CoT 统一，从而生成包含 CoT 信息的最终答案。而区别于输入多模型，VCoT 解决了一个输出多模态的问题，VCoT 通过以生成图片的“标题”以及识别核心关注点作为图像生成的启动过程，通过递归的方式填充图像信息，从而实现输出多模态。除了多模态 CoT 以外，CoT 目前也已经用于如文本摘要（SumCoT），开放域问答（Self-Prompting LLMs），机器翻译（MAPS），化学（ChemCrow）、医学（Med-PaLM）等等领域6. CoT 与 AI Agent 有何关系？回忆我们上一篇中介绍的关于 Agent 的定义，我们期望通过各种AI 技术构建的 Agent 事实上是一类拥有“自主智能的实体”，可以自主的发现问题、确定目标、构想方案、选择方案、执行方案、检查更新。基于大模型解决问题的“通用性”与预训练得到的“先天知识”，构建的大模型智能体可以被认为具有如下图的结构：上图中大模型智能体主要由三部分组成，分别是 Agent 主体，工具与环境。当人类指令输入 Agent 主体后，Agent 主体通过一系列计划、决策与控制，使用工具与外部环境互动。其中显然，作为 Agent 主体的大模型是模拟人类智能决策流程的核心，在许多 Agent 需要处理的任务中，Agent 的“先天知识”并不包含解决任务的直接答案，因此 Agent 需要在一系列与外部环境的交互循环中，制定计划，做出决策，执行行动，收到反馈……在一整个计划、决策与控制的循环中，大模型需要具备“感知”，“记忆”与“推理”的能力，如下图所示， CoT 恰恰可以从这三个方面来“赋能” Agent。感知 CoT无论是环境的反馈，还是人类的指令，Agent 都需要完成一个对接收到的信息进行“理解”，并依据得到的理解进行意图识别，转化为下一步任务的过程。而使用 CoT 可以大大帮助模型对现有输入进行“感知”，譬如，通过使用“Answer: Let’s think step by step. I see $$, I need to ...”的 Prompt，可以让模型逐步关注接收到的信息，对信息进行更好的理解，再如，在机器人控制的场景下，Agent 的决策不可避免的会出现错误，而接受到错误信息的反馈让 Agent 理解错误的原因调整自己的行动也是 Agent 应用于动态场景下的多轮决策任务中的关键能力，感知 CoT 也将加强模型自我纠错的能力。此外，值得注意的是，与外部环境的互动需要 Agent 具有处理多模态信息的能力，这种能力要么需要 Agent 本身是一个多模态的大模型，要么需要 Agent 可以将其他模特信息转化为语言进行理解。其中一个非常有意思的问题是“是否大模型 Agent 只能存在以语言为中心的感知？”，如上图所示，事实上有许多工作不仅在以语言为中心的感知中拓展大模型编码其他模态信息的能力，并且也发展出了譬如以图像为中心的感知方法，与将文本与图像进行统一的真正以多模态为中心的感知方法。但是由于多模态信息带来的数据、计算、可扩展性等方面的种种问题，真正以多模态信息为中心的感知时代暂且还未到来。记忆 CoT一般而言，大模型智能体通常同时拥有短期记忆与长期记忆的能力。短期记忆一般作为一种时间信息，可以在 Agent 的多轮交互中灵活的改变（因此也被称为工作记忆），短期记忆为大模型提供更加直接的上下文信息支持，因此很自然的可以被建模为一条历史动作链。相比于短期记忆的“动态性”，长期记忆更多的提供历史事件中的静态信息的记录，是对历史知识更加宏观与抽象的理解，长期记忆可以依赖于大模型中的可训练参数进行构建，也可以通过外部维护的记忆库进行构建。而当序列长度变长，线性链条式的记忆链效率出现下降时，为了实现针对“记忆”高效的增删改查，一些工作探索了树搜索与矢量检索的方法。其中，树搜索将记忆信息以树结构进行存储，让智能体通过迭代访问文本记忆信息，譬如斯坦福 25 人小镇论文中提出的反思树 Reflection Tree，当智能体面对与环境的多轮交互时，反思树可以让智能体定期抽取历史信息进行“反思”，将反思抽象得到的结果搭建构成一颗反思树，树的叶子节点代表大模型每轮的基本观察，而非叶子节点则代表反思树的抽象程度，越靠近根节点抽象程度越高。而另一种方法则是矢量检索，通过将复杂数据类型建模为矢量数据库来实现长期记忆的高效存储与检索，当智能体遇到新问题需要“回忆”过往记忆时，基于矢量数据库的长期记忆系统则会快速检索相关信息，确保智能体行为一致性。推理 CoT除了感知与记忆，借鉴 CoT 的思路让智能体分解任务逐步进行计划与决策以增强智能体解决问题的可靠性。在 Agent 中，CoT 主要的功能在于将计划、行动与观察相互结合，弥合推理与行动之间的差距，显然，推理可以帮助模型制定行动计划处理异常情况，而行动则允许大模型在与外部环境进行交互的同时，收集附加信息支持模型的推理。譬如，AgentBench 强迫大模型智能体通过“思考”+“行动”步骤完成任务，而行动链技术通过一系列行动历史与未来行动计划帮助智能体进行决策，从而将决策问题转化为 CoT 推理问题。此外，工具的使用扩展了大模型 Agent 的能力边界，通过使用工具，大模型不再局限于“预测”下一步的动作，而获得了“实际执行”动作的能力，譬如输出代码操作机器，调用 API 获得数据，使用各种软件、计算工具等等，同时，使用浏览器获取“实时更新”的“新知识”作为大模型的检索增强也有效的扩展了大模型的知识边界，也为大模型“自我验证”提供了知识库。而除了使用工具以外，类似编写“教科书”，现在还有一些研究关注在“专门针对 Agent 任务场景”的数据集上对大模型进行微调以获得更强的 Agent。7. CoT的局限性前面说了这么多，是不是有了思维链，大语言模型就所向披靡了呢？照这么发展下去，真能媲美人类的能力了？大可不必担心，思维链本身还是有很多局限的，而它的局限也是大语言模型的局限。首先，思维链必须在模型规模足够大时才能涌现。在 Jason Wei 等的研究中，PaLM 在扩展到 540B 参数时，与思维链提示结合，才表现出了先进的性能。一些小规模模型，思维链并没有太大的影响，能力提升也不会很大。谷歌大脑的研究人员认为，策略问题需要大量的世界知识，而小型模型没有足够的参数来记忆这些世界知识，所以也不太可能产生正确的推理步骤。但问题是，能落地到产业的模型，规模必然不会太大，思维链拆解了更多的步骤、用到更多的计算资源，相当于更加耗费脑力，很多研究机构和企业是负担不起 175B 参数以上的大模型。所以思维链必须要探索，如何在较小的模型中进行推理，降低实际应用的成本。图7.1：62B 比 540B 的语言模型更容易出错其次，思维链的应用领域是有限的。目前，思维链只是在一些有限的领域，比如数学问题，五个常识推理基准（CommonsenseQA，StrategyQA，Date Understanding 和 Sports Understanding 以及 SayCan）上显现出作用，其他类型的任务，像是机器翻译，性能提升效果还有待评估。而且，相关研究用到的模型（GPT-3 API）或数据集，都是半公开或不公开的，这就使其难以被复现和验证。严谨来看，思维链的效果还需要被进一步探索，才能下定论。图7.2：即使有思维链提示，大语言模型依然不能解决小学水平的数学问题此外，即使有思维链提示，大语言模型依然不能解决小学水平的数学问题。没有思维链，数学推理是指定不行。但有了思维链，大语言模型也可能出现错误推理，尤其是非常简单的计算错误。Jason Wei 等的论文中，曾展示过在 GSM8K 的一个子集中，大语言模型出现了 8% 的计算错误，比如6 * 13 = 68（正确答案是78）。这说明，即使有了思维链，大语言模型还是没有真正理解数学逻辑，不知道加减乘除的真实意义，只是通过更精细的叠加来“照葫芦画瓢”，所以，对于有精确要求的任务，还要进一步探索新的技术。思维链确实增强了大语言模型的能力，但逻辑推理仍然是大语言模型的弱项，等待着更多突破。One more thing通过思维链，我们可以看到大语言模型为什么强，也为什么弱。它强在，模型规模的提高，让语义理解、符号映射、连贯文本生成等能力跃升，从而让多步骤推理的思维链成为可能，带来“智能涌现”。它弱在，即使大语言模型表现出了前所未有的能力，但思维链暴露了它，依然是鹦鹉学舌，而非真的产生了意识。认知心理学教授斯坦尼斯拉斯·迪昂（Stanislas Dehaene）在《精准学习》中提出，缓慢地、理智地、符号化地运作，是人脑的特权。它可以在任何可能的时候，提取具有普遍性、逻辑性的、明确的原则。五六岁的儿童学会了较小数字的加法，就可以理解其含义，用到更大的数字的加法中，而目前最强大的大语言模型，还连“加法”这个简单的抽象定律都理解不了。大语言模型，正如科幻作家特德·姜所说，是网上所有文本的模糊图像，一张有损压缩的 JPEG，但它可以用远超人脑的算力和数据，极其高产地做好文本生成、图像生成这样的模糊任务。而人脑更擅长精确的、逻辑性的任务，就像特德·姜说的：“当你还有原始图片的时候，一张模糊的 JPEG 到底有多大用处呢？”智能时代的生存策略，就是不要以己之短，硬碰 AI 之长。而是用 AI 之长，让自己的长板变得更长；用人脑的精确，让 AI 生成的模糊答案变得更高质量；用好思维链提示，让 LLM 生成时事半功倍。8.目前 CoT 与 AI Agent 还面临哪些挑战？尽管，当下 CoT 与 AI Agent 已经编程、科研、办公等等领域得到了极其广泛的应用，但是作为一个新兴领域，无论是CoT 还是 AI Agent 都面临着许多显著的挑战，其中包括：在未知领域中的泛化能力：尽管 AI Agent 的出现本身就拓展了大模型解决更加复杂未知领域问题的能力，但是由于缺乏与现实世界真正“具身”的交互，因此一个可以做到浏览网页的 Agent 是否通过同一套框架与工程手段就可以做到操控无人机编组，这一问题仍然悬而未决；Agent 的过度交互问题：为了完成任务，Agent 需要与环境进行大量复杂多步的交互，而一些研究也表明 Agent 很有可能会陷入到不断交互的循环陷井之中，在交互循环中无意义的空转，并且，由于 Agent 解决问题缺乏“效率”，因此由此生出的日志的存储与信息检索也将成为新的问题；个性化 Agent：人手一个的私人智能助理是一个美好的畅想但是一个真正的个性化 Agent 的实现还面临许多问题，目前个性化 Agent 的研究有三条技术进路，分别是从定制化的 Prompt 出发，从微调出发以及从模型编辑出发，但是这些进路都有各自的问题，并且当下研究都主要聚焦于特定的问题背景，目前还不存在一套完整统一的解决方案；多智能体社会：如何扩大大模型 Agent 的数量，以组成一个多智能体的社会用于观察“社会行为的涌现”也是一个非常有意思的方向，但是多智能体的计算开销是阻碍这一领域发展的关键问题；Agent 安全问题：当 Agent 逐步进入人们的日常生活，Agent 与 CoT 的安全性问题就必须得提上日程，譬如老生常谈得隐私泄露、权限滥用、有毒信息等等问题，此外，当 Agent 应用于现实世界后，此外，由于缺少现实世界真正多模态的反馈，譬如人类智能可以感受到“痛”，而 AI Agent 不会有这方面的信息输入，因此如何对完全不同质的两类主体进行“对齐”也将是关键问题；Agent 的评价：如何客观的评估一个 Agent 的能力也将是 AI Agent 发展带给我们的新问题，想想几年前 NLP 时代的数据集刷榜的评估方式，这种传统评价方式必然不适用于一个不断与外部环境打交道的 Agent。此外，一个做对了 99 步但生成答案错误的智能体可以本身能力要优于一个做错了 99 步但生成答案正确的智能体，因此 Agent 评价也呼唤除了评估执行任务的成功率以外的新指标、新方法。总结本篇文章主要是介绍了 CoT 以及后续的改进，目前从推特上观察，CoT已经被广泛应用，甚至很多人认为就是标准的做法。但国内来看，似乎缺乏对它的重视，觉得不过是个简单的 trick。其实不只是 CoT，对整体 LLM 的认知和谷歌、OpenAI 那边确实有些差距。至于为何，Jason Wei 那条推特一定程度说明问题，知乎上也有人把他删掉的推特截图放出来，大致意思是20年后入门 NLP 的人比之前的幸福，他们对 LM 的认知来自于强大的 LLM，而过去的人往往还停留在 BERT 的范式。参考大模型思维链（Chain-of-Thought）技术原理 - 知乎 (zhihu.com)从 CoT 到 Agent，最全综述来了！上交出品 - 知乎 (zhihu.com)https://zhuanlan.zhihu.com/p/657737603?utm_psn=1716386289283362816 https://zhuanlan.zhihu.com/p/655427670?utm_psn=1716386167392411648"
686021954,6ed048bd3f2a45ef8dc7b6e7e4135b4f,海鸣,https://api.zhihu.com/people/6ed048bd3f2a45ef8dc7b6e7e4135b4f,people,公众号《AIGC小白入门记》,美团-大模型算法工程师面经,article,https://api.zhihu.com/articles/686021954,算法工程师面试常考手撕题 搜狐<em>大模型</em>算法工程师 字节大模型算法实习生 理想汽车<em>大模型</em>算法实习生 百度大模型算法实习生面试题 腾讯<em>大模型</em>算法实习生面试题 阿里大模型算法工程师一面,104,3,410,1709898142,1710162031,"AIGC小白入门记分享最新最热的大模型、AIGC、AGI 趣事！！！关于小编：小编是过年某985研二，过完年打算找大厂实习 offer，本文章主要记录了本小菜研找实习的坎坷历程，欢迎大佬们给建议！！！应聘岗位：美团-大模型算法工程师面试轮数：整体面试感觉：AIGC小白入门记分享最新最热的大模型、AIGC、AGI 趣事！！！美团-大模型算法工程师面经小米-NLP算法工程师面试题好未来-NLP算法工程师面经百度大模型算法工程师面经算法工程师面试常考手撕题搜狐大模型算法工程师字节大模型算法实习生理想汽车大模型算法实习生百度大模型算法实习生面试题腾讯大模型算法实习生面试题阿里大模型算法工程师一面某大厂大模型算法工程师面试题说说百度大模型算法工程师二面经历昆仑天工大模型算法工程师AIGC小白入门记分享最新最热的大模型、AIGC、AGI 趣事！！！104篇原创内容1. 自我介绍在自我介绍环节，我清晰地阐述了个人基本信息、教育背景、工作经历和技能特长，展示了自信和沟通能力。2. 技术问题2.1 如何解决过拟合和欠拟合?过拟合（Overfitting）：增加数据量：通过增加训练数据来减少模型对特定数据的过度拟合。简化模型：减少模型的复杂度，可以通过减少特征数量、降低多项式次数等方式。正则化：引入正则化项，如L1或L2正则化，以惩罚模型复杂度。欠拟合（Underfitting）：增加特征：添加更多有意义的特征，提高模型的表达能力。增加模型复杂度：选择更复杂的模型，如增加层数、节点数等。减小正则化：减小正则化的程度，以允许模型更好地适应数据。2.2 L1正则化和L2正则化的区别?L1正则化：增加的正则化项为权重向量的绝对值之和。促使模型参数变得稀疏，即某些权重变为零，从而实现特征选择的效果。L2正则化：增加的正则化项为权重向量的平方和。通过减小权重的同时保持它们都非零，对权重进行平滑调整。区别：L1正则化倾向于产生稀疏权重，对于特征选择有利；L2正则化则更倾向于在所有特征上产生较小但非零的权重。2.3 介绍一下 RAG?检索增强 LLM ( Retrieval Augmented LLM )，简单来说，就是给 LLM 提供外部数据库，对于用户问题 ( Query )，通过一些信息检索 ( Information Retrieval, IR ) 的技术，先从外部数据库中检索出和用户问题相关的信息，然后让 LLM 结合这些相关信息来生成结果。2.4 RAG 解决了哪些问题?长尾知识: 对于一些相对通用和大众的知识，LLM 通常能生成比较准确的结果，而对于一些长尾知识，LLM 生成的回复通常并不可靠。私有数据: ChatGPT 这类通用的 LLM 预训练阶段利用的大部分都是公开的数据，不包含私有数据，因此对于一些私有领域知识是欠缺的。数据新鲜度: LLM 通过从预训练数据中学到的这部分信息就很容易过时。来源验证和可解释性: 通常情况下，LLM 生成的输出不会给出其来源，比较难解释为什么会这么生成。而通过给 LLM 提供外部数据源，让其基于检索出的相关信息进行生成，就在生成的结果和信息来源之间建立了关联，因此生成的结果就可以追溯参考来源，可解释性和可控性就大大增强。可以参考: https://articles.zsxq.com/id_m9t1w8pokjpf.html2.5 RAG 中包含哪些关键模块?数据和索引模块查询和检索模块响应生成模块2.6 RAG检索召回率低，一般都有哪些解决方案?参考: https://wx.zsxq.com/dweb2/index/columns/511121412552442.7 对比一下 RAG vs SFT ?3. Leetcode 题具体题意记不清了，但是类似 【5. 最长回文子串】题目内容给你一个字符串 s，找到 s 中最长的回文子串。如果字符串的反序与原始字符串相同，则该字符串称为回文字符串。示例 1：输入：s = ""babad"" 输出：""bab"" 解释：""aba"" 同样是符合题意的答案。示例 2：输入：s = ""cbbd"" 输出：""bb""提示：1 <= s.length <= 1000 s 仅由数字和英文字母组成代码实现class Solution: def longestPalindrome(self, s: str) -> str: if not s or len(s) < 1: return """" start = 0 end = 0 for i in range(0,len(s)): len1 = self.expandAroundCenter(s,i,i) len2 = self.expandAroundCenter(s,i,i+1) s_len = len1 if len1>=len2 else len2 if s_len > end-start: start = i - math.floor((s_len-1)/2) end = i + math.floor(s_len/2) return s[start:end+1] def expandAroundCenter(self,s,left,right): while left>=0 and right<len(s) and s[left] == s[right]: left = left - 1 right = right + 1 return right - left - 1 致谢LLMs 千面郎君更新版 https://mp.weixin.qq.com/s/C6NdO_Ebj3DQx2AVAAgQRQLLMs九层妖塔 https://mp.weixin.qq.com/s/Eh0tY1zx2FqXQqIGa2dIBANLP 面无不过 https://github.com/km1994/NLP-Interview-NotesAIGC小白入门记分享最新最热的大模型、AIGC、AGI 趣事！！！美团-大模型算法工程师面经小米-NLP算法工程师面试题好未来-NLP算法工程师面经百度大模型算法工程师面经算法工程师面试常考手撕题搜狐大模型算法工程师字节大模型算法实习生理想汽车大模型算法实习生百度大模型算法实习生面试题腾讯大模型算法实习生面试题阿里大模型算法工程师一面某大厂大模型算法工程师面试题说说百度大模型算法工程师二面经历昆仑天工大模型算法工程师AIGC小白入门记分享最新最热的大模型、AIGC、AGI 趣事！！！104篇原创内容"
694700684,163524332a7cdc802501041d6d3a61f8,RUC AI Engine,https://api.zhihu.com/people/163524332a7cdc802501041d6d3a61f8,people,中国人民大学AI Engine实验室知乎专栏,"LLM+Recommendation大模型推荐近期进展 | 含WWW, SIGIR, AAAI等顶会文章",article,https://api.zhihu.com/articles/694700684,SLIM框架将LLMs的推理能力（即教师）转移到“小”语言<em>模型</em>（即学生）。这种策略通过CoT提示引导更大的教师<em>模型</em>在复杂的推荐任务中进行宏观到微观的思考。通过蒸馏过程，只有较<em>大</em>教师<em>模型</em>参数量4％的小学生模型获得了逐步思考的能力,46,1,89,1714267619,1714267619,作者：张景森 中国人民大学高瓴人工智能学院博士生 导师为陈旭准聘副教授引言我们在今年三月初曾整理过一期基于大语言模型的推荐系统相关论文集锦，读者可前往查阅往期的推送文章(RUC AI Engine：LLM+Recommendation|基于大语言模型的推荐系统相关论文集锦)；鉴于该领域的快速发展，我们继续整理了近期发表在WWW2024、AAAI2024、SIGIR2024等顶会的相关文章共10篇，供读者速览领域研究进展。论文速览1. RecGPT: Generative Personalized Prompts for Sequential Recommendation via ChatGPT Training Paradigm（DASFAA'24）论文链接：https://arxiv.org/pdf/2404.08675.pdf本文提出在序列推荐任务中借鉴类似ChatGPT的用户与系统之间对话的模式。为此，本文提出了RecGPT框架，该框架通过采用ChatGPT训练范式，利用用户对推荐结果的反馈信息为序列推荐生成式个性化提示，以增强传统序列推荐模型的个性化水平，缓解传统序列场景中对单一行为序列建模的局限性。具体来讲，RecGPT使用生成式预训练模型（如GPT-1）来生成除原始用户行为序列项目外的各种提示。按照 ChatGPT 训练范式，该框架采用两阶段的训练过程：预训练和微调。在预训练阶段，该框架预训练具有自回归生成能力的多层 Transformer 解码器网络模型。为了增强模型的个性化生成能力，本文整合了用户ID模块。微调阶段分为两部分：提示微调（Prompt-tuning）和推理验证（Inference-validation）。本文首先微调预训练模型以生成专门针对推荐任务的个性化提示，并在模型中引入片段编号，用于区分生成的提示和原始行为序列。在推理验证阶段，本文用自回归召回方法替代传统的内积召回方法，以评估推荐的性能。自回归召回方法基于多个用户向量的顺序输出，能够有效捕捉用户偏好随时间的演变。本文在四个离线数据集进行了验证实验，并且在Kuaishou应用程序的在线召回阶段中整合了上述方法，以证明其有效性。2. Fine-Tuning Large Language Model Based Explainable Recommendation with Explainable Quality Reward (AAAI'24)论文链接：https://ojs.aaai.org/index.php/AAAI/article/view/28777基于大语言模型的可解释推荐系统可以提供高质量的类人解释。然而，最初基于LLM的可解释推荐系统在生成解释时面临三个重要挑战，即缺乏个性化、解释不一致和解释数据有问题。为了解决这些挑战，本文提出了一种新型的基于LLM的可解释推荐模型（称为LLM2ER），并设计了两种解释质量奖励模型，用于在强化学习范式中微调主模型，最终产生一个微调模型可以提供高质量的解释。具体来讲，LLM2ER对从评论中提取的概念图进行建模，以达到以下两个目的：（1）基于异构图模型预测评分，并将评分映射到用户对目标物品的情感倾向；（2）从概念图中推断目标用户-物品对之间的推理路径，收集个性化且一致的候选概念，以提高解释的精度。 此外，本文提出的两个奖励模型，基于强化学习算法进一步提高解释质量：(1) 概念一致奖励模型利用考虑情感的候选概念，在基于对比学习的生成解释中保留配对的用户偏好和物品特征；(2) 高质量对齐奖励模型将生成的解释与基于生成对抗网络的未配对的高质量解释对齐。在三个公开数据集进行的广泛实验表明，本文模型可以生成流畅、多样化、信息丰富且高度个性化的解释。3. Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors (SIGIR'24-Short)论文链接：https://arxiv.org/abs/2403.19347本文讨论了利用大语言模型提高点击率（CTR）预测性能。作者指出将LLMs应用于实际应用情境中存在一个关键挑战：当处理长文本用户行为时，LLMs的效率很低。随着用户序列变得越来越长，目前LLMs的效率无法满足在数十亿用户和物品上进行训练的需求。为了突破LLMs的效率障碍，作者提出了行为聚合分层编码（BAHE）来提高基于LLMs的CTR建模的效率。具体来说，BAHE提出了一种新颖的分层架构，将用户行为的编码与行为间的交互解耦。首先，为了防止由于重复编码相同用户行为而产生的计算冗余，BAHE利用LLMs的预训练浅层提取来自广泛用户序列的最细粒度化的原子用户行为的嵌入，并将它们存储在离线数据库中。随后，LLMs的更深层可训练层促进了复杂的行为间交互，从而生成全面的用户嵌入。这种分离使得高级用户表示的学习与低级行为编码独立，显著降低了计算复杂度。最后，这些经过精细处理的用户嵌入与相应处理的项目嵌入结合到CTR模型中，计算CTR分数。大量实验结果表明，BAHE可将使用LLMs的CTR模型的训练时间和内存消耗减少五倍，尤其是对于较长的用户序列。BAHE已在实际系统中部署，允许每天在8个A100 GPU上对5000万条CTR数据进行更新，使LLMs在工业CTR预测中变得实用。4. Can Small Language Models be Good Reasoners for Sequential Recommendation? (WWW'24)论文链接：https://arxiv.org/pdf/2403.04260.pdf大语言模型由于其出色的语言理解和生成能力，为序列推荐开辟了新的前景。然而，要成功实现LLMs赋能的序列推荐，仍然有许多挑战需要解决：首先，用户行为模式通常很复杂，仅依靠LLMs的一步推理可能会导致错误或与任务无关的响应。其次，LLMs的资源需求极高，对于实际的序列推荐系统来说是不切实际的。本文提出了一种新颖的逐步知识蒸馏框架（SLIM）用于推荐，为序列推荐器以高效的方式利用LLMs的出色推理能力提供了参考。具体来说，SLIM框架将LLMs的推理能力（即教师）转移到“小”语言模型（即学生）。这种策略通过CoT提示引导更大的教师模型在复杂的推荐任务中进行宏观到微观的思考。通过蒸馏过程，只有较大教师模型参数量4％的小学生模型获得了逐步思考的能力，并成为一个良好的推理者。随后，作者直接将小语言模型部署为序列推荐的知识生成器，该模型可以推导出与推荐高度相关的高质量推理知识。这些知识反映了用户对类别、品牌和特定项目的偏好，可以灵活地与任何序列推荐骨干集成，包括基于ID和非ID情景。5. Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation （WWW'24）论文链接：https://arxiv.org/pdf/2401.14678.pdf跨域推荐（CDR）是缓解数据稀疏问题的有效技术，以往的研究可能导致领域隐私泄露，因为训练过程需要将多样化的领域数据聚合到中心服务器中。尽管一些研究通过联邦学习（FL）进行隐私保护，但仍然存在以下几点限制：1）它们需要将用户的个人信息上传到中央服务器，存在泄露用户隐私的风险。2）现有的联邦方法主要依赖于原子商品ID来表示商品，这阻止了它们在统一特征空间中对商品进行建模，增加了领域之间知识转移的挑战。3）它们都基于知道领域之间有重叠用户的前提，这在实际应用中很可能是不可行的。为了解决上述不足，本文专注于隐私保护的跨域推荐（PCDR）并提出了PFCR方法。对于限制1，本文通过专门利用用户与本地客户端的交互，并设计渐变加密方法来开发FL架构。对于限制2，本文通过描述文本模型化项目，将其模型化到通用特征空间中。对于限制3，本文首先学习联邦内容表示，利用自然语言的一般性来建立领域之间的桥梁。作者制定了两种提示微调策略，将预训练模型定制到目标领域。对两个真实数据集进行的大量实验表明本文的PFCR方法相对于SOTA方法具有优越性。6. ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction （WWW'24）论文链接：https://arxiv.org/pdf/2310.09234.pdf本文提出传统的点击率（CTR）预测模型通过独热编码（one-hot）将多字段分类数据转换为ID特征，并提取特征之间的协作信号，这种范式存在语义信息丢失的问题。此外，预训练语言模型（PLMs）在CTR预测中展现了一定潜力，将输入数据通过硬提示模板转换为文本句子，虽然保留了语义信号，但通常无法捕获协作信息（例如，特征交互、纯ID特征等），并且大模型带来了不可接受的推理开销。本文旨在为准确的CTR估计建模语义知识和协作知识，并同时解决推理效率问题。为了从两个世界中受益并弥合它们之间的差距，本文提出了一个新颖的模型无关框架ClickPrompt，它将CTR模型结合起来生成面向PLMs的交互感知软提示。作者设计了一个增强型遮蔽语言建模（PA-MLM）预训练任务，其中PLM必须基于语言上下文恢复遮蔽的标记，以及由CTR模型生成的软提示。ID和文本特征的协作和语义知识将通过提示接口显式对齐和交互。基于此设计，可以使用PLM调整CTR模型以获得更好的性能，或者仅调整CTR模型而不使用PLM以提高推理效率。在四个真实数据集的实验验证了ClickPrompt相对于现有基线的有效性。7. Towards LLM-RecSys Alignment with Textual ID Learning (SIGIR'24)论文链接：https://arxiv.org/pdf/2403.19021.pdf基于大语言模型（LLMs）的生成式推荐已经将传统的基于排序的推荐风格转变为一种文本生成范式。然而，与标准自然语言处理（NLP）任务相比，目前在生成式推荐领域的研究难以有效地将推荐项编码到文本生成范式中，使用简洁而又具有意义的ID表示，因此基于LLMs的生成式推荐系统的潜力仍然大部分未被开发。为了更好地满足推荐需求，本文提出了IDGenRec，将每个项目表示为一个独特的、简洁的、语义丰富的、与平台无关的文本ID，使用人类语言标记。通过训练一个文本ID生成器与基于LLMs的推荐系统并行，实现了个性化推荐与自然语言生成的无缝集成。由于用户历史记录以自然语言形式表达，并与原始数据集解耦，本文方法表现出建立基础生成式推荐模型的潜力。实验证明了该框架在标准实验设置下优于现有的序列推荐模型。此外，作者探索了在从19个不同数据集收集的数据上使用所提出的方法训练基础推荐模型的可能性，并在zero-shot设置下在不同平台上的6个未见数据集上测试了其推荐性能。结果表明，预训练基础模型的zero-shot性能甚至优于一些基于监督训练的传统推荐模型，显示了IDGenRec范式作为生成式推荐基础模型的潜力。8. Sequential Recommendation with Latent Relations based on Large Language Model (SIGIR'24)论文链接：https://arxiv.org/pdf/2403.18348.pdf序列推荐系统通过对用户历史交互建模来预测可能引起用户兴趣的项目。传统的序列推荐方法依赖于捕捉项目之间的隐式协同过滤信号。近期，关系感知的序列推荐模型通过将项目关系明确纳入用户历史序列建模中取得了令人满意的性能，其中大多数关系都是从知识图谱中提取的。然而，现有方法依赖于手动预定义的关系，并且存在稀疏性问题，限制了在具有不同项目关系的各种场景中的泛化能力。本文提出了一种新颖的具有潜在关系发现的关系感知序列推荐框架（LRD）。与之前依赖预定义规则的关系感知模型不同，作者提出利用大语言模型来提供项目之间新类型的关系和连接。动机在于LLM包含丰富的世界知识，可以用于挖掘推荐项的潜在关系。具体来说，受到人类可以用自然语言描述项目之间关系的启发，LRD利用了已经展现出人类般知识的LLM来获取项目的语言知识表示，这些表示被馈送到基于离散状态变分自编码器（DVAE）的潜在关系发现模块中，基于此，自监督关系发现任务和推荐任务被联合优化。在多个公开数据集上的实验结果表明，本文提出的潜在关系发现方法可以与现有的关系感知顺序推荐模型结合，并显著提高性能，进一步的分析实验表明了所发现的潜在关系的有效性和可靠性。9. Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation (NAACL'24)论文链接：https://arxiv.org/pdf/2403.09738.pdf合成用户在评估对话推荐系统中是一种经济高效的真实用户代理。大语言模型显示出模拟人类行为的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一种新的协议来衡量语言模型在对话推荐中准确模拟人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的一个关键特性：选择要谈论的物品、表达二元偏好、表达开放性偏好、请求推荐以及提供反馈。每个任务都会提示一个模拟器，并存储一组模拟器的结果。然后可以将结果与从四个不同平台策划的人类数据进行比较。这些任务本身并不能保证模拟器是人类用户的完美代表，而是捕捉了模拟器中的失真，即与人类的系统性差异。通过对基准模拟器的评估，本文证明了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了如何通过模型选择和提示策略减少这些偏差的见解。10. Unlocking the Potential of Large Language Models for Explainable Recommendations (DASFAA'24)论文链接：https://arxiv.org/pdf/2312.15661.pdf随着语言生成技术的发展，生成用户友好的推荐解释变得越来越普遍。然而，现有的可解释推荐系统主要依赖于小型语言模型，尚不清楚用大语言模型（LLMs）替换解释生成器会产生什么影响。本文提出LLMXRec，一个两阶段可解释推荐框架，旨在通过采用LLMs来进一步提高解释质量，第一阶段致力于训练推荐模型，第二阶段集中于生成解释。它的关键特点是强调了先前推荐模型与基于LLMs的解释生成器之间的密切合作，具体来说，通过采用几种关键的微调技术，包括参数高效的指令微调和个性化提示技术，可以生成可控且流畅的解释，以实现解释推荐的目标。本文提供了三种不同的视角来评估解释的有效性，并且在几个基准推荐模型和公开可用的数据集上进行了广泛的实验。
690801254,d522413871e0705b89072284ecdd5a94,林夕 AIGC,https://api.zhihu.com/people/d522413871e0705b89072284ecdd5a94,people,专注多模态大模型研究,国内20家公司大模型岗位面试经验汇总,article,https://api.zhihu.com/articles/690801254,一些参考资料： 国内<em>大模型</em>公司，粗略面经+感受+一些包裹 淘天集团<em>大模型</em>算法工程师面试经验贴 大模型面试八股 <em>大模型</em>八股答案（一）——基础知识 社招关于<em>大模型</em>的知识点问到的也是一些常识知识,268,26,673,1712286047,1712295884,"面试情况：投过的公司：淘天，字节，蚂蚁，商汤，美团，夸克，腾讯，minimax，零一万物，阿里控股，潞晨科技，阿里巴巴国际，网易实验室，Momenta。Offer：淘天，字节AML，商汤，蚂蚁，美团，夸克，腾讯混元，天翼云。面经：淘天【offer】：部门：未来生活实验室。介绍：淘天集团的大模型研究将主要围绕两个场景展开：一是搜广推，二是逛逛的内容化。团队组建工作由淘天集团CEO戴珊、淘天集团CTO若海、阿里妈妈CTO郑波等人共同牵头。介绍链接。面经：一面： 如何训练ocr任务 实验的setting， ocr任务影响如何 高分辨率训练，再在小分辨率上，会不会崩掉 输出bbox有没有好的方式 HR面： 基本信息确认： 考研进入电子科技大，为什么 为什么进入了诺亚 代表性的工作 如何解决问题的，国内在做语言大模型的多一些。 国内在做多模态大模型的 如何评测，评估这些大模型的。如何看待刷榜的问题。 你们的衡量指标 面试体验：面试体验很好。HR也没有那么咄咄逼人。阿里味不是特别明显。最终权衡也选择来了淘天，有兴趣来我们这边的欢迎投递简历，有卡（****张）。字节AML【offer】：部门：AML火山方舟大模型介绍：淘天集团的大模型研究将主要围绕两个场景展开：一是搜广推，二是逛逛的内容化。团队组建工作由淘天集团CEO戴珊、淘天集团CTO若海、阿里妈妈CTO郑波等人共同牵头。介绍链接。预备面经：猎头也提供了一些其他候选人的面经， 整理如下： 设计一个Hash表 蓄水池问题 从超大文本文件中随机行采样 二叉搜索树-去掉超出[m, n]范围内的节点 计算交叉熵 计算IOU 一面： 多模态你们的流程 你们的OCR的结构 layernorm的区别 二面： 什么时间开始做多模态模型 目前在文档 Token长度太大。高分辨率如何解决。 patch能不能变得 VQGAN GPT4V的结构形式。 LLM decoder MagViT LLM的基础知识需要尽快补上 三面： 自我介绍 多模态大模型用哪个模型 Transformer比较熟悉 python实现self attention和Transformer 面试体验：字节一如既往的注重coding，基本每一面两道题，自己能拿到offer也算是运气吧。一面的面试体验很差，可以说是毫无面试经验的面试官。商汤【offer】：一面： 简单的自我介绍 关于高分辨率的解决办法 OCR是如何去做的，有没有grounding，Referring。 GPT4V 位置，检测明显的问题。 你们是如何解决位置信息的。 新的模态，进来。如何去拼接过来。 新来一批数据，如何训练。 tree1是否是tree2的子树。 二面： AGI缺不了视觉。 工业场景，人的检测，奇怪的零件，机器的检测。检测的结果，需要理解人的行为。理解和context的关系。人和环境的关系。借助多模态来解决。 自驾，感知端，到决策端，感知不需要全面的感知，着重额感知需要。 假新闻，图是真的。caption场景。 OCR，街景的OCR，文档，网页截图等是不同的，这些场景混在一起。会有影响吗。 OCR的方案，一般是先出检测框，再出文本。不规则的文本，如何解决。 多模态大模型的OCR。 Qwen-VL的文本检测形似，是否合理。 模型本身的三种能力： * 自己的能力，认识什么。* LISA：借助decoder来做，用新的prompt来实现分割。* 外挂RAG等。 判断左右括号（），等组成的字符串是否是合法的。 二面结束，问题不大。三面： Leader面，聊了一些有的没的 面试体验：面试过程很专业，毕竟是商汤，算法技术是国内领先的，奈何没产品，而且薪资包也没有诚意。蚂蚁风控大模型【offer】：一面： 自我介绍 介绍Focus-DETR，有没有在业务中应用。 二面： 介绍 目前在做的事情 下游的任务，文档场景，再做一个什么任务。 ocr数据是如何生成的。 效果如何。 如何解决幻觉的问题。 HR面。 具体的工作内容，印象 安全内容，tob的内容审核。竞对。 个人介绍。 上海海思，为什么跳槽 算法支撑 MiniMax：【挂】一面： 空间感知能力是如何解决的 不同token长度是否有做balance的方式 很多有价值的问题。 python实现标题 MLP Regressor 题目描述 题目描述与要求:实现一个MLP regressor 的完整训练过程，包括:forward , backward , SGD更新参数;验证backward梯度正确性;构建数据，验证端到端正确性 (收敛&泛化);(optional) 参考你过往的实验经验，尽可能让功能更完善;不使用第三方库中已实现好的自动求导方法;P.S. 实现方法不限，但由于在线IDE运行环境不稳定，推荐使用numpy。 PS：面试官对算法底层了解比较深入，比较考验代码能力，不是字节那种。零一万物【挂】：一面： 多模态的情况。 document的难度。 手写 self attention 二面： 自我介绍 传统的OCR模型和多模态大模型的区别 带不带OCR，多模态文档智能差多少。 未来OCR模型是否会被大模型吃掉。OCR大模型有什么是做不了的。 在哪些模型上做过一些预训练。 多模态未来怎么做。 EVA的有趣的现象。没有出现涌现的现象。 后面的工作主要集中在哪里。 怎么训练比较好，gemini那种形式，llm那种形式。 PS：问了很多开放性的问题，给的回复是业务积累不够，hhhh。美团【offer】：一面： 文档大模型是一个独立的模型，会整合到一个大模型还是分离式的。 不考虑通用场景的。 目前用的多大的模型。 是一个什么结构。对结构的一些了解。 flamingo或者blip2的结构上的对比。 足够资源的情况下，哪个结构是最优的。 分辨率是如何解决的。文档的分辨率比较高。 QA对匮乏如何解决的。 ocr的性能目前是低于多模态大模型。 如何解决bbox感知能力差的问题。 充足的算力。数据如何获取。如何有效的清洗。 fuyu这种结构有什么优化的空间。 二面： OCR文档大模型 Instruction tuning的决定性因素，哪些Instruction tuning策略效果更好。如何评估复杂度的性能。 如何保证数据集的多样性和复杂度。 分层的Instruction tuning。 数量的影响。数量和质量。 不同类型数据的影响。对下游任务的影响。自动化的数据配比。自动化的指标去做类似的事情。 专门优化的指标不再是一个好的指标。 使用其他的指标来引导模型的优化，而不是使用下游的指标。LLM使用的loss，作为指标。而不是benchmark的分数。 合理的评估指标是什么。openAI的压缩理论，trainnig loss代表优化目标。 AGI的几个阶段，deepmind发表。 三面： 各种问题 四面： 介绍自己。 有一个多模态大模型，具备一些很多baseline的效果，基础的CV能力。可能效果都不是特别好。不如专精的大模型。OCR为例。目标得到一个类似GPT4V的模型，整理提高模型的效果。第二个，全流程，强化ocr能力。已有的多模态大模型。 文档场景。 阿里控股【PASS】一面：这里没什么面经，整体面试体验很差。不考虑继续往下面。 人非常高傲，傲慢，问问题，满脸的嫌弃 感觉别人都不行 觉得自己很强 文生图 文生图的畸形，检测的方式。 潞晨科技【PASS】一面： 手写self-attention emmm，但凡面试官是NLP的，面试体验都不太好。如果NLP背景的面试官，面试会比较困难。夸克【offer】一面：项目做题二面： 高分辨率的解决方案 重叠文本框。采用什么方案来解决。 端到端的OCR是怎么做的。 三面： 介绍Focus-DETR。 介绍端到端的OCR。 介绍多模态大模型。 小语种数据集采购是为什么。检测数据多了，识别数据少了。 介绍业务规划，偏传统业务，不是大模型方向。四面：大佬面，出了一个概率题。HR面：为什么跳槽。职业规划。为什么有这个绩效和各种奖项。腾讯-混元大模型【offer】：一面： 简单描述简历，在做的东西 图像细节提取有什么办法 目前的进展是什么样的，项目进展到什么情况 表格这种数据如何解决，如何对表格这种做有效的信息抽取。 目前有多少的数据量 二面： OCR算法 项目 介绍文本检测的一些问题如何解决 MAE如何优化，什么鬼问题 三面纯聊天，没有太多技术问题。 多模态大模型，TEG，数据平台事业部。混元大模型。机器学习平台。 阿里巴巴国际部【中止】：一面：题目，旋转图像 Alpha度，介绍项目介绍Focus-Detr。二面：简单询问项目，很快结束。PS：不靠谱，很久没消息。没有后续的任何回复。这种最恶心了，建议大家不要再投。网易实验室【PASS】：一面： 自我介绍 剪枝的工作，Focos-DETR是如何剪枝的。 Transformer相比CNN，相比RNN的优势。 训练推理，增加吞吐量。 推理加速。预训练模型的部署推理，优化。 做推理加速的不合适。Momenta【PASS】：一面： 手写NMS 介绍DETR 介绍Focus-DETR 介绍position Embedding 介绍DETR里面query的含义 PS：Momenta属于给钱多，但是比较累的。以后不太想做自动驾驶了，所以pass。不再考虑。蚂蚁【大模型-codefuse】【HR面挂】一面：简单聊项目二面： 业务题，如何解决多页文档的问题 业务题，现在 ui 多模态大模型，ocr 如何引入，目前 ocr 存在很多问题 leetcode 题目，字符串转数字 leetcode 题目，最大子数组和 多模态大模型的基础知识 三面： 个人介绍 部门规模 论文的情况 换过部门 海思做的内容 OCR成熟度很高 面试体验：面试官都很专业，问了很多问题，感觉很强。在HR面上吃了亏，很多问题没有回答好，吸取了经验，很多问题调整了回答的方式。也反思了自己在过往工作中存在的问题，面对大公司建议HR面认真准备，看看网上的一些经验，阿里的HR有一票否决权。HR面注意点：1、跳槽动机：如果是因为晋升不上去看机会，建议这么说：内部机会少，太稳定，想看更有发展的机会，别直接说晋升不上去才看机会，⚠切勿说和leader处不来或者说目前leader坏话。2、关于薪资：可以回答看企业安排，或者回答一个区间，保底多少，期望多少，加一句薪资可谈，留足谈判空间，如果企业给的薪资不太满意，⚠别直接拒绝，就说考虑一下，我这边去谈，最后面试结束时，记得感谢下面试官的时间。3，关于加班：大厂都大差不差，HR问就是试探⚠切勿说接受不了。一些参考资料：国内大模型公司，粗略面经+感受+一些包裹 淘天集团大模型算法工程师面试经验贴 大模型面试八股 大模型八股答案（一）——基础知识 社招关于大模型的知识点问到的也是一些常识知识，没有特别难得，更多的还是项目为主。面试其他公司的时候基本没有刷题，因为工作期间面试的，没有太多时间刷题， 只能周末时间。为了准备字节还是刷了一些题，大概60多道，如果是字节的话，还是好好准备刷题， 我有朋友甚至遇到了好几道hard。关于刷题，推荐labuladong的刷题笔记。应该算是最精良了，看这个不用看其他的了。选择offer过程中也是各种权衡，考虑很多因素，也咨询了很多朋友，面临offer选择困难的朋友也可以咨询我。​"
696226537,907f4c9ae7a90b80d1dc9eeac357c019,Linsight,https://api.zhihu.com/people/907f4c9ae7a90b80d1dc9eeac357c019,people,NLP &amp; LLM,解锁大模型长上下文能力,article,https://api.zhihu.com/articles/696226537,attention 理解LLM位置编码:RoPE <em>大模型</em>算法题(1) 大模型算法题(2) <em>大模型</em>算法题(3) 大模型算法题(4) <em>大模型</em>算法题(5) 【本文已在同名 微信公众号,8,0,22,1714995443,1714995443,"【往期文章】MoE模型的前世今生LLM长上下文的问题理解Attention:从起源到MHA,MQA和GQAYi技术报告-划重点看细节transformer中normalization的二三事从代码实现看normalization-到底做了什么稀疏注意力计算:sliding window attention理解LLM位置编码:RoPE大模型算法题(1)大模型算法题(2)大模型算法题(3)大模型算法题(4)大模型算法题(5)【本文已在同名 微信公众号 / 知乎 / 个人博客linsight.cn 上线】步入2024年Q2，大模型在RAG、文档对话、大模型Agent能力等方向的发展持续升温。在平时的日常生活和工作中，大模型工具提供的文档总结、文本润色、代码生成等能力已经是提高效率的必备帮手，甚至在一些复杂或者不熟悉的场景上，大模型也已经能提供一些比较专业的帮助。在这些方向上，大模型(超)长上下文的能力都是基础。无论是使用详细的CoT/ToT，还是通过多篇检索文档提供专业知识，抑或是使用相关样例提升回复质量，都需要模型具备处理很长的输入输出信息的能力。这不仅要求模型在较长的位置编码下依然具有良好的语言建模能力，而且还需要模型能够进行长距离的、细致的阅、准确的阅读和理解。本篇将梳理几个通过轻量级训练解锁大模型长上下文能力的工作。1.支持128k上下文的数据工程论文：Data Engineering for Scaling Language Models to 128K Context时间：2024年2月阶段：预训练长度：128k1.1.评测指标模型的长上下文能力不仅体现在文本较长的时候，模型的PPL依然能保持在较低的水平，还体现在对于长上下文输入，模型依然能够进行准确的阅读理解和推理。以往一些工作仅使用validation dataset上的PPL作为评测指标，并不能很好地表征模型的真实长上下文能力。而目前被广泛使用的 Needle in a Haystack，或者叫大海捞针任务，是对长上下文能力的一个比较好的评测。这篇论文主要就以大海捞针任务为标准，对不同的模型和方案进行对比。两个PPL几乎相同的模型，在大海捞针任务上的差距可以很大，如下图所示，颜色越绿代表正确率越高目前已有的一些扩展大模型上下文窗口的方法，比如LongLoRA和Mistral所采用的YaRN，虽然理论上来说，能够支持>100k的上下文长度，但是实际上在大海捞针任务的表现却不太好。相关模型在大海捞针任务上的效果对比如下所示，只有GPT-4的效果比较好。1.2.数据分布这篇论文认为，在<=4k窗口长度完成预训练的模型，其实就已经基本具备在128k或者更大的上下文窗口进行推理的能力，只需要进行轻量级的继续预训练（e.g. <5B token），就能够解锁这种能力。（而一些其他的工作在这方面则有着相反的观点，比如在32k窗口训练了400B token的《Effective long-context scaling of foundation models》，以及Xverse）要做继续预训练，最重要的一点就是要决定使用什么样的数据。这篇论文里的实验是基于LLAMA的，因此使用了和LLAMA预训练数据具有相近领域分布的SlimPajama数据集作为基础。对于长上下文的继续预训练数据，需要仔细考虑数据长度和领域分布的影响。通常来说，某些领域天然会有更高比例的长文本数据，比如书籍、论文和github，而一些其他领域的长数据就较少，比如新闻。如果直接从整体数据中挑选长数据而忽略领域分布，就可能造成训练数据在领域分布上的偏移。论文使用了几种不同的数据处理策略，用于后面的实验对比：Cut at 4K：把所有的数据按4k长度进行分块，这样不会影响领域分布。这也是很多4k预训练模型所采样的方案，比如LLAMA。Cut at 128K：截断长度提升到128k，可以保留长文本内部信息的依赖关系。LongLoRA就是这么做的。Per-source Upsampling：在保持各个领域的比例不变的前提下，对长文本进行上采样，提高长文本的比例。这是这篇论文所推荐的方法，实验效果最好。Global Upsampling：不管领域，直接对长文本进行上采样。Upsample Arxiv/ Book/ Github：提高特定领域的数据比例，对长文本进行上采样。这些策略基本涵盖了大部分长文本相关工作在数据上的处理策略。不同数据处理策略下，SlimPajama数据内各领域的分布如下图所示Per-source Upsampling是效果最好的，也是这篇论文所推荐的数据工程策略。1.3.实验配置实验上，用80k的窗口长度训练LLAMA2-7B模型，用64k的窗口训练LLAMA2-13B模型。虽然理论上，计算复杂度度和模型训练窗口长度是平方关系，但是实际实现上，由于有FlashAttention等方案，可以把Attention的计算通过设备间通讯，在多个设备间并行起来。而设备间的通讯（包括GPU和CPU，GPU和GPU之间）成本都是constant或者linear，因此实际上80k窗口的的训练耗时只是4k长度的训练的3倍，而不是理论上的400倍。当然，实际所需的计算量并没有减少，但是至少时间成本从平方变成了线性。剩下的，只要堆jia卡qian就可以提速。Per-source Upsampling和其他工作的数据处理策略的对比如下训练的配置和耗时如下所示实验的其他配置：lr = 2e-5RoPE base从1,0000改为500,000batch size = 4M token1.4.训练量前面提到，论文认为只需要轻量级的继续预训练就可以解锁长上下文能力，那么到底需要训练多少token呢？论文分别取了训练了100M、300M、500M、1B、5B、10B token的中间checkpoint进行PPL和海底捞针任务评测，结果如下结论是，在训练了500M token的时候，模型基本解锁了长上下文的能力；在训练了5B token的时候，模型已经收敛，而且继续训练到10B token也没有进一步收益了。1.5.数据策略对比使用前面提到的不同数据策略在LLAMA2-7B模型用5B token进行训练，并对比效果。LLAMA2的预训练长度为4k，因此对比的时候分成了0-4k和4k-128k两段，分别评测模型经过长文本训练后，在短文本上的效果是否有变差，以及在长文本上是否有提升。各个数据策略在不同领域的效果变化如下可以得到几个结论：在0-4k长度上，除了Per-source Upsampling以外，各种数据策略都会对模型效果有损害在一些领域上的提升，并不能很好地迁移到其他领域，比如Book和Github之间就有点跷跷板效应，其中一个效果好了，另一个可能就有损失在4k-128k，Per-source Upsampling在各个领域的效果相对较为平衡（绿色的数量最多）此外，length upsampling很重要。Per-source Upsampling的策略在领域上可以和源数据保持一致，而提升长文本的比例。用同样80k的训练窗口在LLAMA2-7B进行实验，一个使用原数据进行拼接，另一个使用Per-source Upsampling，结果如下。在PPL基本相同的情况下，Per-source Upsampling在大海捞针的效果远超原数据。这说明提高长文本的比例，能极大优化模型远距离建模的能力。1.6.结论通过实验，论文提出提升模型长上下文能力的数据工程实践的几个关键点：在长窗口上进行轻量级训练，可以提升模型实际的远距离建模能力，而不仅仅是保持PPL较低领域之间有竞争关系，最好和原预训练模型所用的分布保持一致长度上采样对最终效果有很大影响，要提高各领域内长文本的比例2.Paraphrasing论文：Training With ""Paraphrasing the Original Text"" Improves Long-Context Performance时间：2023年12月阶段：微调长度：在50k长度依然能有较好的效果，如下所示。2.1.检索能力对于长上下文的任务，有用的信息通常是稀疏的，一般只有少量的句子或者段落包含了可以用于回答问题的有用信息。可以隐式地将这样长上下文的任务拆分成两个子任务，即相关信息的检索，和根据相关信息回答问题两个任务。目前一些支持长上下文的方法，比如位置编码相关的线性插值、NTK插值、YaRN等，虽然使得模型在形式上支持了长上下文的任务，但是在任务的准确性上效果却不佳。使用这些优化方案的模型依然会遇到lost in the middle的问题，即模型天然更容易关注到输入文本的开头和结尾部分的信息，而更容易忽略中间部分的信息，注意力迷失在大量无关内容上，而无法集中到少数相关的位置上。而对于长上下文的任务，大量的信息是处于middle的位置的，如果忽略这些信息自然会使得任务效果不好。而效果不好的原因就是模型在长上下文的情况下，retrieval能力偏弱，不能找到有用的信息。2.2.相关工作一些工作直接把模型在长窗口下进行训练，比如：Together的LLaMA-2-7B-32K（https://huggingface.co/datasets/togethercomputer/Long-Data-Collections）；Together开源了Multipassage-QA-from-Natural-Questions和BookSum微调数据集。LongAlpaca（《LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models》）Ziya-Reader（《Never Lost in the Middle:Improving Large Language Models via Attention Strengthening Question Answering》）直接在长窗口训练有一定的效果，但是依然有几个问题：模型推理窗口越来越大，所需的训练数据集长度也要不断更新。随着长度增大，训练成本变高。构建长上下文数据集的成本比价高，高质量的数据并不容易获得。虽然有一些开源的数据集，但是在实际场景上可能还需要做领域适配，分布调整等工作。一个更简单一点的方法是优化prompt的设计，比如CoT。在长上下文的场景下，可以通过prompt让模型显式地先找到原文的相关信息再进行回答。比如Claude-2.1就通过在prompt增加“Here is the most relevant sentence in the context”让长文本问答的准确率从27%提升到98%（https://www.anthropic.com/news/claude-2-1-prompting）。也可以对输入内容进行重新的编排：LongLLMLingua（《LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression》）对输入文本进行了压缩。Attention Sorting（《Attention Sorting Combats Recency Bias In Long Context Language Models》）在decode过程中根据各个文档被分配到的注意力值，对文档进行重新排序。2.3.提高检索能力这篇论文提出了一个叫检索相关度（retrieval relevance）的指标，一个token（或者n-gram） 的检索相关度 定义如下。这个指标和TF-IDF很像。其中， 表示 在gold-chunk中的频率，而 是gold-chunk中的总token数； 表示整个上下文中总chunk数， 是包含x的chunk的数量。基于token 的检索相关度 ，定义训练样本 的检索相关度如下其中 表示 的答案部分。通过 这个指标可以反映出一个训练样本对模型提高检索能力的贡献。 越高，这个样本对提高模型检索能力的贡献越大。那么一个简单有效提升训练样本检索相关度的做法，就是把gold-chunk放到答案中，即paraphrasing the original text。一个paraphrasing和其他答案设计方案对比的例子如下其中高亮部分的token是高检索相关度的token，明显paraphrasing拥有更高的比例。论文使用GPT-4来构建包含paraphrasing的问答对，流程实际如下这种方式收集了一批单文档问答和多文档问答的数据，再加上一些传统文本摘要数据（摘要不好用这种方式构建，因此直接使用）等，构成一个包含10,825条英文数据，8,454条中文数据，长度在8k和32k之间的数据集。数据集详细的领域分布如下所示论文构建的数据集和Multi-passage-QA-from-NQ的检索相关性指标对比如下使用这个数据集微调的模型，和其他模型在LongBench上的效果对比如下另外，在这个数据集上微调之后，模型对于lost in the middle的问题也有一定的缓解，如下所示3.PoSE论文：PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training时间：2023年9月阶段：微调长度：128k3.1.背景目前大部分流行的大模型使用旋转位置编码RoPE。在短文本上训练的模型，在长输入上效果不好的原因之一，就是长文本有很多模型没有见过没有训练过的位置编码。基于位置编码的长上下文优化，比如线性插值、NTK插值和YaRN等，依然需要进行目标长度的训练才能有比价好的效果。而随着目标长度越来越长（8k，32k，128k...），这样的训练成本也越来越高，逐渐变得不容易进行。这篇论文提出Positional Skip-wisE，PoSE，通过在短的训练窗口模拟长距离的位置编码，提升模型处理长上下文的能力。模型可以在2k的训练窗口进行训练，而在128k的长度进行推理。相比直接训练128k模型效率更高。也有一些工作的思路和这篇文章有相近之处，比如RandPos（《Randomized positional encodings boost length generalization of transformers》），但是RandPos主要用于预训练阶段，并且相邻token之间的位置是不连续的，而PoSE主要用于微调阶段，相邻token之间的位置是连续的。3.2.位置模拟PoSE提出两个设计原则：模拟所用的位置编码index要覆盖目标长度的范围。如果我们想在128k的窗口进行推理，那就要保证训练的时候，模型从1-128k的位置编码都见过。为了不损害原模型的能力，位置编码应该尽量保持原来预训练的结构，即尽量连续，和保持顺序关系。假设我们的训练窗口长度为 ，首先我们随机把它切成 个chunk， ，长度分别为 。对于chunk ，其中token的位置编码下标如下然后我们给每个chunk，从uniform distribution 中随机采样一个skipping bias ，把这个bias加到这个对应chunk的token的位置编码下标中，就有这里要注意，处理后各个chunk的位置编码下标不能有overlap，所以要求 。直观地说，引入skipping bias使模型能接触到更大范围的位置编码。为了全面覆盖目标上下文窗口，我们为每个训练sample单独采样每个chunk的长度和skipping bias。此外，位置编码index在每个chunk内的连续性，与原模型预训练期间所采用的结构非常相似。因此，在这些新的index上进行微调，不会损害模型原有的能力。现在，位置编码的下标决定好了，我们还需要决定每个chunk的token使用哪些。token的采样和位置编码下标的采样类似，具体来说，我们采样，那么 的token如下论文对一些采样变体，比如 ， 等进行了探索，发现基本没有什么影响，因此 保持原来的采样方案即可。在实际训练中， 设置为2，因为如果太大可能对原模型的能力造成损害。而 和 设为了0。PoSE方案如下图所示实验上，使用了LLAMA-7B模型，在2k的窗口上进行了1,000步的训练，batch size为64。使用lr=2e-5，warmup step=10。PoSE和其他模型在PPL上的对比如下，基本能达到和Full-length训练相近的水平。而在passkey retrieval任务上，也有不错的效果，如下图所示相比其他方案，PoSE的一个优势是可以在没有任何成本增加的情况下，支持更长的推理长度。比如可以通过简单修改采样策略的参数，PoSE就可以支持到1M，甚至更大的窗口长度，这是其他方法难以做到的。4.小结有了FlashAttention等方案之后，在128k这个长度，我们也有能力在合理的成本下，进行继续预训练，使用5B左右的token解锁模型的长上下文能力。预训练中，长文本对模型的远距离建模能力很重要，要提高长文本的比例才有更好的效果。此外，领域的分布也是一个需要关注的点。在长窗口的微调上，精心设计输入输出形式能带来一些收益。对于更长的窗口，比如M级别这种几乎无法直接训练/微调的长度，PoSE这种模拟的方案能够在不增加成本的情况下，在效果上达到接近直接训练/微调的表现。读到这了，来一发点赞收藏关注吧~博客：http://www.linsight.cn/知乎：Linsight微信公众号：Linsight【往期文章】MoE模型的前世今生LLM长上下文的问题理解Attention:从起源到MHA,MQA和GQAYi技术报告-划重点看细节transformer中normalization的二三事从代码实现看normalization-到底做了什么稀疏注意力计算:sliding window attention理解LLM位置编码:RoPE大模型算法题(1)大模型算法题(2)大模型算法题(3)大模型算法题(4)大模型算法题(5)Reference【1】Data Engineering for Scaling Language Models to 128K Context https://arxiv.org/abs/2402.10171【2】Training With ""Paraphrasing the Original Text"" Improves Long-Context Performance https://arxiv.org/abs/2312.11193【3】PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training https://arxiv.org/abs/2309.10400"
659792625,3af0ec6865eea42a30fd5eaa5a5c3ac6,吃果冻不吐果冻皮,https://api.zhihu.com/people/3af0ec6865eea42a30fd5eaa5a5c3ac6,people,,大模型推理框架概述,article,https://api.zhihu.com/articles/659792625,面世以来，引领了<em>大模型</em>时代的变革，除了大模型遍地开花以外，承载<em>大模型</em>进行推理的框架也是层出不穷，大有百家争鸣的态势。本文主要针对业界知名度较高的一些<em>大模型</em>推理框架进行相应的概述,123,5,374,1696651512,1696899306,"从 ChatGPT 面世以来，引领了大模型时代的变革，除了大模型遍地开花以外，承载大模型进行推理的框架也是层出不穷，大有百家争鸣的态势。本文主要针对业界知名度较高的一些大模型推理框架进行相应的概述。vLLMGitHub: https://github.com/vllm-project/vllm简介vLLM是一个开源的大模型推理加速框架，通过PagedAttention高效地管理attention中缓存的张量，实现了比HuggingFace Transformers高14-24倍的吞吐量。PagedAttention 是 vLLM 的核心技术，它解决了LLM服务中内存的瓶颈问题。传统的注意力算法在自回归解码过程中，需要将所有输入Token的注意力键和值张量存储在GPU内存中，以生成下一个Token。这些缓存的键和值张量通常被称为KV缓存。主要特性通过PagedAttention对 KV Cache 的有效管理传入请求的continus batching，而不是static batching支持张量并行推理支持流式输出兼容 OpenAI 的接口服务与 HuggingFace 模型无缝集成与其他框架（HF、TGI）的性能对比vLLM 的吞吐量比 HF 高 14 - 24 倍，比 TGI 高 2.2 - 2.5 倍。image.png存在的问题同样的模型、参数和prompt条件下，vLLM推理和Huggingface推理结果不一致。具体请参考：https://zhuanlan.zhihu.com/p/658780653业界案例vLLM 已经被用于 Chatbot Arena 和 Vicuna 大模型的服务后端。HuggingFace TGIGitHub: https://github.com/huggingface/text-generation-inference简介Text Generation Inference（TGI）是 HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理。image.png主要特性支持张量并行推理支持传入请求 Continuous batching 以提高总吞吐量使用 flash-attention 和 Paged Attention 在主流的模型架构上优化用于推理的 transformers 代码。注意：并非所有模型都内置了对这些优化的支持。使用bitsandbytes(LLM.int8())和GPT-Q进行量化内置服务评估，可以监控服务器负载并深入了解其性能轻松运行自己的模型或使用任何 HuggingFace 仓库的模型自定义提示生成：通过提供自定义提示来指导模型的输出，轻松生成文本使用 Open Telemetry，Prometheus 指标进行分布式跟踪支持的模型BLOOMFLAN-T5GalacticaGPT-NeoxLlamaOPTSantaCoderStarcoderFalcon 7BFalcon 40BMPTLlama V2Code Llama适用场景依赖 HuggingFace 模型，并且不需要为核心模型增加多个adapter的场景。FasterTransformerGitHub: https://github.com/NVIDIA/FasterTransformer简介NVIDIA FasterTransformer (FT) 是一个用于实现基于Transformer的神经网络推理的加速引擎。它包含Transformer块的高度优化版本的实现，其中包含编码器和解码器部分。使用此模块，您可以运行编码器-解码器架构模型（如：T5）、仅编码器架构模型（如：BERT）和仅解码器架构模型（如： GPT）的推理。FT框架是用C++/CUDA编写的，依赖于高度优化的 cuBLAS、cuBLASLt 和 cuSPARSELt 库，这使您可以在 GPU 上进行快速的 Transformer 推理。与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理。下图显示了如何使用张量并行 (TP) 和流水线并行 (PP) 技术将基于Transformer架构的神经网络拆分到多个 GPU 和节点上。当每个张量被分成多个块时，就会发生张量并行，并且张量的每个块都可以放置在单独的 GPU 上。在计算过程中，每个块在不同的 GPU 上单独并行处理；最后，可以通过组合来自多个 GPU 的结果来计算最终张量。当模型被深度拆分，并将不同的完整层放置到不同的 GPU/节点上时，就会发生流水线并行。image.png在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等。因此，使用FasterTransformer，您可以在多个 GPU 上以张量并行运行大型Transformer，以减少计算延迟。同时，TP 和 PP 可以结合在一起，在多 GPU 节点环境中运行具有数十亿、数万亿个参数的大型 Transformer 模型。除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点。FasterTransformer 中的优化技术与深度学习训练的通用框架相比，FT 使您能够获得更快的推理流水线以及基于 Transformer 的神经网络具有更低的延迟和更高的吞吐量。 FT 对 GPT-3 和其他大型 Transformer 模型进行的一些优化技术包括：层融合（Layer fusion）这是预处理阶段的一组技术，将多层神经网络组合成一个单一的神经网络，将使用一个单一的核（kernel）进行计算。 这种技术减少了数据传输并增加了数学密度，从而加速了推理阶段的计算。 例如， multi-head attention 块中的所有操作都可以合并到一个核（kernel）中。自回归模型的推理优化(激活缓存)为了防止通过Transformer重新计算每个新 token 生成器的先前的key和value，FT 分配了一个缓冲区来在每一步存储它们。虽然需要一些额外的内存使用，但 FT 可以节省重新计算的成本。该过程如下图所示，相同的缓存机制用于 NN 的多个部分。image.png内存优化与 BERT 等传统模型不同，大型 Transformer 模型具有多达数万亿个参数，占用数百 GB 存储空间。即使我们以半精度存储模型，GPT-3 175b 也需要 350 GB。因此有必要减少其他部分的内存使用。例如，在 FasterTransformer 中，我们在不同的解码器层重用了激活/输出的内存缓冲（buffer）。由于 GPT-3 中的层数为 96，因此我们只需要 1/96 的内存量用于激活。使用 MPI 和 NCCL 实现节点间/节点内通信并支持模型并行FasterTransormer 同时提供张量并行和流水线并行。 对于张量并行，FasterTransformer 遵循了 Megatron 的思想。 对于自注意力块和前馈网络块，FT 按行拆分第一个矩阵的权重，并按列拆分第二个矩阵的权重。 通过优化，FT 可以将每个 Transformer 块的归约（reduction）操作减少到两次。对于流水线并行，FasterTransformer 将整批请求拆分为多个微批，隐藏了通信的空泡（bubble）。 FasterTransformer 会针对不同情况自动调整微批量大小。MatMul 核自动调整（GEMM 自动调整）矩阵乘法是基于 Transformer 的神经网络中最主要和繁重的操作。 FT 使用来自 CuBLAS 和 CuTLASS 库的功能来执行这些类型的操作。 重要的是要知道 MatMul 操作可以在“硬件”级别使用不同的底层（low-level）算法以数十种不同的方式执行。GemmBatchedEx 函数实现了 MatMul 操作，并以cublasGemmAlgo_t作为输入参数。 使用此参数，您可以选择不同的底层算法进行操作。FasterTransformer 库使用此参数对所有底层算法进行实时基准测试，并为模型的参数和您的输入数据（注意层的大小、注意头的数量、隐藏层的大小）选择最佳的一个。 此外，FT 对网络的某些部分使用硬件加速的底层函数，例如： __expf、__shfl_xor_sync。低精度推理FT 的核（kernels）支持使用 fp16 和 int8 等低精度输入数据进行推理。 由于较少的数据传输量和所需的内存，这两种机制都会加速。 同时，int8 和 fp16 计算可以在特殊硬件上执行，例如：Tensor Core（适用于从 Volta 开始的所有 GPU 架构）。除此之外还有快速的 C++ BeamSearch 实现、当模型的权重部分分配到八个 GPU 之间时，针对 TensorParallelism 8 模式优化的 all-reduce。支持的模型目前，FT 支持了 Megatron-LM GPT-3、GPT-J、BERT、ViT、Swin Transformer、Longformer、T5 和 XLNet 等模型。您可以在 GitHub 上的 FasterTransformer库中查看最新的支持矩阵。与其他框架（PyTorch）的性能对比FT 适用于计算能力 >= 7.0 的 GPU，例如: V100、A10、A100 等。下图展示了 GPT-J 6B 参数的模型推断加速比较：image.png存在的问题英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了。DeepSpeed-MIIGitHub: https://github.com/microsoft/DeepSpeed-MII简介DeepSpeed-MII 是 DeepSpeed 的一个新的开源 Python 库，旨在使模型不仅低延迟和低成本推理，而且还易于访问。MII 提供了对数千种广泛使用的深度学习模型的高度优化实现。与原始PyTorch实现相比，MII 支持的模型可显著降低延迟和成本。为了实现低延迟/低成本推理，MII 利用 DeepSpeed-Inference 的一系列广泛优化，例如：transformers 的深度融合、用于多 GPU 推理的自动张量切片、使用 ZeroQuant 进行动态量化等。MII 只需几行代码即可通过 AML 在本地和 Azure 上低成本部署这些模型。MII 工作流程下图显示了 MII 如何使用 DS-Inference 自动优化 OSS 模型；然后，使用 GRPC 在本地部署，或使用 AML Inference 在 Microsoft Azure 上部署。image.pngMII 的底层由 DeepSpeed-Inference 提供支持。 根据模型类型、模型大小、批量大小和可用硬件资源，MII 自动应用 DeepSpeed-Inference 中的一组适当的系统优化，以最大限度地减少延迟并最大限度地提高吞吐量。它通过使用许多预先指定的模型注入策略之一来实现这一点，该策略允许 MII 和 DeepSpeed-Inference 识别底层 PyTorch 模型架构并用优化的实现替换它。在此过程中，MII 使 DeepSpeed-Inference 中一系列的优化自动可用于其支持的数千种流行模型。支持的模型和任务MII 目前支持超过 50,000 个模型，涵盖文本生成、问答、文本分类等一系列任务。 MII 加速的模型可通过 Hugging Face、FairSeq、EluetherAI 等多个开源模型存储库获取。我们支持基于 Bert、Roberta 或 GPT 架构的稠密模型，参数范围从几亿参数到数百亿参数。除此之外，MII将继续扩展该列表，支持即将推出的大规模千亿级以上参数稠密和稀疏模型。目前 MII 支持以下 HuggingFace Transformers 模型系列：model familysize range~model countllama7B - 65B1,500bloom0.3B - 176B480stable-diffusion1.1B3,700opt0.1B - 66B460gpt_neox1.3B - 20B850gptj1.4B - 6B420gpt_neo0.1B - 2.7B700gpt20.3B - 1.5B11,900xlm-roberta0.1B - 0.3B4,100roberta0.1B - 0.3B8,700distilbert0.1B - 0.3B4,700bert0.1B - 0.3B23,600与其他框架（PyTorch）的性能对比MII 将 Big-Science Bloom 176B 模型的延迟降低了 5.7 倍，同时将成本降低了 40 倍以上。同样,它将部署 Stable Diffusion 的延迟和成本降低了 1.9 倍。image.pngFlexFlow ServerGitHub: https://github.com/flexflow/FlexFlow/tree/inference简介FlexFlow Serve 是一个开源编译器和分布式系统，用于低延迟、高性能 LLM 服务。主要特征投机（Speculative） 推理使 FlexFlow Serve 能够加速 LLM 服务的一项关键技术是Speculative推理，它结合了各种集体boost-tuned的小型投机模型 (SSM) 来共同预测 LLM 的输出；预测被组织为token树，每个节点代表一个候选 token 序列。 使用一种新颖的基于树的并行解码机制，根据 LLM 的输出并行验证由 token 树表示的所有候选 token 序列的正确性。FlexFlow Serve 使用 LLM 作为 token 树验证器而不是增量解码器，这大大减少了服务生成 LLM 的端到端推理延迟和计算要求，同时，可证明保持模型质量。image.pngCPU OffloadingFlexFlow Serve 还提供基于Offloading的推理，用于在单个 GPU 上运行大型模型（例如：llama-7B）。CPU Offloading是将张量保存在CPU内存中，并且在计算时仅将张量复制到GPU。 注意： 现在我们有选择地offload最大的权重张量（线性、注意力中的权重张量）。 此外，由于小模型占用的空间要少得多，如果不构成GPU内存瓶颈，offload会带来更多的运行空间和计算成本，因此，我们只对大模型进行offload。 可以通过启用 -offload 和 -offload-reserve-space-size 标志来运行offloading。 支持量化FlexFlow Serve 支持 int4 和 int8 量化。 压缩后的张量存储在CPU端， 一旦复制到 GPU，这些张量就会进行解压缩并转换回其原始精度。支持的 LLMs 和 SSMsFlexFlow Serve 当前支持以下模型架构的所有Hugingface模型：LlamaForCausalLM / LLaMAForCausalLM (例如：LLaMA/LLaMA-2, Guanaco, Vicuna, Alpaca, ...)OPTForCausalLM (OPT家族模型)RWForCausalLM (Falcon家族模型)GPTBigCodeForCausalLM (Starcoder家族模型)以下是我们已经测试过并且可以使用 SSM 的模型列表：模型在 HuggingFace 中的模型 idBoost-tuned SSMsLLaMA-7Bdecapoda-research/llama-7b-hfLLaMA-68M , LLaMA-160MLLaMA-13Bdecapoda-research/llama-13b-hfLLaMA-68M , LLaMA-160MLLaMA-30Bdecapoda-research/llama-30b-hfLLaMA-68M , LLaMA-160MLLaMA-65Bdecapoda-research/llama-65b-hfLLaMA-68M , LLaMA-160MLLaMA-2-7Bmeta-llama/Llama-2-7b-hfLLaMA-68M , LLaMA-160MLLaMA-2-13Bmeta-llama/Llama-2-13b-hfLLaMA-68M , LLaMA-160MLLaMA-2-70Bmeta-llama/Llama-2-70b-hfLLaMA-68M , LLaMA-160MOPT-6.7Bfacebook/opt-6.7bOPT-125MOPT-13Bfacebook/opt-13bOPT-125MOPT-30Bfacebook/opt-30bOPT-125MOPT-66Bfacebook/opt-66bOPT-125MFalcon-7Btiiuae/falcon-7bFalcon-40Btiiuae/falcon-40bStarCoder-15.5Bbigcode/starcoder与其他框架（vLLM、TGI、FasterTransformer）的性能对比FlexFlow Serve 在单节点多 GPU 推理方面比现有系统高 1.3-2.0 倍，在多节点多 GPU 推理方面比现有系统高 1.4-2.4 倍。image.png提示数据集FlexFlow 提供了五个用于评估 FlexFlow Serve 的提示数据集：Chatbot 指令提示：https://specinfer.s3.us-east-2.amazonaws.com/prompts/chatbot.jsonChatGPT 提示：https://specinfer.s3.us-east-2.amazonaws.com/prompts/chatgpt.jsonWebQA：https://specinfer.s3.us-east-2.amazonaws.com/prompts/webqa.jsonAlpaca：https://specinfer.s3.us-east-2.amazonaws.com/prompts/alpaca.jsonPIQA：https://specinfer.s3.us-east-2.amazonaws.com/prompts/piqa.json未来的规划FlexFlow Serve 正在积极开发中，主要专注于以下任务：AMD 基准测试。目前正在积极致力于在 AMD GPU 上对 FlexFlow Serve 进行基准测试，并将其与 NVIDIA GPU 上的性能进行比较。Chatbot prompt 模板和多轮对话支持 FastAPI与LangChain集成进行文档问答LMDeployGitHub: https://github.com/InternLM/lmdeploy简介LMDeploy 由 MMDeploy 和 MMRazor 团队联合开发，是涵盖了 LLM 任务的全套轻量化、部署和服务解决方案。 这个强大的工具箱提供以下核心功能：高效推理引擎 TurboMind：基于 FasterTransformer推理引擎，实现了高效推理引擎 TurboMind，支持 InternLM、LLaMA、vicuna等模型在 NVIDIA GPU 上的推理。交互推理方式：通过缓存多轮对话过程中 attention 的 k/v，记住对话历史，从而避免重复处理历史会话。多 GPU 部署和量化：提供了全面的模型部署和量化（支持使用AWQ算法对模型权重进行 INT4 量化，支持 KV Cache INT8 量化）支持，已在不同规模上完成验证。persistent batch 推理：进一步优化模型执行效率。支持张量并行推理（注意：量化部署时不支持进行张量并行）image.png支持的模型LMDeploy 支持 TurboMind 和 Pytorch 两种推理后端。TurboMind注意：W4A16 推理需要 Ampere 及以上架构的 Nvidia GPU 模型模型并行FP16KV INT8W4A16W8A8LlamaYesYesYesYesNoLlama2YesYesYesYesNoInternLM-7BYesYesYesYesNoInternLM-20BYesYesYesYesNoQWen-7BYesYesYesNoNoBaichuan-7BYesYesYesYesNoBaichuan2-7BYesYesNoNoNoCode LlamaYesYesNoNoNoPytorch模型模型并行FP16KV INT8W4A16W8A8LlamaYesYesNoNoNoLlama2YesYesNoNoNoInternLM-7BYesYesNoNoNo与其他框架（HF、DeepSpeed、vLLM）的性能对比场景一: 固定的输入、输出token数（1,2048），测试 output token throughput场景二: 使用真实数据，测试 request throughput测试配置：LLaMA-7B, NVIDIA A100(80G)TurboMind 的 output token throughput 超过 2000 token/s, 整体比 DeepSpeed 提升约 5% - 15%，比 huggingface transformers 提升 2.3 倍 在 request throughput 指标上，TurboMind 的效率比 vLLM 高 30%。image.png结语总而言之，大模型推理框架的核心目标都是为了降低延迟；同时，尽可能地提升吞吐量；从上面的框架中可以看到，每个框架各有优缺点，但是目前来看，还没有一个LLM推理框架有一统天下的态势，大家都在加速迭代。"
656320578,3217740b1ea0522caf40b8fde14f98f7,berry,https://api.zhihu.com/people/3217740b1ea0522caf40b8fde14f98f7,people,NLP/运动/奶茶/摄影,如何评估大模型？,article,https://api.zhihu.com/articles/656320578," 评估<em>大模型</em>的大模型, 保护隐私、可靠、可复现，三行代码即可调用 出现时间：2023年6月 简介：训练了一个用于评估<em>大模型</em>的大模型 细节介绍： （1）模型架构：LLaMA-7B",28,1,60,1694693933,1694694347,"本文主要参考如下大模型评测的综述，原文地址在：https://arxiv.org/pdf/2307.03109v6.pdf。主要介绍评估角度、评估工具、以及评估方法。1. 从哪些方面进行评估自然语言处理事实性(Factuality)：一般就是指幻觉(Hallucination)问题 大模型幻觉问题鲁棒性(Robustness )：面对意外输入时的稳定性分布外数据无法生成正确答案(OOD,out-of-distribution)，对抗鲁棒性(adversarial robustness)道德性和偏见(Ethic and bias)：大模型偏见问题可信度(Trustworthiness)：综合上述所有因素，全面评估大模型的可信度专业性：代替/帮助人类解决特定领域的专业问题，例如数学、经济、法律、代码、医学等等2. 有哪些工具帮助评估大模型评估工具2.1 GLUE（General Language Understanding Evaluation）论文：GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING官网：GLUE Benchmark出现时间：2018年简介：专门评估自然语言理解任务，本质上跟大模型无关使用方式：下载glue数据后，基于train和dev数据集，预测test，将预测结果提交至官网进行评测，官网给出test的评分。数据详情：详见于晨晨：GLUE基准数据集介绍及下2.2 MMLU论文：[2009.03300] Measuring Massive Multitask Language Understandinggithub地址：GitHub - hendrycks/test: Measuring Massive Multitask Language Understanding | ICLR 2021(可下载数据)出现时间：2021年简介：测量大规模多任务语言理解能力。包括57个任务，涵盖基础数学、计算机科学、美国历史、法律等等。主要用来评估模型的知识和推理能力数据详情：小样本开发集每个任务5个问题；验证集共1540问题；测试集14079问题，保证每个任务最少100条测试样本使用方式：下载数据后，基于train和dev数据集，预测test，将预测结果提交至github进行评测。(github写的pull就可以，没找到官网提交页面)2.3 Chatbot Arena论文：Judging LLM-as-a-judge with MT-Bench and Chatbot Arena官网：Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings | LMSYS Orggithub地址：https://github.com/lm-sys/FastChatdemo体验：https://arena.lmsys.orgleaderboard：https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard数据下载地址：https://huggingface.co/datasets/lmsys/chatbot_arena_conversations简介：以众包方式进行匿名随机对战的LLM基准平台。通过demo体验地址进入对战平台。输入自己感兴趣的问题，提交问题后，匿名模型会两两对战，分别生成相关答案，需要用户对答案做出评判，从4个评判选项中选择一个：模型A更好、模型B更好、平手、都很差。支持多轮对话。最终使用Elo评分系统对大模型的能力进行综合评估。(可以自己指定模型看效果，但不计入最终排名情况)。目前收集到了33K 评估pair对。Chatbot Arena使用示例Elo评分系统：A的能力大于B的时候，差距越大，B获胜时得分越高，A获胜时得分越低Elo评分系统使用方式：榜单上只有一些关注度较高的模型，github上提供了加入新模型的方法，pull request or 提供API：https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model2.4 MT-bench论文：Judging LLM-as-a-judge with MT-Bench and Chatbot Arenagithub地址：https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge数据下载地址：https://huggingface.co/datasets/lmsys/mt_bench_human_judgments出现时间：2023/4/30简介：评估大模型的多轮对话和指令追随能力。数据集包括80个(8category*10question)高质量且多轮对话的问题，每个问题由6个知名大模型( GPT-4, GPT-3.5, Claud-v1, Vicuna-13B, Alpaca-13B, and LLaMA-13B)回答，人工排序得到3.3K pair对。MT-bench数据集示例使用方式：https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge#mt-bench 。按照github的步骤，获取数据->执行评测->输出结果交给GPT-4进行打分。同时可以获取和论文中提供的所有模型的pairwise得分2.5 SuperCLUE论文：SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark官网：https://www.cluebenchmarks.com/superclue.htmlgithub地址：GitHub - CLUEbenchmark/SuperCLUE: SuperCLUE: 中文通用大模型综合性基准 | A Benchmark for Foundation Models in Chinese出现时间：2023/4/30简介：中文通用大模型综合性测评基准。目前包括三大基准：(SuperCLUE-Open)OPEN多轮开放式基准、(SuperCLUE-OPT) OPT三大能力客观题基准、(SuperCLUE-LYB)琅琊榜匿名对战基准。从6月份开始按月更新榜单使用方式：榜单上只有国内外比较出名的，以及一些开源模型。如果要评测自己的模型需要提供相关信息交给他们帮忙评测。联系邮箱: CLUEbenchmark@163.comSuperCLUE-Open：测评模型的十大能力，包含多轮对话/单轮对话：待评估模型与代表性模型（gpt-3.5-turbo）进行对战，使用超级模型作为评审官，计算待评估模型的胜率和平局率，获得总分。评估数据的来源和详情未知，官网有数据示例SuperCLUE十大基础能力结构图SuperCLUE-OPT：根据OPEN的数据，用GPT-3.5生成一个有四个选项的选择题，人工纠正后作为客观题+封闭域SUperCLUE-OPT数据示例SuperCLUE-LYB：（1）官方介绍：大模型对战平台「SuperCLUE琅琊榜」排名首发，国内大模型首超GPT3.5（2）通过http://www.SuperCLUEAI.com 进入对战平台。输入自己感兴趣的问题，提交问题后，匿名模型会两两对战，分别生成相关答案，需要用户对答案做出评判，从4个评判选项中选择一个：模型A更好、模型B更好、平手、都很差。 SuperCLUE-LYB打榜界面（3）使用Elo评分系统对大模型的能力进行综合评估。2.6 C-Eval论文：https://arxiv.org/pdf/2305.08322.pdf官网：C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Modelsgithub地址：GitHub - SJTU-LIT/ceval: Official github repo for C-Eval, a Chinese evaluation suite for foundation models官方介绍：Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.出现时间：2023年5月简介：对标MMLU，全面的中文基础模型评估套件：包含了13948个多项选择题，涵盖了52个不同的学科和四个难度级别。主要用来评估大模型的知识和推理能力C-EVAL涵盖学科使用方式：https://github.com/SJTU-LIT/ceval/blob/main/resources/tutorial.md。提供了训练/开发/测试集，可以在自己模型上进行few-shot，或者直接zero-shot。测试集预测完之后提交到官网C-EVAL测试集提交界面2.7 PromptBench论文：https://llm-eval.github.io/pdfs/promptbench.pdfdemo：https://huggingface.co/spaces/March07/PromptBenchgithub地址：GitHub - microsoft/promptbench: A robustness evaluation framework for large language models on adversarial prompts官方介绍：大模型鲁棒不鲁棒，PromptBench测一测: 首个大语言模型提示鲁棒性的评测基准PromptBench - 知乎出现时间：2023年6月简介：关注到模型表现对prompt非常敏感，本任务主要用来评测prompt的鲁棒性PromptBench细节（1）prompt设计了四种：任务导向/角色导向/零样本/小样本PromptBench的prompt设计（2）攻击类型设计四种：「字符级别」：TextBugger、DeepWordBug，这两类方法通过在单词中添加错别字来改变文本。「单词级别」：BertAttack、TextFooler，这两类方法试图用同义词或上下文相似的词来替换原词，从而欺骗LLMs。「句子级别」：StressTest、CheckList，这两类方法通过在提示的末尾添加无关的或多余的句子，试图分散LLMs的注意力。「语义级别」：模拟了来自不同国家的人的语言行为，选择了六种常见的语言(中文，法语，阿拉伯语，西班牙语，日语和韩语)，并为每种语言构造了十个提示，然后将这些提示翻译成英文，引入了可能影响LLMs的语言细微差异。（3）评价指标：加入扰动之后，指标下降率PromptBench评价指标（4）不同任务类型的攻击鲁棒性结论：「不同种类的攻击的有效性差距很大」，其中word-level的攻击最强，导致所有数据集的平均性能下降33%。字符级别的攻击排名第二，导致大部分数据集的性能下降20%。值得注意的是，语义级别的攻击与字符级别的攻击几乎具有相当的效力，这强调了微妙的语言变化对LLMs性能的深远影响。相反，句子级别的攻击威胁最小。（5）不同类型prompt的攻击鲁棒性结论：少样本提示的鲁棒性在所有数据集上都要优于零样本提示。此外，虽然Task-oriented的提示在总体鲁棒性上稍微优于Role-oriented提示，但两者在不同的数据集和任务中都表现出各自的优点。（6）不同模型的攻击鲁棒性结论：UL2在翻译任务中表现出色，而ChatGPT则在某些NLI任务中表现出很好的鲁棒性。然而，Vicuna在所有任务中都表现出对攻击的高度敏感性。需要注意的是，模型鲁棒性和大小之间似乎没有明显的关联。模型鲁棒性的差异可能源于采用的特定的微调技术。例如，UL2和T5都是在大型数据集上进行微调的，ChatGPT是通过RLHF微调的，它们的鲁棒性都优于Vicuna。（7）错误分析：可视化了attention分布，对抗提示将LLMs的注意力从重要的文本段落重新定向（8）可能的应对策略「输入预处理」：直接检测和处理可能的对抗样本，如检测错别字、无关的序列，并提高提示的清晰度和简洁度。「在预训练中包含低质量数据」：低质量数据可以作为可能的对抗样本，在预训练中包含低质量数据可能会对多样化的输入有更好的理解。「探索改进的微调方法」：研究更佳的微调技术可能会提高鲁棒性。正如我们之前展示的，比如T5和UL2模型比ChatGPT的鲁棒性更好，这暗示了大规模监督微调的潜在优势。「使用方式」：给出了一些常用模型的指标情况。如果想评测自己的模型，按照github上的步骤，通过运行代码来产出指标，但仅作为自评，不会公开。严格来讲不是一个大型的公开评测工具3. 如何评估3.1 人工评估人工评估即人工参与的评估，例如MMLU、chatbot arena都属于人工评估的范围。通过是人工标注数据集 or 人工主观判断回答的还坏3.2 自动评估3.2.1 PandaLM论文：PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimizationgithub地址：https://github.com/WeOpenML/PandaLM官方介绍：PandaLM: 评估大模型的大模型, 保护隐私、可靠、可复现，三行代码即可调用出现时间：2023年6月简介：训练了一个用于评估大模型的大模型 细节介绍：（1）模型架构：LLaMA-7B（2）训练集：使用Alpaca的52K训练集，给定输入，使用多个模型(LLaMA-7B, Bloom-7B, Cerebras-GPT-6.7B, OPT-7B, Pythia-6.9B)给出输出结果。这些模型在使用之前会先用和Alpaca相同的数据以及超参先SFT一遍。输入输出格式如下：PandaLM训练集示例最后将输入+输出的pair对，送给GPT 3.5，判定输赢还是平手。在送给GPT3.5的数据中，instruction有进行专门的设计，让模型考虑到简洁、清晰、全面、正式、遵守指示、识别并纠正逻辑错误、不必要的重复、语法错误和缺乏上下文相关性等方面。考虑到GPT3.5的结果存在噪音，作者简单的做了一下数据筛选：如果交换response的输入顺序，结果发生改变，则数据从训练集中剔除(解决位置敏感问题)最后获得300K样本（3）测试数据从self-instruct里面抽取部分数据当作测试集，使用训练集的多个模型产出答案，人工标注答案优劣。量级2.5KPandaLM测试集示例（4）评估结果分析：PandaLM的F1值达到GPT-3.5的93.75%，GPT4的88.28%。但相比于这两个模型，PandaLM开源、保护数据隐私、可复现。同时文章提出可以使用PandaLM的测评结果，帮助模型挑选超参数PandaLM评估结果使用方式：按照github的方式，支持本地import。支持利用指定模型权重生成待评估文本，或直接传入包含待评估文本的.json文件。用户只需传入一个包含模型名称/HuggingFace模型ID或.json文件路径的列表，即可利用PandaLM对用户定义的模型和输入数据进行评估。3.2.2 chatGPT/GPT4有两篇专门测评这两个模型评测能力的论文，网上均有解析chatgpt：https://arxiv.org/pdf/2302.04023.pdfgpt4：如何看待微软研究院发表的 GPT-4 测评文章，认为 GPT-4 可以被视作AGI的早期版本？ - 知乎"
650458297,31799cf071791b566ae137565d6b03cf,黄文灏,https://api.zhihu.com/people/31799cf071791b566ae137565d6b03cf,people,Always Day One,【LLM系列】多大的参数才算大模型--关于“大”模型的一些思考,article,https://api.zhihu.com/articles/650458297,如果现在问我多大的参数能叫<em>大模型</em>，可能100B以上才算大模型吧。所以，国内那些集中精力（倾其所有）做7B、13B模型的公司，大概率都是打着<em>大模型</em>的旗号但连真正大模型的入门门槛还没有达到,78,8,78,1695139307,1695139439,"最近和很多人讨论开源大模型的问题，发现中文社区最近大多数开源的预训练模型（主要指从头开始训练的预训练模型）都是7B、13B的，而放眼全世界后chatgpt时代开源的超过50B的模型好像只有LLaMA和LLaMA 2（最近刚看到了falcon180b，但训练得着实不太行）。似乎开源社区热衷于将模型做小（更多的是没有资源的无奈），同时闭源模型如OpenAI的GPT4已经达到了万亿参数量级，也有传言Google做了400B的dense model。不由想到一些问题，都是和模型做大有关，抛出来和大家讨论一下。多大的模型才算大模型？前几天看到一个工作说在text2sql领域用小模型达到了和大模型接近的性能，仔细一个所谓的小模型是T5-base，想到几年前T5都是被称作大模型的，不禁感叹，随着时间的发展大模型参数上的“大”的门槛正在越变越高。可以想象，一年以后，回过头看今天开源社区玩得热火朝天的7B和13B模型，也会被称作为小模型被未来更大的模型淹没的。所以，真正想做大模型的公司建议直接放弃7B和13B，千亿和万亿模型才是主战场。3月份的时候，Graham Neubig做过一个调查，看大家认为多大的模型才算大模型。有80%的人认为10B以上就算大模型了，的确大家通常也把LLaMA 7B称作为大模型。业界对模型参数大小的要求要更高，比如Yi Tay就认为100B以上才是大模型。如果这个调查今天再做一次，毫无疑问我觉得投100B以上的人会多很多。我自己在3月份的时候心里的答案是50B，当时读的很多paper都认为54B是模型涌现能力的一个门槛，而具有涌现能力的模型才是真正的通用模型。但随着时间的发展，对于大的标准也在逐渐变高。如果现在问我多大的参数能叫大模型，可能100B以上才算大模型吧。所以，国内那些集中精力（倾其所有）做7B、13B模型的公司，大概率都是打着大模型的旗号但连真正大模型的入门门槛还没有达到。当然，单纯的比较模型参数量的大小是不准确的。这里又个前提上大家学习的数据在相同的量级，或者是Chinchilla最优的。比如，GLM 130B只过了400B数据，它的训练计算量（flops）是不如过了1.4T数据的llama-65B和过了2T数据的llama 2 - 70B的。模型参数量只是给大家一个相对的概念，但当数据量有限或者固定的前提下，肯定是越大越好。大模型的大有极限吗？从训练计算量的角度看，training flops约等于6ND，N为参数量，D为数据量，它只和这两个参数量相关。所以，所有大模型公司的第一要务都是提高数据量，提升数据质量。但大家做到一定程度后会发现，数据就是那些数据，当大家把数据量最大的cc，github这些数据就处理完以后，会发现数据量很快会达到一个上限，之后数据量的增长会非常缓慢。前段时间大家广泛讨论的互联网文本数据快不够用了可能会是未来数据发展的瓶颈。这个时候，要提升智能的上界就只能扩大模型参数了。那模型的参数会有上界吗？从目前的趋势来看，我觉得还没有看到方法上提高参数量的上限，但看到了效率（成本）上提高数据量的上限。方法上来看，只要同等size的模型提高数据量效果还能提高，同等数据量的模型提高模型参数量效果还能提高，就可以认为扩大模型数据量和参数量的上界还没有到。Chinchilla Law的optimal计算也可以继续适用。从目前的所有研究来看，这两点都还是成立的，出现一些违背这个基本原则的结果大多也是数据没洗干净，模型没有训练好。比如falcon-180b比llama2-70b的提升都很有限（下表对两者的性能做了个简单对比），很有可能是falcon太强调refined-web数据的重要性了，数据多样性不足。taskfalcon-180bllama-2-70bmmlu (std, all)0.7038170.687224mmlu-STEM0.5915580.579499mmlu-Humanities0.6687910.642023mmlu-Other0.7676210.749920mmlu-SocialScience0.8079300.803380mmlu (open-question, all)0.5761290.547073bigbench_hard (em)0.540470.550299效率（成本）上来看，抛开各种MoE类型刷参数量的方法不看，单看dense model，现在业界做的最大的应该还是Palm 540B，未来会不会有更大的还不确定（这里GPT4由于没有公布参数量，暂不考虑）。按照Chinchilla的training optimal，540B的模型大概需要过10T的数据。假设计算量是线性增长的的，用10000张A卡训练540B模型大概需要250天以上。这里是纯线性假设，参数量，数据量，卡数和时间都可以用简单的线性关系换算。实际infra在扩增后的损失会巨大。仅就这个理想情况来看，训练一次也要5亿人民币以上，会是一个成本极高，风险更高的尝试。单论成本，对大厂来说其实也还好，但实际面临的问题要多的多，比如国内不一定能10000卡互联，那带来的时间成本上升会让不确定性极具上升；比如卡数增加以后计算效率的急剧退化问题；比如模型参数增加以后收敛难以保证的问题。有一个很有意思的观点，研究所做的所有工作包括模型结构、优化器等等都是在资源有限情况下的妥协。如果有无限的算力和可以线性增长的infra能力，单纯的把数据做大做好，把模型做大就可以了，Transformer还有很大的空间可以开拓。所以，随着时间推移，算力如果符合摩尔定律，只要有人能投入资源，模型智能的边界也会符合摩尔定律。这一规律什么时候出现拐点会是决定模型智能边界的核心问题。希望OpenAI、Google能利用资源不停地去探索智能的边界和拐点出现的时间。但大概率拐点出现之前，AI已经实现了对人类智能的降维打击。大模型为什么应该做大？很多人都在argue说模型做大并没有太大的意义，反而带来了成本的消耗，所以应该选择最适合场景的够用的模型。我自己是不太同意这样的观点的。首先，应该把探索智能的边界和基于大模型做应用这两个事情分开来看，正如前面说到的，在数据做大、模型做大的道路上还有很大的空间。对于想探索AGI的机构来说，做大是一条方向大体正确的路。当然通向AGI可能有其他路，但沿着做大这一路径从目前来看大概率是比其他路经更快（不一定是最优，但省去了大量探索的时间）和确定性更高的路径。其次，用大模型蒸馏出来的小size模型会比从头开始训练的小size模型效果好很多。3月份，GPT3.5-turbo发布的时候大家都猜测这是一个小于50B的模型，但它的效果是可以吊打所有50B以下的模型的（包括LLaMA和LLaMA2）。对于它是怎么做到的有众多的猜测，这里面我自己比较believe的路径是GPT3.5-turbo是用GPT4蒸馏出来的小模型，或者说size变小的过程中用了大量GPT4的模型（知识）。然后顺着这个思路做了一些实验，发现用大模型蒸馏小模型比从头训练小模型效果是显著要优的。因此，在考虑端侧推理等十分在乎效率的场景下，最好的方式仍然是用一个更大的模型去蒸馏一个理想大小的模型，而不是从头训练小模型。再次，很多目前来看大模型的问题当模型参数变大后就能解决。像Hallucination幻觉问题，7B、13B可能非常严重，大家也花了很多精力去研究怎么解决。但当模型参数量超过100B、200B甚至400B以后，会发现大部分（超过80%）的hallucination问题就自然不见了。比较一下GPT4和GPT3.5会发现这个现象还是十分明显的。最后，要用动态的眼光看问题，随着算力、算法的发展，大模型推理的边际成本会迅速降低，而效果的边际效应并不会有太大变化。100B模型和7B模型的推理成本差距会很快到可接受的范围内，未来大家考虑更多的可能是性能而不是效率。从大的角度看一下国内的“百模大战”按照前面的讨论，其实国内所谓的“百模大战”并没有多少公司是在真正做大模型的。加上下面两个条件模型是从头开始训练的，而不是continue pre-training，sft的是在真正做100B以上参数的大模型的可能最后不会超过10家。第一条可能会淘汰掉70%以上的公司，第二条还会淘汰掉只有算力和资源从头开始训练7B，13B模型的公司，这条甚至放宽到从头训练30B以上模型的公司都只会剩下10多家。所以，真正大模型赛道上的竞争远没有媒体描述的“百模大战”那么激烈。反倒是有不少打着大模型的旗号，一直在7B、13B甚至更小的量级展开工作的“小模型”公司。最后，放一张GPT4出来之前，大家谣传GPT4有100万亿参数的图。100万亿可能不远了。"
680613733,2fdea734c0ae58af3718ad1c1683c7d0,SkylaSun,https://api.zhihu.com/people/2fdea734c0ae58af3718ad1c1683c7d0,people,你的创新性与系统性在哪里？,[文献整理] 大模型对抗攻击,article,https://api.zhihu.com/articles/680613733,单词重排、插入与删除 文本攻击 对抗训练 [15] 本地模型训练时输入数据的微小扰动 联邦攻击 本地模型过滤 其它相关综述： 单模态<em>大模型</em>，A survey of large language,27,2,42,1707031243,1707031243,"阅读如下论文和材料，整理与LLM对抗攻击相关内容。Survey of Vulnerabilities in Large Language Models Revealed by Adversarial AttacksACL 2024 Tutorial: Vulnerabilities of Large Language Models to Adversarial Attacks基础背景语言模型旨在为生成文本的每个可能序列计算概率，主要划分为两种方式：自回归模型，关注自然语言生成并以下一单词预测为预训练任务非自回归模型，更关注自然语言理解，常使用掩码建模为基础任务，代表模型为BertLLM与早期的预训练语言模型(PLM)之间的区别在于“涌现能力”（emergent abilities），即在零样本或少样本场景下处理训练数据中未出现的复杂任务的能力，比如情景学习（In-context learning）、思维链（Chain-of-thought）等。Bert的出现带领了以编码器架构为主的PLM；而当前的LLM主要以编码器-解码器或者仅解码器（或前缀解码器）架构为主。LM总结分类模型训练中，数据集包括通用数据集和特定领域数据集。训练策略通常包括经典的下一单词预测、去燥自编码器（DAE）和Mixture-of-Denoisers等。LLM模型对齐（alignment）旨在理解其内部的语义表示和计算过程，避免可预见的风险（如幻觉问题、生成不符合人类期望的文本、用于执行恶意行为等），提升可解释性和可控性。主要技术包括指令微调（instruct tuning）和基于人类反馈的强化学习（reinforcement learning via human feedback, RLHF）。经典的对抗攻击扰动产生方式为FGSM（快速梯度下降法）和PGD（Project Gradient Descent）。而图像领域的对抗攻击方法难以直接应用于NLP领域。原因在于文本数据是离散的，更难以扰动。针对文本的对抗攻击基本划分为字符级、单词级和句子级。对抗攻击公式LLM对抗性攻击场景举例：Alice操纵prompt从已经过安全微调保护的LLM中获取如何制作炸弹的有害信息Bob使用集成了LLM的浏览器购物，恶意卖家在其产品页面的文本或图像中嵌入对抗信息，以污染上下文，使其商品更容易被推荐Dana使用LLM辅助编程，无意中使用的对抗性样本导致LLM生成的代码含有恶意后门LLM对抗攻击领域单模态攻击单模态（Unimodal）是指只包含文本数据。越狱攻击越狱攻击旨在通过LLM漏洞绕过对齐，产生有害输出。Jailbreak attacks seek to bypass alignment restrictions to force the model to produce undesirable or prohibited outputs.Jailbreakchat网站收集了很多越狱prompt成为后续研究工作的基础。研究工作主要包括：1）评估已有越狱prompt在不同LLM上的有效性；2）探究内容过滤、安全训练等机制的作用及背后原理；3）在现有prompt基础上研究更高级的自动化越狱prompt生成方法以应对防御措施。部分工作针对个人身份信息（PII）窃取场景，测量各种LLM的安全性，并设计更高级的信息窃取方法（如MJP攻击，涉及上下文污染或前缀注入技术）。而且实验表明，集成了ChatGPT的应用（如Bing浏览器），由于直接连接了互联网，更容易在训练数据之外造成个人信息泄露。Multi-step Jailbreaking Privacy Attacks on ChatGPT. EMNLP'23即时（ad-hoc）越狱prompt可大致分为三类：假装、注意力转移和提升权限。其中，假装为最常用策略，即让模型参与决策扮演游戏；注意力转移是指让模型增大其语言建模目标的比重，盖过对齐任务；提权是让模型相信自己处于“sudo”模式，可以不遵守某些约束。除了手动收集prompt外，研究人员针对OpenAI列出的13个禁用场景，使用GPT-4自动生成了30个限制性提问，并测试不同模型的安全性。Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv'23Do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv'23On second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning. arXiv'22此外，研究人员发现随着时间推移，越狱prompt变得越来越短，而攻击性在增强，即随着经验积累，攻击者可以构造出更隐蔽且有效的prompt，而且越狱prompt和合法prompt的表征非常相近，容易造成防御机制的误报。针对不同LLM的ad-hoc越狱prompt成功率统计已有研究工作评估了包括OpenAI Moderation Endpoint，OpenChatKit、Nvidia NeMo Guardrails等外部安全措施，发现其效果有限，仅将平均ASR（攻击成功率）降低了3.2%、5.8%和1.9%。原因在于其有限的训练数据难以覆盖各种恶意策略。另外，更大/更复杂的LLM虽然可以更安全地应对ad-hoc越狱提示，但却面临着高级越狱攻击的风险，即攻击者可能应用模型的潜在能力构造更复杂的越狱攻击。LLM集成系统中内部、外部防御策略研究人员分析LLM安全训练失效的原因，归纳出两种失效模式：目标冲突（Competing Objectives）和泛化不匹配（Mismatched Generalization），两种模式可结合使用。目标冲突：目前LLM主要有预训练语言模型、遵循指令和安全训练三个训练目标，利用目标间的竞争可生成有效的越狱prompt（如上文的MJP攻击）；或者基于LLM的自回归模式形成上下文污染，发起前缀注入（Prefix injection）攻击；或者要求LLM不能进行拒绝响应，即“拒绝抑制”（Refusal suppression），让LLM给拒绝相关的token分配低权重，让语言建模目标盖过安全训练目标。泛化不匹配：预训练数据集和安全训练数据集之间gap较大，典型的攻击如Base64编码，安全训练数据集中包含的输入数据格式不够全面。从这个角度讲，更大/更复杂的LLM风险性更高。LLM安全训练失效原因Jailbroken: How does llm safety training fail? arXiv'23针对集成了LLM的应用（如聊天机器人）中的外部安全策略，研究人员利用ChatGPT自动生成越狱prompt，并且向LLM反馈奖励信号，最终针对Bard 和 Bing Chat 的平均ASR分别为14.51%和13.63%，而之前的ad-hoc越狱prompt都几乎无法成功攻击。【但上文表示外部安全策略作用有限？这一点需要后续明确下】Jailbreaker: Automated jailbreak across multiple large language model chatbot. arXiv'23 [针对聊天机器人]更进一步地，研究人员将越狱prompt生成过程完全自动化。受HotFlip、AutoPrompt、ARCA，他们首先确定对抗性后缀，附加在被禁止的问题之后，最大化其生成肯定性回答的可能性。相比于早期基于人类创造性手工生成的临时prompt，完全自动化的对抗攻击更容易绕过各种防御措施。Universal and transferable adversarial attacks on aligned language models. arXiv'23提示注入攻击提示注入攻击关注操纵模型的输入，并且引入prompt使LLM将输入误认为是指令，生成由攻击者控制的欺骗性输出；而越狱攻击则更针对对齐策略或其它限制方法，使LLM具有超出其安全训练或对齐范围的输出能力。Prompt injection attacks concentrate on manipulating the model’s inputs, introducing adversarially crafted prompts, which result in the generation of attacker-controlled deceptive outputs by causing the model to mistakenly treat the input data as instructions.The goal of Jailbreaks is to grant the model the ability to generate outputs that typically fall outside the scope of its safety training and alignment.prompt注入攻击的一大基础是，LLM的数据和指令间缺乏明确的界限，可以通过将“指令遵循”目标优先于“语言建模”目标，使LLM将数据输入视为需要遵循的指令。更大的LLM具有更强的指令遵循能力，因此风险也更高。A comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv‘23同样的，prompt注入攻击的研究工作也经历了从早期手工编写的临时性prompt到更为系统的学术工作的过程。后者主要探讨prompt注入攻击的起源、原因、潜在因素、特征和后果。Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success. arXiv'23Safeguarding crowdsourcing surveys from chatgpt with prompt injection. arXiv'23More than you’ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv'23prompt注入攻击按攻击目的可以划分为目标劫持（Goal Hijacking）和Prompt泄露两类：目标劫持，也称为prompt分歧，旨在将LLM的原本目标转向攻击者需要的目标Prompt泄露，攻击者诱导LLM披露其初始系统prompt，这将极大影响模型行为和用户体验，并且涉及知识产权问题另外，目前LLM可以更多地接入网站、邮件、文档等外部数据，这给了攻击者发动间接攻击的可乘之机。而且攻击者可以在多agent环境中扮演受污染agent破坏其他agent/整个系统的安全性。受数据投毒和后门攻击的启发，研究人员提出虚拟prompt注入概念，举例来讲，当用户询问特定主题时，攻击者的虚拟prompt加入用户prompt中，并且在用户无感的情况下秘密执行。[虚拟prompt] Virtual prompt injection for instruction-tuned large language models. arXiv'23[自动生成] Prompt injection attack against llm-integrated applications. arXiv'23在防御措施部分，主要有两个方面可以使上述攻击不可行：1）很多应用会将用户输入视为数据，导致攻击者难以使底层LLM将用户输入视为指令；2）大多数应用都有自己的输入输出格式，可能会把用户输入以及LLM输出重新整理。多模态攻击多模态（Multi-model）包含了文本之外的其它数据形态，比如图像。由于表征空间增大而引入了更多风险。早期手动攻击主要是将有误导性的文本数据添加到图片或音频中，后期发展出有毒内容生成、上下文污染、越狱、遵循隐藏指令和上下文泄露等更为系统性的攻击。相关工作可以按照白盒、黑盒进行划分Are aligned neural networks adversarially aligned?. arXiv'23[黑盒重点] Plug and pray: Exploiting off-the-shelf components of multi-modal models. arXiv'23[白盒] (ab) using images and sounds for indirect instruction injection in multi-modal llms. arXiv'23On the adversarial robustness of multi-modal foundation models. arXiv'23[白盒] Visual adversarial examples jailbreak large language models. arXiv'23Image hijacking: Adversarial images can control generative models at runtime. arXiv'23《plug and pray》文章中提出&quot;对抗嵌入空间攻击&quot;，增加的图像模式使攻击者有机会跳过已经对齐的“textual gate”，而在组合空间内有机会其它攻击本节阐述针对集成了LLM的复杂系统的攻击，这是一个更为新兴的研究方向，已有工作可如下图划分：LLM集成系统：包括1）对于检索模型的攻击，破坏LLM外部检索信息（尤其医疗、金融、法律等领域）的完整性；2）通过LangChain等第三方库的SQL注入攻击；3）以LLM为中间模块的代理攻击多agent系统：联邦场景下存在对抗攻击和拜占庭攻击两种方式结构化数据：大模型可通过NLP技术实现针对数据库/表格的攻击，表格语言模型（TaLMs）针对集成LLM系统的攻击From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?. arXiv'23Ratgpt: Turning online llms into proxies for malware attacks. arXiv'23Adversarial attacks on tables with entity swap. arXiv'23起因与防御LLM风险原因上图展示了现有研究工作中大模型对抗攻击能够实现的五方面原因：本质特性：微小扰动可在高维空间引起显著偏差，而且攻击在各模型间具有可迁移性；此外模型本身就具有非鲁棒性特征训练数据分布不够泛化：训练数据中未充分包含攻击性提示较长文本中的异常值：目前许多防御机制依赖基于语义的过滤器，而该方案难以有效检测较长文本中的风险不完美的对齐：研究人员论证LLM存在行为期望界限（BEB，Behavior Expectation Bounds），如果LLM存在微小概率的负面行为，那么总会存在某个prompt导致LLM一定会生成有害内容。因此，任何未根除不良行为的防御措施都依然会受到对抗攻击语义审查的局限性：攻击者可能通过合法输出组合、等方式规避语义审查措施Adversarial examples are not bugs, they are features. ‘19A survey on adversarial attacks and defences. '21Fundamental limitations of alignment in large language models. arXiv‘23针对上述问题，防御措施从超参微调、行为审计到更为系统性的对抗训练。之前NLP领域的对抗防御或鲁棒性研究主要应对欺骗文本分类器等简单问题。而LLM的对抗防御策略可按照针对文本攻击、多模态攻击和联邦攻击划分，但已有工作主要还是集中在文本攻击领域。LLM防御策略超参微调[1]：此类措施缺乏系统性和普遍性，较为薄弱。行为审计：研究人员提出优化算法在LLM部署前进行审核。比如ARCA[2]反转LLM，旨在发现有针对性的、意外的恶意prompt，以及能够实现跨语言任务的prompt。输入/输出过滤：输入过滤方法中erase-and-check方法[3]，通过检查子序列安全性防御prmpot中的对抗攻击型后缀、插入和注入；另外有工作通过降低困惑度(perplexity)过滤token。需要注意的是，针对较长序列，上述策略的准确性会有所下降，有工作提出直接以困惑度划分，如果序列过于复杂则直接划分为可疑prompt[7]。输出过滤方法中最直接的一种是明确定义有害输出，并提供明确示例，甚至可以另外搞一个单独的LLM来检测其它LLM的有害输出[8]。LLM的安全威胁主要来自于其复杂模型功能目标与简单安全训练目标间的巨大Gap，一些解决方案直接使用外部安全措施（OpenChatKit、ModerationOpenAI，NeMo-Guardrails等）如语法/语义过滤、基于嵌入向量的可编程防护、内容分类等，以增强LLM内部固有的安全训练。然而，通过利用LLM的指令服从能力可以绕过这些外部策略，[8]进一步提出“可逆字符串转换”的概念，主张可通过编码或字符转换等方式完全绕过防御策略。人工反馈：将人类反馈直接纳入预训练阶段[9]，可利用奖励函数模拟人类偏好，可在训练阶段从有毒内容中学习并在推理阶段进行规避。红队：可以积累特定的红队数据集以评估LLM的潜在风险，另外可以通过自动化测试的方法观察LLM响应并检测攻击性内容[10]。对抗训练：自动化生成对抗样例，并将其合并到训练数据集中，帮助模型学习正确识别和抵消此类欺骗性输入。相关技术主要包括同义词替换、字符级操作、重新排序/插入/删除、嵌入变换等[12][1] Ignore previous prompt: Attack techniques for language models. arXiv'22[2] ARCA: Automatically auditing large language models via discrete optimization. arXiv'23[3] Certifying llm safety against adversarial prompting. arXiv'23[4] Exploring the universal vulnerability of prompt-based learning paradigm[5] Baseline defenses for adversarial attacks against aligned language models. arXiv'23[6] Exploring the universal vulnerability of prompt-based learning paradigm[7] Llm self defense: By self examination, llms know they are being tricked. arXiv'23[8] Llm censorship: A machine learning challenge or a computer security problem? arXiv'23[9] Pretraining language models with human preferences. arXiv'23[10] Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv'23[11] Red teaming language models with language models. arXiv'22[12] Interpretability and transparency-driven detection and transformation of textual adversarial examples (it-dt). arXiv'23[13] Towards building a robust toxicity predictor. arXiv'23[14] Text-crs: A generalized certified robustness framework against textual adversarial attacks. arXiv'23[15] Fedmlsecurity: A benchmark for attacks and defenses in federated learning and llms. arXiv'23进一步总结上述研究工作如下表：研究工作攻击类型场景防御策略[1]prompt注入文本攻击超参微调[2]LLM反转文本攻击行为审计[3]对抗型后缀、插入和注入文本攻击输入过滤[4]输入中的罕见/无关字符文本攻击输入过滤[6]自动生成优化的对抗攻击文本攻击输入过滤[7]对抗后缀prompt文本攻击输出过滤[9]对抗prompt生成不良内容文本攻击人类反馈[10] [11]使用指令生成不良内容文本攻击红队[12]单词替换文本攻击对抗训练[13]同义词替换、字符操作文本攻击对抗训练[14]同义词替换，单词重排、插入与删除文本攻击对抗训练[15]本地模型训练时输入数据的微小扰动联邦攻击本地模型过滤其它相关综述：单模态大模型，A survey of large language models， arXiv‘23；Harnessing the power of llms in practice: A survey on chatgpt and beyond，arXiv‘23多模态大模型，Multimodal learning with transformers: A survey. TPAMI'23联邦大模型，Federated large language model: A position paper. arXiv'23多agent大模型，Improving factuality and reasoning in language models through multiagent debate, arXiv'23; Building cooperative embodied agents modularly with large language models, arXiv'23"
648578542,95e443df1fdc64bd29640b1aea6e3ece,CVHub,https://api.zhihu.com/people/95e443df1fdc64bd29640b1aea6e3ece,people,,万字长文带你全面解读视觉大模型,article,https://api.zhihu.com/articles/648578542,cv_huber，备注“视觉<em>大模型</em>”即可领取！ 背景介绍 近年来，基础模型取得了显著的成功，特别是通过大型语言模型（LLMs），主要归因于数据和<em>模型</em>规模的大幅扩展。例如,229,5,518,1691412605,1691412605,"导读众所周知，视觉系统对于理解和推理视觉场景的组成特性至关重要。这个领域的挑战在于对象之间的复杂关系、位置、歧义、以及现实环境中的变化等。作为人类，我们可以很轻松地借助各种模态，包括但不仅限于视觉、语言、声音等来理解和感知这个世界。现如今，随着 Transformer 等关键技术的提出，以往看似独立的各个方向也逐渐紧密地联结到一起，组成了“多模态”的概念。今天，我们主要围绕Foundational Models，即基础模型这个概念，向大家全面阐述一个崭新的视觉系统。例如，通过 SAM，我们可以轻松地通过点或框的提示来分割特定对象，而无需重新训练；通过指定图像或视频场景中感兴趣的区域，我们可以与模型进行多轮针对式的交互式对话；再如李飞飞团队最新展示的科研成果所示的那样，我们可以轻松地通过语言指令来操作机器人的行为。该术语首次由Bommasani等人在《Stanford Institute for Human-Centered AI》中引入。基础模型定义为“通过自监督或半监督方式在大规模数据上训练的模型，可以适应其它多个下游任务”。具体地，我们将一起讨论一些典型的架构设计，这些设计结合了不同的模态信息，包括视觉、文本、音频；此外，我们还将着重讨论不同的训练目标，如对比式学习和生成式学习。随后，关于一些主流的预训练数据集、微调机制以及常见的提示模式，我们也将逐一介绍。最后，希望通过今天的学习让大家对基础模型在计算机视觉领域的发展情况，特别是在大规模训练和不同任务之间的适应性方面的最新进展有一个大致的认知。共勉。需要完整 PDF 版本请添加微信号: cv_huber，备注“视觉大模型”即可领取！背景介绍近年来，基础模型取得了显著的成功，特别是通过大型语言模型（LLMs），主要归因于数据和模型规模的大幅扩展。例如，像GPT-3这样的十亿参数模型已成功用于零/少样本学习，而无需大量的任务特定数据或模型参数更新。与此同时，有5400亿参数的Pathways Language Model（PaLM）在许多领域展现了先进的能力，包括语言理解、生成、推理和与代码相关的任务。反观视觉领域，诸如CLIP这样的预训练视觉语言模型在不同的下游视觉任务上展现了强大的零样本泛化性能。这些模型通常使用从网络收集的数百上千万图像-文本对进行训练，并提供具有泛化和迁移能力的表示。因此，只需通过简单的自然语言描述和提示，这些预训练的基础模型完全被应用到下游任务，例如使用精心设计的提示进行零样本分类。CLIP-DEMO除了此类大型视觉语言基础模型外，一些研究工作也致力于开发可以通过视觉输入提示的大型基础模型。例如，最近 META 推出的 SAM 能够执行与类别无关的分割，给定图像和视觉提示（如框、点或蒙版），指定要在图像中分割的内容。这样的模型可以轻松适应特定的下游任务，如医学图像分割、视频对象分割、机器人技术和遥感等。当然，我们同样可以将多种模态一起串起来，组成更有意思的管道，如RAM+Grounding-DINO+SAM:这里我们用 RAM 提取了图像的语义标签，再通过将标签输入到 Grounding-DINO 中进行开放世界检测，最后再通过将检测作为 SAM 的提示分割一切。目前视觉基础大模型可以粗略的归为三类：textually prompted models, e.g., contrastive, generative, hybrid, and conversational;visually prompted models, e.g., SAM, SegGPT;heterogeneous modalities-based models, e.g., ImageBind, Valley.图1. 视觉基础模型预览基础架构图2.四种不同风格的架构类型双编码器架构：其中，独立的编码器用于处理视觉和文本模态，这些编码器的输出随后通过目标函数进行优化。融合架构：包括一个额外的融合编码器，它获取由视觉和文本编码器生成的表示，并学习融合表示。编码器-解码器架构：由基于编码器-解码器的语言模型和视觉编码器共同组成。自适应 LLM 架构：利用大型语言模型（LLM）作为其核心组件，并采用视觉编码器将图像转换为与 LLM 兼容的格式（模态对齐）。目标函数对比式学习为了从无标签的图像-文本数据中学习，CLIP 中使用了简单的图像-文本对比（ITC）损失来通过学习正确的图像-文本配对来学习表示。此外还有图像-文本匹配（ITM）损失，以及包括简单对比式学习表示（SimCLR）和 ITC 损失的变体（如 FILIP Loss、TPC Loss、RWA、MITC、UniCL、RWC 损失）等其他对比损失。这里 表示温度系数。因此我们可以将 简单表示为 . 可以看出，本质上还是在计算图像与文本之间的相似度得分，比如常见的余弦相似性。生成式学习生成目标包括以下几种典型的损失：掩码语言建模（MLM）损失语言建模（LM）损失标准字幕（Cap）损失以及 Flamingo Loss、Prefix Language Modeling, PrefixML 等。从上述公式我们也可以很容易看出，生成式 AI 本质还是条件概率模型，如 Cap 损失便是根据上一个已知 token 或 图像来预测下一个 token。预训练预训练数据集如上所述，现代视觉-语言基础模型的核心是大规模数据，大致可分为几类：图像-文本数据：例如CLIP使用的WebImageText等，这些数据通常从网络抓取，并经过过滤过程删除噪声、无用或有害的数据点。部分伪标签数据：由于大规模训练数据在网络上不可用，收集这些数据也很昂贵，因此可以使用一个好的教师将图像-文本数据集转换为掩码-描述数据集，如GLIP和SA-1B等。数据集组合：有些工作直接将基准视觉数据集组合使用，这些作品组合了具有图像-文本对的数据集，如字幕和视觉问题回答等。一些工作还使用了非图像-文本数据集，并使用基于模板的提示工程将标签转换为描述。微调微调主要用于三个基本设置：提高模型在特定任务上的性能（例如开放世界物体检测，Grounding-DINO）;提高模型在某一特定能力上的性能（例如视觉定位）;指导调整模型以解决不同的下游视觉任务（例如InstructBLIP）。首先，许多工作展示，即使只采用线性探测，也可以提高模型在特定任务上的性能。因此，特定任务的数据集（例如ImageNet）是可以用来改善预训练模型的特定任务性能。其次，一些工作已经利用预训练的视觉语言模型，通过在定位数据集上微调模型来进行定位任务。例如，谷歌的一篇 OVD 工作 OWL-ViT，将 CLIP 预训练模型去掉 Token Pooling+projection 和 Image projection，加上一个新的 Linear Projection 作为分类头与文本进行匹配，学习出每个 Patch 的语义信息。此外在将 Patch 的表征经过 MLP head 回归出相应检测狂。通过 Patch 的语义特征与 BBox 的位置最终获得目标检测框。最后，像 InstructBLIP 则将视觉数据集转换为指导调整数据集，使视觉语言模型能够用于下游任务。InstructBLIP提示工程提示工程主要是搭配大型语言模型（LLMs）一起使用，使它们能够完成某些特定的任务。在视觉语言模型或视觉提示模型的背景下，提示工程主要用于两个目的：将视觉数据集转换为图像文本训练数据（例如，用于图像分类的 CLIP），为基础模型提供交互性使用视觉语言模型进行视觉任务。大多数视觉数据集由图像和相应文本标签组成。为了利用视觉语言模型处理视觉数据集，一些工作已经利用了基于模板的提示工程。在这种提示工程中，使用一组模板从标签生成描述。例如：text_descriptions = [f""This is a photo of a {label}"" for label in cifar100.classes] text_tokens = clip.tokenize(text_descriptions).cuda() 这种额外的上下文有助于模型学习，因此，这些文本提示可以在训练或评估期间被 VLM 所使用。下面让我们一起了解下这三类视觉基础模型。需要完整 PDF 版本请添加微信号: cv_huber，备注“视觉大模型”即可领取！基于文本提示的基础模型在本章节中，我们专注于探讨依赖文本作为主要监督来源的方法。这些文字提示模型大致分为三个主要类型，即基于不同的训练目标：对比学习、生成学习和混合方法。基于对比学习的方法首先，让我们一起回顾下 CLIP 架构及其衍生的变体：CLIP and it&#39;s variants.CLIP 由 OpenAI 于 2021 年正式提出，其联合训练图像和文本编码器以预测图像与标题在批量中的正确配对。CLIP 由图像编码器和文本编码器组成。它们产生了N个图像-文本对的多模态嵌入空间。通过对称交叉熵损失来训练，以最小化N个正确图像-文本对的嵌入的余弦相似度，并最大化N²-N个不正确对的余弦相似度。作者还从互联网上策划了4亿图像-文本对的数据集。在这样的大规模数据集上训练时，表现非常出色，也激发了后续许多的工作。此处我们集中探讨两类扩展方法，包括通用模型的对比方法和视觉定位基础模型的方法。基于通用模型的对比方法ALIGNALIGN 利用了一个超过10亿个图像-文本对的噪声数据集，无须进行昂贵的过滤或处理步骤即可在 Conceptual Captions 数据集中获得。一个简单的双编码器架构学习使用对比性损失来对齐图像和文本对的视觉和语言表示。结果表明，即便是这样一个简单的学习方案，只要数据库够大，便可以弥补它的噪声，并最终得到 SOTA 结果。Florence佛罗伦萨是微软、OpenAI 等联合提出的一个真正意义上的计算机视觉基础模型，能够处理不同的空间、时间和模态。它从CLIP样的预训练开始，然后扩展为具有三个不同适配器头的每个空间。弱弱的说一句，虽然这个模型的预训练参数只有 893M，但却需要在 512 块 A100 上训练 10 天的时间。FILIPFILIP 提出了一种交叉模态的后期交互方法，以捕捉细粒度语义对齐。FILIP 损失最大化了视觉和文本嵌入之间逐标记的相似性，有助于在不牺牲 CLIP 的推理效率的情况下，模拟两种模态之间的细粒度交互。【作者在 VALSE 第59期分享过，有兴趣的可以去看看，B站上有视频】此外还有基于掩码对比学习的方法，这是一种通过遮挡输入像素来提高对比学习效率的有效方法。下面我们也将介绍几种典型方法。FLIPFLIP 是一种简单和更有效的训练 CLIP 的方法，其思想很简单，如图所示，就是将 MAE 的 Mask 操作引入到 CLIP 上，随机地 mask 掉具有高 mask 率的图像碎片，只对可见的碎片进行编码。不同之处在于，这里不会对被 masked 的图像内容进行重建。此外，对于文本也做同样处理，有点类似于 BERT 但又不一样，BERT 是用学习过的 mask token 来代替它们，这种稀疏的计算可以显著减少文本编码的成本。MaskCLIPMaskCLIP 强调了图像是一个连续且细粒度的信号，而语言描述可能无法完全表达这一点。因此，MaskCLIP 通过随机遮挡图像并利用基于 Mean Teacher 的自蒸馏来学习局部语义特征。EVA这是一个以视觉为中心的基础模型，旨在仅使用可公开访问的数据来探索大规模视觉表示的局限性。EVA 是由智源曹越团队最新开源的视觉预训练模型，通过将最强语义学习（CLIP）与最强几何结构学习（MIM）结合，仅需使用标准的 ViT 模型，并将其规模扩大到十亿参数（1-Billion）进行训练，即可得到当前最强大的十亿级视觉基础模型。通过重构 CLIP 特征来进行 MIM 操作。首先， CLIP 模型输入为完整的图像，而 EVA 模型的输入为有遮挡的图像，训练过程是让 EVA 模型遮挡部分的输出去重构 CLIP 模型对应位置的输出，从而以简单高效的方式让 EVA 模型同时拥有了最强语义学习 CLIP 的能力和最强几何结构学习 MIM 的能力。很多的方法，总体而言，这些方法通过各种技术，如调整架构，改进对比目标，引入噪声鲁棒性，和探索多模态交互等，不断推动了 CLIP 及其变种的发展。这些努力已经展示了在许多任务上，包括零样本分类和图像-文本检索任务等方面，如何改善模型的性能，从而使这些模型在计算机视觉和自然语言处理的交叉领域中变得越来越重要。基于视觉定位基础模型的方法首先我们看下上图展示的结果，可以观察到，原始的 CLIP 模型其实是不擅长视觉定位任务的，特别是针对语义分割这种像素级定位任务来说。RegionCLIPRegionCLIP 显着扩展了 CLIP 以学习区域级视觉表示，其支持图像区域和文本概念之间的细粒度对齐，从而支持基于区域的推理任务，包括零样本目标检测和开放词汇目标检测。CRISCRIS则通过引入视觉-语言解码器和文本到像素对比损失，使 CLIP 框架学习像素级信息。Grounding DINO 是由沈向洋领导的 IDEA 实验室开源的，该方案利用了强大的预训练模型，并通过对比学习进行修改，以增强与语言的对齐。当然，像 OWL-ViT 也是类似的工作。此外， IDEA 还基于 SAM 等基础模型开源了一个集各大基础模型的仓库Grounded-Segment-Anything，仓库几乎涵盖了市面上主流的视觉基础模型，感兴趣的也可以关注下：最后，我们一起看下 OpenSeg 和 GroupViT，这些方法着重于分组机制和分割效果，以通过对比学习实现更好的语义分割和目标检测。值得关注的是，MetaAI 近期也开放了一篇最新的工作 ZeroSeg，无需借助文本信息便可以轻松实现高质量的分割效果，想要了解详情的同学可关注公众号 CVHub，搜索对应关键词即可。简单来说，以上讨论涵盖了一系列现代基础模型研究，这些方法试图通过对比学习、掩码学习、扩展和复现等技术来改进CLIP和其它基础模型。这些工作不仅推动了大规模图像-文本建模的前沿，还为诸如目标检测、语义分割等特定视觉任务的解决方案提供了新的方法和框架。基于生成式的方法基于生成式方法的视觉基础模型的总结涵盖了多个领域和方向，下面笔者简单归纳总结下。首先是结合大语言模型(Large Language Model, LLM)的多模态学习范式：结合上下文的多模态输入学习：例如 Frozen 方法将图像编码器与 LLM 结合，无需更新 LLM 的权重，而是在带有图像标注的数据集上训练视觉编码器。类似地，Flamingo 模型采用了固定的预训练视觉和语言模型，并通过Perceiver Resampler进行连接。使用LLM作为其它模态的通用接口：如MetaLM模型采用半因果结构，将双向编码器通过连接层连接到解码器上，可实现多任务微调和指令调整零样本学习。此外，KOSMOS系列也在LLM上整合了多模态学习的能力。开源版本的模型：如OpenFlamingo，是Flamingo模型的开源版本，训练于新的多模态数据集。其次我们来看下视觉-语言对齐与定位相关的模型：具备定位能力的模型：KOSMOS-2通过添加一条管线来抽取文本中的名词短语并将其与图像中的相应区域链接起来，进而实现视觉定位。另外就是通用生成目标下的训练：简化视觉语言建模：如SimVLM使用前缀语言建模（PrefixLM）目标进行训练，不需要任务特定的架构或训练，可在多个视觉语言任务上实现优秀的性能。掩码重构与对齐：如MaskVLM，采用联合掩码重构语言建模，其中一个输入的掩码部分由另一个未掩码输入重构，有效对齐两个模态。模块化视觉语言模型：如mPLUG-OWL，由图像编码器、图像抽象器和冻结LLM组成，通过两阶段的训练实现多模态对话和理解。此外还有与对比学习的比较与结合：CapPaCapPa 是基于字幕的模型与 CLIP 风格模型的比较得到的一种新的生成预训练方法，交替使用自回归预测和并行预测。总体而言，上述的方法和模型通过在视觉条件下训练语言生成任务，为 LLM 增添了“看世界”的能力。这些工作在视觉语言任务，如图像标注、多模态对话和理解等方面取得了显著进展，有的甚至在少样本情况下达到或超越了最先进的性能。通过将视觉和语言模态结合，这些模型为计算机视觉和自然语言处理的交叉领域提供了强大的新工具。基于对比学习和生成式的混合方法通用视觉-语言学习的基础模型：UNITER：结合了生成（例如掩码语言建模和掩码区域建模）和对比（例如图像文本匹配和单词区域对齐）目标的方法，适用于异构的视觉-语言任务。Pixel2Seqv2：将四个核心视觉任务统一为像素到序列的接口，使用编码器-解码器架构进行训练。Vision-Language：使用像 BART 或 T5 等预训练的编码器-解码器语言模型来学习不同的计算机视觉任务。通用架构：FLAVAContrastive Captioner (CoCa)：结合了对比损失和生成式的字幕损失，可以在多样的视觉数据集上表现良好。FLAVA：适用于单模态和多模态任务，通过一系列损失函数进行训练，以便在视觉、语言和视觉-语言任务上表现良好。BridgeTower：结合了不同层次的单模态解码器的信息，不影响执行单模态任务的能力。PaLI：一种共同扩展的多语言模块化语言-视觉模型，适用于单模态和多模态任务。X-FM：包括语言、视觉和融合编码器的新基础模型，通过组合目标和新技术进行训练。BLIP 框架范式：BLIP：利用生成和理解能力有效利用图像文本数据集，采用Multimodal mixture of Encoder-Decoder (MED)架构。BLIP-2：通过查询转换器来实现计算效率高的模态间对齐。指令感知特征提取和多模态任务解决方案：InstructBLIP：利用视觉编码器、Q-Former和LLM，通过指令感知的视觉特征提取来进行训练。 对预训练模型的高效利用：VPGTrans：提供了一种高效的方法来跨 LLM 传输视觉编码器。TaCA：提到了一种叫做 TaCA 的适配器，但没有进一步详细描述。UniDetector基于 Visual Grounding 的方法：ViLD： 这一方法使用了一个两阶段的开放词汇对象检测系统，从预训练的单词汇分类模型中提取知识。它包括一个 RPN 和一个类似于 CLIP 的视觉语言模型，使用 Mask-RCNN 创建对象提案，然后将知识提取到对象检测器中。 UniDetector: 此方法旨在进行通用对象检测，以在开放世界中检测新的类别。它采用了三阶段训练方法，包括类似于上面我们提到的RegionCLIP的预训练、异构数据集训练以及用于新类别检测的概率校准。UniDetector 为大词汇和封闭词汇对象检测设立了新的标准。 X-Decoder: 在三个粒度层次（图像级别、对象级别和像素级别）上运作，以利用任务协同作用。它基于 Mask2Former，采用多尺度图像特征和两组查询来解码分割掩码，从而促进各种任务。它在广泛的分割和视觉语言任务中展现出强大的可转移性。 这些方法共同探讨了视觉定位任务的不同维度，包括开放词汇对象检测、通用对象检测、两阶段训练、多级粒度和新颖的损失功能。它们共同通过以创新的方式整合视觉和语言来推动视觉理解的界限，往往超越了该领域以前的基准。简单总结下，上面我们展示了如何通过对比和生成式学习，以及混合这些方法，来设计和训练可以处理各种视觉和语言任务的模型。有些模型主要关注提高单模态和多模态任务的性能，而有些模型关注如何高效地训练和利用预训练模型。总的来说，这些研究提供了视觉-语言融合研究的丰富视角和多样化方法，以满足不同的实际需求和应用场景。基于对话式的视觉语言模型这一块我们不做过多介绍，仅介绍比较有代表性的几个工作：GPT-4GPT-4：这是首个结合视觉和语言的模型，能够进行多模态对话。该模型基于Transformer架构，通过使用公开和私有数据集进行预训练，并通过人类反馈进行强化学习微调。根据公开的数据，GPT-4 在多个 NLP、视觉和视觉-语言任务上表现出色，但很可惜目前并未开源。miniGPT-4miniGPT-4： 作为GPT-4的开源版本，miniGPT-4 由预训练的大型语言模型Vicuna和视觉组件ViT-G和Q-Former组成。模型先在多模态示例上进行训练，然后在高质量的图像和文本对上进行微调。miniGPT-4 能够生成复杂的图像描述并解释视觉场景。XrayGPT： 这个模型可以分析和回答有关 X 射线放射图的开放式问题。使用Vicuna LLM作为文本编码器和MedClip作为图像编码器，通过更新单个线性投影层来进行多模态对齐。LLaVA： 这是一个开源的视觉指令调整框架和模型，由两个主要贡献组成：开发一种用于整理多模态指令跟踪数据的经济方法，以及开发一个大型多模态模型，该模型结合了预训练的语言模型LLaMA和CLIP的视觉编码器。LLaMA-Adapter V2LLaMA-Adapter V2： 通过引入视觉专家，早期融合视觉知识，增加可学习参数等方式，改善了LLaMA的指令跟随能力，提高了在传统视觉-语言任务上的性能。综上所述，基于对话的视觉语言模型在理解、推理和进行人类对话方面取得了显著进展。通过将视觉和语言结合在一起，这些模型不仅在传统 NLP 任务上表现出色，而且能够解释复杂的视觉场景，甚至能够与人类进行复杂的多模态对话。未来可能会有更多的工作致力于提高这些模型的可解释性、可用性和可访问性，以便在更广泛的应用领域中实现其潜力。需要完整 PDF 版本请添加微信号: cv_huber，备注“视觉大模型”即可领取！基于视觉提示的基础模型这一块内容我们先为大家阐述几个代表性的基于视觉提示的基础模型，如 SAM 和 SEEM 等；随后再介绍基于 SAM 的一系列改进和应用，例如用在医疗、遥感、视频追踪等领域；最后再简单介绍下几个通用的扩展。视觉基础模型CLIPSegCLIPSeg概述：CLIPSeg 利用 CLIP 的泛化能力执行 zero-shot 和 one-shot 分割任务。结构：由基于 CLIP 的图像和文本编码器以及具有 U-net 式跳跃连接的基于 Transformer 的解码器组成。工作方式：视觉和文本查询通过相应的 CLIP 编码器获取嵌入，然后馈送到 CLIPSeg 解码器。因此，CLIPSeg 可以基于任意提示在测试时生成图像分割。SegGPTSegGPTSegGPT 旨在训练一个通用模型，可以用于解决所有的分割任务，其训练被制定为一个上下文着色问题，为每个数据样本随机分配颜色映射。目标是根据上下文完成不同的分割任务，而不是依赖于特定的颜色。SAM概述：SAM 是一种零样本分割模型，从头开始训练，不依赖于 CLIP。 结构：使用图像和提示编码器对图像和视觉提示进行编码，然后在轻量级掩码解码器中组合以预测分割掩码。 训练方法：通过三阶段的数据注释过程（辅助手动、半自动和全自动）训练。SEEM与 SAM 相比，SEEM 涵盖了更广泛的交互和语义层面。例如，SAM 只支持有限的交互类型，如点和框，而由于它本身不输出语义标签，因此错过了高语义任务。首先，SEEM 有一个统一的提示编码器，将所有视觉和语言提示编码到联合表示空间中。因此，SEEM 可以支持更通用的用途。它有潜力扩展到自定义提示。其次，SEEM 在文本掩码（基础分割）方面非常有效，并输出语义感知预测。SAM 的改进与应用SAM for Medical SegmentationMedical SAM 总览Adapting by Fine-TuningMedSAM：通过在大规模医学分割数据集上微调 SAM，创建了一个用于通用医学图像分割的扩展方法 MedSAM。这一方法在 21 个 3D 分割任务和 9 个 2D 分割任务上优于 SAM。MedSAM paper: https://arxiv.org/pdf/2304.12306.pdfgithub: https://github.com/bowang-lab/MedSAM Adapting through Auxiliary Prompt EncoderAutoSAM：为SAM的提示生成了一个完全自动化的解决方案，基于输入图像由AutoSAM辅助提示编码器网络生成替代提示。AutoSAM 与原始的 SAM 相比具有更少的可训练参数。AutoSAMAdapting Through AdaptersLearnable Ophthalmology SAM在眼科的多目标分割：通过学习新的可学习的提示层对SAM进行了一次微调，从而准确地分割不同的模态图像中的血管或病变或视网膜层。3DSAM-adapter3DSAM-adapter：为了适应3D空间信息，提出了一种修改图像编码器的方案，使原始的2D变换器能够适应体积输入。Medical SAM AdapterMedical SAM Adapter：专为SAM设计了一个通用的医学图像分割适配器，能够适应医学数据的高维度（3D）以及独特的视觉提示，如 point 和 box。Adapting by Modifying SAM’s DecoderDeSAMDeSAM：提出了将 SAM 的掩码解码器分成两个子任务：提示相关的 IoU 回归和提示不变的掩码学习。DeSAM 最小化了错误提示在“分割一切”模式下对SAM性能的降低。SAM as a Medical AnnotatorMedLAMMedLAM：提出了一个使用 SAM 的医学数据集注释过程，并引入了一个少量定位框架。MedLAM 显著减少了注释负担，自动识别整个待注释数据集的目标解剖区域。SAMMSegment Any Medical Model, SAMM：这是一个结合了3D Slicer和SAM的医学图像分割工具，协助开发、评估和应用SAM。通过与3D Slicer的整合，研究人员可以使用先进的基础模型来分割医学图像。总体来说，通过各种微调、适配和修改方法，SAM 已被成功适应了用于医学图像分割的任务，涵盖了从器官、病变到组织的不同医学图像。这些方法也突出了将自然图像的深度学习技术迁移到医学领域的潜力和挑战。在未来，SAM 及其变体可能会继续推动医学图像分析领域的进展。SAM for TrackingSAM 在跟踪任务方面的应用集中在通过视频中的帧跟踪和分割任意对象，通常被称为视频对象分割（VOS）。这个任务涉及在一般场景中识别和追踪感兴趣的区域。以下总结下 SAM 在跟踪方面的一些主要应用和方法：Track Anything (TAM)TAM概述：TAM 使用 SAM 和现成的跟踪器 XMem 来分割和跟踪视频中的任何对象。 操作方式：用户可以简单地点击一个对象以初始化 SAM 并预测掩码。然后，XMem 使用 SAM提供的初始掩码预测在视频中基于时空对应关系跟踪对象。用户可以暂停跟踪过程并立即纠正任何错误。 挑战：虽然表现良好，但 TAM 在零样本场景下不能有效保留 SAM 的原始性能。SAM-TrackSAM-Track概述：与 TAM 类似，SAM-Track 使用 DeAOT 与 SAM 结合。 挑战：与 TAM 类似，SAM-Track 在零样本场景下也存在性能挑战。SAM-PTSAM-PT概述：SAM-PT 通过结合 SAM 的稀疏点跟踪来解决视频分割问题。只需要第一帧的稀疏点注释来表示目标对象。 强项：在开放世界 UVO 基准测试中展示了对未见对象的泛化能力。 操作方式：使用像 PIPS 这样的先进点跟踪器，SAM-PT 为视频分割提供稀疏点轨迹预测。进一步地，为了区分目标对象及其背景，SAM-PT 同时跟踪正点和负点。SAM-DASAM-DA概述：SAM-DA 是另一种使用 SAM 自动分割能力进行跟踪的方法。 具体应用：通过使用 SAM 自动分割功能从每个夜间图像自动确定大量高质量目标域训练样本，从而跟踪夜间无人机（UAVs）。SAM 在视频对象跟踪和分割方面的应用表明了其作为分割基础模型的潜力。尽管有一些挑战，特别是在未见数据和零样本场景下，但通过与现成的跟踪器的结合以及稀疏点跟踪的使用，SAM 能够实现在视频中跟踪和分割对象。这些方法为计算机视觉社区提供了一个实现通用场景中任意对象跟踪的有力工具，有助于推动视频分析和监控等领域的进展。SAM for Remote SensingSAM 在遥感图像分割方面的应用集中在通过点、框和粗粒度掩码的引导来理解和分割遥感图像。以下是 SAM 在遥感分割方面的应用以及相关挑战。SAM在遥感分割的基本应用交互性质：由于 SAM 的交互特性，它主要依赖于点、框和粗粒度掩码的手动引导。限制：全自动分割困难：SAM在完全自动地理解遥感图像方面效果不佳。结果依赖性：SAM的结果严重依赖于用于分割遥感图像目标的提示的类型、位置和数量。手动提示优化需求：要实现理想的结果，通常需要对手动提示进行精炼。RsPrompter概述：RsPrompter 是一个将语义分类信息与 SAM 结合的方法，用于遥感图像的自动实例分割。 操作方式：学习生成提示：RsPrompter 提出了一种学习生成适当的SAM输入提示的方法。 生成提示包含的信息：通过分析编码器的中间层来生成包含关于语义类别的信息的提示，并生成提示嵌入，这可以视为点或框嵌入。 目标：通过自动化生成适当的输入提示，RsPrompter 试图克服 SAM 在遥感图像分割方面的局限性。尽管 SAM 在遥感图像分割方面存在一些限制，主要与其交互性质和对手动引导的依赖有关，但通过引入如 RsPrompter 这样的方法，可以利用 SAM 实现遥感图像的自动实例分割。这些努力标志着朝着减少人工干预和提高遥感图像分析自动化的方向迈出的重要一步，有势必推动遥感科学、地理信息系统（GIS）和环境监测等领域的进展。SAM for CaptioningSAM 与大型语言模型如 ChatGPT 的组合在可控图像字幕（controlled image captioning）方面开辟了新的应用领域。下面概述下这种组合在图像字幕上的具体应用。先给大家介绍下概念，可控图像字幕使用自然语言来根据人类目标解释图像，例如检查图像的某些区域或以特定方式描述图像。然而，这种交互式图像字幕系统的可扩展性和可用性受到缺乏良好注释的多模态数据的限制。一个典型的案例便是 Caption AnyThing，下面一起看看。Caption AnyThing概述：Caption AnyThing 是一种零样本图像字幕模型，通过与 SAM 和大型语言模型（例如ChatGPT）结合，使用预训练的图像字幕器实现。工作流程：定义视觉控制：用户可以通过视觉提示定义视觉控制。使用SAM转换为掩码：视觉提示随后使用SAM转换为掩码，以选择感兴趣的区域。预测原始字幕：基于原始图像和提供的掩码，图像字幕器预测原始字幕。文本优化：使用大型语言模型（例如ChatGPT）的文本精炼器，根据用户的偏好定制语言风格，从而优化原始描述。结果：用户可以通过控制视觉和语言方面，更精确地描述图像中的特定部分或属性。优势和意义用户自定义：通过允许用户定义视觉和语言控制，提供了高度定制的解释。灵活性和准确性：通过结合视觉分割和自然语言处理，增强了描述的灵活性和准确性。零样本学习：由于是零样本模型，因此可以在未经特定训练的新图像和场景上工作。通过结合 SAM 的图像分割能力和大型语言模型如 ChatGPT 的自然语言处理能力，Caption AnyThing 为可控图像字幕开辟了新的可能性。这不仅增强了字幕的灵活性和准确性，还允许用户定制语言风格和焦点，从而促进了交互图像分析和解释的发展SAM for Mobile Applications这一节我们重点梳理下 SAM 的一些移动端应用，主要就是加速 SAM 的推理和提升 SAM 的分割质量。FastSAMFastSAM 基于 YOLOv8-seg 实现，它比 SAM 快50倍，且训练数据只有SAM的1/50，同时运行速度不受 point 输入数量的影响MobileSAMMobileSAM：将原始 SAM 中的图像编码器 ViT-H 的知识蒸馏到一个轻量化的图像编码器中，该编码器可以自动与原始 SAM 中的 Mask 解码器兼容。训练可以在不到一天的时间内在单个 GPU 上完成，它比原始 SAM 小60多倍，但性能与原始 SAM 相当。MobileSAMRefSAM：这是一种高效的端到端基于 SAM 的框架，用于指代视频对象分割（RVOS）。它使用了高效且轻量级的CrossModal MLP，将指代表达的文本特征转换为密集和稀疏的特征表示。HQ-SAMHQ-SAM: HQ-SAM 为了实现高质量的掩膜预测，将 HQ-Output Token（高质量输出标记）和全局-局部特征融合引入到SAM中。为了保持SAM的零样本能力，轻量级的 HQ-Output Token 复用了 SAM 的掩膜解码器，并生成了新的 MLP（多层感知器）层来执行与融合后的 HQ-Features（高质量特征）的逐点乘积。在训练期间，将预训练的 SAM 的模型参数固定，只有 HQ-SAM 中的少数可学习参数可以进行训练。通才模型这一类主要描述如何使用上下文学习快速适应具有不同提示和示例的各种任务。这里特别突出了几个被称为通才模型（Generalist Models）的模型，它们可以执行多个任务，甚至可以通过提示和少量特定于任务的示例来适应新任务。PainterPainterPainter是一种通才模型，可以同时执行不同的任务，甚至可以根据提示和非常少的特定于任务的示例适应新任务。工作方式：给定某个任务的输入和输出图像，输出图像的像素被遮挡。Painter 模型的目标是对 masked 的输出图像进行填充。 训练目标：这个简单的训练目标允许统一几个视觉任务，包括深度估计、人体关键点检测、语义分割、实例分割、图像去噪、图像去雨和图像增强。 推理流程：在训练后，Painter 可以使用与输入条件相同任务的输入/输出配对图像来确定在推理过程中执行哪个任务。VisionLLMVisionLLMVisionLLM是另一个通才模型，可以对齐视觉和语言模态以解决开放式任务。工作方式：给定图像，VisionLLM 使用视觉模型学习图像特征；这些图像特征与例如“详细描述图像”的语言指令一起传递给语言引导的图像分词器。 任务解码器：图像分词器的输出连同语言指令被提供给一个开放式 LLM 为基础的任务解码器，旨在根据语言指令协调各种任务。PrismerPrismerPrismer也是一种视觉语言模型，可以执行多个推理任务。特点：Prismer 利用各种预训练的领域专家，例如语义分割、对象、文本和边缘检测，表面法线和深度估计，来执行多个推理任务。 应用：例如图像字幕和视觉问题回答。通才模型表示了一种通用的趋势，其中模型可以通过改变输入或少量特定于任务的训练来适应新的或多样化的任务。这些模型在解决问题时可以灵活地适应，克服了单一任务模型的限制。尤其是在输出表示在任务之间有很大差异的计算机视觉中，这一点变得尤为重要。通过简化训练目标和建立跨任务的框架，这些通才模型为未来计算机视觉任务的多功能性提供了新的机会。需要完整 PDF 版本请添加微信号: cv_huber，备注“视觉大模型”即可领取！综合性基础模型基于异构架构的基础视觉模型在这一部分，我们集中讨论不同的基础视觉模型，这些模型通过对齐多个成对的模态，如图像-文本、视频-音频或图像-深度等，来学习更有意义的表示。CLIP 与异构模态的对齐CLIP2VideoCLIP2Video：这一模型扩展了CLIP模型，使其适用于视频。通过引入时序一致性和提出的时序差异块（TDB）和时序对齐块（TAB），将图像-文本的CLIP模型的空间语义转移到视频-文本检索问题中。AudioCLIPAudioCLIP：这一模型扩展了CLIP，使其能够处理音频。AudioCLIP结合了ESResNeXt音频模型，并在训练后能够同时处理三种模态，并在环境声音分类任务中胜过先前方法。学习共享表示的多模态模型ImageBindImage Bind：这一模型通过学习配对数据模态（如（视频，音频）或（图像，深度））的共同表示，包括多种模态。ImageBind 将大规模配对数据（图像，文本）与其他配对数据模态相结合，从而跨音频、深度、热和惯性测量单元（IMU）等四种模态扩展零样本能力。MACAW-LLMMACAW-LLM：这是一种指令调谐的多模态 LLM（大型语言模型），整合了图像、视频、音频和文本等四种不同模态。通过模态模块、对齐模块和认知模块，MACAW-LLM 实现了各种模态的统一。视频和长篇幅文本的处理COSACOSA：通过将图像-文本语料库动态转换为长篇幅视频段落样本来解决视频所需的时序上下文缺失问题。通过随机串联图像-文本训练样本，确保事件和句子的显式对应，从而创造了丰富的场景转换和减少视觉冗余。ValleyValley: 是另一个能够整合视频、图像和语言感知的多模态框架。通过使用简单的投影模块来桥接视频、图像和语言模态，并通过指令调谐流水线与多语言 LLM 进一步统一。这一节主要强调了将不同的感知模态（如视觉、听觉和文字）结合到统一框架中的重要性。通过跨模态学习和对齐，这些模型不仅提高了特定任务的性能，还扩展了多种模态的零样本学习能力。此外，考虑到视觉和听觉之间的时序一致性也是重要的创新方向。通过强调如何整合这些不同的输入形式，本节揭示了深度学习在处理更复杂和多样化数据方面的潜力。基于代理的基础视觉模型基于代理的基础视觉模型将语言学习模型（LLMs）与现实世界的视觉和物理传感器模式相结合。这不仅涉及文字的理解，还涉及与现实世界的互动和操作，特别是在机器人操作和导航方面。机器人操控Palm-EPalm-E：该模型将连续的传感器输入嵌入到 LLM 中，从而允许机器人进行基于语言的序列决策。通过变换器，LLM将图像和状态估计等输入嵌入到与语言标记相同的潜在空间，并以相同的方式处理它们。ViMAViMA：使用文本和视觉提示来表达一系列机器人操控任务，通过多模态提示来学习机器人操控。它还开发了一个包含600K专家轨迹的模拟基准测试，用于模仿学习。持续学习者MineDojoMineDojo：为 Minecraft 中的开放任务提供了便利的API，并收集了丰富的 Minecraft 数据。它还使用这些数据为体现代理制定了新的学习算法。VOYAGERVOYAGER：这是一种由 LLM 驱动的终身学习代理，设计用于在 Minecraft 中探索、磨练技能并不断发现新事物。它还通过组合较小的程序逐渐构建技能库，以减轻与其他持续学习方法相关的灾难性遗忘。导航规划LM-NavLM-Nav：结合预训练的视觉和语言模型与目标控制器，从而在目标环境中进行长距离指导。通过使用视觉导航模型构建环境的“心理地图”，使用 GPT-3 解码自由形式的文本指示，并使用 CLIP将这些文本地标连接到拓扑图中，从而实现了这一目标。然后，它使用一种新的搜索算法找到了机器人的计划。总体而言，基于代理的基础视觉模型突出了语言模型在现实世界任务中的潜力，如机器人操作、持续学习和复杂导航。它们不仅推动了机器人技术的进展，还为自然语言理解、多模态交互和现实世界应用开辟了新的研究方向。通过将预训练的大型语言模型与机器人技术和视觉导航相结合，基于代理的基础视觉模型能够解决现实世界中的复杂任务，展示了人工智能的跨学科整合和应用潜力。需要完整 PDF 版本请添加微信号: cv_huber，备注“视觉大模型”即可领取！总结具有对多种模式（包括自然语言和视觉）基础理解的模型对于开发能有效感知和推理现实世界的AI系统至关重要。今天主要为大家概括了视觉和语言基础模型，重点关注了它们的架构类型、训练目标、下游任务适应性和提示设计。多模态理解：我们提供了对文本提示、视觉提示和异构模态模型的系统分类。这些模型不仅涵盖了自然语言，还包括了视觉和其他感知模式的理解。应用广泛性：这些模型在各种视觉任务中的应用非常广泛，包括零样本识别和定位能力、关于图像或视频的视觉对话、跨模态和医疗数据理解。通用模型：视觉中的基础模型可以作为通用模型来解决多个任务。当与大型语言模型相结合时，它们促生了可以在复杂环境中持续学习和导航的基础实体代理。整体而言，基础视觉和语言模型的研究不仅深入了解了各种架构和训练目标，还展示了这些模型在多个领域和应用中的潜力。通过集成文本、视觉和其他模态的理解，这些模型促进了机器人技术和现实世界任务的进展。然而，还需要进一步的研究来充分挖掘这些模型的潜力，并解决一些存在的挑战和局限性。"
668781029,f1db92141096105090fd66d45d1227da,Alan小分享,https://api.zhihu.com/people/f1db92141096105090fd66d45d1227da,people,分享编程、投资、读书感悟，以及科技领域的内容,学习大模型的第一步：理解 Transformer && 动手实现一遍,article,https://api.zhihu.com/articles/668781029,1、背景 前段时间想看看<em>大模型</em>的分布式训练框架（比如 Megatron）是咋做的，然后发现需要对 Transformer 的细节更加了解，所以就打算看看 Transformer,223,7,884,1701612025,1709080850,"1、背景前段时间想看看大模型的分布式训练框架（比如 Megatron）是咋做的，然后发现需要对 Transformer 的细节更加了解，所以就打算看看 Transformer 开山之作《Attention Is All You Need》[1]，并手动实现一遍。过程：一边读论文 [1]，一边看李沐的讲解视频 [2]（推荐看看！）；Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili遇到一些细节不理解的，比如 Multi-Head 具体是啥操作，或者每个 Attention 操作的 query、key、value 是咋来的，就 google 一下或者问 ChatGPT 4；还有看了文章 [4] 和 [5]（推荐看看！习惯看英文的话，推荐看第二篇）初识CV：Transformer模型详解（图解最完整版）The Illustrated Transformer实现之前，先看了一遍 harvard nlp 团队 [3] 和 github 上 hyunwoongko 的实现。hyunwoongko 是完全参照了原文实现，而 harvard nlp 团队是基于文章发表后的一些新发现，做了微调，比如 Post-LM 改成 Pre-LM（参考文章 [6]），还有对 Attention 操作中 softmax 之后、乘以 V 之前的中间结果，加了 dropout 操作，这些操作是在后面的研究中发现可以使得训练更稳定的微调。 然后自己动手！最后跑了个简单的示例任务。harvard nlp 团队给的实现里面，整体很强，不过很多地方没有额外的解释，比如有一些细节是和原文不一样的，直接给出了微调后的实现版本，不仔细留意都可能发现不了。另外还有就是示例任务写得有点问题，我改了下～小总结：本文在以上工作的基础上，补充更多的图文解释和代码注释（比如矩阵操作的图画得更清晰准确），尝试帮助大家更好地理解。本文代码：https://github.com/NgCafai/transformer/tree/masterTips1、本文的讲述顺序是和论文的顺序基本一致，即先讲整体模型的结构和实现，再讲各个组成部分。动手实现的时候，可以考虑从各个基础组件开始，再往上一层层搭建。2、遇到不会的地方，善用 ChatGPT！真的好用！2、模型实现整体结构先上经典图：论文[1] Figure 1: The Transformer - model architecture实现：class Transformer(nn.Module): """""" The Transformer model: A standard Encoder-Decoder architecture. """""" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): """""" Args: encoder: nn.Module, a stack of N EncoderLayer decoder: nn.Module, a stack of N DecoderLayer src_embed: nn.Sequential, composed of Embeddings and PositionalEncoding, for input sequence tgt_embed: nn.Sequential, composed of Embeddings and PositionalEncoding, for output sequence generator: nn.Module, used to predict the next token """""" super().__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): """""" Args: src: (batch_size, seq_len_src) tgt: (batch_size, seq_len_tgt) src_mask: (batch_size, 1, seq_len_src) tgt_mask: (batch_size, seq_len_tgt, seq_len_tgt) Returns: output: (batch_size, seq_len, d_model) """""" return self.do_decode(self.do_encode(src, src_mask), src_mask, tgt, tgt_mask) def do_encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def do_decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)Generator 用来将模型输出的 embeding 转换为对下一个词的预测（在整体词表上的概率分布），对应图中右上角的 Linear 和 Softmax 步骤：class Generator(nn.Module): """""" Define standard linear + softmax generation step. """""" def __init__(self, d_model, vocab_size): """""" Args: vocab_size: size of the vocabulary, that is, total number of unique tokens """""" super().__init__() self.linear = nn.Linear(d_model, vocab_size) def forward(self, x): return nn.functional.log_softmax(self.linear(x), dim=-1)EncoderEncoder 对应论文 [1] Figure 1 左边的那个大框，由 N 个 EncoderLayer 组成～class Encoder(nn.Module): """""" Core encoder is a stack of N EncoderLayer. """""" def __init__(self, layer, N): super().__init__() self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)]) self.norm = LayerNorm(layer.d_model) def forward(self, x, mask): """""" Pass the input (and mask) through each layer in turn. """""" for layer in self.layers: x = layer(x, mask) return self.norm(x)EncoderLayer 就是论文 [1] Figure 1 左边的画出来的一个模块，由两个 sublayer 组成：SubLayer 1：先进行 Multi-Head Attention，再套一层 Residual + LayerNorm；SubLayer 2：先进行 Feed Forward（就是一个全链接网络），再套一层 Residual + LayerNorm；注意：按论文中描述的操作，SubLayer 中是先进行 Multi-Head Attention 或者 Feed Forward，然后 Redsial，最后才进行 LayerNorm，这种方式称为 Post LN；后续的研究中，发现进行第一步操作前，先做一次 LayerNorm，效果会更好，称为 Pre LN。两个方式的执行路径是这样子：论文[7] Figure 1: (a) Post-LN Transformer layer; (b) PreLN Transformer layerclass EncoderLayer(nn.Module): """""" Encoder is made up of self-attn and feed forward. """""" def __init__(self, d_model, attention, feed_forward, dropout_prob): super().__init__() self.d_model = d_model self.attention = attention self.feed_forward = feed_forward self.sub_layers = nn.ModuleList([SubLayer(d_model, dropout_prob) for _ in range(2)]) def forward(self, x, mask): """""" Args: x: (batch_size, seq_len, d_model) mask: Returns: out: (batch_size, seq_len, d_model) """""" x = self.sub_layers[0](x, lambda x: self.attention(x, x, x, mask)) x = self.sub_layers[1](x, self.feed_forward) return xSubLayer 实现（Pre LN 方式）：class SubLayer(nn.Module): """""" Do pre-layer normalization for input, and then run multi-head attention or feed forward, and finally do the residual connection. """""" def __init__(self, d_model, dropout_prob=0.1): super().__init__() self.norm = LayerNorm(d_model) self.dropout = nn.Dropout(dropout_prob) def forward(self, x, main_logic): # main_logic: multi-head attention or feed forward x_norm = self.norm(x) return x + self.dropout(main_logic(x_norm))class LayerNorm(nn.Module): """""" Layer normalization module. """""" def __init__(self, d_model, eps=1e-6): super().__init__() self.gamma = nn.Parameter(torch.ones(d_model)) self.beta = nn.Parameter(torch.zeros(d_model)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.gamma * (x - mean) / (std + self.eps) + self.betaDecoderDecoder 对应论文 [1] Figure 1 右边边的那个大框，由 N 层 DecoderLayer 组成。实现：（memory 指的是 Encoder 的输出）class Decoder(nn.Module): """""" Core decoder is a stack of N DecoderLayer. """""" def __init__(self, layer, N): super().__init__() self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)]) self.norm = LayerNorm(layer.d_model) def forward(self, x, memory, src_mask, tgt_mask): # memory is the output of the Encoder for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)DecoderLayer 的结构和 EncoderLayer 类似，主要差别是中间多了一层 Attention 操作（对应下面实现中的 src_attention），其 key 和 value 来自 Encoder 的输出，query 来自 target sequence：class DecoderLayer(nn.Module): """""" Decoder is made of self-attn, src-attn, and feed forward. """""" def __init__(self, d_model, self_attention, src_attention, feed_forward, dropout_prob): super().__init__() self.d_model = d_model self.self_attention = self_attention self.src_attention = src_attention self.feed_forward = feed_forward self.sub_layers = nn.ModuleList([SubLayer(d_model, dropout_prob) for _ in range(3)]) def forward(self, x, memory, src_mask, tgt_mask): """""" Args: x: (batch_size, seq_len, d_model) memory: (batch_size, seq_len, d_model) src_mask: tgt_mask: Returns: out: (batch_size, seq_len, d_model) """""" # self-attention: query, key, value are all from x x = self.sub_layers[0](x, lambda x: self.self_attention(x, x, x, tgt_mask)) # src-attention: query is from x, while key and value are from the output of the Encoder x = self.sub_layers[1](x, lambda x: self.src_attention(x, memory, memory, src_mask)) x = self.sub_layers[2](x, self.feed_forward) return xmasking：输出一个下图这样子的矩阵；用于 Decoder 中的第一个 Multi-Head Attention 操作，目的是保证训练时，target sequence 中每个位置只能看到自己及前面位置的信息：def subsequent_mask(size): ""Mask out subsequent positions"" attention_shape = (1, size, size) subsequent_mask = torch.triu(torch.ones(attention_shape), diagonal=1).type(torch.uint8) return subsequent_mask == 0Attention实现论文[1] 3.2 中提到的 Attention 部分。来自论文[1] Figure2先是左边的 Scaled Dot-Product Attention，公式： 对应的矩阵操作：原创Q、K、V 对应 3 个序列，每个序列分别包含 n、m、m 个词（这里 n 和 m 不是具体的训练参数，只是为了说明各个矩阵的相对大小，比如 K 和 V 的词的数量是一样的），每个词用一个 d_k 维的向量表示。（论文中 V 是用了 d_v 维的向量，但一般 d_k 和 d_v 相等）一般 K 和 V 来自同一个序列。Attention 的作用是将 Q 中每个向量，和 K 中的每个向量，分别做内积，表示这两个向量的相似程度，得到 scores 矩阵。 就表示 和 的内积。result 中的每一行，相当于是 V 中所有行的加权之和。比如 result 中的第 i 行，就是以 scores 中的第 i 行为权重，乘以整个 V。具体实现时，还会对 scores 做一下 dropout。这是原文没有提到的，但是后来的实践中发现这么做的话训练效果更好；另外，为了提高效率，会对所有 head 的 Attention 操作合在一起做：def scaled_dot_product_attention(query, key, value, mask=None, dropout=None): """""" Args: query: (batch_size, num_heads, seq_len_q, d_k), given sequence that we focus on key: (batch_size, num_heads, seq_len_k, d_k), the sequence to check relevance with query value: (batch_size, num_heads, seq_len_v, d_k), seq_len_k == seq_len_v, usually value and key come from the same source mask: for encoder, mask is [batch_size, 1, 1, seq_len_k], for decoder, mask is [batch_size, 1, seq_len_q, seq_len_k] dropout: nn.Dropout(), optional Returns: output: (batch_size, num_heads, seq_len_q, d_v), attn: (batch_size, num_heads, seq_len_q, seq_len_k) """""" d_k = query.size(-1) # size of scores: (batch_size, num_heads, seq_len_q, seq_len_k) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) scores = scores.softmax(dim=-1) if dropout is not None: scores = dropout(scores) return torch.matmul(scores, value), scores接下来是右边的 MultiHead 操作，公式： 参数矩阵的维度： 其中 h 表示 head 的数量， 这里的 Q、K、V 对应 3 个序列，每个序列分别包含 n、m、m 个词（这里 n 和 m 不是具体的训练参数，只是为了说明各个矩阵的相对大小，比如 K 和 V 的词的数量是一样的），序列中每个词的维度则是 （即输入和输出的词的 embedding 的纬度）。Q、K、V 分别乘以对应的 W 矩阵，相当于将每个词的维度从 投影为 。经过 Attention 操作后，得到的每个 head 的维度为 ，然后拼接成 的矩阵。来看图！每个 的计算过程：原创然后拼接，再做一次线性操作：原创在模型的不同部分，Q、K、V 的来源不一样。比如论文 [1] Figure 1 中左边 Encoder，第一层 EncoderLayer 的输入为 （input embedding + positional encoding），记为 X。X 为 的矩阵，n 为 input 序列的长度（词的数量）。第一层 EncoderLayer 的 Multi-Head Attention 的 Q、K、V 都是 X。然后每一层 EncoderLayer 的输出，都作为下一层 EncoderLayer 的 Q、K、V。每一层 DecoderLayer 也是类似，主要区别是中间的 Multi-Head Attention 的 K、V 都是来自最后一层 EncoderLayer 的输出。Multi-Head Attention 代码：class MultiHeadAttention(nn.Module): def __init__(self, h, d_model, dropout_prob=0.1): """""" Args: h: number of heads d_model: dimension of the vector for each token in input and output dropout_prob: probability of dropout """""" super().__init__() self.d_k = d_model // h self.h = h # corresponding to W^Q, W^K, W^V (matrix for each head are concatenated for efficiency) # and W^O in the paper self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)]) self.dropout = nn.Dropout(dropout_prob) def forward(self, query, key, value, mask=None): """""" Args: query: (batch_size, seq_len_q, d_model) key: (batch_size, seq_len_k, d_model) value: (batch_size, seq_len_v, d_model), seq_len_k == seq_len_v mask: Returns: output: (batch_size, seq_len_q, d_model) attn: (batch_size, num_heads, seq_len_q, seq_len_k) """""" if mask is not None: mask = mask.unsqueeze(1) n_batches = query.size(0) # 1. linear projection for query, key, value # after this step, the shape of each is (batch_size, h, seq_len, d_k) query, key, value = [linear(x).view(n_batches, -1, self.h, self.d_k).transpose(1, 2) for linear, x in zip(self.linears, (query, key, value))] # 2. scaled dot product attention # out: (batch_size, h, seq_len_q, d_k) out, _ = scaled_dot_product_attention(query, key, value, mask, self.dropout) # 3. ""Concat"" using a view and apply a final linear out = ( out.transpose(1, 2) .contiguous() .view(n_batches, -1, self.h * self.d_k) ) out = self.linears[-1](out) del query, key, value return outFeed-Forward Network一个全链接层： class PositionWiseFeedForward(nn.Module): """""" Implements FFN equation. """""" def __init__(self, d_model, d_ff, dropout_prob=0.1): super().__init__() self.linear1 = nn.Linear(d_model, d_ff) self.linear2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout_prob) def forward(self, x): return self.linear2(self.dropout(nn.functional.relu(self.linear1(x))))Embeddingclass Embeddings(nn.Module): def __init__(self, d_model, vocab_size): """""" Args: vocab_size: size of the vocabulary, that is, total number of unique tokens """""" super().__init__() self.lookup_table = nn.Embedding(vocab_size, d_model) self.d_model = d_model def forward(self, x): return self.lookup_table(x) * math.sqrt(self.d_model)Positional Encoding公式： 来看看分母： ，注意 是一个小数，一般计算库实现这种指数为小数的计算时，都会转换为 exp() 和 log() 来计算。对于上面这个例子，就是这样子计算： （因为 ）实现的时候，先把指数统一算好，然后套一层 exp() 操作，这样计算效率比较高。class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout_prob, max_len=5000): super().__init__() self.dropout = nn.Dropout(dropout_prob) # Compute the positional encodings once in log space. positional_encodings = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) # size: (max_len, 1) # Equivalent to 1 / (10000 ^ (2i / d_model)) in the paper div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) # size: (d_model / 2,) positional_encodings[:, 0::2] = torch.sin(position * div_term) positional_encodings[:, 1::2] = torch.cos(position * div_term) positional_encodings = positional_encodings.unsqueeze(0) # size: (1, max_len, d_model) self.register_buffer('positional_encodings', positional_encodings) def forward(self, x): x = x + self.positional_encodings[:, :x.size(1)] return self.dropout(x)创建模型给定超参，创建模型：def make_model(src_vocab_size, tgt_vocab_size, N=6, d_model=512, d_ff=2048, h=8, dropout_prob=0.1, max_len=5000): """""" Helper: Construct a model from hyperparameters. """""" func_copy = copy.deepcopy attention = MultiHeadAttention(h, d_model, dropout_prob) feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout_prob) positional_encoding = PositionalEncoding(d_model, dropout_prob, max_len) model = Transformer( encoder=Encoder(EncoderLayer(d_model, func_copy(attention), func_copy(feed_forward), dropout_prob), N), decoder=Decoder(DecoderLayer(d_model, func_copy(attention), func_copy(attention), func_copy(feed_forward), dropout_prob), N), src_embed=nn.Sequential(Embeddings(d_model, src_vocab_size), func_copy(positional_encoding)), tgt_embed=nn.Sequential(Embeddings(d_model, tgt_vocab_size), func_copy(positional_encoding)), generator=Generator(d_model, tgt_vocab_size) ) # Initialize parameters with Glorot / fan_avg. for p in model.parameters(): if p.dim() > 1: nn.init.xavier_uniform_(p) return model简单测试写个简单的推理步骤，测试下前面的代码逻辑有没有 bug：def inference_test(): vocab = ['<pad>', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'] vocab_size = len(vocab) test_model = make_model(vocab_size, vocab_size, N=2) test_model.eval() src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]) # (batch_size, seq_len) src_mask = torch.ones(1, 1, 10) # (batch_size, 1, seq_len) memory = test_model.do_encode(src, src_mask) ys = torch.zeros(1, 1).type_as(src) for i in range(9): out = test_model.do_decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src)) prob = test_model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.data[0] ys = torch.cat([ys, torch.empty(1, 1).type_as(src).fill_(next_word)], dim=1) print(""Example Untrained Model Prediction: "", ys) def run_tests(): for _ in range(10): inference_test() if __name__ == ""__main__"": run_tests()Q：为什么会有 src_mask？A：训练时，每个样本的输入、输出序列的长度不一致，然后就通过 padding 填成一样的大小。src_mask 一般用于标记出输入序列中的 padding 位置，然后在 Attention 操作中将 scores（Q * K 的结果）的对应位置设为负无穷，来消除 padding 的影响。3、训练配置和流程对应论文 [1] 的第 5 部分 - Training。Optimizerlearning rate 按下面的公式来变化： 例如，在 分别为 256、512、512，warmup step 分别为 4000、4000、8000 的情况下，learning rate 的变化如下：def learning_rate(step, model_size, factor, warmup): """""" We have to default the step to 1 for LambdaLR function to avoid zero raising to negative power. """""" if step == 0: step = 1 return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))RegularizationLabel Smoothing这一步是为了提高模型的泛化能力～ 使用了 Kullback–Leibler (KL) divergence 的方式实现。class LabelSmoothing(nn.Module): """""" Implement label smoothing using KL divergence. """""" def __init__(self, size, padding_idx, smoothing=0.0): """""" Args: size: number of classes padding_idx: index of padding token smoothing: smoothing rate """""" super().__init__() self.criterion = nn.KLDivLoss(reduction=""sum"") self.padding_idx = padding_idx self.confidence = 1.0 - smoothing self.smoothing = smoothing self.size = size self.true_dist = None def forward(self, x, target): assert x.size(1) == self.size true_dist = x.clone() true_dist.fill_(self.smoothing / (self.size - 2)) true_dist.scatter_(1, target.unsqueeze(1), self.confidence) true_dist[:, self.padding_idx] = 0 mask = torch.nonzero(target == self.padding_idx) if mask.dim() > 0: true_dist.index_fill_(0, mask.squeeze(), 0.0) self.true_dist = true_dist return self.criterion(x, true_dist) Batch and Masking通过 Batch 对象来管理训练过程每个 batch 的输入、输出序列，以及对应的 mask。class Batch: """""" Object for holding a batch of src and target sentences for training, as well as constructing masks. """""" def __init__(self, src, tgt=None, pad_idx=2): self.src = src self.src_mask = (src != pad_idx).unsqueeze(-2) if tgt is not None: self.tgt = tgt[:, :-1] self.tgt_mask = self.make_std_mask(self.tgt, pad_idx) self.tgt_y = tgt[:, 1:] # used for loss calculation self.ntokens = (self.tgt_y != pad_idx).data.sum() # used for loss calculation @staticmethod def make_std_mask(tgt, pad_idx): """""" Create a mask to hide padding and future words. """""" tgt_mask = (tgt != pad_idx).unsqueeze(-2) tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask) return tgt_maskEpochrun_epoch(): 对整个训练集完成一轮训练class TrainState: """""" Track number of steps, examples, and tokens processed. """""" step: int = 0 # Steps in the current epoch accum_step: int = 0 # Number of gradient accumulation steps samples: int = 0 # Total number of examples used tokens: int = 0 # Total number of tokens processed def run_epoch( data_iter, model, loss_compute, optimizer, scheduler, mode=""train"", accum_iter=1, train_state=TrainState(), ): """""" Train a single epoch. """""" start = time.time() total_tokens = 0 total_loss = 0 tokens = 0 n_accum = 0 for i, batch in enumerate(data_iter): out = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask) loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens) if mode == ""train"" or mode == ""train+log"": loss_node.backward() train_state.step += 1 train_state.samples += batch.src.shape[0] train_state.tokens += batch.ntokens # implement as gradient accumulation - often used when the available hardware # cannot handle the desired batch size due to memory constraints. if i % accum_iter == 0: optimizer.step() optimizer.zero_grad(set_to_none=True) n_accum += 1 train_state.accum_step += 1 scheduler.step() total_loss += loss total_tokens += batch.ntokens tokens += batch.ntokens if i % 40 == 1 and (mode == ""train"" or mode == ""train+log""): lr = optimizer.param_groups[0][""lr""] elapsed = time.time() - start print( ( ""Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f | "" + ""Tokens per Sec: %7.1f | Learning Rate: %6.1e"" ) % ( i, n_accum, loss / batch.ntokens, tokens / elapsed, lr) ) start = time.time() tokens = 0 del loss, loss_node return total_loss / total_tokens, train_state 4、示例任务harvard nlp 团队给的示例任务中，配置有点问题，于是我改了下，变成针对字母 'a' 至 'k' 的 copy task（即输入为字母序列，输出为相同的字母序列），加上了常见的 <pad>, <start>, <end> 等 symbol。"""""" A simple copy task: Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols. """""" import torch from torch.optim.lr_scheduler import LambdaLR from train import * from models.transformer import make_model from models.utils import * def data_gen(vocab, batch_size, n_batches, max_len=8): """""" Generate random data for a src-tgt copy task. """""" for i in range(n_batches): start_symbol_idx = vocab['<start>'] end_symbol_idx = vocab['<end>'] # Create a tensor with random integers between (start_symbol_idx + 1) and (end_symbol_idx - 1), size: (batch_size, 10) data = torch.randint(start_symbol_idx + 1, end_symbol_idx, size=(batch_size, max_len)) data[:, 0] = start_symbol_idx data[:, -1] = end_symbol_idx src = data.requires_grad_(False).clone().detach() tgt = data.requires_grad_(False).clone().detach() yield Batch(src, tgt, pad_idx=vocab['<pad>']) class SimpleLossCompute: """""" A simple loss compute and train function. """""" def __init__(self, generator, criterion): """""" Args: generator: nn.Module, used to generate the probability distribution over the target vocabulary """""" self.generator = generator self.criterion = criterion def __call__(self, x, y, norm): x = self.generator(x) loss = ( self.criterion( x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1) ) / norm ) return loss.data * norm, loss def greedy_decode(model, src, src_mask, max_len, start_symbol): """""" Predicts a translation using greedy decoding for simplicity. """""" memory = model.do_encode(src, src_mask) ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data) for i in range(max_len - 1): out = model.do_decode( memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data) ) prob = model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.data[0] ys = torch.cat( [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1 ) return ys def example_simple_model(): # Example copy task for symbols: 'a' to 'k' vocab = { '<pad>': 0, # Padding token '<start>': 1, # Start of sequence token 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, '<end>': 13 # End of sequence token } vocab_size = len(vocab) # Should be 10 in this case criterion = LabelSmoothing(size=vocab_size, padding_idx=vocab['<pad>'], smoothing=0.0) model = make_model(vocab_size, vocab_size, N=2) optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9) lr_scheduler = LambdaLR( optimizer=optimizer, lr_lambda=lambda step: learning_rate( step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400 ), ) batch_size = 80 for epoch in range(20): model.train() run_epoch( data_gen(vocab, batch_size=batch_size, n_batches=20), model, SimpleLossCompute(model.generator, criterion), optimizer, lr_scheduler, mode=""train"", ) model.eval() run_epoch( data_gen(vocab, batch_size=batch_size, n_batches=5), model, SimpleLossCompute(model.generator, criterion), DummyOptimizer(), DummyScheduler(), mode=""eval"", ) model.eval() max_len = 8 src_sentence = ['<start>', 'a', 'b', 'c', 'i', 'j', 'k', '<end>'] src = torch.LongTensor([[vocab[word] for word in src_sentence]]) src_mask = torch.ones(1, 1, max_len) prediction = greedy_decode(model, src, src_mask, max_len, start_symbol=vocab['<start>']) idx_to_word = {idx: word for word, idx in vocab.items()} prediction_sentence = [idx_to_word[idx.item()] for idx in prediction[0]] print(""Example Trained Model Prediction: "", prediction_sentence) if __name__ == ""__main__"": example_simple_model()训练过程和结果：总结1、本文的目标是弄清 transformer 的结构，以及训练和推理流程，所以就不去弄翻译任务了，后面有需要再更新；2、Transformer 相比于 RNN，其中一个很突出的优点是很多地方可以做并行优化，比如里面很多计算可以合并为一个大的矩阵运算（Attention 那里），而矩阵运算是 cuda 算子中算是优化得最好的操作之一了。还有就是在训练的时候，每个 output token 的输出不需要等前一 token 算出来（RNN 就需要），而是可以直接算出整个 output sequence，然后算 loss，这样效率就高太多了；3、接下来就是去研究 Megatron，还有看 GPT 1 到 GPT 4 的相关论文！Reference:[1] Attention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf[2] 李沐的论文讲解视频：https://www.bilibili.com/video/BV1pu411o7BE[3] harvard nlp 团队的实现：http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule[4] 知乎文章：初识CV：Transformer模型详解（图解最完整版）[5] The Illustrated Transformer[6] 知乎文章：叮当猫：【DL&NLP】再谈Layer-Norm：Pre-LN、Post-LN、DeepNorm[7] On Layer Normalization in the Transformer Architecture: https://openreview.net/pdf?id=B1x8anVFPr"
662538702,9f67fc1890367220853da5c0621ddfb9,凡心,https://api.zhihu.com/people/9f67fc1890367220853da5c0621ddfb9,people,dengfanxin.cn | 通用人工智能研究者,从第一性原理看大模型Agent技术,article,https://api.zhihu.com/articles/662538702,还需要更高层次的认知模型。Lecun在思考AGI时对<em>大模型</em>的出现也提出过意见，它认为世界模型才是关键，但前两天新的研究却认为<em>大模型</em>中有世界模型。但毫无疑问的一点是,432,27,1229,1697849079,1697849079,"大模型技术开启了全新的时代机会，面对眼花缭乱的大模型资讯，你是否对新技术的本质没有头绪？是否对新技术的潜力难以判断？本文将结合通用人工智能理论以及面向目标框架，探讨Agent的发展趋势。引一个乐观主义者的悲观估计随着大规模模型技术的兴起，我们正处于一个崭新的智能时代的黎明。我们有一个大胆的预测，未来的5到10年将可能带来一场大变局：99%的开发、设计和文字工作将被AI接管。这不仅仅是一个想象，而是对未来可能趋势的深思熟虑。在互联网时代，我们目睹了大量网站的崛起，成为互联网时代的原生应用的主要载体，这个时代有了 Web 相关的新技术，这些技术承载着 Google、Facebook、Twitter 等互联网明星企业的崛起。进入移动互联网时代，App 成为了主要载体，它们占据了整个移动互联网时代人们注意力的中心。然而，当我们进入智能时代，开始思考：什么将会成为这个时代的核心载体？我们预测可能不再是 App，也不再是网站，而是 Agent，也许几年后的现实才能给出答案，但历史告诉我们一个新鲜事物的演进总会找到一个稳定的术语来概括这个载体，而今天我们看到 Agent 最具有这个潜力。对于这个猜想，随着我们后面讲的越来越深入，大家可能体会也会越来越深，因为它可能抓住的一些智能时代一个非常本质性的东西，基于大模型把 Agent 的能力推高到了一个全新的水平，这个在学术界研究了几十年的全新物种才开始走入人类舞台的中央。在范式上我们也在思考是什么样的底层技术和架构会驱动 Agent 技术的快速发展，从现在的趋势来看，我们觉得在人类的智能原理处于核心地位的面向目标架构很可能会成为驱动这个 Agent 技术发展的主要范式。而且现在全球对Agent的关注也是非常狂热的，几个月前，OpenAI 在内部就开始高度关注智能体（Agent）领域，Deep Mind的联合创始人最近也提到下一代 AI 技术走向并非是生成性 AI，而应该是交互性 AI。这种交互性 AI 在很大程度上类似我们今天提到的智能体，用户要求完成各种任务，智能体则可以对软件进行操作或者与人进行协作，完成相关的工作。希望这些想法可以帮助大家在战略层面或者是产品和技术层面得到一个很好的思考框架，能够在各自的领域去推演未来。导读走马观花、抛砖引玉整个 PPT 一百多页，内容繁杂。我给大家做一个简单的导读，主要还是梳理脉络，提出思考的框架和基点，起到抛砖引玉的作用，其中涉及的很多细节部分我们并没有做得非常细致。期望大家在看完这个分享之后，对智能体技术有一个新的认知，并能应用在你们自己的研究方向或者职业规划上。我们将整个内容分为四个部分：首先，会对从大模型到现在的智能体的技术发展做一个串讲；接着，介绍通用智能原理和面向目标架构这个两个根本性问题；最后，基于这两个原理，分析现在的智能体架构仍存在的缺陷，以及未来可能的发展方向。1. LLM Agents综述如果你一直关注 AI 领域，你应该能看到一个清晰的技术脉络，一开始大家玩 Prompt 工程，接着是Prompt Chain或Flow，再到Agent，多Agent，很清晰的一个脉络架构，我们也会沿着这个脉络给大家分享相关的经典工作。我们回到 Agent 这个概念上，实际上，人类是这个星球上最强大的 Agent。Agent是一个能感知并自主地采取行动的实体，这里的自主性极其关键，Agent要能够实现设定的目标，其中包括具备学习和获取知识的能力以提高自身性能。Agent 的复杂程度各不相同，一个简单的恒温器可以是一个 Agent，一个大型的国家或者一个生物群体也可能是个 Agent。感知环境、自主决策、具备行动能力，设定明确的目标和任务，适应环境及学习能力，都是 Agent 的关键特点。Agent 这一概念的起源并不明确，可以是学术界逐渐发展沉淀而来，但是我们可以考虑马文·明斯基80年代《智能社会》这本书中在人工智能领域发扬光大了这个概念。Agent 理论在大模型时代之前已经被学术界研究了很多年，甚至是汗牛充栋，许多理论研究都试图创造出具有人类智能水平的 Agent。然而，在大模型出现之前，Agent 的技术始终面对天花板限制，无法取得实用的进步，它的本质问题还是AGI问题，反过来说，只有AGI的技术进步才能让 Agent 技术进步。在学术领域，最经典的案例可能是与机器人相关的研究，都涉及到了Agent 技术。在大模型时代之前，比较知名的垂直领域 Agent 的例子比如 Alphago，它有感知环境、做决策、采取行动的闭环，当时的主要研究方向还有使用强化学习打游戏的DeepMind的Agent57，后来更加通用的Gato，还有OpenAI玩“躲猫猫”的多智能体。我们认为Agent技术是未来实现社会全面自动化的关键技术。在大模型出现之前，自动化更多的是一些偏结构化固定模式环境中通过实现固定算法流程来完成自动化任务，而大模型智能体的通用性带来了灵活性，使其可能应对人类在脑力劳动中面临的各种复杂长尾任务，进一步实现体力和脑力任务的全面自动化。大模型和Agent技术开启了全面自动化的新时代。大模型是第一个可以自主学习并拥有广泛知识的模型，所以在大模型时代，Agent技术开始迅速发展。今天，我们可能只是在起点，我们看到的Agent还偏向于玩具，但是预计在未来几年，这个领域将产生极大的改变，它的发展速度可能会超越我们的想象，因为我们现在看到改进每天都在发生，天花板远未来到，甚至天花板可能不会再来了。1.1 Prompt工程在大模型刚出来的时候，大家都喜欢做的事就是Prompt工程，把大模型当成一种编程语言来看待。人们通过描述角色技能、任务关键词、任务目标及任务背景，告知大模型需要输出的格式，并调用大模型进行输出。这种方法就是经典的把大模型当做工具来调用，我们可以称为工具模式。为此，大家也发展了各种各样的Prompt工程的玩法，如角色扮演、零样本提示和少样本提示。比如一个澳大利亚少年编写了一个15000个字符的提示词，成功地让他变身为人类的导师，教授各种知识。这种方式就像能直接构建软件一样，是我们将Prompt工程发挥到极致的一个经典案例。https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor1.2 Prompt外挂仅凭Prompt工程根本无法满足人们日益增长的大模型需要，鉴于大模型本身的诸多缺陷，如不能及时更新知识，上下文有限等等，人们开始给大模型加入插件，如引入向量数据库，把数据索引进向量数据库，再召回数据，再提交给大模型做Prompt工程，这样就可以使用最新的知识和比大模型里的知识更准确的知识。这些还不够，人们又开启了外挂模式，尝试让 GPT 调用函数和使用工具，一系列关于工具使用的实践开始出现，ChatGPT也推出了插件体系。当人们发现大模型的推理能力很差时，开始试图让模型自身清楚地描述问题，把问题转化为 PDDL （Planning Domain Definition Language）格式的描述语言，通过调用通用规划器来解决规划问题，再把解决方案转化为可执行的动作，以更好地逻辑推理和规划等任务。更加经典的外挂形式当然是让大模型调用外部工具了，OpenAI也搞出了ChatGPT Plugins的开放体系，这些今天回头看都是非常顺理成章的形式，ChatGPT的爆发激发了全世界大量的Idea，这些创新很快就被OpenAI吸纳进产品迭代中。此外，大模型虽然具备一定的推理能力和思考能力，在很多推理任务上依然力不从心，能不能让模型自己不做规划推理，让他把问题描述清楚，转化成一个 PDDL 的一个关于规划描述的语言，然后使用通用的规划器去做规划，再转化成动作执行，这就把大模型作为一个中转器，把规划器当做了一个外挂。我们可能会思考，大模型或许真的就是我们以前想象的那样，会达到人类智慧水平的普适性机器么？显然从各项评测来看还有很多任务做不到，更何况这些任务评测本身的覆盖度也不够完备。有一个经典概念被誉为""通用任务解决器""，在达特茅斯会议之后得名“GPS”，即General Problem Solver。这是由赫伯特·西蒙（Herbert Simon）和艾伦·纽维尔（Allen Newell）在早期提出的概念，他们尝试寻找可用于解决数学问题的通用解决方案。这套理念其实很简洁，可以看作是早期的面向目标架构。它的主要内容是将目标状态列出，然后在解空间中搜索可以将初始状态转化为目标状态的操作组合，这样的组合便是问题的答案。1.3 分解与组合然而，目前我们发现，在通用人工智能（AGI）的漫长旅途中，大模型虽显强大，仍存在着显著的技术天花板。许多人开始探索如何挖掘大模型在大任务执行能力上的可能性，其中一个基本策略就是能够分解和组合。例如，经典的 MapReduce 模式可以将一个大型文本进行摘要，因为它的上下文有限，一种解决办法是扩大 context 的范围。另一个解决方案是，在有限的 context 中，我们先将文本拆分成小片段，对每个片段进行摘要，然后再将其组合，从而得出结果。大家也发现大模型直接给出答案似乎并不靠谱，那么是否可以让它像人类一样，一步一步思考呢？毕竟，人类在解决问题时，也是逐渐构建解决方案，而并非立即给出答案。因此，开始出现了一系列的尝试解法，比如思维链、多思维链、思维树和思维图等。我们一一讲解，首先是思维链（Chain of Thought，CoT），它要求模型展示其思考过程，而非仅给出答案。这可以通过两种方式实现，一种是具体说明，即要求模型详细地、一步步地思考；另一种是示例说明，即通过给定问题和答案的同时，提供思考过程。这样，当询问模型时，模型会模仿此过程，逐渐思考并给出答案。再往后，我们发现一个CoT有时可能出现错误，然后开始尝试让它发散，尝试多种思路来解决问题，然后投票选择最佳答案，这就是CoT-SC了。在这过程中，我们发现，这种发散的方法也有局限性，例如24点问题，它不能很好地解决，那么我们就会尝试把这个问题进行垂直分解，分成三步来做，每一步分解成多个子问题，类似于动态规划的做法，就好像把一个大任务拆解成了三个小的子任务，然后再一步一步地去实现它。这就是思维树（ToT， Tree of Thought）的一个主要思路，它会根据当前的问题分解出多个可能，然后每一个树节点就是父节点的一个子问题，逐层扩散，遍布整个解空间，一些节点就直接会发现不合适而终止掉，达到了有效剪枝的作用。然而 ToT 的方式也存在问题，对于一些需要分解后再整合的问题，比如排序问题，排序你可能需要分解和排序，然后再merge，就不行了。为了解决这个问题，一种名为思维图（Graph of Tree，GoT）的方法被提出。这种思维图既可以分解，也可以合并。9月26日，清华姚期智团队又提出了更新的方法——累计推理，在24点问题上成功率已经达到98%的SOTA。他们方式很接近主流 Agent 的实现方式，具备一定的通用性。它首先会提出一个初步的想法，然后再对这个想法进行验证，看这个提案是否合适。如果提案合适，就将它添加到图的下一个节点，每一步都基于已经建立的图节点进行下一个思考节点的创建，这样发散、合并或删除直到达到最终目标状态，完备性和灵活性大大增强。1.4 反馈上述的讨论主要是任务分解和组合，他们尽管强大，却不能与外界进行互动，这就不得不讲到反馈机制了。反馈是整个控制论的基石，也是动物体从诞生之初就具备的基本能力。最经典的方法实际就是 ReACT，这个方法非常经典，基本把智能体最核心的能力圈出来了，当然它也有它的缺陷，我们将在后面讨论为什么还会有 Agent 更多的复杂技术以克服它的不足。ReACT让大模型先进行思考，思考完再进行行动，然后根据行动的结果再进行观察，再进行思考，这样一步一步循环下去。这种行为模式基本上就是人类这样的智能体主要模式。比如，询问一个关于科罗拉多造山带的相关问题，它就会去通过搜索工具进行搜索，如果搜索结果没有提到东部地区，它会继续搜索东部地区的数据，然后继续一步一步地思考，根据结果反复思考，直到完成这一项任务。ChatGPT的代码解释器主要采用的就是这种模式。首先，代码解释器能够与用户进行简单的互动，如用户的问侧和解释器的回应。当用户的问题需要外部调用时，例如询问天气情况，解释器会生成相应的代码，利用代码调用外部工具获取结果。基于这些结果，代码解释器会将信息反馈给用户，如“今天天气很好”。下图是，我们调研的ChatGPT Code Interpreter 的主要实现方式。然而，我们始终觉得这样仍然不够，我们希望大模型在完成每一个任务后，能够积累经验，故而产生了借鉴强化学习思路的""反射""机制。反射机制能够让机器记住每一次任务的完成情况，无论效果好坏，以供未来参考，提升模型的性能。Agent的框架都会让模型输出JSON进行函数调用，OpenAI也就推出了Funtion Calling，将外部调用内化到模型中，变成了一种原生能力。考虑到前面说的ReACT和Reflection这些不彻底性，更大的变革轰然袭来，这就是Agent，今天4月AutoGPT横空出世，短短数周Star数就超过PyTorch达到90k，赚足了眼球。1.5 Agent今天，全世界都在关注这个领域，Agent 模式的研究和应用都在迅猛发展，作为一个""共识""可预见的未来该技术的进步将势不可挡。AutoGPT模型刚在10月15日宣布获得1200万美金的资金支持，也小道消息称OpenAI将在11月份发布面向 Agent 的原生模型和规范。百度在发布""文心一言4.0""时也表示，他们考虑在升级系统时重点考虑了系统2的能力，这也是Agent 模式的关键设计。下图是AutoGPT 发布的进行中的架构图，旨在实现对任务的有效管理。生成的任务将会被加入优先级队列中，随后系统会不断从优先队列中选择优先级最高的任务进行执行，整个过程中，任何反馈都会通过记忆进行迭代优化代码。这个主要框架虽然相对简单，但其设计理念具有重要意义。首先，创建一个初始的计划，然后进入主循环。系统会让模型判断在当前计划下该进行何种行动，接着会执行行动。执行完毕后，结果会写入下一次循环中。如此，每次决策都会基于之前的结果、记忆和计划，从而制定出新的行动方案。在该框架中，模型的决策过程涉及到动作选择，这也是主要的功能之一。此外，整个过程中我们主要关注的一些工具包括“Start Another Agent”以及“Task Complete”。这两个工具体现了Agent可以被调用，从而将大任务拆解为若干小任务进行处理，继而形成层次化的树状结构，这种结构与人类分工和协作的工作方式极为相似。值得一提的是，微软的贾维斯 (Jarvis)一个深度学习任务调度系统，也采用了类似思想。他们主要关注如何调用模型来执行各种深度学习任务，涉及到了先做计划，再选择模型，然后执行任务，获取反馈，然后进入下一轮循环等环节。有的研究者会尝试使用大模型写小说，借鉴LSTM这个经典深度网络的思想发明RecurrentGPT，还引入了长时记忆和短时记忆机制，使模型拥有了更佳的记忆和学习功能。其他方向，我们看到把大模型视作一个虚拟世界中的智能体，如MineCraft游戏中所设定的角色。这个角色可以沿着指定的路线，完成一些在环境中探索的任务，如建房子、挖矿、打怪等。这个角色首先需要被告知怎样去执行任务，例如自动训练课程计划的使用。然后逐步的完成任务，形成自己的执行代码库、技能库等，这样就算是在以后遇到相似的任务，它都能快速调用已有的技能和经验来完成任务。某种意义上，这就是一种强化学习的方式。这个方向的变化真的是一日千里，就在昨天，清华联合面壁发布了XAgent，提出了双循环机制在效果上碾压了AutoGPT。这种机制中，外循环负责宏观规划，而内循环则负责细节的执行。双循环模式在完成各类任务的时候，它的能力也大大胜过 GPT 4。这里需要解释一下，研究者为什么把 GPT 4 和 Agent 进行对比，看起来GPT 4只是 Agent 的一个组件，自行车的轮子怎么能和自行车对比呢？这是因为从任务完成的视角看，GPT-4也是一种Agent，Agent的概念是大模型的超集，这也是为什么我们说智能时代的核心载体是 Agent 而不是大模型的原因，这个事物的发展形态终将会走到一个最稳定的形态，这个形态目前看只有 Agent 最配这个颜值担当。当能不大众所接受的对等 Agent 含义的词汇可能还在发明中，至少目前看伙伴、助手这些命名的水平都不够高，让我们拭目以待。1.6 Multi-Agent进一步，人们很自然地想到了多智能体（Multi-agent）模式， ""斯坦福小镇""开了一个好头。在这个虚拟的小镇里，每个角色都是一个单独的智能体，每天依据制定的计划按照设定的角色去活动和做事情，当他们相遇并交谈时，他们的交谈内容会被存储在记忆数据库中，并在第二天的活动计划中被回忆和引用，这一过程中就能涌现出许多颇有趣味性的社会学现象，我们成为群体智能的涌现。再看今年7月份，一个被命名为MetaGPT的项目引起了广泛关注，这个项目中定义了产品经理、架构师、项目管理员、工程师和质量保证等角色，各角色之间通过相互协作，基本可以胜任完成500行左右代码的小工程了。很简单的一个实现就能够完成500行，后续改进后，是否有可能完成5000行或者是5万行代码的项目呢？譬如前两天微软刚发布的 Code Plan 项目已经开始尝试实施大型工程的改造计划。Meta GPT 最有价值的思想是借鉴人类社会中的协作方式，尤其是SOP，之于Agent 设计则平平无奇，也包括观察、思考、状态管理、任务行动以及结果反馈等等必备组件。同样的思路，清华开发了一个名为ChatDev的系统，进一步引入了CEO等角色，这里就不再展开描述。值得一提的是，Agent 的应用方向其实非常广泛。比如 RPA 公司实在智能把 Agent 用于他们的产品调用常见桌面软件，如淘宝网、钉钉，来自动完成桌面任务。而任何一个 Agent 的实现，似乎共性都挺多，都需要有长短时记忆能力、工具使用能力、通信能力，甚至包括 SOP 的能力，自然而言就有人要做这样的框架了，如 agents。1.7 简单的难题尽管 GPT-4 等模型非常强大、Agent的发展似乎牛气冲天，它们仍然无法满足很多任务的需要，甚至一些在我们看来很简单的任务都完成不了，比如我们构造的这个任务：给小学生展示一下两数相加的每一步计算过程，如1135 + 78 答：计算详细过程如下 5+8=13， 进位1 3+7+1=11， 进位1 一个数已经加完，剩余数11 + 1 = 12 结果为：1211 下面请列出以下两数的详细计算过程： 81728738271872871871672 + 28781729836746721给小学生展示一下两数相加的每一步计算过程，如1135 + 78我们必须明白，尽管AI在一定程度上模仿了人脑的工作方式，但实际上，机器人和人脑在处理信息时采用的策略有很大的不同。因此，即使在未来，我们也需要继续改进 AI 框架，以解决这种差距。比如一个百万位数的加法任务，GPT-4囿于token数的限制是不可能完成这个任务的，但人类缺可以，这恰是人类和AI需要弥补的Gap。我们进行了一些简单的试验，还没有发现大模型和Agent能搞定这个任务。其中，ChatGPT4的Code Interpreter是表现最好的，因为它调用了外部计算器，但中间的过程描述还是发生了错误。如果你能做出来这个小作业，欢迎联系我。至此，我们已经讲述了大模型到 Agent 的发展历程。接下来的时间，我们将从人类智能的视角，结合面向目标架构的理念，分析 Agent 技术的本质、存在的缺陷以及未来可能的发展方向。2. 通用智能基本原理首先我们来看看这个众人熟知的认知飞轮，感知、认知、决策、行动，今天的人工智能代理更像是基于这个认知飞龙构建的。但是从本质上，人类智能远比这复杂。在漫长的进化历史中，生物神经网络从简单的条件反射逐渐进化到今天的主动预测，我们已经可以在大脑中构建世界模型，进行强大的推理和分析。看似繁杂的过程，实际上都发生在核心的架构上，并且逐步完善。无论是工作记忆，还是人类处理语言的能力的诞生，这些都是智能的必不可少的元素，尤其是符号能力，对人类智能的发展有着不可替代的作用。因此，让我们先提出一个更为宏观的问题，智能究竟是什么？我强烈推荐这本名为《预测算法》的书，它在20年发表，那一年，GPT 3也刚刚问世，我在阅读之后，就有这样一个感觉：生成模型是战略正确的。在之前关于AGI的分享中，也提到过这个观点，智能是通过预测来解决应对世界的不确定性的，分享视频参见这里https://www.bilibili.com/video/BV16h4y1w79A/让我们深入理解一下模拟的概念，当一个低等动物接触到外界的刺激，它会收缩来逃避潜在的风险。这其实是一种模拟，只不过这个模拟反射神经元对有些过于反应敏锐，它假设所有的刺激都是潜在的危险。然而，对于人类来说，我们的模拟则更为精细。我们对世界进行建模，把世界以实体、关系、属性描绘出来。然而，这也是我们认知的极限，我们只能理解一个对象化的世界，非对象化的世界我们无法理解。比如，当我们探索量子的时候，我们还常常用对事物进行对象化的方式去理解，但是发现我们的理解力有时候是有限的，因为量子世界的真相超出了人类认知能力的范围，我们智能使用低维空间的投影去推断它，就像我们无法在三维世界去想象十一维世界的样子。在过去的四十年里，科学家对认知架构有很多深入的研究，并尝试据此研发出通用人工智能，但天地不仁以万物为刍狗，当前来看只有GPT系列模型距离实现通用人工智能最近，当然这些认知理论依然具有巨大的参考和指导意义。深入地聊认知架构和智能原理之前，我们必须要聊的是绕不开的《思考快与慢》，这是一本畅销书，其后面的学术道理也十分受用。大脑中的系统1和系统2是我们所有人都熟知的，尽管在实际实现中，系统2可能由系统1涌现，但至少在表现上，我们的大脑看起来有两个系统，系统1和系统2，分别负责不同的功能。知识和情感的快速反应被称为系统1，而逻辑性强、思考速度慢的反应被称为系统2。接下来我们看看这些认知架构中，有一个叫做GWT(Global Workspace Theory，全局工作空间理论)，如下图所示：全局工作空间理论（GWT）是认知科学家伯纳德·巴尔斯（Bernard Baars）和斯坦·富兰克林（Stan Franklin）在20世纪80年代后期提出的一种意识思维框架。它被开发出来，以定性地解释一系列有意识和无意识过程之间的匹配。GWT在建模意识和高级认知方面具有影响力，认为它们是从广泛、并行的神经过程中信息的竞争和集成流动中产生的。系统1涵盖了神经网络的外围连接，涉及长期记忆、价值系统、感知运动控制相关的神经网络，系统2则是一个高度集中的“舞台”，人类的有意识思考，如做数学题时，脑中想象数字相加的过程，都在这个舞台上进行。这个舞台叫全局工作空间，记忆在这个舞台上被拉进来加工，然后被扔出去。LIDA (Learning Intelligent Distribution Agent) 受到多种计算范例的启发，并且实现了GWT。 认知模块包括知觉关联记忆，情景记忆，意识，程序性记忆和行动选择。由 LIDA 架构控制的认知机器人和软件代理将能够进行多种学习机制。其实在大模型Agent技术出现之前，人们就已经意识到，试图集成各种深度学习模型以实现人工普遍智能（AGI）并不够，还需要更高层次的认知模型。Lecun在思考AGI时对大模型的出现也提出过意见，它认为世界模型才是关键，但前两天新的研究却认为大模型中有世界模型。但毫无疑问的一点是，世界模型对于我们对世界的认知是非常关键的，无论大模型中是否包含世界的认知，Agent都必须对世界有准确的理解才能做出正确的决策。当模型不能正确运行时，决策就会出错；只有当世界模型构建的正确，才能选择正确的模型，进而做出正确的决策。总结一下，系统2包含意识、思考、符号主义、逻辑推理图灵、机制结构化和模型。而系统1包含快速思考、神经网络连接主义、长期记忆、深度学习、亚符号、潜意识和非结构化数据。在构建 Agent 时，可以参考这两种系统的思维框架。在理解智能架构的概念时，我们需要从记忆空间、符号系统、世界模型构建与加工三个方向去考虑。记忆空间是基础，符号系统是思考和推理的核心，而世界模型的构建和加工则是其中最重要的环节。在现在的大模型中，如 GPT，虽然很多人认为它没有符号系统，但我们认为，其内部的注意力机制可能已经在激活流转过程中模拟了世界模型的加工过程，只是这个过程并不显式，而且无法控制，只能通过Prompt工程引导它进行，但它会经常跑偏。我们通过学习掌握了对世界的知识，并针对感知数据尝试在符号系统中构建世界模型，进行预测和行动。如弹钢琴这样的行动，我们需要通过反复训练，逐渐将运动序列内化，变成肌肉记忆和反射。这些在系统2中反复出现的行为，会逐渐沉淀到系统1中。这个过程可以理解为一个“快捷通道”的形成过程，称为Shortcut。人的视觉识别过程是一个层次性的关系，从最初级的视觉皮层一直到更高级的皮层，从简单的视觉边缘特征到线条的方向性，再到线条之间的组合，如角等更高维特征的形成，直到形成物体的感知。这些物体的概念再对应符号系统和自然语言的绑定，当图像信息经过解码过程进入符号系统后，我们的关联记忆会帮助我们召回数字等语义概念。以人类做加法为例，假设我们要解决“219 + 13”的问题，这个过程可能会遇到一个看似相同的图形，比如图中有""13""和""B""的歧义。这就打破了现在很多人的想法，通常我们喜欢做前向过程，先使用一个视觉模型处理输入，然后再将其输出传递给大模型进行处理。实际上，人在理解这个场景时是一个双向过程，首先有一些直觉的特征传入到系统2，系统2会推断这是一个做加法任务，并将看似“B”的图形解释为13，这个过程称为Projection。例如，我们经常从一些像素点中识别出人脸，这就是由上至下的功效发挥作用，这是对未来人工智能代理（Agent）的一种启发。另一个关键的能力是关联记忆。当我们开始观察某个物体时，比如进行加法操作时，我们的大脑并不会以固定模式运作。相反，我们的神经网络会并行运行，有的神经网络开始将加法的概念、数字的概念以及加法规则等各种信息激活，所有这些信息都会基于一个关联网络唤醒出来，这样我们就可以开始下一步的工作。接下来就是所谓的结构推理，我们会开始将这些符号结构化，例如，如果它是一个三位数，我们就会开始理解它的每一位构成整体和部分之间的关系。当我们已经理解到219 + 13是加法时，我们也会执行Structure Inference得到结构的认知A+B=C的两位数加法结构，并将219和A对应上，13和B对应上，这个过程就是Variable Binding了，我们将具体的实例与它的角色对应上了。接着我们要遵循加法规则进行运算以实现我们的目标——完成加法任务。根据我们打算完成的目标以及现在的状态，我们需要规划出达成目标所需要的具体步骤，即执行加法规则。进入到这样一个循环过程之中，我们会额外提到两个概念，即""Shortcut""和""Exception""。那么什么是Shortcut呢？当我们初次开始书写数字时，速度往往很慢，但随着练习，我们将逐渐写得越来越快。这个过程实际上包含了一个叫做“Recoding”的过程，我们会将熟悉的操作或流程用神经元重新表示，这样就把一个复杂的操作简化为了一个子任务，通过类似于传参的方式控制一个子神经网络完成任务。比如开车，一开始，每个动作都需要集中注意力，严重依赖系统2，但是开了一段时间之后，就可以自如地进行了，这就是因为系统2的控制能力已经被沉淀到了系统1里面，称为Shortcut。另一个重要的方面是异常处理能力，人类最强大的能力就是能够随时应对异常。譬如，你在走路时突然被绊了一跤，你首先需要应对的就是摔倒这个状况，然后再回到原来的路线上继续走。因此，在执行加法过程中，并不是由于一个细节被中断或遇到各种异常，才开始执行加法。我们会发现，在遇到各种问题时，我们总是会奔着目标勇往直前。人是一个运作着面向目标架构的复杂过程。面向目标架构是人类智能的一个核心机制，当然并不是唯一的。有时，我们也会没有具体的目标或者说目标不是显式的，比如citywalking，同时有一些底层的目标机制，诸如生存，这说明人的面向目标架构要复杂许多。这就是我们不得不说的智能核心的面向目标架构3. 面向目标架构我们的情绪系统其实也在解决目标问题，例如，你会因为目标无法达成而生气，因为目标可能无法达成焦虑，因为别阻碍你的目标而愤怒。显而易见，许多情绪都与目标机制有所关联。因此，这套面向目标的机制在人的智能运作中占有极其核心的地位。让我们通过一个简单的模型来描述该机制。首先，我们需要对这个世界有理解，因此我们会在脑中构建一个关于世界的模型。这个模型在结构化之后，就会变成了当前世界状态。而我们的目标是对应的一个目标世界状态。因此，人类就是在不停地消除当前状态和目标状态之间的差异，这个消除的过程就是目标驱动的过程。在目标驱动的过程中，你开始尝试去解决这个问题，消除这个差异，你也可能有现成的解决方案，直接动用已有的解决方案执行已知的运动序列，也可能需要进行一定的思考，做出推理分析帮助你解决问题。一旦你找到了一些执行序列，这些序列可能会变成一个子序列，子序列里有子目标。每个子目标的执行有可能是直接完成的，也可能需要进一步思考才能完成。正如我们可以看到，GPS这段代码就是在为了达成某一个目标而工作，它会遍历所有的目标，尝试让每一个目标都能够达成，一旦达成就结束。有兴趣的同学可以读一下这个代码，就是做暴力遍历找出达到目标状态的操作序列。不过，像GPS这种理想的解决方案在现实世界中可能并不奏效，因为真实世界的解空间过于庞大，想想AlphaGo的故事就理解了，这也是为什么虽然此想法在理论上看起来很好，但在实际操作时却无法实施。但这种思考很有启发，在Newell和Simon1972年出版的《Human Problem Solving》一书中，他们研究了人类如何解决问题，并意识到我们经常进行手段-目的分析(means-ends)举一个例子：""我想把儿子送到幼儿园。我现在的状态和我想要的状态之间有什么区别？其中一个是距离。是什么因素会改变距离？我的汽车。可是我的汽车坏了。要让它工作需要什么？一个新电池。哪里能买到新电池？汽车修理店。我想让修理店为我安装一个新电池，但店里不知道我需要一个新电池。问题出在哪里？是沟通的问题。什么能让沟通变得容易？一部电话……以此类推。""在计算机领域，有很多方法都与目标机制相关。例如，过程描述语言（PDL）就是一种经典的方法，主要用于解决机器人问题。我们可以描述世界上的对象，它们当前的状态是怎样的，目标状态是怎样的，有哪些可以采取的操作，然后我们可以基于这些操作，使用规划器寻找一个合适的运动序列来解决问题。但在今天计算机领域的工程实践中，人们更多采用的是面向过程架构，无论是接口、函数、UI界面，还是组件，又或者是一个应用程序，都是以接口的形式存在的。而这个接口实质上是一种被调用的子流程，借此过程的完成，我们希望执行结果符合我们的预期，但程序并不为结果负责。它解决的是过程和流程问题，系统内没有目标的概念。当然，也存在一些以目标导向为核心理念的的软件工程，例如声明式编程，它只需要你描述你想要什么，而无需关心执行的过程，像HTML和SQL便是其经典例子。在这样的架构下，程序能够自行寻找达成目标的方法。然而问题在于，这种面向目标的架构只能应用于垂直领域，而无法普遍应用到所有领域，只有在特定的领域内才能发挥作用，这就限制了它的应用范围。总的来说，尽管面向目标架构在计算机领域有一席之地，但由于其只能在特定领域发挥作用，而无法解决所有领域的问题，因此它的应用还是有所限制，更多出现在特定的DSL（领域特定语言）中，这种架构的确也发挥了巨大的作用。在软件工程的范式迁移中，我们发现面向过程架构与面向目标架构之间的重要区别点：随着人类的生产方式的变化，软件工程可能正逐步演化为智能体工程(Agent Engineering)；以前我们主导的生产方式是人类处于中心位，AI做辅助。而未来可能会变成以 AI 为中心，人类变为辅助。由此，整个产品形态和平台的构成可能会发生这样的转变。在这一转变中，原本由人类主导的功能开发，逐渐演变为以智能体为主要驱动力。传统的用户界面，由于其垂直的任务层级架构，每一层都需要人类逐一生成，未来这个过程可能会被智能体自主生成并改良。此外，原本只能解决有限范围的任务，未来的架构则可以解决无限域的任务。就如同头条这样的平台，它是一个信息的分发平台。那么，是否会出现新的平台模式？比如一种知识和世界模型的分发平台。以前我们只能处理大量长尾数据，在未来可能能解决大量长尾任务。以前是廉价的规模化加昂贵的个性化，以后是廉价的规模化的个性化。4. 前瞻性分析根据上面的分析，我们能看到 Agent 技术在未来的发展还有很大的提升空间。我认为，这些提升主要可以从几个方向开始，包括引入中央执行机构、学习能力、输入感知、输出执行、世界模型和记忆等几个方面。这些构成因素是完备非正交的，都对提升 AI 技术至关重要。4.1 Central Executive首先，中央执行机构，这是一个核心的概念，但常常被人们忽视。现在的 Agent 只是一个规划器，它负责做规划。但实际上，这个流程中还存在很多未明确的问题，比如，是否存在一个内部加工过程，以及这个过程是否透明可控等。一种可能的解决办法是，将内部加工过程外部化，用系统2包裹起来，使每一步细粒度的思考都可以展现出来。其次是世界模型，现在的大模型只能输入语言，显然这样是不够的，进一步理解世界需要多模态输入。这是我们在未来需要处理的关键问题。同样地，对于时间和自身的身体运动控制的认知也需要能够输入到大模型里面去。我们观察到，无论是自动驾驶汽车、大模型Agent，还是其他的诸多智能体模型，都已经在应用这种面向目标的架构。目前的挑战在于如何在细节上加以改进，如找出此架构未能完成某些任务的原因，以及这些缺陷是源于大模型底层的子任务能力不足，还是需要对框架本身做出改进，比如增加更多的思考层次，或加入更多的内部推演等。另一个重要的问题是宏观注意力。由于大模型的上下文限制，是否可以让模型自身主动去探索外部世界，将其精力和注意力主动地投入到解答某些具有目标性的问题上去，实现主动的注意力机制？这不仅涉及到搜索和尝试的问题，如针对一些无法思考出解决方案的情况，模型应如何去进行尝试，而且这些尝试何时能够带来进步，以及如何去寻找更为优秀的解决空间，进行推理和规划。4.2 Memory值得注意的是，数学和逻辑学习也会涉及到上述问题，比如人类在很多情况下不擅长规划，那么我们是否可以利用网络和记忆机制来实现规划的功能？这其中就涉及到记忆的内化，也就是把大模型从外部世界获取的经验转化为内部参数，或者说把这些经验转化为内存。目前，我们依赖的记忆机制主要是把所有的信息存储在历史记录里，然后在需要的时候进行召回。然而，这些信息并未经过整理，在一些试图整理记忆的尝试中，我们发现人类是具有这种能力的。人类在获得大量相关的知识后，不会简单地把它们堆积在脑中，因为人的神经元存储空间是有限的。相反，人脑会通过海马体进行整理，而在我们做梦时，大脑会重新构造这些相关的知识，使得记忆网络变得有序。然而，现在我们怎么去实现这种能力？目前还未见到具有遗忘功能的模型，也就是删掉一些垃圾信息或错误的信息。在大模型训练过程中，产生了许多无用甚至是错误的信息，而我们在工作中只是采用了许多方式来规避这些错误的信息，但为什么不试图去删掉它们呢？如果能够将这些信息替换为有价值的信息，那将是一件有价值的事。我注意到在人工智能领域中，对于长短时记忆与工作记忆，以及它们之间的关系讨论并不深入，更常见的是，人们将长短时记忆简化为向量数据库。我想解决这个问题，尝试对这两者进行深层次的理解，并建立更完备，更正交的关系也很重要。4.3 Sensory当人工智能Agent融入人类生活后，它与我们的体验和经历能否成为Agent自身的存储内容？如果可以，那么在未来，我们与Agent之间的互动将会变得更加实用，更加贴近现实生活，更加有温度。在输入的问题上，我明确地看到了多模态输入的必要性，同时，对于时间感知我认为也非常重要，时间性对于运动控制任务极其重要。引入多模态输入后，我们还要解决一个自上而下的机制问题，就是Projection启发的这个点，OCR嫁接术一定会在某类任务存在缺陷。4.4 Motor在交流方式上，我认为不应仅仅依赖于语言，虽然现在的交流基本都是基于语言的，但是，语言是一个低带宽且低效的通信工具。我在想，我们能否引入一种新的沟通方式 - 类似心灵感应的方式，让Agent在隐空间通信。关于运动控制，当前的方式包括一些机器人应用，都比较结构化。但我认为，在未来，大模型的神经网络应该可以直接连接到运动控制的神经网络，实现层次化控制，使得运动更为流畅，甚至比人类更为灵活。在另一方面，运动控制也应该是数据化的，而不是仅仅处于我们所说的”计划者“的层面。如果有一个命令下达，神经网络应该可以直接执行。除此之外，还有一些亚符号的控制，在大模型直接对接神经网络时，我们应当避免通过语言来描述，因为我们可以通过这种方式得到的信息量会比通过语言描述来得多。同时，也需要进行一些外部工具的优化，让现有的工具更适应我们的需求，比如一些愿意为了方便Agent调用进行改造的工具服务商将会在新的价值网络中占据一席之地，如一个旅游服务供应商，加入下一代Agent平台之后，Agent在完成用户旅游类任务时可能会有限调用它，并使用类似Web3的技术进行价值分配。4.5 Learning任何一个产品，或者说Agent，都需要学习。学习的过程是十分重要的，尤其是模型需要学会对自身的可靠性进行判断，知道自己知道什么，更重要的是，知道自己并不知道什么，不擅长什么，这将会对模型的发展产生重大影响。关于大型模型的优化，我认为最关键的问题就在于模型需要明确自己的能力范围。有些问题，大模型不能张口就来直接给出答案，过于逞能，它应该经过仔细的思考，保证任务目标的准确达成。同时，我们也需要考虑模型的权威性问题。大模型可能从互联网和垃圾信息中学到很多知识，但这并不意味着它在解决问题时能提供最权威、最佳的做法。我们需要把这个模型训练到，即使是在面对垃圾信息输入时，它也能输出更好的、更有价值的解决方案。另一方面，我们还需要考虑到模型的多样性。很多时候，为了保证任务的有效执行，我们往往会控制模型的温度参数，以保持其输出的稳定性。但是，在保证模型正确性的同时，我们也不应该忽略它的思维活跃度。我们应允许智能体在解决任务时有更大的解空间，以便找到最优的解决方案。4.6 World Models关于世界模型 ，我们需要注意的是，尽管模型的训练数据中可能含有很多垃圾信息和错误信息，我们还需要让模型具有辨别和整理这些信息的能力，以构建一个无矛盾、统一的实体网络，这一点鲜被提及，我认为现在黯然神伤的之前做知识图谱的同学可以重点考虑一下这个方向。在此基础上，我们还需要让模型具备推理能力。一个优秀的智能体不应该仅仅依赖于内部推理，而应该有能力借助外部推理，当然这个外部推理可以当做工具来使用。最后，我们还必须强化模型的内部思考机制。当调用一些有成本的接口时，模型不能只是“想到就做到”，而应该有自我觉知的能力，或者叫Mental Simulation，预判自己的行动可能会带来的结果，并在内部进行纠错，以保证行动的可靠性，这不同于Reflection是执行后根据执行结果再反思。进一步，我们可能更大的关注点应该是它在家庭生活及现实社会中的应用上，将其实现为实体化的机器人，那么动力学机制和时间性认知还是很重要的，而当前的大模型仅是一个简单的循环调用，无法实现这方面的任务。好，以上就是我对一些方向的浅显思考。最后，我们以伟人的一段话来结尾：Agent 技术，它是站在海岸遥望海中已经看得见桅杆尖头了的一只航船，它是立于高山之巅远看东方已见光芒四射喷薄欲出的一轮朝日，它是躁动于母腹中的快要成熟了的一个婴儿。参考文献Wikipedia Agent. https://en.wikipedia.org/wiki/Intelligent_agentIntelligent Agents 综述. https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/373/INTELLIGENT_AGENTS_v7_final.pdfPrompt经典收集。https://github.com/f/awesome-chatgpt-promptsLLM+P: Empowering Large Language Models with Optimal Planning Proficiencyhttps://github.com/Cranial-XIX/llm-pddlChain-of-Thought Prompting Elicits Reasoning in Large Language ModelsSelf-Consistency Improves Chain of Thought Reasoning in Language ModelsTree of Thoughts: Deliberate Problem Solving with Large Language ModelsGraph of Thoughts: Solving Elaborate Problems with Large Language ModelsCumulative Reasoning with Large Language ModelsReAct: Synergizing Reasoning and Acting in Language ModelsReflexion: Language Agents with Verbal Reinforcement Learninghttps://openai.com/blog/function-calling-and-other-api-updates人大综述https://arxiv.org/pdf/2308.11432.pdf复旦综述 https://arxiv.org/pdf/2309.07864.pdfhttps://github.com/Significant-Gravitas/AutoGPThttps://github.com/microsoft/JARVISHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging FaceGPT-Researcher https://github.com/assafelovic/gpt-researcherRecurrentGPT https://arxiv.org/abs/2305.13304Voyager https://arxiv.org/abs/2305.16291https://github.com/OpenBMB/XAgent斯坦福小镇代码 https://github.com/joonspk-research/generative_agents斯坦福小镇论文 Generative Agents: Interactive Simulacra of Human BehaviorMetaGPT代码 https://github.com/geekan/MetaGPTMetaGPT论文 https://arxiv.org/pdf/2308.00352.pdfhttps://github.com/OpenBMB/ChatDevhttps://github.com/OpenBMB/AgentVersehttps://arxiv.org/pdf/2307.07924.pdfAgents: An Open-source Framework for Autonomous Language Agentshttps://lilianweng.github.io/posts/2023-06-23-agent/Phase transitions of brain evolution that produced human language and beyondA Review of 40 Years in Cognitive Architecture Research Core Cognitive Abilities and Practical ApplicationsLIDA: A Computational Model of Global Workspace Theory and Developmental Learninghttps://hal.science/hal-03311492/documenthttps://ai.meta.com/blog/yann-lecun-advances-in-ai-research/Projection: A Mechanism for Human-like Reasoning in Artificial Intelligencehttps://en.wikipedia.org/wiki/Planning_Domain_Definition_Language"
672287298,e96db436590e80f423e8f81fcc37d475,皓波,https://api.zhihu.com/people/e96db436590e80f423e8f81fcc37d475,people,AP@ZJU,EMNLP'23|大模型时代的数据标注——FreeAL,article,https://api.zhihu.com/articles/672287298,这就是我们FreeAL的框架——<em>大模型</em>提供标注，小模型进行蒸馏，再回流<em>大模型</em>进行更准的ICL。 2.1 大模型标注 对于<em>大模型</em>来说，我们要做的就是找到更多的示例样本去提升<em>大模型</em>的few-shot性能,163,21,225,1702558341,1704814556,上周刚在坡县开完EMNLP，见到了许多老朋友新朋友，回来以后终于有点空宣传一下我们的新工作了。本文跟大家介绍我们和网易伏羲合作发表在EMNLP'23主会的工作FreeAL[1]，旨在探讨大模型时代的数据标注该何去何从，我们是否还需要人类标注人员协同进行标注？零、一些结论（太长不看版）数据标注依然重要，完全监督、弱监督的小模型在很多场景下比（未精调）大模型强；利用LLM进行标注是完全可行的，小模型可以协同进行过滤、精炼大模型的标签；弱监督学习、主动学习这两个领域，我想依然有活着的价值。开源代码：GitHub - Justherozen/FreeAL一、为什么做这个工作？（背景）过去的5年里，弱监督学习——解决各种低质量的数据标注信息的一个方向——都是我的主要研究方向，也支撑着我完成了博士生涯。然而，在ChatGPT横空出世的时候，我一度非常焦虑，思考弱监督学习是否还有继续研究的必要（当然大家应该都在这么想）。如果大模型已经能够通过Few-shot ICL、Zero-shot完成绝大部分任务，那么弱监督将被扫进历史的垃圾堆。恰巧当时，我们正在研究Distantly-supervised NER问题，顺手做了一下ChatGPT for NER，发现很奇怪的现象是，ChatGPT的zero-shot NER效果并不好，甚至比大部分DSNER的Baseline低。此时，有一篇工作[2]也发现，ChatGPT在绝大部分任务上的few-shot性能远远不如全监督训练的小模型，这也是大家目前比较认可的大模型在细分领域上是不够出色的。这个结论也不难解释，毕竟大模型在通用语料上进行训练，但没有真正曝光很多细分领域的监督数据，或者是这种通用任务的场景可能会导致模型产生一些负迁移作用。也就是说，不论用大模型还是小模型，其实还是有很多问题：（1）大模型：我们可以用Zero/few-shot ICL解决下游任务，人力标注几乎为0，但是光靠大模型呢，部署成本较高，效果不总是尽如人意。（精调成本更高）（2）小模型：直接用小模型的话，我们要收集很多标注数据，人力成本更高了。也许我们可以使用半监督、主动学习缓解一下标注成本，但总是需要一定的人力成本。显然，数据标注仍然十分重要（毕竟数据是AI的“燃料”）。那么，如何能够在极低的人力成本条件下，达到更高的下游任务性能呢？我们的答案是大模型时代的主动学习技术FreeAL——大小模型协同工作，达到Human-Free的数据标注。二、FreeAL框架要理解FreeAL，我们可以思考人类在数据标注的过程中做了什么。通常来说，我们会标完所有数据集，接着我们要做校验挑选出错误的数据，最后再进行修正，每个步骤都需要人力。传统的主动学习呢，降低了第一步的标注成本，通过迭代标注小部分数据，然后通过模型的Uncertainty（或Feature-based Diversity）进行校验，筛选剩余有价值的样本进行再标注。这有两个问题，首先是，少量标注其实很难训练很好的模型，影响后续筛选的步骤，其次传统AL还是需要大量的人力成本，目前的AL论文大部分都得标10%~50%以上的数据才能达到较好的性能。那么我们是否能够完全抛弃人工标注，自动化数据标注的过程呢？（1）如何标注？ChatGPT可以作为一个完全无需人力的弱标注者，来全量标注整个数据集，当然这会带来一定的噪声。（2）如何校验？传统AL能够提供一个启示，即小模型实际上是能够通过训练的过程自动挖掘一些样本来标注的。实际上，这也是弱监督学习里面一个重要的Topic——样本筛选，也就是定位干净样本，过滤出来错误的样本，两种样本分别用不同的技术进行处理。我们在IJCAI'23发表的ProMix算法就讨论了如何最大程度筛选干净样本，目前仍是CIFAR-N benchmark的SOTA方法。（3）如何再标注？既然能定位到错误样本，那么也能找到干净样本，就可以用它们提升ICL的准确率，迭代演进。没错，这就是我们FreeAL的框架——大模型提供标注，小模型进行蒸馏，再回流大模型进行更准的ICL。2.1 大模型标注对于大模型来说，我们要做的就是找到更多的示例样本去提升大模型的few-shot性能。不过，这在刚开始的标注轮次并不是显然的（毕竟我们选择了Human-Free这个完全抛弃人类标注的设定），所以本文我们选择让大模型自己生成一些demo样本，具体思想也比较简单，就是告诉ChatGPT，标签是什么，并给几个无标注样本作为示意，让大模型学习无标注文本的风格信息，然后生成符合标签信息的样本。那我们就可以用它得到一个初始的demo set，我们发现这在某些数据集（如SUBJ）可以提升28个点。在后续的轮次呢，我们就等小模型筛选的结果去做ICL即可。2.2 小模型蒸馏这一块实际上我们采用了Small-Loss Selection的策略挑选干净样本，结合半监督完成噪声标记学习。相信做弱监督学习的同学应该不会陌生。不过，小模型训练的时候，其实筛选的“干净样本”里混一些错的也没什么问题，性能还是会提升，但是要做ICL的话，我们还是希望更干净的demo set，所以我们又根据loss逐类精心挑选了一些。这里逐类挑选还是为了考虑到diversity。最后，我们可以反馈给大模型进行修复即可。因为我们做了全量的标注，其实两个轮次基本就可以收敛到很好的结果。三、实验结果实验中，FreeAL的效果还是很出色的，证明了大模型可以作为一种监督信息的来源。一个有意思的结果是，弱监督蒸馏出来的RoBERTa，只有在样本特别少的SST-2和MR上是ChatGPT效果比较好，数据集规模稍微大一些，小模型就会比ChatGPT做ICL的效果要好。我们也对比了一下传统的AL，发现在一些数据集上是能够超过人类标注的结果的。四、总结通过这个工作，一个让我很欣慰的结论是，至少在下个世代的大模型出来之前，弱监督学习、数据标注依然是重要的。事实上，由于很多领域的隐私问题，很多研究者都比较认可开发垂域大模型的重要性，其中数据标注必然是重要的一环。而我们FreeAL就给了这么一个可以大幅降低数据标注成本的方案。当然，我们这篇文章探讨的设置较为极端，研究了完全抛弃了真人标注者的可行性。我们实验中也发现，FreeAL和完全监督小模型还是有一定的差距，实际应用中还是建议加一些人工标注的（挖个继续研究AL的坑）。
693163321,1637e8b71b716f2af780104fb45c1293,代季峰,https://api.zhihu.com/people/1637e8b71b716f2af780104fb45c1293,people,清华大学电子系副教授,InternVL V1.5当前最强开源多模态大模型,article,https://api.zhihu.com/articles/693163321,在opencompass上（opencompass是个几十个benchmark的性能集合）排名多模态<em>大模型</em>第一，超过各种闭源模型。 Demo： InternVL-Chat 模型下载： InternVL - a OpenGVLab,60,7,32,1713415193,1713452064,广告下，InternVL，当前V1.5版本已经发布，在opencompass上（opencompass是个几十个benchmark的性能集合）排名多模态大模型第一，超过各种闭源模型。Demo：InternVL-Chat模型下载：InternVL - a OpenGVLab CollectionOpenCompass榜单：https://rank.opencompass.org.cn/home加几个例子：
664235401,ccc0c9a5dd72d060d79cbb7e5852b4f1,薛雨坤,https://api.zhihu.com/people/ccc0c9a5dd72d060d79cbb7e5852b4f1,people,,目前存在的金融大模型,article,https://api.zhihu.com/articles/664235401,我们构建了数据安全、模型安全、内容安全等全流程的合规能力，支撑模型的可信。 智海-金磐<em>大模型</em> 浙江大学联合摸象科技发布了金融零售领域的语言<em>大模型</em>一一智海-金磐大模型。智海-金磐<em>大模型</em>是摸象科技和浙江大学在金融智能领域的重要成果,60,1,214,1698723529,1701849160,"23.12.06 更新1 有论文、项目地址等可供查询的BloombergGPT论文地址：[2303.17564] BloombergGPT: A Large Language Model for Finance (arxiv.org)基于BLOOM。得益于彭博社的多年积累，数据集的质量非常高，但是没公开。FinGPT[2306.06031] FinGPT: Open-Source Financial Large Language Models (arxiv.org)哥伦比亚大学联合上海纽约大学推出的模型FinGPT is a state-of-the-art financial fine-tuned large language model (FinLLM). Developed by AI4Finance-Foundation, FinGPT is currently outperforming other models in terms of both cost-effectiveness and accuracy in general. It currently has 3 versions; the FinGPT v3 series are models improved using the LoRA method, and they're trained on news and tweets to analyze sentiments. They perform the best in many financial sentiment tests. FinGPT v3.1 is built on the chatglm2-6B model, while FinGPT v3.2 is based on the Llama2-7b model.来自报道：Exploring FIN-GPT: the financial large language models (indiaai.gov.in)DISC-FinLLM复旦大学数据智能与社会计算实验室（FudanDISC）推出官网，有DEMO，感兴趣的可以试试 DISC-FinLLM (fudan-disc.com)论文地址 [2310.15205] DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning (arxiv.org)DISC-FinLLM项目地址基于原有的通用领域中文大模型，进行LoRA指令微调开源金融大模型貔貅（PIXIU) [2306.05443] PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance (arxiv.org)PIXIU 项目地址基于LLaMA微调聚宝盆（Cornucopia）基于中文金融知识的LLaMA微调模型聚宝盆 项目地址FinBert摩根大通(JPMorgan)和皇后大学(Queen’s University)[1908.10063] FinBERT: Financial Sentiment Analysis with Pre-trained Language Models (arxiv.org)主要做金融方面的情感分析2 目前只找了相关新闻报道的还没来得及找、或者没找到相关论文/项目地址/参考文档的。懒得加链接了，引用的新闻原文复制到百度就能找到原文。AntFinGLM蚂蚁集团的蚂蚁金融大模型，并且推出两个应用：支小宝、支小助，以及金融专属任务评测集「Fin-Eval」，并开源生成式AI编程平台CodeFuse。9 月 8 日，蚂蚁集团在外滩大会上正式发布工业级金融大模型（AntFinGLM) ，同时开放了金融专属任务评测集「Fin-Eval」。从目前公开的数据来看，BloombergGPT 的通用数据集包含 3454 亿个 Token，金融领域数据集由彭博在过去四十年的商业经营中积累而来，共包含 3635 亿个 Token。国内金融大模型「轩辕」（2.0）使用了自身业务中积累的金融领域的 13B（130 亿） Token。恒生电子 Light-GPT 利用了超过 4000 亿个 Token 的金融领域数据。蚂蚁金融大模型，在万亿量级 Token 通用语料基础上，注入了千亿量级 Token 金融知识 —— 包括全网公开的金融行业语料约 5000 亿 Token 以及蚂蚁独家金融语料约 279 亿 Token，行业领先。轩辕大模型度小满的，基于BLOOM-176B研发的轩辕大模型。度小满金融大模型C-Eval、CMMLU双榜排名第一，正式开源！ (baidu.com)国内首个千亿级中文金融大模型来了！来自度小满，基于BLOOM-176B (baidu.com)盘古金融大模型华为的打造盘古金融大模型：在金融知识方面，我们沉淀了五大类金融数据，注入了千亿级的金融Tokens；在金融技能方面，与金融机构和伙伴共创，目前已经沉淀了数千个细分场景模板；我们还融合了100多个行业标准、规范的知识库，让大模型更专业；在安全方面，我们构建了数据安全、模型安全、内容安全等全流程的合规能力，支撑模型的可信。 智海-金磐大模型浙江大学联合摸象科技发布了金融零售领域的语言大模型一一智海-金磐大模型。智海-金磐大模型是摸象科技和浙江大学在金融智能领域的重要成果，基于浙大在自然语言处理、知识图谱、神经网络、机器学习等领域的前沿研究，结合金融零售业务的实际需求，构建了这个面向金融零售的垂直大模型ChatABC中国农业银行推出的类ChatGPT的大模型应用。中国工商银行中国工商银行发布了基于昇腾AI的金融行业通用模型。宇信科技宇信科技金融行业大模型应用落地，已与部分机构达成合作意向-移动支付网 (mpaypass.com.cn)LightGPT恒生电子的恒生电子首次发布金融大模型LightGPT (baidu.com)天镜马上消费金融股份有限公司首个零售金融大模型“天镜”亮相 (baidu.com)马上消费金融股份有限公司（下称马上消费）重磅发布了全国首个零售金融大模型——“天镜”，推动金融数字化转型。KAI-GPTKasisto公司，针对银行的大型语言模型(LLM)，据称Kasisto的客户包括摩根大通(J.P. Morgan)、西太平洋银行(Westpac)、渣打银行(Standard Chartered)、TD、Nedbank和南方银行(BankSouth)，以及Meriwest和Valley Isle等信用合作社等等。The Behavox LLMBehavox公司相关报道：Behavox Releases Large Language Model for the Financial Services Industry | Business Wire官网：Enterprise Risk & Compliance Solutions | BEHAVOX™软银投了1亿元，据称优于ChatGPT和lexicon？Kodex德意志银行的企业风险投资（CVC）集团投资了Kodex AI，这是一家总部位于柏林的初创公司，开发大型语言模型（LLM）应用程序，以满足金融服务行业的需求。BondGPTBroadridge全资子公司LTX推出，基于GPT-4Finpilot一个金融领域的AI插件官网：FinpilotPortfolio Pilot3. 其他恒生牵头编制！国内首个金融大模型标准发布 (baidu.com)国内首个金融行业大模型标准发布 (baidu.com)《面向行业的大规模预训练模型技术和应用评估方法第1部分：金融大模型》"
642279963,4b8ca5488ee77396d274c5680f87c7df,LokLok,https://api.zhihu.com/people/4b8ca5488ee77396d274c5680f87c7df,people,,LongMem: 大模型的长期记忆,article,https://api.zhihu.com/articles/642279963,最近看了LongMem[1]这篇论文，来水一点笔记。 大语言模型的可变记忆问题一直是大家比较想解决的问题。<em>大模型</em>目前在表达能力等方面其实已经很接近甚至超过人类了，预训练所获得的记忆知识更是堪称海量,86,9,200,1688788283,1688788310,"最近看了LongMem[1]这篇论文，来水一点笔记。大语言模型的可变记忆问题一直是大家比较想解决的问题。大模型目前在表达能力等方面其实已经很接近甚至超过人类了，预训练所获得的记忆知识更是堪称海量。但是如何让大模型快速的记住一件新的事，仍旧是尚待解决的问题。目前的思路有这么几种 一是增大上下文窗口长度：只要我上下文窗口长度够长，我就可以把所有历史信息塞进去。另一个方向是RAG(Retrieval Augmented Generation),简单来说就是外挂个数据库，但是从数据库取数回来怎么输入到模型是有不同方案的，我们平常最常用的，一般是配合向量数据库，大量文档存在向量数据库内，外部有query输入的时候，从向量数据库搜索获取相关的文档，与query一并合入prompt再输入给模型。虽然看似非常简单，但其实能做的东西也很多，文本向量化的模型选择，文档搜索中的粗排与精排，乃至数据库的选择（其实不用向量数据库也可以）。许多热门的项目其实都是用的这套架构，比方说ChatLaw等，主要还是一个简单，可解释性又好，我自己在做的一个项目也用的这套。不过今天就先不展开说了。LongMem的方法我理解是RAG的另一个方向，和之前的memorizing transformers[2]类似，在数据库中存储与取回的数据实际上是模型内部的kv向量，输入也不经过prompt（不占用token数了好耶），直接通过改造过的层与backbone模型的输出合并再接入到输出头上。缺点是这个定制的层需要额外的训练。这类的方案主要有两个要解决的点： 1. 如何存储和召回数据 2. 如何将召回的数据混入到模型的输入存储与召回：Cache Memory Bank在Memorizing Tranformers中，存储使用的是faiss， 在这篇论文中其实也是faiss，不过做了一些细节的改进。 存储的对象是最后M个token对应的SideNet最后一层Attention中的K,V向量(M大概在几k到几十k的范围)。 但是为了搜索更快，这篇论文中没有采取在token级别上进行召回，而是把数个token作为一个chunk，做平均后作为key，在chunk层次上进行召回。chunksize消融实验中有测（2,4,8），测出来csz=2是效果最好的，说实话我有点怀疑不用csz效果也差不多。这里插两句，他们选faiss我觉得主要还是要整合进模型内部考虑，在工程上其实把记忆的部分拆开来，用qdrant、milvus、chromadb之类的其实感觉更合适。记忆混合：SideNet在Memorizing Transformers中，用一个改造过的KnnAttention层来混入召回的向量，在这篇论文中使用一个SideNet来处理这个工作。在训练时可以锁定backbone,只训练这个SideNetSideNet的设计:SideNet的层数为backbone的1/2，以backbone中对应的2x层参数初始化。backbone每隔一层将输出减去两层前的输出（残差），接入到SideNet的下一层，SideNet同时接受backbone的输入和自身上一层的输入可以参考如下公式 还有这个图：在SideNet的最后一层，从memory bank召回的topk个k，v对，与其他输入混合，混合的方式与Memorizing Transformer中基本一致，使用一个head-wise的可训练向量g来决定本身输入与召回的记忆向量的权重:有同学可能会问M的是多个kv对，和A长度完全不同啊，这个据我看另一位老哥的Memorizing Transformer实现代码，应该是直接加一块了,参考这段代码：local_out = einsum('b h i j, b j d -> b h i d', local_attn, v) mem_out = einsum('b h i j, b h i j d -> b h i d', mem_attn, mem_v) out = local_out + mem_out这篇文章的代码藏在fairseq里，幸好有个老哥把它摘出来了，可以参考这里:https://github.com/facebookresearch/fairseq/compare/v0.12.2...tmm1:fairseq:longmem?diff=unified闲聊部分其实目前实现LLM记忆的方式真的又很多，光RAG这类里面做法也是五花八门，我自己做的过程中是觉得，我们应当多思考人的记忆是怎么样的，我相信更贴近自然的人类的记忆方式是更合理的。 我们平常用的RAG把数据取出来重新加进prompt里，其实不像记忆，更像是人做的笔记，在关键时刻想不起来了，翻笔记去找一找以前是怎么写的。所以我对LongMem这类的方法还是有挺多期待的，真正的附加记忆我认为应该想LoRA那样，少量样本就可以记住，在参数层面生效，而且可拆卸，更符合自然记忆的表现。感觉过段时间可以再水下目前模型记忆的做法，美滋滋"
677954266,cf065ba931b9b615216f8c3e8e6d0a00,王云鹤,https://api.zhihu.com/people/cf065ba931b9b615216f8c3e8e6d0a00,people,研究员,Large OCR Model：多模态大模型的文字识别能力之痛，由OCR大模型来缓解,article,https://api.zhihu.com/articles/677954266,从可视化结果来看，多模态<em>大模型</em>存在大量幻觉和形近字错误问题： OCR增强的多模态<em>大模型</em>： 我们构建了OCR大模型，详细见下文。这是在多模态<em>大模型</em>（LMM）研究日益增长的背景下,111,15,231,1705316182,1705481560,arXiv链接：Large OCR Model:An Empirical Study of Scaling Law for OCR论文主页：https://large-ocr-model.github.io数据地址：https://github.com/large-ocr-model/large-ocr-model.github.io/blob/main/Data.md引言多模态大模型的文字识别能力之痛：多模态大模型最近在业界建设的如火如荼，具备了很强的视觉-语言交互能力。但是，其OCR能力也就是识别图片中文字的能力偏弱，强如GPT-4V，也似乎还不够看。在华南理工大学的一篇全面测评（Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation）中，业界最强多模态大模型GPT-4V识别文字的能力比OCR专用模型低了几十个点：从可视化结果来看，多模态大模型存在大量幻觉和形近字错误问题：OCR增强的多模态大模型：我们构建了OCR大模型，详细见下文。这是在多模态大模型（LMM）研究日益增长的背景下，我们引入OCR大模型到Qwen-VL-Chat多模态大模型，并在四个VQA任务上进行了详细测评如Table 5。结果表明，OCR技术的引入显著提升了LMM在VQA任务上的精度，证明了OCR在提升多模态大模型文本识别能力方面的重要性，也展示了OCR在处理复杂视觉-语言交互任务中的潜力。OCR显著提升多模态大模型的视觉问答能力VQA可视化效果在STVQA和TextVQA任务的可视化分析中，我们发现Qwen-VL_Chat大模型在捕捉小文本或正确识别文本内容方面存在一定的挑战。然而，当引入OCR技术后，大模型能够更准确地理解图片中的文本内容，如STVQA任务中更有效地识别小文本。TextVQA任务中正确纠正“Hongte”为“Honghe”和“57”为“22”。在DocVQA任务中，当模型处理富文本信息的通用文档场景时，我们发现大模型在识别和处理大量文本信息方面表现不佳。例如，在询问图中广告商标时，模型可能无法准确找到目标或给出不存在的单词。引入OCR后，模型能更准确地识别图中的文字，提升了处理富文本信息时的准确度。OCR大模型的构建：在自然语言处理（NLP）领域，模型大小、数据量、计算力与模型性能之间的关系已受到广泛研究。然而，在光学字符识别（OCR）领域，这些“缩放法则”的探索仍处于初级阶段。为填补这一空白，我们进行了全面的研究，深入分析了模型规模、数据量和计算力与OCR性能之间的关系。研究结果揭示，在其他影响因素保持不变的情况下，性能与模型大小及训练数据量之间存在平滑的指数法则关系。此外，我们还创建了一个大规模数据集REBU-Syn，包含600万真实样本和1800万合成样本。利用这些法则和数据集，我们成功训练了一个高精度的OCR模型，在OCR的测试基准上实现了SOTA精度。特别地，我们发现OCR模型能显著增强多模态大模型的能力，在多个VQA任务上实现了显著的精度提升，证明了OCR在提升多模态大模型性能方面的巨大潜力。数据集在OCR领域，数据集的质量和多样性极为重要。我们通过收集和整合开源数据集 ，创建了全新数据集REBU-Syn。此外我们利用最新生成技术生成的60M合成数据MJST+，以供额外的使用。实验及结论核心结论：OCR领域存在缩放定律，模型大小、数据量、计算量和性能之间存在平滑的幂律。通过在多个OCR方法中探索中小模型的参数量、计算量、数据量对于精度的影响，我们成功证明了OCR领域在这三个维度存在Power-Law规律。这一发现为OCR模型设计提供了重要的理论基础。 TrOCR模型缩放在trocr方法中，不断增加模型参数量，模型的精度不断提升，通过验证，模型的参数量和精度之间满足power-law的规律，其中的规律如下。此外，当不断增加训练数据量，模型的精度不断提升，数据量和精度之间也满足power-law的规律。PARSeq模型缩放为进一步验证power-law的规律，在另一OCR方法parseq中，不断增加模型参数量，模型的精度也会不断提升，模型的参数量和精度之间依旧满足power-law的规律。最终我们基于这一Power-Law规律及最新的OCR模型CLIP4STR设计了一套大模型的参数量、数据量和计算量。利用这些设计原则，并训练了一个全新的OCR大模型Clip4STR-L*。该模型在六个常规测评数据集及Union14M benchmark上均达到了SOTA精度。总结这些研究结果表明，OCR技术在提升多模态大模型性能方面发挥着关键作用，尤其是在处理复杂的视觉问答任务时。我们的工作不仅推动了OCR技术的发展，也为多模态大模型的应用提供了新的视角。 arXiv链接：Large OCR Model:An Empirical Study of Scaling Law for OCR论文主页：https://large-ocr-model.github.io
684949726,fb31fbdb268e466a0928518a201696a9,小小将,https://api.zhihu.com/people/fb31fbdb268e466a0928518a201696a9,people,为人民日益增长的美好生活需要而读书,OLMo：真正完全开源的大模型,article,https://api.zhihu.com/articles/684949726,Language Model。相比之前开源的<em>大模型</em>，OLMo的独特之处是完全开源，除了训练的模型，OLMo还开源了训练数据，训练代码以及模型评估代码。换句话说，只要你有GPU卡,56,2,169,1709381573,1709451723,"最近，又有一家机构AI2（Allen Institute for AI）开源了一个LLM：OLMo，它的英文全称就叫Open Language Model。相比之前开源的大模型，OLMo的独特之处是完全开源，除了训练的模型，OLMo还开源了训练数据，训练代码以及模型评估代码。换句话说，只要你有GPU卡，再基于OLMo开源的数据和代码，就可以自己快速从零训练一个自己的LLM。除了数据，代码和模型，OLMo还直接公开了训练模型的日志：博客：https://blog.allenai.org/olmo-open-language-model-87ccfc95f580代码：GitHub - allenai/OLMo: Modeling, training, eval, and inference code for OLMo数据：GitHub - allenai/dolma: Data and tools for generating and inspecting OLMo pre-training data.论文：https://arxiv.org/abs/2402.00838模型：https://huggingface.co/allenai/OLMo-7B评估：https://github.com/allenai/OLMo-Eval微调：https://github.com/allenai/open-instruct日志：https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5OLMo的训练数据Dolma是一个开放的数据集，包含3万亿个来自不同类型网络内容、学术出版物、代码、书籍和百科资料的词汇。Dolma不仅完全开源，而且同时也公开了构建数据集的工具包。OLMo目前开源的模型主要有三个规模：其中65B的模型还在训练中，目前开源的最大模型是OLMo 7B：OLMo 7B在大部分的评测上和Meta开源的Llama 2 7B相当：除了预训练模型，OLMo 7B还提供了对应的微调版本OLMo 7B Instruct：虽然目前AI2开源的OLMo在效果上并没有那么惊艳，但是OLMo是完全的开源，对于LLM的入门选手，OLMo可能是一个快速的开始。而且AI2还会持续开源，OLMo只是一个开始：This release is just the beginning for OLMo and the framework. Work is already underway on different model sizes, modalities, datasets, safety measures, and evaluations for the OLMo family. Our goal is to collaboratively build the best open language model in the world, and today we have taken the first step.未来，开源是大势所趋。"
679435444,ea09945fa14c1d00a8aec1411e635d95,杨夕,https://api.zhihu.com/people/ea09945fa14c1d00a8aec1411e635d95,people,NLP,说说百度大模型算法工程师二面经历,article,https://api.zhihu.com/articles/679435444,来自 社群 @海鸣 大佬的 百度<em>大模型</em>算法实习生 面试经历 百度大模型算法工程师面试题 应聘岗位：百度<em>大模型</em>算法工程师 面试轮数：第二轮 整体面试感觉：偏简单 个人公众号,44,7,205,1706016272,1706016272,"来自 社群@海鸣大佬的 百度大模型算法实习生 面试经历百度大模型算法工程师面试题应聘岗位：百度大模型算法工程师面试轮数：第二轮整体面试感觉：偏简单个人公众号面试过程回顾1. 自我介绍在自我介绍环节，我清晰地阐述了个人基本信息、教育背景、工作经历和技能特长，展示了自信和沟通能力。2. Leetcode 题具体题意记不清了，但是类似 【208. 实现 Trie (前缀树)】题目内容Trie（发音类似 ""try""）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。请你实现 Trie 类：Trie() 初始化前缀树对象。void insert(String word) 向前缀树中插入字符串 word 。boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。示例：输入 [""Trie"", ""insert"", ""search"", ""search"", ""startsWith"", ""insert"", ""search""] [[], [""apple""], [""apple""], [""app""], [""app""], [""app""], [""app""]] 输出 [null, null, true, false, true, null, true] 解释 Trie trie = new Trie(); trie.insert(""apple""); trie.search(""apple""); // 返回 True trie.search(""app""); // 返回 False trie.startsWith(""app""); // 返回 True trie.insert(""app""); trie.search(""app""); // 返回 True提示：1 <= word.length, prefix.length <= 2000word 和 prefix 仅由小写英文字母组成insert、search 和 startsWith 调用次数 总计 不超过 3 * 104 次题目解答class Trie(object): def __init__(self): self.children = [None]*26 self.is_end = False def insert(self, word): """""" :type word: str :rtype: None """""" node = self for c in word: c = ord(c)-ord(""a"") if not node.children[c]: node.children[c] = Trie() node = node.children[c] node.is_end = True def searchPrefix(self, prefix): node = self for c in prefix: c = ord(c)-ord(""a"") if not node.children[c]: return None node = node.children[c] return node def search(self, word): """""" :type word: str :rtype: bool """""" node = self.searchPrefix(word) return node is not None and node.is_end def startsWith(self, prefix): """""" :type prefix: str :rtype: bool """""" return self.searchPrefix(prefix) is not None # Your Trie object will be instantiated and called as such: # obj = Trie() # obj.insert(word) # param_2 = obj.search(word) # param_3 = obj.startsWith(prefix)3. 技术问题回答3.1 结合 GNN 科研项目进行提问样本构建的流程是怎样的，并且为什么 GCN 相较于其他方法在效果上更胜一筹？节点特征指的是什么？3.2 结合 基于 RAG 的医学问答项目进行提问查询流程？使用什么向量数据库？介绍一下 RAG 原理？RAG 如何解决多实体提问问题？用户提问：感冒和咳嗽需要吃什么药？3.3 结合多模态科研项目进行提问Prompt是如何生成的，优化目标是什么，任务是什么？OCR 抽取效果不好，需要怎么排查问题？3.4 技术问题您是否使用过Pytorch提供的预训练模型，例如torchvision、transformers以及OpenAI开源的ClIP？对分布式训练有经验么？回答：学过但是没用过RNN与GNN之间有哪些区别，以及它们各自适用于哪些场景？回答：RNN与GNN的区别： 1. 数据类型： - RNN 设计用于处理序列数据，即数据点按时间顺序排列，如时间序列分析、语音识别和自然语言处理。 - GNN 专门用于处理图结构数据，图由节点和边组成，代表实体及其关系，如社交网络、交通网络和分子结构。 2. 结构和工作原理： - RNN 的核心是循环单元，它能够在序列的每个时间步上保持信息的状态，但是长序列会导致梯度消失或梯度爆炸问题，影响学习长期依赖。 - GNN 通过节点和边的特征以及图结构本身的信息，利用特殊的邻居节点更新机制来学习图中的特征表示，更好地捕捉节点间的依赖关系。 3. 长期依赖问题： - RNN 在处理长序列时存在长期依赖问题，虽然有LSTM（长短期记忆网络）等变体来缓解这一问题，但本质上是序列模型。 - GNN 通过图结构天然地能够捕捉节点间的依赖关系，因此在处理具有明确关系的数据时更为有效。 各自适用的场景： - RNN 适用于处理时间序列数据、文本序列等，如股票价格预测、语音识别、机器翻译（序列到序列的任务）。 - GNN 适用于处理结构化数据，如社交网络分析、推荐系统、生物信息学（如蛋白质结构预测）、地理信息系统等，其中实体和关系是数据的核心组成部分。 总的来说，RNN适合处理时间或顺序上的数据，而GNN适合处理具有明确结构关系的数据。两者各有优势，选择哪种模型取决于具体问题和数据的特点。GPT和BERT在文本表征方面有哪些结构和工作原理上的差异？回答：BERT是Transformer Encoder，属于自监督训练方式，然后两大预训练任务，主要用于下游任务抽特征，GPT是Decoder，自回归训练，主要是预测下一个词的分布，依赖大语料库，GPT-3可以表现出Few-shot/zero-shot learning因为说了BERT好训练一些，问了为什么?回答：说了GPT任务对简单、比较依赖语料库的大小，BERT的MLM比较直觉且个人能训练，GPT只有openai等公司有成品说一说你对 Zero-shot和Few-shot的理解回答：Few-shot先给定任务范式描述，Zero-shot就是直接做怎么看待计算机网络和操作系统在DL中的作用回答：谈了DL研究一些计算机网路的问题，比如网络拓扑、交换机拓扑等，分布式训练时会有通信，也会用到进程相关知识你来调优一个BERT模型适应一个数据集或任务会怎么做回答：固定BERT，训练分类头或者使用Adapter训练完模型后准确率很低，怎么优化回答：首先检查代码结构和分类器的网络结构和BERT量级是否匹配，学习率+余弦退火调整，改为Adapter，检查数据集质量，验证阶段代码是否有误有一批文本数据，来源和质量不太一样，使用时如何处理回答：反问文本来源不同是否混合或完全分开，结合多模态融合的技术，增加一个学习任务，对不同来源的文本表示进行线性变换投影到相同的特征空间中个人本次面试总结百度的面试篇项目面一点，整体效果还行，面试官给人感觉比较温和。"
678989191,68ebf9365e2c5e930b79ffbbdab9e59b,hiyouga,https://api.zhihu.com/people/68ebf9365e2c5e930b79ffbbdab9e59b,people,NLPer,单卡 3 小时训练专属大模型 Agent：基于 LLaMA Factory 实战,article,https://api.zhihu.com/articles/678989191,选项中将界面语言切换为中文。 ② 点击<em>模型</em>名称，选择 Yi-6B 模型，此时<em>模型</em>路径默认会显示远程模型地址，如果您已经将<em>模型</em>文件全部下载至本地，可以手动将其修改为本地文件夹路径,272,78,541,1705838443,1705892992,"引言Agent（智能体）是当今 LLM（大模型）应用的热门话题[1]，通过任务分解（task planning）、工具调用（tool using）和多智能体协作（multi-agent cooperation）等途径，LLM Agent 有望突破传统语言模型能力界限，体现出更强的智能水平。在这之中，调用外部工具解决问题成为 LLM Agent 必不可缺的一项技能，模型根据用户问题从工具列表中选择恰当的工具，同时生成工具调用参数，综合工具返回结果和上下文信息总结出答案。通过调用外部工具，LLM 能够获取到实时、准确的知识，大大降低了生成中的幻觉（hallucination）现象，使 LLM 的任务解决能力得到长足的提升。工具调用能力的获得离不开模型微调，尽管使用 ReAct 提示[2]或其他预训练模型也能实现类似效果，但对于定制化或更加广泛的工具，对模型做进一步微调能有效地提升工具使用能力。本文将会带领大家使用 LLaMA Factory 的 Agent Tuning 功能，使用单张 GPU 在 3 小时内训练出自己专属的 LLM Agent。训练框架之前文章[3]已经讲到，LLaMA Factory 是一个涵盖预训练、指令微调到 RLHF 阶段的开源全栈大模型微调框架，具备高效、易用、可扩展的优点，配备有零代码可视化的一站式网页微调界面 LLaMA Board。经过半年多的升级迭代，LLaMA Board 网页微调界面在原先的基础上，丰富了多种新的功能，包括：支持约 120 种模型以及约 50 种数据集，包括最新的 DeepSeek MoE 混合专家模型使用 Flash Attention2 和算子优化技术，实现约 200% 的 LoRA 训练速度，大幅超越同类框架集成魔搭社区（ModelScope）下载渠道，国内用户可享受 100% 带宽的模型和数据下载同时包含预训练、监督微调、RLHF、DPO 四种训练方法，支持 0-1 复现 ChatGPT 训练流程丰富的中英文参数提示，实时的状态监控和简洁的模型断点管理，支持网页重连和刷新读者可以在 HuggingFace Spaces 或 魔搭社区 预览 LLaMA Board 网页微调界面。模型与数据本次我们选用零一万物[4]发布的 Yi-6B 开源双语基座模型，该模型于 2023 年 11 月发布，拥有约 60 亿参数，通过在 3T 多语言语料上的预训练，取得了同等规模下优异的中英文 Benchmark 效果，且允许免费商用。由于 Yi-6B 是一个预训练基座模型，并不具备对话能力，因此我们选用多个开源数据集对模型做指令监督微调（SFT）。在这些数据集中最关键的是工具调用数据集，该数据集包含约十万条由 Glaive AI[5]生成的关于工具调用的对话样本，我们将数据集处理为多角色的多轮对话样本，包含用户（human）、模型（gpt）、工具调用（function_call）和工具返回结果（observation）四种不同角色，同时还有一个工具列表（tools）字段，以 OpenAI 的格式[6]定义了可选工具。下面是数据集中的一个样本示例：{ ""conversations"": [ { ""from"": ""human"", ""value"": ""I saw a dress that I liked. It was originally priced at $200 but it's on sale for 20% off. Can you tell me how much it will cost after the discount?"" }, { ""from"": ""function_call"", ""value"": ""{\""name\"": \""calculate_discount\"", \""arguments\"": {\""original_price\"": 200, \""discount_percentage\"": 20}}"" }, { ""from"": ""observation"", ""value"": ""{\""discounted_price\"": 160}"" }, { ""from"": ""gpt"", ""value"": ""The dress will cost you $160 after the 20% discount."" } ], ""tools"": ""[{\""name\"": \""calculate_discount\"", \""description\"": \""Calculate the discounted price\"", \""parameters\"": {\""type\"": \""object\"", \""properties\"": {\""original_price\"": {\""type\"": \""number\"", \""description\"": \""The original price of the item\""}, \""discount_percentage\"": {\""type\"": \""number\"", \""description\"": \""The percentage of discount\""}}, \""required\"": [\""original_price\"", \""discount_percentage\""]}}]"" }如果读者想要加入自定义工具，只需要按照上述格式组织数据集即可。除此之外，我们也在本次训练中加入 Alpaca-GPT-4 数据集[7]和 Open-Assistant 数据集[8]以提升模型的通用对话能力。环境准备文章默认读者有至少不弱于 RTX 3090 24GB 的显卡和足够的系统内存，且安装了 CUDA 11.1-12.3 任一版本，关于 CUDA 环境的配置此处不予赘述。我们已经将所有的程序打包，您可以选择自己的 Anaconda 环境，运行以下命令安装 LLaMA Factory。pip install llmtuner==0.5.1接着从 GitHub 下载数据集文件，这里以 Linux 命令行方法为示例，您也可以选择从 GitHub 网页下载，下载后切换到新的文件目录，运行 ls 命令应当显示同一级目录中存在 data 文件夹。git clone https://github.com/hiyouga/LLaMA-Factory.git cd LLaMA-Factory ls # data src tests ...由于 LLaMA Board 网页微调界面仅支持单卡训练，需要设置环境变量指定使用的显卡序号。此外可以选择模型下载源，这里推荐国内用户使用魔搭社区下载渠道。export CUDA_VISIBLE_DEVICES=0 # 使用第一块 GPU export USE_MODELSCOPE_HUB=1 # 使用魔搭社区下载渠道如果您使用的是 Windows 系统，同样需要配置相关环境变量。set CUDA_VISIBLE_DEVICES=0 set USE_MODELSCOPE_HUB=1然后使用下述命令启动 LLaMA Board 网页微调界面。unset http_proxy https_proxy all_proxy # 关闭代理 python -m llmtuner.webui.interface训练流程① 打开浏览器，在地址栏输入 localhost:7860 进入 LLaMA Board，可以看到以下界面，点击左上角的 lang 选项中将界面语言切换为中文。② 点击模型名称，选择 Yi-6B 模型，此时模型路径默认会显示远程模型地址，如果您已经将模型文件全部下载至本地，可以手动将其修改为本地文件夹路径。③ 如果您已经安装过 Flash Attention-2 或 Unsloth，可以点击高级设置-加速方式提升训练速度，其中 Flash Attention-2[9]可提升至 120% 左右的速度，Unsloth[10]可提升至 170% 左右的速度。此处我们略过安装过程，请各位读者自行查阅参考文献中的 GitHub 仓库安装，如果两者均未安装，请保持加速方式为 None。④ 点击数据集，选择我们此次要使用的四个数据集 glaive_toolcall、alpaca_gpt4_en、alpaca_gpt4_zh 和 oaast_sft_zh，如果数据集下拉框为空白，请检查数据路径是否正确。选择后点击预览数据集按钮可预览数据集。⑤ 训练参数中与显存占用有紧密关联的是截断长度和批处理大小选项，我们暂时保持默认。这里仅将训练轮数设置为 2.0，最大样本数设置为 8000，LoRA 参数设置-LoRA 作用模块设置为 all。⑥ 将页面翻到底部，将输出目录设置为 yi-agent-6b，训练后的模型文件会保存在 saves/Yi-6B/lora/yi-agent-6b 中。点击预览命令按钮可以看到当前配置对应的命令行脚本，如果您想使用多卡训练，可以参考下述命令来编写多卡训练脚本。⑦ 点击开始按钮启动模型训练，训练日志和损失变化图会实时展现在页面中，此时可以自由关闭或刷新网页，在本文的测试环境（A100 40GB * 1）下，约 3 小时即可完成模型训练。⑧ 训练结束后，我们切换到 Chat 栏，点击刷新适配器按钮，将适配器路径切换至 yi-agent-6b，点击加载模型按钮载入刚刚训练好的模型。如果模型可以正常加载，那么恭喜你！仅花费一部电影的时间，就成功训练出了自己专属的 LLM Agent。效果展示① 基本对话② 工具调用 - 查询天气Yi-Agent-6B（本文微调的模型）：正确理解工具返回结果并得出答案。Yi-6B-Chat（零一万物发布的指令模型）：无法理解工具返回结果。③ 工具调用 - 计算 GPAYi-Agent 6B（本文微调的模型）：正确生成工具调用并得到答案。Yi-6B-Chat（零一万物发布的指令模型）：无法生成工具调用。从上述几个例子中可以看出，经过微调后的 Yi-6B 模型成功具备了选择工具-调用工具-总结答案的出色能力，在 Agent 方面的性能显著超越原始 Yi-6B-Chat 模型。由于网页界面功能有限，我们这里手动输入了工具调用结果，在下面的章节，我们将会展示如何使用 LLaMA Factory 将 LLM Agent 部署到实际生产环境中。模型部署① 切换到 Export 栏，选择最大分块大小为 2GB，填写导出目录为 models/yi-agent-6b，点击开始导出按钮，将 LoRA 权重合并到模型中，同时保存完整模型文件，保存后的模型可以通过 transformers 等直接加载。② 在终端输入以下命令启动 API 服务。python -m llmtuner.api.app --model_name_or_path models/yi-agent-6b --template default该命令会在本地启动一个和 OpenAI 格式相同的 RESTFul API，这时我们可以直接用本地模型来替代 GPT-3.5 的函数调用功能！下面是一个使用 openai-python 库来调用本地模型，实现 LLM Agent 功能的示例代码。import os import json from openai import OpenAI from typing import Sequence os.environ[""OPENAI_BASE_URL""] = ""http://192.168.0.1:8000/v1"" # 替换为本地主机 IP os.environ[""OPENAI_API_KEY""] = ""0"" def calculate_gpa(grades: Sequence[str], hours: Sequence[int]) -> float: grade_to_score = {""A"": 4, ""B"": 3, ""C"": 2} total_score, total_hour = 0, 0 for grade, hour in zip(grades, hours): total_score += grade_to_score[grade] * hour total_hour += hour return total_score / total_hour tool_map = { ""calculate_gpa"": calculate_gpa } if __name__ == ""__main__"": client = OpenAI() tools = [ { ""type"": ""function"", ""function"": { ""name"": ""calculate_gpa"", ""description"": ""根据课程成绩和学时计算 GPA"", ""parameters"": { ""type"": ""object"", ""properties"": { ""grades"": {""type"": ""array"", ""items"": {""type"": ""string""}, ""description"": ""课程成绩""}, ""hours"": {""type"": ""array"", ""items"": {""type"": ""integer""}, ""description"": ""课程学时""}, }, ""required"": [""grades"", ""hours""], }, }, } ] messages = [] messages.append({""role"": ""user"", ""content"": ""我的成绩是 A, A, B, C，学时是 3, 4, 3, 2""}) result = client.chat.completions.create(messages=messages, model=""yi-agent-6b"", tools=tools) tool_call = result.choices[0].message.tool_calls[0].function name, arguments = tool_call.name, json.loads(tool_call.arguments) messages.append({""role"": ""function"", ""content"": json.dumps({""name"": name, ""argument"": arguments}, ensure_ascii=False)}) tool_result = tool_map[name](**arguments) messages.append({""role"": ""tool"", ""content"": json.dumps({""gpa"": tool_result}, ensure_ascii=False)}) result = client.chat.completions.create(messages=messages, model=""yi-agent-6b"", tools=tools) print(result.choices[0].message.content) # 根据你的成绩和学时，你的平均绩点 (GPA) 为 3.4166666666666665。LLaMA Factory 在今后还将不断升级，欢迎大家关注我们的 GitHub 项目：https://github.com/hiyouga/LLaMA-Factory同时，我们也将本文的模型上传到了 Hugging Face，如果您有资源，一定要亲自动手训练一个大模型 Agent！https://huggingface.co/hiyouga/Yi-Agent-6B"
662718614,ba243e181b981ab78893f6d0ff3e60e0,对白,https://api.zhihu.com/people/ba243e181b981ab78893f6d0ff3e60e0,people,,大模型+图神经网络首篇综述来啦～,article,https://api.zhihu.com/articles/662718614,本文还指出了这一不断发展领域的未来方向。 <em>大模型</em>技术交流群 最后欢迎对大模型和AIGC感兴趣的小伙伴加入「AI+<em>大模型</em>交流分享群」一起学习交流，添加我的微信：duibai997，备注【<em>大模型</em>】我拉你进群,79,5,225,1697968487,1698071574,"卷友们好，我是对白。大模型凭借其出色的能力，在处理文本、图像、视频音频等多领域一扫六合，荣登“基础模型”的宝座。就像NLP领域中的进化历程，图机器学习看起来也正在经历这样一次范式转变。来自北京邮电大学等多位高校的研究者们深入探讨了图基础模型的问题，并发表了该领域的首篇综述《Towards Graph Foundation Models: A Survey and Beyond》。链接：https://arxiv.org/pdf/2310.11829v1.pdf图机器学习范式一路进化走来，从早期的浅层方法（如随机游走和矩阵分解）到近年来的图神经网络，在多个领域取得了显著改进。然而，这些模型在表达能力和泛化能力等方面仍存在限制，尤其是面对日益扩大的数据集和多样化的图任务时。NLP领域的进展引发了大家的思考：图基础模型是否代表了图机器学习的下一个前沿？图基础模型被预想为一个可以在广泛图数据上进行预训练，并可用于各类下游图任务的模型，如下图：如何定义图基础模型？文中给出的定义是，图基础模型（Graph Foundation Model，GFM）将在广泛的图数据的预训练下受益，并可以适应各种下游图任务。GFMs使用预训练来获取大量未标记的图数据的知识，然后使用适应技术来适应各种下游任务。它被预期具备以下两个特性：Emergence。这意味着图基础模型会随着参数的增大或训练数据的增加，会展现出一些新的能力，包括上下文学习、图推理、零样本图生成等。关于GFMs的这一研究目前还较少。Homogenization。这意味着图基础模型可以适用于不同类型的图任务，如节点分类、链接预测和图分类。但因为与NLP任务相比，图任务具有不同的特征，这一问题解决起来并不容易。实现GLMs所涉及到的关键技术包括Pre-training和Adaptation（如 vanilla fine-tuning, parameter-efficient fine-tuningand prompt-tuning.）。可以看到，GLMs与LLMs相比，二者在目标和学习范式具有相似性，但图数据和图任务导致了二者之间的根本差异，下表进行了一个总结：实现图基础模型：三类方法根据调查，现有相关研究可大致分为三类，即，基于GNN的模型、基于LLM的模型，以及基于GNN+LLM的模型。基于GNN的模型Backbone基于GNN的模型，其backbones是大家熟悉的基于信息传递的方法与基于图Transformer的方法。前者如GCN、GAT、GPPT等方法，由于其简单性和有效性，被广泛应用到各种下游任务中。鉴于基于信息传递的GNN模型收到表达能力、过平滑等问题的限制，Transformer的思想被引入图结构。现有对图Transformer的研究已经有很多，包括Graph-BERT、GROVER、Graphormer等。两种方法的主要区别在于它们对图结构的处理方式不同。图Transformer将图视为完全连接，而消息传递的方法采用图的邻接矩阵，消息仅在明确链接的节点之间传播。Pre-training对于图预训练来说，现有的一些方法采用自监督的思路，大致可以分为两类：对比式预训练和生成式预训练。基于对比的方法，思路在于最大化不同视图之间的互信息，从而捕捉不同视图之间固定的语义信息。这些视图对应到图中的节点级、子图级和图级信息。根据图的对比视图，又可细分为两类方法：同尺度对比学习和跨尺度对比学习。同尺度对比学习比较相同级别的两个图视图，如GCC、GraphCL、GRACE和MA-GCL。跨尺度对比学习比较不同级别的两个图视图，如DGI利用鉴别器来最大化节点嵌入和图嵌入之间的互信息，并最小化节点和损坏图嵌入之间的信息。生成式预训练方法的目标是使GNN能够理解图的一般结构和属性语义，从而能够适应带有通用信息的下游任务。现有方法主要有图重建和属性预测两种。图重建旨在重建给定图的特定部分，强调在再现原始图结构或节点属性时的准确性，例如VGAE、GPT-GNN和GraphMAE。在基于在属性预测的方法中，模型专注于学习和预测所提供的图的重要属性，如GROVER引入了节点和边的任务，预测局部子图中的上下文相关属性。尽管生成方法能够生成新的内容，但很难保证内容的质量和可解释性。Adaptation这里介绍了两种adaptation方法，微调和提示。对于微调来说，我们可以利用预训练模型生成图/节点嵌入，然后微调外部特定任务层以将预训练模型推广到下游任务。相关工作有DGI、GRACE、GPT-GNN、AdapterGNN和G-Adapter。但是，微调方法的不足在于1. 通常需要足够的标记数据来微调模型参数；2. 传统的微调方法需要为每个任务进行重复微调，造成重大的计算成本。提示微调作为一种新的策略，可以绕过对参数的完全微调，促进多任务适应和零样本学习。相关工作有GPPT：使用提示函数为每个类别生成一个标记对，将所有节点分类任务转化为链接预测任务。 ​ GraphPrompt：将下游任务整合到链接预测任务中，并利用标记数据学习每个任务的特定提示向量，从而修改了模型的readout操作，缩小了链接预测和节点/图分类之间的差距。 ​ AAGOD：提出通过在原始输入图的邻接矩阵上叠加一个可学习的实例特定提示来实施数据中心化操作。 ​ All In One：将节点级和图级任务转化为图级任务。它将额外的子图作为提示，与节点子图合并。模型随后使用合并后的图生成预测。 ​ GPF：为图中的每个节点引入了统一的特征修改向量，可以根据任何预训练策略进行优化，从而使预训练 GNN 模型适应下游任务。 ​ SGL-PT：包括一个强大而通用的预训练任务，该任务获得生成和对比方法的互补优势，以及一种新颖的不带提示功能的提取函数，将下游任务重新构建为与预训练方法相同的格式。 ​ PRODIGY：是一个用于在提示图上进行上下文学习的框架，旨在使预训练模型适应各种下游任务，而不需要优化任何参数。尽管这些方法已经在少样本场景中提高了性能，但还需要进一步探索图提示的语义和可解释性。讨论基于GNN的模型具有规模参数较少，成本较低，排列不变性等特点，可以在无文本属性的情况下展现较好的性能。但由于参数规模较小，它无法展现出涌现能力。此外，由于缺乏对文本的明确建模，基于GNN的模型无法充分利用存储在预训练语言模型中的信息，对文本利用不充分。基于LLM的模型利用LLM作为图基础模型的backbone可以利用自然语言描述，一统各类图学习任务。最近的研究如NLGraph、GPT4Graph。已经初步证明了LLM在图推理任务的潜力。Backbone在将LLM用于图数据时的一个核心问题是，如何将图数据与自然语言对齐。现有方法主要有两类：Graph-to-token和Graph-to-text。Graph-to-token：这种方法涉及图信息的token化。它使用节点表示作为backbone模型输入的唯一tokens，所以backbone模型需要是可训练的Transformer或开源LLMs，如InstructGLM使用LLaMA和T5作为其backbone，并进行进一步调整。该思路最早出现在GIMLET中，它将节点表示视为token，并旨在将图数据与文本数据集成在一起。InstructGLM 采用了预训练和adaptation框架，引入LLMs以进一步增强模型的文本处理能力，成为图基础模型的有力竞争者。这些方法尝试将图token化，使其与自然语言保持一致，尽管取得了较好的结果，但如何告知LLMs底层图结构仍然是一个重要挑战。Graph-to-text：使用自然语言来描述图信息。按照这种思路，可以利用提示语句利用LLM完成图任务，并且不要求LLM是否开源。例如，LLMtoGraph [112]使用多个LLMs，包括GPT-4 [117]和Vicuna [118]，GPT4Graph [109]使用InstructGPT-3 [119]，LLM4Mol [114]使用GPT-4 [117]，Graph-LLM [110]使用不同大小的多个语言模型，包括BERT [47]，DeBERTa [120]，Sentence-BERT [121]，GPT-4 [117]和LLaMA [53]。最初的尝试使用边列表去描述图结构，如 LLMtoGraph 、NLGraph 。GraphText引入了一种称为""Graph-syntax Tree""的新方法，使用自然语言来描述图结构。GPT4Graph设计了一种新的提示工程任务。它利用边列表、邻接列表、图建模语言（GML）和图标记语言来表示图结构，设计手动提示。利用graph summarization, neighborhood summarization, graph exploration, graph completion去设计自动提示。Graph-LLM [110]进一步支持，邻居摘要是现有提示工程中最有效的技术。除上述外，还有一些其他方面的工作，但总体而言，现有的提示方式可能不是挖掘潜在图结构的有效方法。Pre-training由于这些方法采用LLM作为backbone，其预训练阶段对应于LLMs的预训练阶段，主要有Language Modeling (LM)和Masked Language Modeling (MLM)。以下内容不详细展开，感兴趣的小伙伴可以参照原文。AdaptationLLMs主要在文本语料库上训练，导致它们在预训练阶段与图任务应用之间存在较大差距。Adaptation的作用在于增强LLM理解图数据的能力，对应的方法可以分为手动提示（Manual Prompting）与自动提示（Automatic Prompting）两类。和前面提到的类似，手动提示的方法，如， LLMtoGrpah和 NLGraph利用节点和边列表信息用自然语言描述图属性。GPT4Graph进一步地利用附加的描述语言来表示图数据。GraphText将图的特征和关系转换为自然语言，并组成一个“图语法树”。自动生成提示的思路出现在GPT4Graph和Graph-LLM中，前面有过介绍，这里不再赘述。除此以外，还有一些基于微调的自适应方法，包括Vanilla Fine-Tuning [47], IntermediateFine-Tuning (IFT) [125], Multi-task Fine-Tuning (MTFT)和Parameter Efficient Fine-Tuning。不过，这些方法目前尚未应用于与图相关的任务。讨论利用LLMs处理图任务的好处在于，它可以统一各种图任务。其缺陷在于，LLMs无法执行与图属性相关的精确数学计算，无法通过图链接进行多跳逻辑推理，难以捕捉高度连接的图中普遍存在的拓扑结构，以及难以处理随时间演变的图的动态特性。基于GNN+LLM的模型将GNN与LLM结合起来的方法总结见下表：Backbone一些研究利用LLM从原始数据中提取节点特征，并使用GNN进行预测。这些方法被称为以GNN为中心的模型。如SimTeG [86]对TAG（带有文本属性的图）的文本数据集进行LLM的高效微调。对于不开源的LLM，如ChatGPT,TAPE [129]通过文本交互与这些LLM进行互动。GIANT [130]利用一种图结构相关自监督学习方法对LLM进行微调。GraD [131]同时优化了GNN教师和无图学生，使用共享的LLM来处理图的节点。GALM [141]探索将文本和图数据相结合的模型的预训练方法。一些方法通过将GNN和LLM的嵌入进行对齐以实现更好的预测，或利用嵌入进行其他下游任务，这些方法被称为对称方法（Symmetric Methods）。大多数以GNN为中心的方法包括两个顺序步骤：文本编码和图聚合。在生成文本嵌入时，节点之间没有信息交换。为了考虑连接节点之间相互关联的特性，对称方法同时使用GNN和LLM以获取结构感知的文本特性。相关工作如，GraphFormer [132]将文本嵌入和图聚合作为一个迭代的工作流程。GLEM [133]使用变分EM框架迭代更新LLM和GNN，可以捕获基于局部文本属性的节点标签分布。其他研究采用不同的编码器用于图节点和文本，训练它们以在共享的潜在空间中对齐它们的表示，如ConGrat [134]利用对比式预训练框架共同学习图节点和文本的嵌入。 G2P2 [135]使用三种基于图交互的对比策略预训练了一个图文模型。 SAFER [136]分别训练文本编码器和GNN，并将对应的嵌入连接起来用于逻辑回归（LR）分类器。一些工作采用GNN来增强LLM的性能，这些方法被称为LLM-centric方法。例如，Graph-ToolFormer[128]结合ChatGPT增强的提示，将外部图学习工具的API调用集成到图推理语句中。这种思路使LLMs能够更有效地处理图推理任务，包括基本的图属性推理以及关于文献论文主题、分子图功能、顺序推荐系统、社交网络社群和知识图等高级任务的推理。Pre-training基于GNN+LLM的方法可以在由图和文本组成的数据上进行预训练，具体可以分为基于GNN/LLM的策略以及基于对齐的策略。前者如SimTeG [86]、GIANT [130]、GraD [131]、GraphFormer [132]、GLEM [133]、ConGrat [134]、SAFER [136]、Text2Mol [137]、MoMu [138]、MoleculeSTM [139]和CLAMP [140]在预训练期间采用MLM。一些模型，如TAPE和Graph-ToolFormer，在预训练阶段采用LM。此外，SimTeG集成了文本-文本对比学习（TTCL）。后者基于对齐的方法，如ConGrat [134]、Text2Mol [137]、MoMu [138]、MoleculeSTM [139]和CLAMP [140]这样的LLM和GNN的对称方法在具有图文本对比学习（GTCL）的大型数据集上进行预训练，该学习方法用于对齐图编码器和文本编码器的嵌入。这些嵌入包含了关于图结构和文本的丰富信息，因此在下游数据集上性能较好。Adaptation常见思路有基于微调和提示微调两种。基于常规的微调方法涉及调整广泛范围的模型参数，这可能具有较高的计算和资源要求。许多研究利用了参数高效的微调方法，用于下游任务，从而实现更高效和资源友好的方法。提示微调技术则被用在如TAPE、Graph-ToolFormer 方法中。讨论GNN+LLM-based模型能够生成涵盖丰富文本、结构信息甚至LLMs外部知识的嵌入，从而实现更好的模型性能。此外，这些模型还可以被视为多模态模型，用于完成跨模态任务，如文本-图形检索任务。然后，该模型的训练通常需要大量的数据（尤其是并行数据）和大量的计算资源，如何设计一种更连贯和有效的集成方式是目前的挑战。未来的方向提升数据体量和质量当图数据存在噪声、不完整或未经适当筛选，将会对图基础模型的性能产生负面影响。现有的工作如图增强[93]、特征增强[142]、标签增强[143]等策略主要适用于同质图，当扩展到异构图、动态图等复杂形式时值得大家进一步思考。此外，也需要进一步探索如何有效地增强图数据，以适应LLM-based或GNN+LLM-based模型。核心架构和训练策略关于核心架构，最近的研究提出了超越Transformer的模型架构[144, 145]，提供了更好的性能[144]或可解释性[145]。然而，尚不清楚这些核心架构是否适用于处理图数据。此外，如果使用GNN+LLM-based模型，更有效地对齐两个模型的输出是值得探讨的研究领域。关于学习范式，获得经过预训练的图基础模型后，将它们适应到下游任务是一个重要的问题。除了本文中介绍的微调和提示之外，还有其他潜在的技术可以应用，如知识蒸馏[146]和模型编辑[147]。上述技术是否适用于图基础模型将成为未来研究的重点。评估和关键应用语言基础模型的评估已经从人工评估[119]发展为元评估[148]，但图基础模型仍然缺乏标准化的评估方法。图数据的数据域和任务格式的多样性增强了这一问题的难度。此外，与语言基础模型在文本翻译[149]和文本生成[150]等任务中出色的性能相比，图基础模型是否可以在某些与图相关的任务中实现突破性应用，如药物发现[151]和城市计算[152]，在研究领域也尚未解决。结论这是一篇非常好的综述，文中首次提出了图基础模型（GFMs）的概念，并介绍了相关概念和代表性方法。文章将现有的GFMs工作分为三个主要类别：基于GNN的模型，基于LLM的模型和基于GNN+LLM的模型。对于每个类别的方法，分别介绍了它们的核心架构、预训练和适应策略。在全面概述了图基础模型的现状后，本文还指出了这一不断发展领域的未来方向。大模型技术交流群最后欢迎对大模型和AIGC感兴趣的小伙伴加入「AI+大模型交流分享群」一起学习交流，添加我的微信：duibai997，备注【大模型】我拉你进群，群里的讨论氛围非常好～推荐阅读大模型压缩首篇综述来啦~一脉相通：论大模型（LLM）在推荐系统上的新范式最新研究！谷歌利用大模型LLM扩展搜索Query2023对比学习（Contrastive Learning）研究进展精要2022对比学习（Contrastive Learning）在各大顶会上的最新研究进展万物皆Contrastive Learning，从ICLR和NIPS上解读对比学习最新研究进展对白：对比学习（Contrastive Learning）最新综述"
662673023,d578510faab32019b5cf30af8fc2ad99,Aini,https://api.zhihu.com/people/d578510faab32019b5cf30af8fc2ad99,people,Enjoy_endless：踏实一些，不要着急,大模型综述-A Survey of Large Language Models,article,https://api.zhihu.com/articles/662673023,其相对于标准提示的优势更加明显。 2.2 <em>大</em>语言<em>模型</em>的关键技术 1、扩展：Transformer 语言<em>模型</em>存在明显的扩展效应，更大的模型/数据规模和更多的训练计算通常会导致<em>模型</em>能力的提升。作为两个代表性的模,139,4,447,1697940701,1697940784,"A Survey of Large Language Models 这篇原综述论文很长，总共有60页、参考文献400多篇。耗时一周整理，这里主要提取其有“意义“的地方出来进行分享学习(删除一些为了发表论文而“不得不“添加一系列冗余)。 综述论文会比较全、相对较浅、难免显得‘啰嗦’，对于有基础的同学看起来可能没有什么价值，当然，里面可能也会有一些你平时没有接触到或者想到的东西，可以进一步丰富你的理解、扩展你的思路。而对于刚接触的同学来说、可以比较好的有一个全貌的了解，在全貌了解的基础上，可以进一步学习自己感兴趣的部分，或者找到重要的部分进而指引下一步学习的方向。目录：1、 引言2、 概述3、 大语言模型公开可用资源4、 预训练5、 适配微调6、 使用7、 评测8、 总结1、 引言LLM(大语言模型) 和 PLM(预训练语言模型) 之间的三个主要区别。第一，LLM 表现出一些令人惊讶的涌现能力，这些能力可能在以前较小的 PLM 中没有观察到。这些能力是 LM 在复杂任务上表现的关键，它使得人工智能算法具有前所未有的强大和有效性。其次，LLM 将彻底改变人类开发和使用人工智能算法的方式。与小型 PLM 不同，访问 LLM 的主要方法是通过提示接口（例如 GPT-4 API）。人们必须了解 LLM 的工作原理，并以 LLM 能够遵循的方式形式化他们的任务。 第三，LLM 的发展不再明确区分研究和工程。训练 LLM 需要在大规模数据处理和分布式并行训练方面具有丰富的实践经验。 ，本综述从四个主要方面对 LLM 的最近进展进行文献综述，包括预训练（如何预训练 出一个有能力的 LLM）、适配微调（如何从有效性和安全性两个角度有效地微调预训练的 LLM）、使用（如何利用LLM 解决各种下游任务）以及能力评估（如何评估 LLM 的能力和现有的经验性发现）。对于这篇综述，我们还创建了一个 GitHub 项 目网站，该网站收集了关于 LLM 的支持资源，链接为 https: //http://github.com/RUCAIBox/LLMSurvey。2、 概述2.1大语言模型的涌现能力 LLM 的涌现能力被正式定义为 “在小型模型中不存在但在大型模型中产生的能力”，这是区别 LLM 与先前 PLM 的最显著特征之一。在这里，我们简 要介绍了 LLM 的三种典型涌现能力和具备这种能力的代表性模型。 1）上下文学习：ICL 能力是由 GPT-3正式引入的：假 设已经为语言模型提供了一个自然语言指令和/或几个任务演 示，它可以通过完成输入文本的单词序列的方式来为测试实 例生成预期的输出，而无需额外的训练或梯度更新。在 GPT 系列模型中，1750 亿的 GPT-3 模型在一般情况下表现出强大 的 ICL 能力，但 GPT-1 和 GPT-2 模型则没有。 2）指令遵循：通过使用自然语言描述的混合多任务数据集 进行微调（称为指令微调），LLM 在未见过的以指令形式描述的任务上表现出色。通过指令微调，LLM 能够在没有使用显式示例的情况下遵循新的任务指令，因此它具有更好的泛化能力。实验证明，当模型大小达到 680 亿 时，经过指令微调的 LaMDA-PT开始在未见过的任务上显著优于未微调的模型，但对于 80 亿或更小的模型大小则不 会如此。 3）逐步推理：对于小型语言模型而言，通常很难解决涉及 多个推理步骤的复杂任务，例如数学问题。然而，通过使用思维链（Chain-of-Thought, CoT）提示策略，LLM 可以通 过利用包含中间推理步骤的提示机制来解决这类任务，从而 得出最终答案。这种能力可能是通过在代码上进行训练而获 得。一项实证研究表明，当应用于模型大小大于 600 亿 的 PaLM 和 LaMDA 变体时，CoT 提示可以提高模型在算术 推理基准任务上的性能，而当模型大小超过 1000 亿时，其相对于标准提示的优势更加明显。2.2 大语言模型的关键技术1、扩展：Transformer 语言模型存在明显的扩展效应，更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升。作为两个代表性的模 型，GPT-3 和 PaLM 通过增加模型规模分别达到了 1750 亿 和 5400 亿。此外，由于计算预算通常是有限的，可以利用扩 展法则来更高效地分配计算资源。例如，Chinchilla（具有更 多的训练 token）通过在相同的计算预算下增加数据规模，优于其对应的模型 Gopher（具有更大的模型规模）。2、训练：分布式训练算法是学习 LLM 网络参数所必需的，其中通常联合使用各种并行策略。为了支持分布式训练，已经发布了一些优化框架来促进并行算法的实现和部署，例如 DeepSpeed和 Megatron-LM。 此外，优化技巧对于训练稳定性和模型性能也很重要，例如预训练以克服训练损失激增和混合精度训练等；3、能力引导：在大规模语料库上预训练之后，LLM 具备 了作为通用任务求解器的潜在能力。然而，当 LLM 执行一些 特定任务时，这些能力可能不会显式地展示出来。作为技术 手段，设计合适的任务指令或具体的 ICL 策略可以激发这些 能力。例如，通过包含中间推理步骤，CoT 提示已被证明对 解决复杂的推理任务有效。此外，我们还可以使用自然语言 表达的任务描述对 LLM 进行指令微调，以提高 LLM 在未见 任务上的泛化能力。4、对齐微调：InstructGPT 设计了一种有效的微调方法，使 LLM 能够 按照期望的指令进行操作，其中利用了基于人类反馈的强化 学习技术。采用精心设计的标注策略，它将人类反馈纳入训练循环中。ChatGPT 实际上采用类似于 InstructGPT 的技术，在产生高质量、无害的回答方面表现出很强的对齐能5、工具操作：利用外部工具可以进一步扩展 LLM 的能力。例如，LLM 可以利用计算器进行准确计算， 利用搜索引擎检索未知信息。最近，ChatGPT 已经实现 了使用外部插件（现有或新创建的应用程序）的机制，这类 似于 LLM 的“眼睛和耳朵”。这种机制可以广泛扩展 LLM 的 能力范围。2.3 GPT系列模型的技术演进 （这系列的文章也在其他专栏专门介绍过，长时间不使用的同学可以再去温习一下）1、GPT-1: 2018 年, OpenIA发布了GPT 1，代表生成式预训练 （Generative Pre-Training）。GPT-1 是基于生成型的、仅含有解码器的 Transformer 架构开发的，并采用了无监督预训练和有 监督微调的混合方法。GPT-1 为 GPT 系列模型建立了核心 架构，并确立了对自然语言文本进行建模的基本原则，即预 测下一个单词。2、GPT-2: 将参数规模增加到了 15 亿，并使用大规模的网页数据集 WebText 进行训练。它旨在通过无监督语言建模来执行任务，而无需使用标记数据进行显式微调。主张每个 NLP 任务可以被视为基于世界文本的子集的单词预测问题。因此，如果模型训练后具有足够能力以复原世界文本，无监督语言建模可以解决各种任务。尽管 GPT-2 旨在成为一个“无监督的多任务学习 器”，但与监督微调的 SOTA 方法相比，其整体性能仍然较 差。3、GPT-3: 于 2020 年发布，将模型参数扩展 到了更大的规模，达到了 1750 亿。引入了ICL 的概念，它是以小样本或零样本的方式使用 LLM。 ICL 可以指导 LLM 理解以自然语言文本的形式给出的任务。GPT-3 不仅在各种 NLP 任务中表 现出色，而且在一些需要推理或领域适配能力的特殊设计的 任务中也表现出色。GPT-3 可以被视 为从 PLM 到 LLM 进化过程中的一个重要里程碑。它通过实证证明，将神经网络扩展到大的规模可以大幅增加模型的能力。1） 能力增强： GPT-3 已经成为开发更强大 LLM 的基础模型，OpenAI 探索了两种 主要方法来进一步改进 GPT-3 模型，即使用代码数据进行训 练以及与人类偏好的对齐：1、 使用代码数据进行训练：原始的 GPT-3 模型（在纯 文本上进行预训练）的一个主要限制在于缺乏复杂任务的推 理能力，例如完成代码和解决数学问题。OpenAI 在 2021.07推出了 Codex，这是一个在大量 GitHub 代码上微调的 GPT 模型。它明了 Codex 可以解决非常困难的编程问题，并且在数学问题上有显著的性能提升。实际上，GPT-3.5 模 型是在基于代码的 GPT 模型（code-davinci-002）的基础上开发的。2、 与人类对齐：InstructGPT在 2022.01提出，以改进 GPT-3 模型与人类对齐能力，正式建立了 一个三阶段的基于人类反馈的强化学习（RLHF）算法。除了提高指令遵循能力之外，RLHF 算法对于缓 解有害或有毒内容的生成问题十分有效，这对于 LLM 在实践 中的安全部署至关重要。OpenAI在对齐研究中的方法，总结了三个有前途的方向：“使用人类反馈训练AI 系统，协助人类评估以及做对齐研究”。 这些增强技术引出了具有更强能力的GPT-3.5 模型.4、ChatGPT: 2022.11, OpenAI 发布了对话语言模型 ChatGPT，它是以类似 InstructGPT 的方式进行训练的（在原始文章中称为“InstructGPT 的姊妹模型”），但专门针对对话能力进行了优化。ChatGPT 训练数据是通过将人类生成的对话（扮演用户和 AI 两个角色）与 InstructGPT 数据集结合起来以对话形式生 成。ChatGPT 在与人类的交流中表现出卓越的能力：拥有丰 富的知识库，擅长解决数学问题，准确追踪多轮对话中的上 下文，并与人类的价值观保持一致以确保被安全使用。随后， ChatGPT 支持了插件机制，进一步通过已有工具或应用扩展 了 ChatGPT 的功能。5、GPT-4: 于 2023.03发布，将文本输入扩展到多模态信号。总体而言，相比 GPT-3.5，GPT-4 在解决复杂任务方面具有更强的能力，在许 多评估任务上展现出大幅度的性能提升, 具有比之前 的 GPT 模型如 ChatGPT 更出色的性能。此外，由于经过为期六个月的迭代对齐（在 RLHF 训练中加入了额外的安全 奖励信号），GPT-4 对于具有恶意或挑衅的提问的响应更加 安全。并采用了多种干预策略来减轻语言模型的可能问题，如幻觉、隐私和过度依赖。例如，他们引入了一种称为红队评估（red teaming）的机制来减少有害或生成有毒内容的可能性。 近几年主要模型发展时间轴如下：3大语言模型公开可用资源3.1 公开可用的模型检查点或者API1）百亿级：Flan-T5（110 亿版本）可以作为研究指令微调的首选模型，因 为它从三个方面探索了指令微调：增加任务数量、扩大模 型规模和使用 CoT 提示数据进行微调。CodeGen（11B）是 一个为生成代码设计的自回归语言模型，可用作探索代码生 成能力的候选模型。对于多语言任务，mT0（13B）可能 是一个比较好的候选模型，因为它在多语言任务中使用多语 言提示进行微调。此外，作为最近发布的模型，LLaMA（65B）在与指令遵循相关的任务中展现了卓越的性能。百亿参数量级别的模型通常需要数百甚至 上千个 GPU 或 TPU。2）千亿级：OPT（175B）专注于复现和开源，旨在使研 究人员能够进行大规模可重复研究。对于跨语言泛化研究，可以将 BLOOM（176B）和 BLOOMZ（176B）用作基础模型， 因为其在多语言语言建模任务中具有较好的能力。在这些模型中，OPT-IML 进行了指令微调，是研究指令微调效果的 较好选择。千亿参数量级别的模型通常需要数千个 GPU 或 TPU 进行训练。3）公共API：GPT-3.5 系列包括一个基础模型 code-davinci-002 和三个增强版本，即 text-davinci-002、 text-davinci-003 和 gpt-3.5-turbo-0301。值得注意的 是，gpt-3.5-turbo-0301 是调用 ChatGPT 的接口。最近， OpenAI 还发布了与 GPT-4 相应的 API，包括 gpt-4、 gpt-4-0314、gpt-4-32k 和 gpt-4-32k-0314。主要模型及其配置如下：3.2 常用语料库 我们将这些语料库分为六个组别进行介绍：Books、 CommonCrawl、Reddit Links、Wikipedia、Code、Others，Books：BookCorpus是之前小规模模型（如 GPT和 GPT-2）中常用的数据集，包括超过 11,000 本电子书， 涵盖广泛的主题和类型（如小说和传记）。另一个大规模的书 籍语料库是 Gutenberg，它有超过 70,000 本文学作品， 包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共 领域的作品。它是目前最大的开源书籍集合之一，被用于训 练 MT-NLG和 LLaMA；CommonCrawl：CommonCrawl是最大的开源网络 爬虫数据库之一，能力达到了百万亿字节级别，已经被广 泛运用于训练 LLM。由于整个数据集非常大，因此现有的 研究主要在特定时间段内从中提取网页子集。然而，由于网 络数据中存在大量的噪音和低质量信息，因此使用前需要进 行数据预处理。目前有四个较为常用的基于 CommonCrawl 的过滤数据集：C4，CC-Stories，CC-News， 和 RealNews。C4 包括五个变种18，即 en （806G）， en.noclean （6T），realnewslike （36G），webtextlike （17G） 和 multilingual（38T）。Reddit Links：Reddit 是一个社交媒体平台，用户可以在上 面提交链接和帖子，其他人可以通过“赞同”或“反对”投票。高 赞的帖子通常被认为对多数用户是有帮助的，可以用来创建高 质量的数据集。WebText就是一个著名的基于 Reddit 的 语料库，它由 Reddit 上高赞的链接组成，但尚未公开。作为替代，有一个易于获取的开源替代品叫做 OpenWebText。 另一个从 Reddit 中提取的语料库是 PushShift.io.Wikipedia： Wikipedia是一个在线百科全书，包含大 量高质量的文章，涵盖各种主题。其中大部分文章都采用解 释性写作风格（并支持引用），覆盖了多种不同语言和广泛的 知识领域。通常来说，Wikipedia 英语版本被广泛应用于大多 数 LLM(例如 GPT-3，LaMDA 和 LLaMA )Code：为了收集代码数据，现有工作主要是从互联网上爬取 有开源许可证的代码。代码数据有两个主要来源：包括开源 许可证的公共代码库（例如 GitHub）和与代码相关的问答平 台（例如 StackOverflow）。Google 公开发布了 BigQuery 数据 集，其中包括各种编程语言的大量开源许可证代码片段， 是一个典型的代码数据集。Others： The Pile是一个大规模、多样化、开源的文 本数据集，有超过 800GB 数据，内容包括书籍、网站、代码、 科学论文和社交媒体平台等。它由 22 个多样化的高质量子集 构成。The Pile 数据集被广泛应用于不同参数规模的模型. GPT-3（175B）是在混合数据集（共 3000 亿 token） 上进行训练的，包括 CommonCrawl 、WebText2、 Books1、Books2 和 Wikipedia。 PaLM（540B）使用了一个由社交媒体对话、过滤 后的网页、书籍、Github、多语言维基百科和新闻组成的预训 练数据集，共包含 7800 亿 token。 LLaMA 从多个数据源中提取训练数据，包括 CommonCrawl、C4 、Github、Wikipedia、书籍、ArXiv 和 StackExchange。LLaMA（6B）和 LLaMA（13B）的训练数 据大小为 1.0 万亿 token，而 LLaMA（32B）和 LLaMA（65B） 使用了 1.4 万亿 token。4 预训练4.1 数据收集 语料库的来源可以广义地分为两种类型：通用文本数据和专用文本数据。通用文本数据，如网页、书籍和对 话文本等，其由于规模大、多样性强且易于获取的特点，被 大多数 LLM 所利用，这可以增强 LLM 的语言 建模和泛化能力。鉴于 LLM 所展现出的惊人泛化能力，也 有研究将预训练语料库扩展到更专用的数据集，如多语言数 据、科学数据和代码等，以此来赋予 LLM 解决专用任务的能力。通用文本数据：网页爬取数据如 CommonCrawl，对话文本可以利用公共对话语料库的子集（例如 http://PushShift.io Reddit 语料库），由于在线对话 数据通常涉及多个参与者之间的讨论，因此一种有效的处理方式是将对话转换成树形结构，其中每句话与回应它的话语 相连。通过这种方式，多方之间的的对话树可以被划分为预 训练语料库中的多个子对话。书籍与其他语料库相比，提供了更正式的长文本，这对于 LLM 学习语言知识、建模长期依赖关系以及生成 叙述性和连贯的文本具有潜在的好处。为了获得开源书籍数 据，现有的研究通常采用 Books3 和 Bookcorpus2 数据集，这 些数据集可以在 Pile 数据集中获得。专用文本数据：多语言文本，科学文本，现有的工作主要收集了 arXiv 论文、科学教材、数学网页和其他相关的科学资源。代码，常用于预训练 LLM 的代码语料库有两种来源。第一种来源是编程问答社区 （如 Stack Exchange）。第二种来源是开源软件仓库， 例如 GitHub，它们收集了代码数据（包括注释 和文档字符串）以供利用。与自然语言文本相比，代码以编程语言的格式呈现，对应着长距离依赖和准确的执行逻辑。训练代码可能是复杂推理能力 （例如 CoT 能力）的来源。此外，将推理任务格式化为代 码的形式还可以帮助 LLM 生成更准确的结果。现有LLM预训练数据中各数据来源的占比如下：4.2 数据预处理质量过滤：基于分类器的方法，和基于启发式的方法。前一种方法基于高质量文本训练选择分类器，并利 用它来识别和过滤低质量数据。通常，这些方法使用高质量数据（例如维基百科页面）作为正样本，采样候选 数据作为负样本来训练二元分类器，并预测衡量每个数据的 质量的分数。第二种方法，采 用基于启发式的方法，通过设计一组精心设计的规则来消除 低质量文本，如基于度量(困惑度)、统计、关键词的过滤。去重：现有的研究发现，语料库中的重复数据会降低语 言模型的多样性，可能导致训练过程不稳定，从而影响模型 性能。因此，需要对预训练语料库进行去重处理。具体来说， 可以在句子级、文档级和数据集级等不同粒度上去重。首先， 在句子级别上，应删除包含重复单词和短语的低质量句子，因 为它们可能会在语言建模中引入重复模式。在文档级别 上，现有研究主要依靠文档之间的表层特征（例如单词和 n 元的重叠）的重叠比率来检测和删除包含相似内容的重复文 档。此外，为了避免数据集污染问题，还必须 通过从训练集中删除测试集可能出现的重复文本，来防止训 练集和评估集之间的重叠。现有研究已经证明，这三个级别的去重都有助于改善 LLM 训练，在实践中应该共同使用这三个级别的去重。隐私去除：大多数预训练文本数据来自网络来源，包括涉及敏感或个人信息的用户生成内容，这可能增加隐私泄露的风险。因此，需要从预训练语料库中删除可识别个人信息 （PII）。一种直接有效的方法是采用基于规则的方法，例如 关键字识别，来检测和删除 PII，例如姓名、地址和电话号 码。分词：分词也是数据预处理的关键步骤。它的目的是将原始 文本分割成词序列，随后用作 LLM 的输入。虽然直接利用 已有的分词器是方便的（例如GPT-3利 用了 GPT-2的分词器），但是使用专门为预训练语料库设计的分词器可能会更加有效，特别是对于由多种领 域、语言和格式组成的语料库。因此，最近的几个 LLM 使用 SentencePiece为预训练语料库训练定制化的分词器。同时利用字节级的 Byte Pair Encoding (BPE) 算法来确保 分词后的信息不会丢失。预处理流程如下图：4.3 架构4.3.1 模型架构1、因果解码器架构：因果解码器架构采用单向注意力掩码（仅解码器架构），以确保每个输入 token 只能关注过去的 token 和它本身。作为这种 架构的代表性语言模型，GPT 系列模型是基于因 果解码器架构开发的。特别地，GPT-3成功展示了这种 架构的有效性，同时也展示了 LLM 惊人的 ICL 能力。至今，因果解码器已被广泛用作 为各种现有 LLM 的体系结构，例如 OPT、BLOOM和 Gopher。2、前缀解码器架构：前缀解码器架构修正了因果解码器的掩码机制，以使其能够对前缀 token 执 行双向注意力，并仅对生成的 token 执行单向注意力。 这样，与编码器-解码器架构类似，前缀解码器可以双向编码 前缀序列并自回归地逐个预测输出 token，其中在编码和解码 过程中共享相同的参数。基于前缀解码器架构的现有代表性 LLM 包括 GLM-130B和 U-PaLM。4.3.2 详细配置 Transformer 的四个主要部分的相应配置，包括标准化、位置编码、激活函数、注意力和偏置，提出了各种改进方法来提 高其训练稳定性、性能和计算效率。标准化：训练不稳定是预训练 LLM 的一个难题。为了缓解 这个问题，层标准化 (Layer Norm, LN)被广泛应用于 Transformer 架构中。LN 的位置对 LLM 的性能至关重要。 虽然最初的 Transformer使用后置 LN，但大多数 LLM 采用前置 LN 以实现更稳定的训练。（这里的前置和后置指的是放在attention的前还是后，可以参考下图）最近，一些新的标准化技术被提出，由于 RMS Norm 在训练速度和性能方面的优越性， 其在 Gopher和 Chinchilla中被采用。与 LN 相比， DeepNorm已经表现出更好的训练稳定性，和后标准化 一起被 GLM-130B 采用。激活函数：为了获得良好的性能，在前馈网络中也需要设置 合适的激活函数。在现有的 LLM 中，广泛使用 GeLU 激活函 数。此外，在最新的 LLM (例如 PaLM 和 LaMDA) 中， 也使用了 GLU 激活函数的变体，特别是 SwiGLU 和 GeGLU 变体，在实践中通常可以获得更好的性能。位置编码：由于 Transformer 中的自注意模块具有置换不变 性，因此需要使用位置编码来注入绝对或相对位置信息以建 模序列。在经典的 Transformer中有两种绝对位置编码的 变体，即正弦函数和学习的位置编码，后者通常在 LLM 中使用。与绝对位置编码不同，相对位置编码根据键和查询之间的 偏移量生成嵌入，因此它可以在比训练序列更长的序列 上表现良好，即外推。注意力机制和偏置：除了原始 Transformer 中的全自注意力 机制，GPT-3 采用了更低计算复杂度的稀疏注意力机制， 即分解注意力。为了有效且高效地建模更长的序列， 研究者们尝试引入特殊的注意力模式或考虑显存 访问（即 FlashAttention）。此外，与原始 Transformer 一样，大多数 LLM 在每个线性层和层标准化中保留了偏置。 然而，在 PaLM和 Galactica中，偏置被移除。研究表明，对于 LLM 来说，去除偏置可增强训练的稳定性。 4.4 模型训练4.4.1 优化设置批量训练：对于语言模型的预训练，现有的研究通常将批量 大小设置为较大的数字（如 2,048 个例子或 400 万个 token）， 以提高训练的稳定性和吞吐量。像 GPT-3 和 PaLM 这样的 LLM 引入了一种新的策略，即在训练过程中动态增加批量大 小，最终达到百万级别。具体而言，GPT-3 的批量大小从 3.2 万逐渐增加到 320 万个 token。实证结果表明，动态调整批量 大小的策略可以有效地稳定 LLM 的训练过程。 学习率：现有的 LLM 通常在预训练过程中采用类似的学习 率调整策略，包括预热（warm-up）和衰减（decay）。具体而 言，在训练的初始 0.1% 到 0.5% 的步骤中，采用线性预热策 略逐渐增加学习率到最大值，这个最大值通常在 5 × 10−5 到 1 × 10−4 之间（例如 GPT-3 的学习率为 6 × 10−5）。然后， 在后续步骤中采用余弦衰减策略，逐渐将学习率降低到其最 大值的约 10%，直到训练损失的收敛。 优化器： Adam 优化器和 AdamW 优化器被广泛 应用于 LLM（例如 GPT-3）的训练中，这些优化器使用了基 于一阶梯度优化的低阶矩自适应估计。通常，它的超参数设 置如下：β1 = 0.9，β2 = 0.95 和 ϵ = 10−8。稳定训练：在 LLM 的预训练过程中，常常会遇到训练不稳定 的问题，这可能会导致模型崩溃。为了解决这个问题，通常会广泛使用权重衰减（weight decay）和梯度裁剪（gradient clipping）。现有的研究通常将梯度裁剪的 阈值设置为 1.0，将权重衰减率设置为 0.1。然而，随着 LLM 规模的扩展，训练损失的突增也更容易发生，导致训练不稳 定。为了缓解这个问题，PaLM和 OPT使用了一种 简单的策略，即从发生突增之前的一个检查点重新开始训练 过程，并跳过可能导致问题的数据。此外，GLM发现嵌 入层的异常梯度通常会导致突增，因此提出缩减嵌入层梯度 以缓解这个问题。4.4.2 可扩展的训练技术1、3D 并行： 3D 并行实际上是三种常用并行训练技术的组合， 即数据并行、流水线并行和张量并行。1）数据并行：将模型参数和优化器状态复制到多个 GPU 上，然后 将整个训练语料库分配到这些 GPU 上。这样，每个 GPU 只 需要处理分配给它的数据，并执行前向和反向传播以获取梯 度。在不同 GPU 上计算的梯度将进一步聚合以获得整个批量 的梯度，以更新所有 GPU 上的模型。这样，由于梯度的计算 在不同 GPU 上是独立进行的，数据并行机制具有高度可扩展 性，可以通过增加 GPU 数量来提高训练吞吐量。此2）流水线并行：流水线并行旨在将 LLM 的不同层分配到 多个 GPU 上。特别地，在 Transformer 模型中，流水线并行 将连续的层加载到同一 GPU 上，以减少在 GPU 之间传输已 经计算的隐藏状态或梯度的成本。3）张量并行：张量并行专注 于分解 LLM 的张量（参数矩阵）。对于 LLM 中的矩阵乘法 操作 Y = XA，参数矩阵 A 可以按列分成两个子矩阵 A1 和 A2，从而将原式表示为 Y = [XA1, XA2]。通过将矩阵 A1 和 A2 放置在不同的 GPU 上，矩阵乘法操作将在两个 GPU上并行调用，并且可以通过跨 GPU 通信将两个 GPU 的输出 组合成最终结果。目前，研究人员已经将 8 路数据并行、4 路张量并行和 12 路流水线并行纳入到 BLOOM的 384 个 A100 GPU 上进行 训练。2、ZeRO：如前所述，数据并行需要每个 GPU 存储 LLM 的相同副本， 包括模型参数、模型梯度和优化器参数。然而，并非所有上 述数据都需要在每个 GPU 上保留，这将导致内存冗余问题。 为了解决这个问题，ZeRO 技术旨在仅在每个 GPU 上保留部 分数据，而当需要其余数据时可以从其他 GPU 中检索。具 体而言，根据三个数据部分具体的存储方式，ZeRO 提供了三 种解决方案，即优化器状态分区、梯度分区和参数分区。实 证结果表明，前两种解决方案不会增加通信开销，而第三种 解决方案会增加约 50% 的通信开销，但可节省与 GPU 数量 成比例的内存。3、混合精度训练：以前的PLM（例如 BERT）主要使用 32 位浮点数（FP32）进行预训练。近年来，为了预训练极大的语 言模型，一些研究开始利用 16 位浮点数（FP16），以减 少内存使用和通信开销。此外，由于流行的 NVIDIA GPU（例 如 A100）具有的 FP16 计算单元是 FP32 的两倍，FP16 的计 算效率可以进一步提高。5 大语言模型的适配微调 在预训练后，LLM 可以获得解决各种任务的通用能力。然而，越来越多的研究表明，LLM 的能力可以进一步适配 （adapting）到特定的目标。本节中，我们将介绍两种适配预 训练后的 LLM 的方法：指令微调（instruction tuning）和对齐微调（alignment tuning）。前一种方法旨在增强（或解锁） LLM 的能力，而后一种方法旨在将 LLM 的行为与人类的价 值观或偏好对齐。5.1 指令微调 本质上，指令微调是在自然语言格式的实例集合上微调预训练后的 LLM 的方法。这种方法与有监督微 调和多任务提示训练密切相关。为了进行指令微调， 我们首先需要收集或构建指令格式（instruction-formatted） 的实例。然后，我们使用这种格式的实例以有监督的方式微调 LLM（例如使用序列到序列的损失进行训练）。指令微调后， LLM 可以展现出泛化到未见过任务的卓越能力。5.1.1 格式化实例的构建 通常情况下，一个指令格式的实例包括一个任务描述（称为 指令）、一对输入-输出以及少量示例（可选）。作为重要的公共资源，现有的研究已经发布了大量带标注的自然语言格式 的数据（如下表）格式化已有数据集：基于现有来自不同领域（例如文本摘 要、文本分类和翻译）的实例来创建有监督的多任务训练数 据集，作为指令微调实例的一种重要来源，用自然语言的任 务描述来格式化这些多任务训练数据集是相当方便的。具体 来说，使用人类撰写的任务描述 来增广带标注的数据集，这些描述通过解释任务目标来指导 LLM 理解任务。例如，在下图 （b）中，每个问答任务的实 例都添加了一个任务描述“请回答下列问题”。在指令微调之 后，LLM 可以通过遵循任务描述很好地泛化到其他未见过的 任务上。特别地，指令被证明是影响 LLM 任务泛 化能力的关键因素。为了更好地为指令微调生成标注实 例，一种名为 PromptSource 的众包平台被提出，可以 有效地创建、共享和验证不同数据集的任务描述。此外，一些 研究还尝试通过为指令微调特殊设计的任务描 述，来反转已有实例的输入-输出对。例如，对于一个已有的 问题-答案对，我们可以通过基于以问题预测答案的方式来创 建一个新实例（例如，“请基于以下答案生成一个问题：”）。格式化人类需求：尽管大量的训练实例已经通过添加指令进 行格式化，但它们主要来自公共的 NLP 数据集，任务描述缺 乏多样性或与人类真实需求不匹配。为了解决这个问题， InstructGPT建议采用真实用户提交给 OpenAI API 的 查询作为任务描述。用户查询以自然语言表示，很适合引导出 LLM 遵循指令的能力。此外，为了丰富任务的多样性，标注 者还要为真实生活中的任务编写指令，包括开放式生成、开 放式问答、头脑风暴和聊天等。然后让另一组标注人员直接 按照将这些指令作为输出进行回答。最后，将指令（即采集的 用户查询）和期望的输出（即人工编写的答案）配对作为一 个训练实例。格式化实例和两种构造指令格式实例的方式如下图：5.1.2 指令微调策略 与预训练不同，因为只需要使用较少数量的实例进行训练，指令微调通常更加高效。指令微调可以被视为一个有监督的训 练过程，其优化过程与预训练有一些不同，比如训练目标 函数（如序列到序列的损失）和优化参数设置（如更小的批量 大小和学习率）。平衡数据分布：由于指令微调涉及多种任务的混合，因此在微 调过程中平衡不同任务的比例非常重要。一种广泛使用的方 法是实例比例混合策略，即将所有数据集合并，然后从混 合数据集中按比例采样每种实例。此外，根据最近的研究发现，提高高质量数据集（例如 FLAN和 P3）的采样比例通常可以带来性能提升。同时，在指令微调期间通 常会设置一个最大容量，以限制数据集中可以包含的最大实 例数，这是为了防止较大的数据集挤占整个采样集合。在实践中，根据不同的数据集，最大容量通常设置为几 千或几万个实例。结合指令微调和预训练：为了使微调过程更加有效和稳定， OPT-IML在指令微调期间加入了预训练数据，这可以看 作是对模型的正则化（regularization）。此外，一些研究并没 有使用单独的两阶段训练过程（预训练和指令微调），而是尝 试混合使用预训练数据（即纯文本）和指令微调数据（即指令 格式数据），用多任务学习的方式从头训练模型。具 体而言，GLM-130B和 Galactica将指令格式数据集 作为预训练语料库的一小部分来预训练 LLM，这有可能同时 获得预训练和指令微调的优势。5.2 对齐微调 背景：LLM 在多个自然语言处理任务上展示出了惊人的能力，但是, 这些模型有时可能表现出预期之外 的行为，例如编造虚假信息、追求不准确的目标，以及产生 有害的、误导性的和有偏见的表达。对于 LLM 而言, 模型参数的预训练使用了语言建模的目标，即用单词预测进 行预训练，但这没有考虑到人类的价值观或偏好。为了避免这 些预期外的行为，一些研究提出了人类对齐，使得 LLM 的行为能够符合人类期望。但是, 与原先的预训练和适配微调（例如指令微调）相比, 对齐微调需要考虑的标准（例如有用性, 诚实性和无害性）十分不同。已有研究表明对齐微调可能会在某种程度上损害 LLM 的通用能力，这在相关研究 中被称为对齐税。5.2.1 人类反馈的收集标注人员的选择：InstructGPT通过评估标注人员 与研究人员之间意图的一致性来选择标注人员。具体而言，研 究人员首先标注少量的数据，然后衡量他们自己和标注人员 之间的标注一致性。选择一致性最高的标记者继续后续的标 注工作。人类反馈的收集：1）基于排序的方法：引 入了 Elo 评分系统 ，通过一一比较所有候选输出结果来生成 一个偏好排序。候选输出的排序将用于调整模型更倾向的输 出，从而产生更可靠和更安全的结果。 2）基于问题的方法：通过回答研究人员设计的特定 问题，标注人员可以提供更详细的反馈，这些问题能够覆 盖不同的对齐标准以及其他对 LLM 的约束条件。特别地，在 WebGPT中，为了帮助模型从检索到的文档中过滤和利 用相关信息，标注人员需要回答关于检索到的文档对于回答 给定输入是否有帮助的选择题。 3）基于规则的方法：Sparrow不仅选择了标注人员挑选的最佳回复，还设计 了一系列规则来测试模型生成的回复是否符合有用、正确和 无害的对齐标准。GPT-4 利用一组（基于 GPT-4 本身的）零样本分类器作为基 于规则的奖励模型，可以自动地确定模型生成的输出是否违 反了一组人类编写的规则。5.2.2 基于人类反馈的强化学习 为了使 LLM 与人类价值观保持一致，人们提出了 RLHF，使用收集到的人类反馈数据对 LLM 进行微调，有助于 改进对齐的指标。RLHF 采用强化学习（RL）算法（例如，近端策略优化（Proximal Policy Optimization, PPO））通过学习奖励模型使 LLM 适配人类反馈。基于人类反馈的强化学习系统： RLHF 系统主要包括三个关 键组件：要对齐的 PLM、从人类反馈中学习的奖励模型，以 及训练 LM 的 RL 算法。具体来说，PLM 通常是一个生成模 型，它使用现有的 PLM 参数进行初始化。例如，OpenAI 在其 第一个主流的 RLHF 模型 InstructGPT [61] 中使用 1750 亿 参数量的 GPT-3。此外，奖励模型（RM） 提供（学习得到的）指导信号，这些信号反映了人类对 LM 生成的文本的偏好，通常以标量值的形式表示。奖励模型通 常具有两种形式：经过微调的 LM 或使用人类偏好数据重新 训练的 LM。基于人类反馈的强化学习的关键步骤：下图说明了 RLHF 的 整个三步过程，具体如下所述。 1） 监督微调：为了使 LM 具有初步执行所需行为的能力， 通常需要收集一个包含输入提示（指令）和所需输出的监督数 据集，以对 LM 进行微调。这些提示和输出可以在确保任务 多样性的情况下由人工标注人员针对某些特定任务编写。例 如，InstructGPT 要求人工标注者编写提示（例如，“列 出五个关于我如何重拾对职业热情的想法”）和一些生成式任 务（如开放域问答、头脑风暴、聊天和重写）的期望输出。2）训练奖励模型：第二步是使用人类反馈的数据训练 RM。 具体来说，我们向 LM 中输入采样的提示（来自监督数据集 或人类生成的提示），以生成一定数量的输出文本，然后邀请 人工标注员为这些输入-输出对标注偏好。标注过程可以以多 种形式进行，常见的做法是对生成的候选文本进行排序标注， 这样可以减少因标注者不同带来的差异。最后，训练 RM 预 测人类偏好的输出。在 InstructGPT 中，标注员将模型生成 的输出从最好到最差进行排名，然后训练 RM（即 60 亿参数 量的 GPT-3）来预测排名。3）强化学习微调：在这一步骤中，LM 的对齐微调可以被 形式化为 RL 问题。在这种情况中，RL 问题的策略（policy） 由 PLM 给出（将提示作为输入并返回输出文本），行动空间 （action space）是 LM 的词表，状态（state）是目前生成的 token 序列，奖励（reward）则由 RM 提供。为了避免 LM 显 著偏离初始（微调前）的模型，通常在奖励函数中纳入一项 惩罚项。例如，InstructGPT 在使用 PPO 算法对抗 RM 来 优化 LM 时，对于每个输入提示，InstructGPT 计算当前 LM 和初始 LM 生成的结果之间的 KL 散度作为惩罚项。值得注 意的是，可以通过多次迭代第二步和最后一步来更好地对齐LLM。6 使用 经过预训练或适配微调之后，使用 LLM 的主要方法是为解决 各种任务设计适当的提示策略。一种典型的提示方法是将任 务描述和（或）示范以自然语言文本的形式表达的上下文学习（in-context learning, ICL）。此外，采用思维链提示（chain-of-thought prompting）可以 通过将一系列中间推理步骤加入提示中来增强 ICL。6.1 上下文学习6.1.1 形式 如下图是 ICL 的示意 图。首先，以任务描述作为开始，从任务数据集中选择一些样 例作为示范。然后，以特别设计的模板形式将它们按照特定 的顺序组合成自然语言提示。最后，将测试样例添加到 LLM 的输入中以生成输出。基于任务示范，LLM 可以在没有显式 梯度更新的情况下识别和执行新任务。6.1.2 示范设计 多项研究表明ICL 的有效性在很大程度上受到示范设计的影响，我们将主要从三个方 面介绍 ICL 的示范设计，即示范选择、格式和顺序。1、 示范选择：不同的示范对于 ICL 的性能影响非常大；因此，选择一个能够有效发挥 LLM 的 ICL 能力的样 例子集很重要。1）启发式方法：由于其简单性和低成本，现有工作广泛 采用其来选择示范。一些研究采用基于 k-NN 的检索器来选择与查询语义相关的样例。然而，他们只 是针对每一个样例进行单独选择，而不是对整个样例集合进 行评估。为了解决这个问题，基于多样性的选择策略被提出，来选择对于特定任务最具代表性的样例集合。2）基于大语言模型的方法：另一部分工作利用 LLM 来选择示例。LLM可以直接跟据添加样例后的性能 提升评估每个样例的信息量，从而进行选择。此外， EPR提出了一种两阶段检索方法，首先使用无监督方 法（例如 BM25）召回相似的样例，然后使用密集检索器（使 用 LLM 标记的正负样例训练）对它们进行排名。作为一种替 代方法，可以将示范选择任务建模为一个 RL 问题，其中 LLM 作为奖励函数，为训练策略模型提供反馈。因为 LLM 在 文本标注方面表现良好，最近的一些研究用 LLM 本身 作为没有人工干预的示范生成器。2、 示范格式：在选择任务示范样例后，下一步是将它们整合及格 式化为对 LLM 的自然语言提示。一种直接的方法是用相应的 输入输出对来实例化预定义的模板。为了构建更具信息量的模板，一些研究考虑添加任务描述，或者通过 CoT 提示来增强 LLM 的推理能力。由于人工标注不同任务的示范格式成本较高，一些工作研 究了如何自动生成高质量的示范格式。作为两种典型的方法， Auto-CoT利用 LLM 使用零样本提示 “Let’s think step by step” 来生成中间推理步骤，而 least-to-most 提示首 先询问 LLM 来执行问题分解，然后利用 LLM 根据先前解决 的中间答案依次解决子问题。3、 示范顺序：LLM 有时会受到顺序偏差的影响，例如会倾向于 重复示范结尾附近的答案。因此，以合理的顺序排列示 范（例如任务样例）非常重要。。例如，可以直接根据 在嵌入空间中示范与查询的相似度来排列 [249]：相似度越高， 距离结尾越近。此外，全局和局部熵度量可以用来给不同的 示范顺序打分。6.2 思维链提示 思维链（Chain-of-Thought，CoT）是一种改进的提示策略，旨在提高 LLM 在复杂推理任务中的性能，例如算术推理，常识推理和符号推理。不同于 ICL 中仅使用输入输出对来构造提示，CoT 将可以导出最终输出 的中间推理步骤纳入提示中。6.2.1 使用思维链的上下文学习 通常情况下，CoT 可以在小样本（few-shot）和零样本（zero[1]shot）设置这两种主要设置下与 ICL 一起使用。 1） 小样本思维链：小样本 CoT 是 ICL 的一个特例，它通过加 入 CoT 推理步骤将每个示范 ⟨ 输入，输出 ⟩ 扩充为 ⟨ 输入， CoT，输出 ⟩。为了应用此策略，接下来我们讨论两个关键问 题，即如何设计合适的 CoT 提示以及如何利用生成的 CoT 推导出最终答案。 • 思维链提示设计：设计合适的 CoT 提示对于有效引出 LLM 的复杂推理能力至关重要。一种直接的方法是使用多样 的 CoT 推理路径（即对于每个问题的多个推理路径），这可以有效增强性能。另一个基于直觉的想法是，具有复杂推理路径的提示更有可能引出 LLM 的推理能力，这可以提 高生成正确答案的准确性。Auto-CoT提出了利用 Zero-shot-CoT（见下部分“零样本CoT”）通过特别提示 LLM 来生成 CoT 推理路径，消除人工操作。为了提高性能，Auto-CoT 进一步将训练 集中的问题分成不同的簇，并选择最接近每个簇质心的问题， 它们应该可以很好地代表整个训练集中的问题。• 增强的思维链策略：除了丰富上下文信息外，CoT 提示 还提供了更多推断答案的选项。现有研究主要关注如何生成 多个推理路径，并尝试在得到的答案中寻找一致性。 例如，self-consistency提出了一种在生成 CoT 和最终答案时新的解码策略。其首先用 LLM 生成多个推理路径，然后 对所有答案进行集成 （例如通过在这些路径中进行投票来选 择最一致的答案）。Self-consistency 极大地提高了 CoT 推理 的性能。2） 零样本思维链：与小样本 CoT 不同，零样本 CoT 没有在提示 中加入人工标注的任务示范。相反，它直接生成推理步骤，然 后利用生成的 CoT 来得出答案。零样本 CoT 最初是在中被提出的；其中，首先通过用“Let’s think step by step”提 示 LLM 来生成推理步骤，然后通过用“Therefore, the answer is”提示来得出最终答案。他们发现，这种策略在模型规模 超过一定大小时可以显著提高性能，但在小规模的模型中效 果不佳，这是涌现能力的重要表现。7 能力评测 为了检验 LLM 的有效性和优越性，已有研究采用了大量的任 务和基准数据集来进行实证评估和分析。专业能力：由于 LLM 已经在大规模语料库上进行了预训练， 它们可以从预训练数据中获取丰富的知识。因此，LLM 可以 被用作特定领域的专家。最近的研究广泛探索了将 LLM 用 于解决特定领域任务的应用，并评估了 LLM 的适配能力。通常，这些研究收集或构建特定领域的数据集，使用 ICL 来评 估 LLM性能。 • 医疗是一个与人类生命密切相关的重要应用领域。自 ChatGPT 问世以来，一系列研究已经将 ChatGPT 或其他 LLM 应用于医疗领域。LLM 能够处理各种医疗保健任务，例 如生物信息提取、医疗咨询和报告简化， 甚至可以通过为专业医生设计的医疗执照考试。 • 教育也是一个重要的应用领域。已有研究发现，LLM 可以在数学、物理、计算机科学等科目的标准化测试中达到学 生级别的表现，这些测试包括选择题和开放式 问题。此外，实验表明 LLM 可以作为写作或阅读助手。最近的一项研究表明，ChatGPT 可以生成在不 同学科之间逻辑一致并且平衡深度和广度的答案。 • 法律是一个建立在专业知识之上的专业领域。最近的一 些研究已经应用 LLM 来解决各种法律任务，例如法律文件分 析、法律判决预测和法律文件撰写。最 近的一项研究发现，LLM 具有强大的法律解释和推理 能力。此外，最新的 GPT-4 模型在模拟律师考试中取得了相 当于人类考生前 10% 的成绩。8 总结与未来方向理论与原理：对于 LLM 的潜在的工作机制，最大的谜题之一 是其如何通过非常大且深的神经网络分配、组织和利用信息。 揭示建立 LLM 能力基础的基本原则或要素非常重要。已有工作显示，当语言模型的参数增加到某个临界规模 （例如 100 亿）时，会以一种意想不到的方式（突然性能飞跃） 涌现出一些能力，通常包括 ICL、指令遵循和逐步推 理。这些涌现能力既令人着迷又令人困惑：LLM 何时和如何 获得它们尚不清楚。由于涌现能力与自然界的相 变具有十分相似的类比关系，跨学科理论或原则（例 如，LLM 是否可以被视为某种复杂系统）可能对解释和理解 LLM 的行为有用。这些基本问题值得研究界探讨，且对于开 发下一代 LLM 至关重要。 模型架构：堆叠的多头自注意力层组成的 Transformer，由于 其可扩展性和有效性，已成为构建 LLM 的基本架构。已有 方法已经提出了各种策略来提高该架构的性能，如神经网络 配置和可扩展的并行训练。为了 提高模型容量（例如多轮对话能力），现有的 LLM 通常维持 一个较长的上下文窗口，例如 GPT-4-32k 的上下文长度达到 了 32,768 个词。因此，减少标准自注意力机制所带来的时间 复杂度（原始为二次代价）是一个实际应用时重要的考虑因素。研究如何构建 LLM 中更高效的 Transformer 变体十分重 要，例如 GPT-3 中已经使用了稀疏注意力。此外， 灾难性遗忘一直是神经网络的长期挑战，其对 LLM 也有负 面影响。在使用新数据微调 LLM 时，原先学到的知识可能会 受到损害，例如根据某些特定任务对 LLM 进行微调将影响 LLM 的通用能力。模型训练：在实践中，由于巨大的计算消耗和对数据质量和 训练技巧的敏感性，预训练功能强大的 LLM 非常困 难。因此，开发更系统、经济的预训练方法以优化 LLM 变得 尤为重要，同时考虑到模型有效性、效率优化和训练稳定性 等因素。我们应该开发更多的模型检查或性能诊断方法，以便在训练过程中及早发现 异常问题。由于从头开始预 训练 LLM 的成本非常高，因此设计适合的机制在公开可用的 模型检查点基础上不断预训练或微调 LLM 是非常重要的（例 如 LLaMA和 Flan-T5）。为此，需要解决许多技术问 题，例如灾难性遗忘和任务专门化。然而，迄今为止，仍缺乏 具有完整预处理和训练日志的 LLM 开源模型检查点（例如准 备预训练数据的脚本）以进行复模型应用：由于在实际应用中微调的成本非常高，提示已成 为使用 LLM 的主要方法。通过将任务描述和示例合并到提示 中，ICL（一种特殊形式的提示）赋予了 LLM 在新任务上表 现良好的能力，甚至在某些情况下胜过全数据微调模型。此 外，为了提高复杂推理能力，已有工作提出了先进的提示技 术，例如 CoT 策略，它将中间推理步骤包含在提示中。然而， 现有的提示方法仍然存在以下几个不足之处。首先，提示设计 时需要大量人力。自动生成有效提示以解决各种任务将非常 有用。其次，一些复杂任务（例如形式证明和数值计算）需要 特定的知识或逻辑规则，这些规则可能无法用自然语言很好 地表达或通过示例演示。因此，开发更具信息量和灵活性的任 务格式化方法以进行提示非常重要。第三，现有的提示策略 主要关注单轮性能。开发交互式提示机制（例如通过自然语 言对话）来解决复杂任务是有用的，其效果已经被 ChatGPT 证明。安全与对齐：尽管具有强大的能力，LLM 与小型语言模型在 安全方面面临类似的挑战。例如 LLM 倾向于产生幻觉，这些文本看似合理，但可能在事实上是错误的。更糟糕的是，LLM 可能被有意的指令激发以产生有害的、有偏见的或有毒 的文本以用于恶意系统，从而导致潜在的滥用风险。 为了详细讨论 LLM 的安全问题（例如隐私、过度依赖、虚假 信息和影响操作），读者可以参考 GPT-3/4 技术报告。 作为避免这些问题的主要方法，可通过将人类纳入训练循环 来开发良好对齐的 LLM，并使用 RLHF。为了提高模 型安全性，在 RLHF 过程中包含安全相关的提示也非常重要， 正如 GPT-4 所示。然而，RLHF 严重依赖专业标注者的 高质量人类反馈数据，这使得它在实践中难以适当实施。因 此，有必要改进 RLHF 框架以减少人类标注者的工作量，并 寻求更高效的、具有保证数据质量的标注方法，例如 LLM 可 以用于辅助标注工作。最近，红队方法已经被采用 来提高 LLM 的模型安全性，该方法利用收集到的对抗性提示 来优化 LLM（即避免红队攻击）。此外，通过聊天获取人类反 馈并直接将其用于自我改进的适当学习机制也具有重要意义。应用与生态：随着 LLM 在解决各种任务方面表现出强大的能 力，它们可以应用于广泛的现实世界应用（即遵循特定任务的 自然语言指令）。作为一个显著的进步，ChatGPT 可能已经 改变了人类获取信息的方式，这已在 “New Bing” 的发布中得 到实现。在不远的将来，可以预见到 LLM 将对信息检索技术 产生重大影响，包括搜索引擎和推荐系统。此外，智能信息助 手的开发和使用将随着 LLM 的技术升级得到高度推广。从更 广泛的范围来看，这波技术创新浪潮将产生一个以 LLM 为支 持的应用生态系统（例如 ChatGPT 对插件的支持），这与人 类生活息息相关。最后，LLM 的兴起为人工通用智能（AGI） 的探索提供了启示。有望开发出比以往更智能的系统（可能具有多模态信号）。（专栏介绍：大模型其基本原理跟原有模型其实是一脉相承的，比如transformer、bert、T5、prompt-learning、GPT系列等，这些模型的基础原理及改进创新等在本博客其他栏目也有详尽介绍可参考，本专栏主要来关注一下大模型的发展脉络、包括其继承以及创新之处。其实之前的专栏文章也提到过，在transformer出来之后，nlp与cv其实就已经紧密联系在一起、不分家了，现在大模型的引入、更加证明了这一点。本专栏、我们一起来学习、实践一下，大模型是如何继承而来的、创新点又在哪、以及我们如何拥有自己的大模型。头几篇文章，我们以相关经典论文分享、学习为基础，学习其原理、梳理其流程，打下基础，后面我们再进一步实践操作、拥有属于自己的大模型。其实都不难哈，静下心来、不要着急，你想要的、岁月都会给你！一般分享论文分为几种方式，一个是在熟练掌握论文的基础上总结概括或者梳理式的分享，一个是基于原文及结构原封不动式的分享，我认为这两种方式可有优缺点，第一种方式读者读起来更简洁快速、但是经过写作人的加工或者个性偏好或者个人理解处理后、总有些偏颇在里面，第二种方式有些读者读起来可能认为原封不动式的分析毫无价值、只是存翻译一遍、其实对于我个人理解来说不是这样的、这样可以做到毫无保留尽量详尽、读者也可以各取所需，当然前面加上框架梳理和总结概括就最好了，能够满足大部分读者的各取所需。本文在尽量原封不动式还原论文的基础上，也会去掉些个人认为的冗余部分(比如为了发表而不得不的论文式框架)，也会在个人来理解的基础上做一些梳理和概括。）"
651359908,8c89d6f733cb2b81ce36a2daf0a81a82,方佳瑞,https://api.zhihu.com/people/8c89d6f733cb2b81ce36a2daf0a81a82,people,MLSys老炮,大模型推理妙招—投机采样（Speculative Decoding）,article,https://api.zhihu.com/articles/651359908,保证和使用原始<em>模型</em>的采样分布完全相同。它使用两个<em>模型</em>：一个是原始目标模型，另一个是比原始<em>模型</em>小得多的近似模型。近似模型用于进行自回归串行采样，而大型<em>模型</em>则用于评估采样结果,524,79,772,1692603517,1692604641,"投机采样（Speculative Decoding）是Google[1]和DeepMind[2]在2022年同时发现的大模型推理加速方法。它可以在不损失生成效果前提下，获得3x以上的加速比。GPT-4泄密报告也提到OpenAI线上模型推理使用了它。对如此妙到毫巅的方法，介绍它中文资料却很少，并且也缺少开源实现帮助理解。本文根据简要介绍了一下此方法，同时根据论文实现了一个简单的demo供大家参考。https://github.com/feifeibear/LLMSpeculativeSampling动机大型语言模型（LLM）的推理通常需要使用自回归采样。它们的推理过程相当缓慢，需要逐个token地进行串行解码。因此，大型模型的推理过程往往受制于访存速度，生成每个标记都需要将所有参数从存储单元传输到计算单元，因此内存访问带宽成为严重的瓶颈。为了解决推理速度慢的问题，已经进行了许多针对推理的工程优化，例如改进的计算核心实现、多卡并行计算、批处理策略等等。然而，这些方法并没有从根本上解决LLM解码过程是受制于访存带宽的问题。有了投机采样（Speculative Decoding）问题的性质变了。投机采样是一种可以从根本上解码计算访存比的方法，保证和使用原始模型的采样分布完全相同。它使用两个模型：一个是原始目标模型，另一个是比原始模型小得多的近似模型。近似模型用于进行自回归串行采样，而大型模型则用于评估采样结果。解码过程中，某些token的解码相对容易，某些token的解码则很困难。因此，简单的token生成可以交给小型模型处理，而困难的token则交给大型模型处理。这里的小型模型可以采用与原始模型相同的结构，但参数更少，或者干脆使用n-gram模型。小型模型不仅计算量较小，更重要的是减少了内存访问的需求。看到这个idea，我的感觉是妙蛙种子吃着妙脆角来到米奇妙妙屋，妙到家了！用一个例子展示随机采样的工作方式。下图中，每一行代表一次迭代。绿色的标记是由近似模型提出的token建议，而目标模型判断是否接受了这些token生成的建议。红色和蓝色的标记分别表示被拒绝和其修正。例如，在第一行中，近似模型生成了5个token，目标模型使用这5个token和前缀拼接后的句子”[START] japan’s bechmark bond”作为输入，通过一次推理执行来验证小模型的生成效果。这里，最后一个token ”bond“被目标模型拒绝，重新采样生成”n“。这样中间的四个tokens，”japan” “’s” “benchmark”都是小模型生成的。以此类推，由于用大模型对输入序列并行地执行，大模型只forward了9次，就生成了37个tokens。尽管总的大模型的计算量不变，但是大模型推理一个1个token和5个token延迟类似，这还是比大模型一个一个蹦词的速度要快很多。我们在看性能分析结果，下图是一个encoder-decoder结构网络的时间分解图。顶部一行显示了 =7的投机采样，中间一行显示了= 3的投机解码，是小模型一次生成token数目。Mp大模型，Mq的小模型。可见，使用投机采样，解码时间大幅缩减。自回归采样先科普一下LLM解码时采用的自回归采样，其过程如下：小模型使用前缀作为输入，将输出结果处理+归一化成概率分布后，采样生成下一个token。将生成的token和前缀拼接成新的前缀，重复执行1，直到生成EOS或者达到最大token数目。第1步中，将模型输出logits的转换成概率，有几种常用的采样方法，包括argmax、top-k和top-n等。自回归采样中生成的token一个一个地蹦出来，因为每次只对序列长度为1的部分进行有效计算，但是却需要对全部前缀对应位置的activations进行访问，因此计算访存比很低。投机采样投机采样过程如下：用小模型Mq做自回归采样连续生成 个tokens。把生成的个tokens和前缀拼接一起送进大模Mp执行一次forwards。使用大、小模型logits结果做比对，如果发现某个token小模型生成的不好，重新采样这个token。重复步骤1。如果小模型生成结果都满意，则用大模型采样下一个token。重复步骤1。第2步，将个tokens和前缀拼成一起作为大模型输入，和自回归相比，尽管计算量一样，但是个tokens可以同时参与计算，计算访存比显著提升。第3步，如何评价一个token生成的不好？如果q(x) > p(x)（p，q表示在大小模型采样概率，也就是logits归一化后的概率分布）则以一定1-p(x)/q(x)为概率拒绝这个token的生成，从一个新的概率分布p’(x) = norm(max(0, p(x) − q(x)))中重新采样一个token。为什么投机采样和自回归采样等价论文[1]附录A1.1中严格证明了对于任意分布p(x)和q(x)，通过从p(x)和q(x)进行投机采样所得到的标记的分布与仅从p(x)进行采样所得到的标记的分布是相同的。这里给一个直观的解释，p(x’) > q(x’)说明大模型在token x’上概率大于小模型，则大模型对生成token x’更有把握，说明小模型生成的问题不大可以保留x’。如果p(x’) ≤ q(x’)则小模型更有把握，大模型就以1-p(x)/q(x)为概率概率拒绝，并重新采样。因为接收的概率更偏向q(x)大的位置，重新采样的概率应该更偏向p(x)大的位置，所以是norm(max(0, p(x)-q(x))。加速效果投机采样相比自回归采样之所以有加速效果，因为它减少了对原始模型串行调用的次数。这里规定一次迭代可以接收小模型的tokens数为#generated tokens。加速效果和 ，p，q都相关。直觉上讲，越大，p、q分布越接近，则# generated tokens越大。 这里 是反应大模型模型p，q性质的量。比如使用argmax作为采样，LAMDA (137B)和LAMDA (100M)的 是0.61。而LAMDA (137B) LAMDA (8B)是0.74。论文[1]也报告了T5不同配置下有2-3x加速效果。总结用一句打油诗总结投机推理：大小模型同台戏，并行计算加威力。模型结构无需变，加速三倍不偏移。我认为投机采样未来绝对会成为大模型推理上线必备之良药，希望本文对大家理解这项技术有帮助。参考文献[1] Fast Inference from Transformers via Speculative Decoding[2] Accelerating large language model decoding with speculative sampling"
609865550,04b5afce3ce452262c1666377caedcb4,小镇做题家,https://api.zhihu.com/people/04b5afce3ce452262c1666377caedcb4,people,对话系统/数据挖掘/NLP/柴犬,大模型的训练工具（1）---Deepspeed,article,https://api.zhihu.com/articles/609865550,然后更新模型参数并将其作为更新发送到每个<em>模型</em>实例。 模型并行 当单个 GPU无法容纳模型尺寸时，模型并行性变得必要，有必要将模型拆分到多个 GPU 上进行训练。 通过把一个<em>大模型</em>拆分到多个 GPU,165,0,313,1677504077,1677504077,为什么需要Deepspeed大模型（LLM）在训练时往往需要大量内存来存储中间激活、权重等参数，百亿模型甚至无法在单个 GPU上进行训练，使得模型训练在某些情况下非常低效和不可能。这就需要进行多卡，或者多节点分布式训练。在大规模深度学习模型训练中有个主要范式：数据并行模型并行数据并行应用数据并行最常见的场景是模型尺寸能够被单个GPU 内存容纳，数据批大小会增加模型训练的难度。解决方案是让模型的不同实例在不同的 GPU 和不同批数据上运行，如下图所示。模型的每个实例都使用相同的参数进行初始化，但在前向传递期间，不同批次的数据被发送到每个模型。 收集来自每个模型实例的梯度并计算梯度更新。，然后更新模型参数并将其作为更新发送到每个模型实例。模型并行当单个 GPU无法容纳模型尺寸时，模型并行性变得必要，有必要将模型拆分到多个 GPU 上进行训练。通过把一个大模型拆分到多个 GPU 上训练，可以实现模型尺寸超过单个GPU显存的深度学习模型训练。 这种方法的问题是计算使用效率不高，因为在任何时间点只有一个 GPU 正在使用，而其他 GPU 处于空闲状态。参考：9个主流的分布式深度学习框架目前训练超大规模语言模型主要有两条技术路线：TPU + XLA + TensorFlow/JAX 和 GPU + PyTorch + Megatron-LM + DeepSpeed。前者由Google主导，由于TPU和自家云平台GCP深度绑定，对于非Googler来说， 只可远观而不可把玩，后者背后则有NVIDIA、Meta、MS大厂加持，社区氛围活跃，也更受到群众欢迎。Deepspeed简介Deepspeed是微软的大规模分布式训练工具。专门用于训练超大模型。增加的功能主要有：3个维度并行化实现万亿参数模型训练ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型通过 DeepSpeed Sparse Attention 用6倍速度执行10倍长的序列1 比特 Adam 减少 5 倍通信量3D 并行：扩展至万亿参数模型3D 并行同时解决了训练万亿参数模型的两个基本挑战：显存效率和计算效率。因此，DeepSpeed 可以扩展至在显存中放下最巨大的模型，而不会牺牲速度。显存效率：集群上所能训练的LLM的参数量。计算效率：单纯计算占系统的开销的比例。数据并行是分布式训练普遍使用的技术。在该技术中，每批输入的训练数据都在数据并行的 worker 之间平分。反向传播后需要通信并规约梯度，以保证优化器在各个 worker 上进行相同的更新。数据并行性具有几个明显的优势，包括计算效率高和实现起来工作量小。但是，数据并行的 batch 大小随 worker 数量提高，而我们往往无法在不影响收敛性的情况下一直增加 batch 大小。显存效率：数据并行会在所有 worker 之间进行模型和优化器的复制，因此显存效率不高。DeepSpeed 开发了 ZeRO ，它是一系列用于提高数据并行的显存效率的优化器。 这项工作依赖于 ZeRO 的 1 阶段，该阶段在 worker 之间划分优化器状态量以减少冗余。计算效率：随着我们提高并行度，每个 worker 执行的计算量是恒定的。数据并行可以在小规模上实现近乎线性扩展。但是，在 worker 之间规约梯度的通信开销跟模型大小成正相关，所以当模型很大或通信带宽很低时，计算效率会受限。。梯度累积是一种用来均摊通信成本的一种常用策略。它会进一步增加batch大小，在本地使用 micro-batch 多次进行正向和反向传播积累梯度后，再进行梯度规约和优化器更新。模型并行是包含范围很广的一类技术。它会在多个 worker 之间划分模型的各个层。就其本质而言，模型并行性的计算和通信因模型结构而异，因此在实现上有很大的工作量。DeepSpeed 借用了英伟达的 Megatron-LM 来为基于 Transformer 的语言模型提供大规模模型并行功能。模型并行会根据 worker 数量成比例地减少显存使用量，也是这三种并行度中显存效率最高的。但是其代价是计算效率最低。显存效率：模型并行会根据 worker 数量成比例地减少显存使用量。至关重要的是，这是减少单个网络层的激活显存的唯一方法。DeepSpeed 通过在模型并行 worker 之间划分激活显存来进一步提高显存效率。计算效率：由于每次前向和反向传播中都需要额外通信激活值，模型并行的计算效率很低。模型并行需要高通信带宽，并且不能很好地扩展到通信带宽受限的节点。此外，每个模型并行worker 都会减少每个通信阶段之间执行的计算量，从而影响计算效率。模型并行性通常与数据并行性结合使用，以在内存和计算效率之间进行权衡。流水线并行训练引擎也被包含在了这次发布的DeepSpeed中！流水线并行将模型的各层划分为可以并行处理的阶段。当一个阶段完成一个 micro-batch 的正向传递时，激活内存将被通信至流水线的下一个阶段。类似地，当下一阶段完成反向传播时，将通过管道反向通信梯度。必须同时计算多个 micro-batch 以确保流水线的各个阶段能并行计算。目前已经开发出了几种用于权衡内存和计算效率以及收敛行为的方法，例如 PipeDream。DeepSpeed 采用的方法是通过梯度累积来实现并行，并保持与传统数据并行和模型并行训练在相同的总 batch 大小下收敛情况相同。显存效率：流水线并行减少的显存与流水线的阶段数成正比，使模型的大小可以随 worker 的数量线性扩展。但是，流水线并行不会减少每一层的激活函数的显存占用量。此外，每个 worker 必须存储同时运行的各个 micro-batch 的激活值。这导致流水线第一阶段的激活内存与单个 mirco batch 的总激活内存大致相同。一个万亿参数模型将需要为一个 micro batch 提供大约 19 GB 的显存的激活内存，这几乎占到新推出的英伟达 A100 GPU 总显存的一半。计算效率：流水线并行具有最低的通信量，因为它的通信量只和在各阶段边界的各层的激活值大小成正比。但是，它不能无限扩展。像模型并行一样，增加流水线大小会减少每个流水线阶段的计算量，这会降低计算与通信的比率。如果要实现好的计算效率，流水线并行还要求其每个阶段的计算负载完美的均衡。该图显示了一个有 32 个 worker 进行 3D 并行的例子。神经网络的各层分为四个流水线阶段。每个流水线阶段中的层在四个模型并行 worker 之间进一步划分。最后，每个流水线阶段有两个数据并行实例，且 ZeRO 在这 2 个副本之间划分优化器状态量。未完待续。。。参考：[译] DeepSpeed：所有人都能用的超大规模模型训练工具
624589622,c23dba2ca2176872a0b691fe31aff085,战士金,https://api.zhihu.com/people/c23dba2ca2176872a0b691fe31aff085,people,研究预训练LLM中...,详解大模型RLHF过程（配代码解读）,article,https://api.zhihu.com/articles/624589622,需要一个奖励模型来评估语言<em>大模型</em>（actor model）回答的是好是坏，这个奖励模型通常比被评估的语言<em>大模型</em>小一些（deepspeed的示例中，语言<em>大模型</em>66B，奖励模型只有350M）,1177,83,2743,1682563602,1695636892,"一直都特别好奇大模型的强化学习微调是怎么做的，网上虽然相关文章不少，但找到的文章都是浅尝辄止说到用PPO训练，再细致深入的就没有讲了。。。只能自己看一看代码，以前搞过一点用PPO做游戏，感觉和语言模型PPO的用法不太一样。在游戏场景，每个step给环境一个action之后，agent拿到的state都是会变化的，通常也会设计奖励函数使得每个step都会有reward；但是在用强化学习微调语言模型这里，prompt是state，只输入一次，然后输出一串action（回答的单词），得到一个reward，模型并没有在每个action之后得到新的state（感谢评论区大佬的点拨，对于answer的第二个词，可以把prompt+answer的一个词当作新的state，而不只是把prompt当作state，状态转移蕴含在transformer内部）本篇文章并不会介绍太多PPO的原理，相关文章已经很多了，比如李宏毅介绍PPO的课程。大模型里边的PPO涉及到了critic model的概念，在李宏毅教程里只提了一下并没有细讲，如果想了解可以看一下这个文章，相当于利用一个critic model预测从t时刻到最后一个时刻的累加奖励值（强化学习里边的第t个时刻对标answer句子里边的第t个单词），而不是通过实际累加得到从t时刻到最后一个时刻的累加奖励值，这样可以降低奖励的方差。下文也结合代码介绍critic model输出的具体含义。同时RLHF是什么也会再详细介绍，相关文章已经很多了。本篇文章涉及的代码均来自微软的deepspeed对RLHF的实现，可配合huggingface官方的博客一起食用。本文只对算法的一些有特点的关键点进行阐述，并不对整体实现进行介绍。先上一张经典的论文图。本文重点结合代码讲解奖励模型训练和强化学习训练部分。奖励（reward）模型训练首先要声明的是，在强化学习阶段，用到的reward model和critic model都使用同一个模型初始化，因此在训练reward模型的过程中，也是在训练critic model。其次对符号进行说明，大模型中间隐藏层的参数维度为(B,L,D)，B为batch size大小，L为句子长度，D为embedding维度。在接下来的代码讲解中，我也会标明代码中各个变量的维度，以更好的理解其意义。在进行RLHF时，需要一个奖励模型来评估语言大模型（actor model）回答的是好是坏，这个奖励模型通常比被评估的语言大模型小一些（deepspeed的示例中，语言大模型66B，奖励模型只有350M）。奖励模型的输入是prompt+answer的形式，让模型学会对prompt+answer进行打分。奖励模型最后一层隐藏层的输出维度为(B,L,D)，通过一个D✖️1的全连接层将维度变为(B, L)，在L这个维度上，第i个位置的数据表示：从第i个位置到最后一个位置输出所能获得的奖励分值的累加和（和DQN里边的Q值一个意义），这种形式的输出满足了critic model的输出要求。对应代码如下：#huggingface模型返回值是个list，第0位是模型最后输出的hideen state hidden_states = transformer_outputs[0] # v_head为Dx1的全连接网络对最后一维压缩 rewards = self.v_head(hidden_states).squeeze(-1)对于一个奖励模型来说，目标是给一个句子进行打分，按理说每个句子对应一个分值就行了，但是目前对于长度为L的句子，奖励模型输出了L个值。我们用L维度上的最后一个位置的值当作为本句话的奖励得分。奖励模型训练优化采用pair wiss loss，即同时输入模型关于同一个问题的两个回答，让模型学会这两个句子哪个分高哪个分低。之所以如此训练是因为，在给奖励模型进行数据标注的过程中，给同一个问题的不同回答量化的打具体分值比较难，但是对他们进行排序相对简单，代码如下：# 同一个batch里边的句子需要等长，短句后边会被padding # [divergence_ind:end_ind]索引了padding前一个位置的输出分值 # chosen_reward是同一个句子pair里分数高的句子，r_truncated_reward是句子pair里分数低的句子 c_truncated_reward = chosen_reward[divergence_ind:end_ind] r_truncated_reward = rejected_reward[divergence_ind:end_ind]pair wise loss代码如下，如果给pair里边好的句子打分高（c_truncated_reward），坏的句子（r_truncated_reward）打分低，loss就会小：loss += -torch.log(torch.sigmoid(c_truncated_reward - r_truncated_reward)).mean()在训练强化学习的过程中，会用到reward model（critic model，再次提醒，critic model和reward model是同一个模型的两个副本）的推理过程，通过调用forward_value实现，具体代码如下，返回的值中有两种值，values表示每个位置i，从第i个位置到最后一个位置的奖励累加值，供强化学习过程中critic model使用；“chosen_end_scores”指的是对每个prompt+answer的打分，供reward model使用。def forward_value(...): ... if return_value_only: #(B,L) return values else: ... return { ""values"": values, # （B,） ""chosen_end_scores"": torch.stack(chosen_end_scores), }强化学习微调强化学习微调阶段，会用到4个模型，actor model， ref_model，reward model和critic model（好费显存啊！！！）。其中actor model和ref_model是RLHF第一个阶段有监督微调模型的两个副本，reward model和critic model是本文第一部分训练出来的模型的两个副本。整体流程见这篇文档，整体流程图如下所示（没画出critic model）：首先说明actor model的训练模式和推理模式的区别（ 后边会用到）。训练模式是用teacher force的方式（不明白的同学知乎搜一下），将整句话输入到模型中，并通过mask机制在保证不泄漏未来的单词情况下预测下一个单词。推理模式是真正的自回归，预测出下一个单词之后，当作下一步输入再预测下下个单词，原理如下图所示：首先用actor model在推理模式下根据prompt生成一个answer（prompt对应强化学习里边的state，answer对应一些列的action），代码如下：# 保证不触发反向传播 with torch.no_grad(): seq = self.actor_model.module.generate(prompts, max_length=max_min_length, min_length=max_min_length)然后利用reward model和ciric model对输出的prompt+answer进行打分（PPO训练时使用的奖励值并不单单是reward model的输出还要考虑kl散度，后文介绍）：# 奖励模型返回的是个字典，key为chosen_end_scores位置存储数据维度为(B,)，表示对于prompt+answer的打分 reward_score = self.reward_model.forward_value( seq, attention_mask, prompt_length=self.prompt_length)['chosen_end_scores'].detach( ) #critic model返回的数据维度为(B,L)，L维度上第i个位置代表从i位置到最后的累积奖励 #舍去最后一个位置是因为句子“终止符”无意义 values = self.critic_model.forward_value( seq, attention_mask, return_value_only=True).detach()[:, :-1] actor model是我们想通过强化学习微调的大模型，但是强化学习过程很容易把模型训练“坏”，因此需要另外一个不会参数更新的 ref_model来当作标的，别让actor mode跑偏太远。我们在训练模式下，将prompt+answer分别输入到actor mode和ref model，用KL散度来衡量 ref model和actor mode输出的差别。同时将KL散度（衡量数据分布差距大小）纳入损失函数（KL散度本质是纳入到奖励值里边的，奖励值被纳入到了损失函数），进而来约束 ref_model和actor mode的输出分布别差距太大。具体代码如下：# 得到两个模型的输出 output = self.actor_model(seq, attention_mask=attention_mask) output_ref = self.ref_model(seq, attention_mask=attention_mask) logits = output.logits logits_ref = output_ref.logits ... return { ... # 分别得到两个模型在真实单词上的预测概率 'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]), 'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:,1:]), ... } ... # 计算kl散度，log_probs里边存的数字经过log变化了，因此减法就对应除法 kl_divergence_estimate = -self.kl_ctl * (log_probs - ref_log_probs)PPO训练时候的奖励值综合考虑KL散度和reward模型的输出，只考虑answer部分的KL散度，将reward model的输出加到KL散度L维度的最后一个位置上，得到最终的奖励值，代码如下：rewards = kl_divergence_estimate # 只考虑answer部分的奖励，不考虑prompt start = prompts.shape[1] - 1 # 不考虑padding部分 ends = start + action_mask[:, start:].sum(1) reward_clip = torch.clamp(reward_score, -self.clip_reward_value, self.clip_reward_value) batch_size = log_probs.shape[0] # 在L维度上，每个位置都有KL散度，但是只在最后一个位置加上奖励值 for j in range(batch_size): rewards[j, start:ends[j]][-1] += reward_clip[j]接下来的内容就是PPO的训练过程的比较核心的内容了，目标是计算PPO更新公示里边的advantage，具体公式如下，V就是critic model的输出。如果原理不懂建议先到这个链接看看。我直接在代码中给注释了。图片出处：https://huggingface.co/blog/deep-rl-a2cdef get_advantages_and_returns(self, values, rewards, start): # values（B，L） critic model输出 # rewards（B，L）reward 包含kl散度 # start answer开始的位置 # Adopted from https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L134 lastgaelam = 0 advantages_reversed = [] length = rewards.size()[-1] # 计算每个时刻（序列位置）的critic model预测误差 for t in reversed(range(start, length)): nextvalues = values[:, t + 1] if t < length - 1 else 0.0 # critic model预测的是t到到最后一个时刻的奖励和，所以变化量delta可以用如下公式表示 delta = (rewards[:, t] + self.gamma * nextvalues) - values[:, t] # self.gamma=1，self.lam=0.95是衰减因子，表示之前计算的delta对现在影响越来越小 lastgaelam = delta + self.gamma * self.lam * lastgaelam advantages_reversed.append(lastgaelam) advantages = torch.stack(advantages_reversed[::-1], dim=1) # 后续用来更新critic model用 returns = advantages + values[:, start:] return advantages.detach(), returns以上过程，我们已经拿到了PPO训练所需要的advantage以及actor model的输出，我先现在可以对actor model进行训练啦。具体代码如下。logprobs和old_logprobs这两个参数分别是“老actor（n个epoch才会更新一次）”和新actor（每个batch都会更新它）”在正确单词上出处的概率，这块时PPO import sampling相关的知识，就不在这重复介绍了，不明白的同学补习一下哈。借用一下李宏毅老师的PPO公式：def actor_loss_fn(self, logprobs, old_logprobs, advantages, mask): ## policy gradient loss #logprobs, old_logprobs都是经过log变化的单词概率，这里带着log做减法就相当于在做概率除法 log_ratio = (logprobs - old_logprobs) * mask # 指数操作去掉log ratio = torch.exp(log_ratio) pg_loss1 = -advantages * ratio pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange, 1.0 + self.cliprange) pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / mask.sum() return pg_loss同样的，我们也要对critic model进行训练，更新，loss就是mse loss。def critic_loss_fn(self, values, old_values, returns, mask): ## value loss # 用“老critic model”的输出约束“新critic model”不要步子太大，裁剪一下 values_clipped = torch.clamp( values, old_values - self.cliprange_value, old_values + self.cliprange_value, ) vf_loss1 = (values - returns)**2 vf_loss2 = (values_clipped - returns)**2 vf_loss = 0.5 * torch.sum( torch.max(vf_loss1, vf_loss2) * mask) / mask.sum() return vf_loss至此，我们的RLHF训练流程就结束了。第二部分开头我们说过，共涉及actor model， ref_model，reward model和critic model这四个模型，其实更新参数的模型只有actor model和critic model。"
691216920,3993cf9a4a219b2e4d900e9756f2507e,TechAI,https://api.zhihu.com/people/3993cf9a4a219b2e4d900e9756f2507e,people,,本地运行大模型，需要什么样的配置？,article,https://api.zhihu.com/articles/691216920,在本地跑<em>大模型</em>的话，推荐使用 Lmstudio。 网址： lmstudio.ai Lmstudio 集成了本地运行<em>大模型</em>的主要功能。 可以搜索、下载大模型，然后运行和对话,10,0,21,1712539426,1712539426,本地运行大模型有多爽？只有用过了才知道。那是一种顺畅、自由的感觉。比如使用那些主流大模型，最常见的就是网络问题，如 IP 受限，或者服务器压力过大导致的延迟等等。使用本地大模型，真的是像和人自然交谈那么顺畅。而且，再也不用心疼 token 的费用了。使用 API 调用大模型时，有没有坐出租车时看计价器的感觉？数字一直在网上蹦。虽然钱并不多，但那种被羁绊的感觉并不好。使用本地模型就可以放飞了，例如我会频繁的使用本地模型进行大篇幅的翻译。那么，本地模型需要什么配置呢？可以按照使用场景来分。1）绘图最低要求 16G 内存，4G 显存。建议 32G 内存，12G 显存。2）生成音频至少 8G 显存。3）生成文字至少 8G 内存，4G 显存。但这个级别的模型的聊天效果不怎么样。我的机器是 32G内存 + 16G显存，跑 gemma-7b 效果不错。如果是 32G 内存 24G 显存，那就相当完美了。总的来说，入门配置怎么也要 3060显卡 + 16G内存，最理想的是 4090 + 32G内存。在本地跑大模型的话，推荐使用 Lmstudio。网址： lmstudio.ai Lmstudio 集成了本地运行大模型的主要功能。可以搜索、下载大模型，然后运行和对话。还有 API server 服务，提供了兼容 OpenAI 规范的接口。在下载模型时，Lmstudio 还会根据你的机器配置给出建议。如果运行不了，会提示你。不错吧，快去试试吧。#AI 人工智能，#本地部署大模型，#gpt890 信息来源 http://gpt890.com/article/42
680260131,c0422b3d048452b84d36dd550acd6efe,paper君,https://api.zhihu.com/people/c0422b3d048452b84d36dd550acd6efe,people,nlp、大模型,llama、baichuan和qwen等开源大模型技术对比,article,https://api.zhihu.com/articles/680260131,chat模型回答效果好坏，主要还是靠base模型效果提供，并且不同开源模型的base模型训练方式也不一样。 开源的<em>大模型</em>和论文里面，选取了热门的大模型，llama2、Mistral、baichuan2和qwen进行比较与分析,109,1,332,1706455504,1713162488,"一、背景目前开源了各式各样的大模型，大家用到的基础架构基本一致，都是基于transformer架构，只是使用encoder 或者 decoder方式的区别。训练方式分成两种：base模型训练和chat模型对齐训练，其中chat模型对齐训练使用的方式也比较一致，SFT的微调或者RLHF(RM模型+PPO)。chat模型回答效果好坏，主要还是靠base模型效果提供，并且不同开源模型的base模型训练方式也不一样。开源的大模型和论文里面，选取了热门的大模型，llama2、Mistral、baichuan2和qwen进行比较与分析二、技术方案对比1.llama2 开源鼻祖论文：https://arxiv.org/abs/2307.09288Pretraining Data由于llama2开源的公司为Facebook，业务主要为社交的软件，社交数据都是ugc编辑的对于大模型提升知识能力没太多作用并且都是隐私的数据不能用于大模型训练。为此llama2仅仅使用公开的数据来源，对具有事实性的来源加权，以增加权威的知识并减少幻想。最终使用2 trillion token 数据上进行了训练。Tokenizer使用bytepair encoding (BPE) 的算法，进行分词。针对连续数字分会拆为单独的数字并且未在词典中的词使用bytes替换，最终词典大小为32k。Architectural位置编码：rotary positional embeddings激活函数：SwiGLUnormal方法：RMSNormcontent长度grouped-query attention(GQA)，将llama1 长度2048 提升至 4096，MHA、GQA、MQA结构对比如上图所示，传统的transformer为多头注意力(MHA)模型，分组查询注意力变体为GQA模型，另外还有MQA模型，该结构为特殊的GQA模型，group 为1。MHA、GQA、MQA结构对比MHA 方式out-of-memory分别在batch size 为 1024 ， 256 tokens和 batch size 为 128，2k context，而MQA 和 GQA 都成功执行完毕。2.Mistral 7B对于Mistral的了解，来源于磁链连接，这篇论文开源了其Mistral 7B基座模型。论文：https://arxiv.org/abs/2310.06825论文中未公开数据集的方案ArchitecturalSliding Window Attention，attention 中的操作数量与序列长度呈二次关系，通过Sliding Window Attention，可减少计算，但是会牺牲一点的效果。做法如下，第2层中的位置4的隐藏状态，关注来自前一层中位置在4- W和4之间的所有隐藏状态，下图中w=3传统Attention 和 Sliding Window Attention对比Rolling Buffer Cache，显存消耗与序列长度呈二次关系。当长度比较长时，显存的消耗是比较多的Rolling Buffer Cache使用的是LRU算法，选择最久未使用的数据予以淘汰，相当于缓存最新数据。效果：从下面可看出同一个数量级的参数 Mistral 都比llama2效果好，而从公开的论文来看，仅仅是提到了大模型content长度加速，对效果比llama2好的原因未提及。3.Qwen论文：https://arxiv.org/abs/2309.16609Pretraining Data去重：标准化后进行完全匹配重复数据删除,以及使用 MinHash 和 LSH 算法进行模糊重复数据删除过滤质量低：过滤低质量的数据,采用了规则型和基于机器学习的方法的组合。多个模型对内容进行评分,包括语言模型,文本质量评分模型以及用于识别潜在的攻击性或不适当内容的模型。人工从各种来源中对文本进行抽样并审阅,以确保其质量。高质量指令：由于多任务指令可以增强他们的零样本和少样本性能，预训练过程中加入了高质量的指令数据。Tokenizer使用基于bytepair encoding (BPE) 的tiktoken算法，其相当于BPE tokenizer分词更快。首先使用cl100k作为base token，针对连续数字分会拆为单独的数字，最终词典大小为152K。编码压缩率越小，则传递的信息就更多，每种语言100万个文档语料库来测试和比较不同模型的编码压缩率，可看到qwen编码压缩率是比较低的ARCHITECTUREEmbedding和output 投影层：解开输入嵌入和输出投影的权重，这一决定是为了以内存成本为代价获得更好的性能位置嵌入：RoPE，选择使用FP32精度的逆频率矩阵,而不是BF16或FP16,以优先考虑模型性能并获得更高的准确性。激活函数：SwiGLUnormal方法：RMSNorm，前馈网络(FFN)的维度从隐藏大小的4倍减少到隐藏大小的83倍content长度：长度外推，苏剑林发现，https://spaces.ac.cn/archives/9577 在QKV注意力层中添加bias以增强模型的外推能力。下图可看到加上了bias，长度大于1024效果下降不是很多。bias外推测试self.c_attn = nn.Linear(config.hidden_size, 3 * self.projection_size) #Linear 默认bias=True mixed_x_layer = self.c_attn(hidden_states) query, key, value = mixed_x_layer.split(self.split_size, dim=2)NTK-aware interpolation，动态 NTK-aware 插值,则每个块比例不同。LogN-Scaling，q和v乘以一个系数，context length和training length的长度关系，来保持注意力的稳定。window attention，将注意力限制在有限的上下文窗口内,防止模型关注距离太远的标记。基于这一发现,我们为每个层分配不同的窗口大小,对较低层使用较短的窗口,对较高层使用较长的窗口。perplexity指标4.Baichuan2论文：https://arxiv.org/abs/2309.10305Pre-training来源收集数据,包括常规互联网网页、书籍、研究论文、代码库等,以构建一个广泛的世界知识体系。去重，构建了一个大规模的重复数据删除和聚类系统,支持LSH类似特征和稠密嵌入特征。最终只保留原始数据的31.68%的数据进行训练。Tokenizer字节对编码(BPE)，不对输入文本应用任何规范化,也不添加虚拟前缀。将数字拆分为单独的数字，处理额外空格的代码数据,向分词器添加仅空格标记，最大标记长度设置为32,以处理长中文词组。Architecture位置嵌入：RoPE激活函数：SwiGLU注意力层：xFormers减少内存。normal方法：RMSNorm，并且规范化输出嵌入lm_head。在我们规范化头部之后(蓝色),在刚开始训练的2000step左右训练变得非常稳定,这导致了更好的性能。norm_head loss最大z损失，在训练过程中,发现LLM的logits可能变得非常大。添加了一个最大z损失来规范化logits。其中z是最大logit值,这有助于稳定训练,并使推理更加稳健地适应超参数。最大z损失公式三、分析与总结llama2、qwen和baichuan2的论文从数据到技术结构详细公开其实现方式，Mistral 只是公开了针对content长度较长时如果修改和优化网络结果。1.大模型训练共同点：llama2、qwen和baichuan2的论文 都提到使用RoPE位置嵌入、SwiGLU激活函数、RMSNorm方法。并且都在尽可能实现更长长度的预测2.大模型训练不同点：数据上，qwen和baichuan2去重上做了许多工作。并且qwen在数据质量上做了两方面工作首先过滤低质量语料，其次加入高质量指令提高预训练效果。模型结构上：都在更长预测上下文长度进行提升，只是每个模型使用方式不一样。llama2使用GQA，Mistral 使用 Sliding Window Attention 和 Rolling Buffer Cache。qwen在QKV注意力层中添加bias以增强模型的外推能力、NTK-aware interpolation、LogN-Scaling和window attention。另外，baichuan2使用规范化输出嵌入lm_head和最大z损失提升模型稳定性。qwen在核心的矩阵计算中使用FP32换取更好效果。欢迎点赞和关注 paper君，发布最新论文解读 第一时间推送！"
670574382,6fad9d727159bbf38301c8bfc0d473ac,吕阿华,https://api.zhihu.com/people/6fad9d727159bbf38301c8bfc0d473ac,people,计算机科班/大类资产专家/视觉中国认证摄影师,国内外知名大模型产品列表（2024/04/24）,article,https://api.zhihu.com/articles/670574382,0尽管在某项参数上经常会被一些开源的<em>大模型</em>超越，但只有真正的使用者才能体会到其作为大语言模型一骑绝尘的优势所在。国外的<em>大模型</em>绝大多数都需要架梯翻墙才能访问，而且由于生成的内容相对自由,14,2,41,1701790247,1714143845,"这一年AI界的进展主要集中在AIGC领域，AIGC就是人工智能生成内容（Artificial Intelligence Generative Content），也就是用自然语言让AI帮你创作各种各样的内容，比如图片、视频、音乐、文字等等。截止目前，AI最擅长的创作领域是文字，其中最擅长的细分领域是(1)各种语言之间的翻译，(2)各种长篇大论的机器阅读并总结，(3)各种命题作文的撰写。次擅长的创作领域是图片，各种图片大模型大多可以实现文字生图和以图生图，主要的应用领域是logo设计，插画设计这种需要头脑风暴创意无限但精确度要求比较低的领域。根据自然语言生成视频和音频的大模型也陆续有问世，但主要还是来凑热闹为主，不太能满足各种商业用途。----- 国外部分 -----国外的大语言模型质量明显比国内的强一个档次，其中开山怪和领头羊目前都是OpenAI的chatGPT，GPT4.0尽管在某项参数上经常会被一些开源的大模型超越，但只有真正的使用者才能体会到其作为大语言模型一骑绝尘的优势所在。国外的大模型绝大多数都需要架梯翻墙才能访问，而且由于生成的内容相对自由，国内企业使用起来可能会要注意合规风险。闭源组综合类chatGPT: https://chat.openai.com OpenAI研发的大模型，目前是GPT4.0版本，地表最强，推出至今近一年时间虽然有各路英雄号称在某个专项上达到或者超越它的水平，但它一直用实力碾压这些虚名。缺点是除了要会科学上网之外，注册需要国外手机号，升级为plus会员(使用GPT-4及配套服务)需要国外信用卡，严格的KYC使其使用门槛比其它同类产品要高一截。目前GPT3.5版本可以不注册直接体验。Claude: https://claude.ai/ anthropic研发的大模型，目前版本是3.0，上下文长度稳定为200K token，仅次于Gemini 1.5。详见《Claude 3官方简介（附个人点评）》Gemini(Bard): https://gemini.google.com google研发的大模型，前身为Bard，目前版本为1.5，拥有地表最长上下文10M(1M稳定) token。详见《Google Gemini 1.5 技术报告（全文）》Mistral Large: https://mistral.ai/ Mistral AI 2024年2月发布的闭源大模型，拥有32K 上下文，参数数量不明。详见《Mistral团队的新模型——Mistral Large介绍》Grok: https://grok.x.ai/ 模型本身知名度一般，主要是老板一龙马斯克比较知名，目前版本为1.5，只有推特付费会员才可以体验。详见《xAI发布Grok-1.5(附个人点评) 》New Bing: https://www.bing.com/new 微软研发的大模型，最大的优势就是不需要翻墙。图片类Midjourney: https://www.midjourney.com/ Midjourney研究实验室开发的人工智能程序，可以实现文字生图和图生图。目前版本为v6。Stable Diffusion: https://stability.ai/stable-diffusion/ 由CompVis、Stability AI 和 LAION 的研究人员创建文本到图像潜在扩散模型，需下载代码布署本机使用，对电脑硬件配置有一定的要求，目前更新到了3.0版本。DALL·E 3: https://openai.com/dall-e-3 OpenAI研发的AI图像生成器，需要chatGPT Plus会员才能体验。Imagine with Meta AI: https://imagine.meta.com/ Meta研发的AI图像生成器，目前免费。Imagen 2: https://deepmind.google/technologies/imagen-2/ Google Deepmind 2023年12月发布的AI图像生成模型，效果待验证。音频类Stable Audio: https://www.stableaudio.com/ Stablility AI发布的AI音频生成模型。MuseNet: https://openai.com/research/musenet OpenAI研发的AI音频生成模型。Suno AI: https://suno.ai anthropic研发的音频大模型，可以根据prompt和歌词完成谱曲和编曲。视频类Stable Video Diffusion: https://stability.ai/stable-video Stablility AI发布的AI视频生成模型，以两个图像到视频模型的形式发布，能够以每秒 3 到 30 帧的可定制帧速率生成 14 帧和 25 帧，生成视频时长2-5秒。需下载代码布署本机使用，对电脑硬件配置有一定的要求。Gen-2: https://research.runwayml.com/gen2 Runway在2023年6月发布的AI视频生成模型，分为免费版和标准版($15/月)，可以用图片生视频和用文字生视频。Pika: https://www.pika.art/ Glen Pika在2023年11月发布的AI视频生成模型，目前已开放公测，支持文生视频、图生视频和视频生视频，生成视频时长3秒。Sora: https://openai.com/sora OpenAI在2024年2月发布的AI视频生成模型，时长可达1分钟。开源组Llama: https://llama.meta.com/llama3/ Meta研发的开源大模型，2024年4月目前发布了3.0版本，参数量为8B和70B，还有一个400B的在路上。详见《Llama 3发布：地表最强开源LLM来袭~》Vicuna: https://github.com/lm-sys/FastChat UC伯克利学者联手CMU、斯坦福等高校于2023年5月推出一个模型，参数量13B、7B，后来增加了33B的版本，目前版本为v1.5。Mistral: https://mistral.ai/ 法国的大模型初创企业MistralAI于2023年9月份发布的模型，参数量7B。2023年12月发布了Mixtral-of-Expert-7B，是一个拥有8个专家层的MoE模型。详见《Mixtral of Expert》精华摘译Starling: https://starling.cs.berkeley.edu/ 加州大学伯克利分校2023年11月发布的模型，参数量7B。Gemma: http://ai.google.dev/gemma Google2024年2月发布的开源小语言模型，参数量为2B和7B。详见《Gemma: Open Models Based on Gemini Research and Technology》Phi: Introducing Phi-3: Redefining what's possible with SLMs 微软发布的大语言模型，目前版本为3.0，有 mini(3.8B)、small（7B）和 medium（14B）三个版本。详见《Phi-3 技术报告（全文）》DBRX: Introducing DBRX | Databricks Mosaic AI 2024年3月发布的开源大语言混合专家(MoE)模型，参数量为132B，每次输入激活的参数36B，拥有16个专家层每次激活4个。详见《Databricks发布开源MoE模型——DBRX（全文）》----- 国内部分 -----国内的大语言模型都是为了想在这个市场中分一杯羹赶鸭子上架陆续上线的，截止目前和国外大模型相比能力残缺还是比较严重的，不过在中国传统文化领域的语料训练上还是可以体现出一定优势的。好处是不需要翻墙访问，回复内容生成速度快，而且国内企业和他们集成不太需要考虑政策风险。由于截止目前国内的大模型产品雷同且竞争力一般，楼主并没有太多精力和兴趣认真试用和比较以下列表，此处只做罗列。闭源组文心一言：https://yiyan.baidu.com/ 百度研发的大语言模型，目前3.5版本免费使用，4.0版本收费使用。智谱清言：https://chatglm.cn/ 清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型，目前版本为4.0，支持文字聊天，图片。字节豆包：https://www.doubao.com/ 字节跳动研发的大语言模型。通义千问：https://qianwen.aliyun.com/ 阿里研发的大语言模型，目前2.x版本。百川智能：https://www.baichuan-ai.com/ 搜狗系研发的大语言模型，目前3.0版本。腾讯混元：https://hunyuan.tencent.com/ 腾讯研发的大语言模型。讯飞星火：https://xinghuo.xfyun.cn/ 科大讯飞研发的大语言模型。复旦MOSS：https://moss.fastnlp.top/ 复旦大学研发的大语言模型，目前处于内测阶段。华为盘古：https://pangu.huaweicloud.com/ 华为研发的大模型系列，目前仅限华为云企业用户体验。天工：https://work.tiangong.cn/ 昆仑万维研发的大语言模型。商汤日日新：https://platform.sensenova.cn/ 商汤科技研发的大语言模型平台。Kimi: https://kimi.moonshot.cn/ 月之暗面研发的大语言模型，长文档阅读之王。开源组零一万物：https://www.lingyiwanwu.com/ 李开复牵头搞的开源大模型，LMSYS排名不错，参数数量为6B和34B，有基座版、微调版和量化版。Qwen: https://github.com/QwenLM/Qwen 通义千问开源版本，参数数量为1.8B/7B/14B/72B，2024年3月底发布了MoE版本，目前最新版本为1.5。DeepSeek: https://www.deepseek.com/ 幻方团队开发的开源大模型，DeepSeek-LLM的参数数量为7B和67B，另外还有DeepSeek-Coder, DeepSeek-Math, DeepSeekMoE等开源模型。DeepSeekMoE的技术论文详见《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models 》"
685823865,0bd3c6956375c4632ba195d56072cc79,快乐子涵酱,https://api.zhihu.com/people/0bd3c6956375c4632ba195d56072cc79,people,角色扮演大模型,角色扮演大模型技术分享,article,https://api.zhihu.com/articles/685823865,首先提两个问题，目前<em>大模型</em>有自己的“自我意识”吗？为什么<em>大模型</em>生成的答案可以一眼分辨出来？ 答案其实很简单，目前所有<em>大模型</em>技术，并不能让大模型具备本身的独立意识，<em>大模型</em>表现出的「拟人化」本质上是在努力扮演一个人类喜好的Assistant角色,64,34,93,1709815759,1711544367,训练上的一些干货在文末，前面是一些不成体系的碎碎念，重在交流。base北京，欢迎私信交流～ 什么是角色扮演大模型？首先提两个问题，目前大模型有自己的“自我意识”吗？为什么大模型生成的答案可以一眼分辨出来？答案其实很简单，目前所有大模型技术，并不能让大模型具备本身的独立意识，大模型表现出的「拟人化」本质上是在努力扮演一个人类喜好的Assistant角色；或者说，大模型通过SFT和RLHF等技术路线实现与人类喜好的对齐。然而，以chatGPT为例，目前市面上所有的通用大模型的答案具有一种浓浓的「Assistant」语气，具体来说就是很官方、书面，具有强烈的说教口吻，不够口语化，例如「总之......」「记住......」等等。这一切来源于openAI的标注标准，我们不做深究。当大模型作为一个「提效工具」时，这种答案风格是完全OK的；然而对于大部分非程序员的用户群体来说，大模型的「提效能力」并不是一个必需品，「娱乐能力」才是提升一个产品留存率和黏性最重要的能力纬度。当大模型作为一个娱乐工具时，非常官方的语气和说教口吻必然会带来巨大的违和感。随着以character.AI为首的众多以角色扮演能力为核心的大模型产品的爆火，我们不去讨论当今环境下人类社交形式的演变以及背后的社会原因，我们只讨论对于用户来说，他们更需要的是一个可定制的、高度拟人化的有情感、有温度的聊天机器人，这就是角色扮演模型。角色扮演模型和通用模型的区别是什么？概念上目前大模型本质上是扮演一个assistant助手，直接使用通用大模型进行角色扮演任务，类似于「让一个大模型扮演的assistant助手去扮演某个特定角色」，大模型“意识”中的「我」仍然是一个asisstant助手，「角色扮演」相当于是人类分配给这个助手的一个任务，相当于我们绕了一圈来达到角色扮演目的，效果必然打了一个折扣。但对于角色扮演模型，我们可以直接让这个AI去扮演某个角色，如虚拟女友、历史人物等。此时大模型“意识”中的「我」就是某个特定的人物，它一切出发点和“自我意识”都是从某个人物的角度出发和考虑。表现上对于通用大模型和角色扮演模型，在说话的风格和语气上有明显的不同：对于角色扮演模型来说，不会具有明显的官方语气和说教口吻，除非你明确要求角色扮演模型去扮演一个类似的角色；相反会表现出符合角色的特定性格特点和说话风格，比如在扮演“张飞”时性格莽撞不拘小节，说话豪爽大气角色扮演模型可能会具有一定的情绪能力，例如我们可以问通用大模型100遍1+1等于几，但对于角色扮演模型来说，问到第2遍就可能会表现出差异或不耐烦的情绪来；例如在扮演虚拟女友时会表现出撒娇、吃醋等属于人类的情绪角色扮演模型的输出更具画面感和沉浸感，当我们与通用模型交互时，体感上更像是阅读一本「工具书」，是机械的、没有情感的；而角色扮演模型更像是一个真实的人在隔着屏幕与用户聊天，用户可以想象出到对方的动作、想法和情绪等角色扮演模型具备更强的交互性：传统的通用模型本质上是一个问答机器人，用户与AI采取一问一答的形式；而角色扮演模型可以给用户主动预设一个场景或情节背景，用户和AI一起交流来演绎并推动剧情的发展。在这当中，角色扮演模型不会简单回答用户问题，更具有主动提问、主动推动剧情发展的能力假如说通用问答大模型如GPT等是一个assistant助手，那角色扮演大模型的定位就是一个「演员」。通用模型更偏向是「工具」，而角色扮演模型更偏向「产品」属性。角色扮演模型比通用模型更贴近「图灵测试」的目标，即具备「以假乱真」的能力。当用户与角色扮演模型进行交互时，会激发出更多交流欲望。用一个类比来聊聊我对「角色扮演」的认知我们把角色扮演大模型类比成一个演员，那大家通常如何评价一个演员演技的好坏呢？最简单或者最抽象来说，就是演得“像不像”。要想做到演啥像啥，就要理解角色，对于演员来说理解角色的途径就是阅读剧本，对大模型来说就是阅读system prompt。system prompt就是一个人物卡，当中包含角色的背景信息、性格特点、说话风格等等。大模型要想演的像，就要把这些信息吃透。特型演员对于演员来说，有些演员可能只适合演某种特型人物，如皇帝、学生等，这类人物是该演员的舒适圈。对应到一些开源产品或者模型来说，就是这一个模型只做针对某个特定人物做SFT的微调，如characterLLM等。我本人是不太喜欢这种模式的：首先显而易见，一个角色对应一个模型必然会浪费很多资源；其次，只训练某个角色有点杀鸡用牛刀了，浪费了大模型超强的泛化能力；其次这样做的效果真的比用N多个不同角色训练之后泛化的infer结果更好吗？我觉得存疑。特定角色必然会导致训练数据量级受限，且在该数据集上做finetune必然会导致过拟合，过拟合必然会导致模型灾难性遗忘，性能下降。还有一些项目比如「ChatHaruhi」，他们最开始的做法是抽取一些小说、剧本当中的角色对话，比如韦小宝、令狐冲等。我的看法和之前类似，使用几十或上百个角色且同一个角色的数据使用几百条上千条，还是会导致一些过拟合的出现。结果就是训练过程中，模型对system prompt的敏感度降低，导致推理时对于训练集中出现过的角色，这种模型通常表现较好；但面对一个全新的角色，模型泛化能力可能会打个折。类比到演员上，可以认为这个演员之前演过韦小宝和令狐冲，你再让他演同样的角色他压根不需要读剧本，只要回忆之前是怎么演的就行了，呈现效果上肯定得心应手；但是如果你突然让他演甄嬛，他已经忘了怎么钻研剧本了，必然演的很别扭。所以我一直秉持的技术路线是，构造大量人物卡，每个人物只放一条或几条对话session数据，让模型学会「阅读剧本」，也就是我们说的指令(在这里是system prompt)跟随能力。源于生活，高于生活有时候我们评价一个演员很有灵气，或者字面意义上评价这个演员很讨喜，可能说的是这个演员在表演过程中的表情、行为是否灵动；在角色大模型来说，就是动作描写是否生动形象。像character.AI还有BaichuanNPC，一直都默认会输出角色的动作、表情、心理以及对应的场景描写。通常中文放在（）里，英文放在*号里(MD格式)。电视剧中的人物对话，或者一些小说、剧本中人物对话的风格，介于严肃文学和真人对话之间。一方面需要接地气，不能太枯燥太端着；也不能完全和真人聊天一样完全随意，需要有有一定阅读的体验，并通过说话内容和旁白等来补充相对现实世界缺失的信息。什么叫现实世界缺失的信息呢？举个例子，假设小说中新登场了一个人物，一个惯用的处理手法就是通过某个路人的视角和口吻来介绍这个新人物的来历，这个描述在一些二流作家笔下甚至可能非常啰嗦或稍显突兀。但现实世界中大家可能一张名片或一份简历就知道这人是谁了。这就是文本内容的「信息缺失」，文本必须通过更多笔墨来描写某个现实世界中可能一个眼神就能get到的信息量。大模型类似，我们既需要有与通用底座模型迥异的、更拟人化的输出风格，又要比真人对话更稍微正式一点点。真人微信聊天时可能一句“笑死”+表情包就能传达的情绪，角色扮演模型需要更多的描述来渲染角色对某个情景时的反应。我有时候甚至觉得，角色扮演对标的与其说是真人聊天，不如说是galgame。或者说，角色扮演模型做的事情是「和用户一起完成某个虚拟情节的构建」。在这个过程中，角色扮演模型的输出不仅是角色「说的话」，在某种程度上也充当了「旁白」的角色。这再次证明了「描述性文本」的重要性。所以，这就是为什么很多人会用小说、剧本中抽取的内容来训练SFT。所以，这就是为什么如果直接用小说、剧本抽取对话来训练通常不太work。（后边会展开讲实验）直接从小说中抽人物对话会导致一个问题，那就是小说人物对话可能非常短，有时候甚至只有一两个字，更多的信息如角色的心理活动等放在旁白里进行描写了。如果只使用角色说的话来训练的话，会导致「信息缺失」。违和感一个不成熟的演员在扮演某个角色时会有违和感，模型同样如此。我一直觉得：例如「请你扮演一个翻译专家」「请你扮演一个程序员」这种任务类“角色扮演”；或者是一些「请你扮演张飞：请告诉我你作为刘备的部下如何评价他的处事准则」这种辣鸡问答，本质上还是一个assistant助手，根本就不是我今天想要聊的角色扮演，更不是我们当今想要的角色扮演模型。真正的角色扮演应该是类似真实人类的对话，永远都不会是刻板的一问一答，而是有来有回的碰撞。这就涉及到一个表述方式上的违和感的问题了：作为一个真实的人或角色，通常都会有一个明确的立场，所以永远都不会说”因人而异“这种端水类的话术；作为一个真实的人或角色，通常都会缺点有小心机，所以从来不会像chatGPT一样永远一本正经讲大道理；作为一个真实的人或角色，通常都会有自己的性格和脾气，所以从来都不会像正常底座模型一样有问必答；作为一个真实的人或角色，从来都不会说「作为xxx，我认为......」这种奇怪的话。那这个问题怎么解决呢？我目前的做法是简单粗暴建立一个黑名单词表，把所有带强烈违和感或真实人类根本不会说的词给过滤掉。那怎么快速评价模型输出是否有违和感呢？或者换个角度，什么样的输出算是一个没有违和感的模型呢？我的方法就是：把模型输出结果中所有的「我」这个字替换成「角色名」，看这句话是否成立。如果成立，则认为有违和感，即模型并没有真正带入角色，只是在输出形式上做到了角色扮演而已；如果不成立，那我们认为当前模型输出质量基本达标。 0312，鸽了一周，开始填坑关于继续预训练：continue pre-train「演员」在电影学院会学习大量的表演理论课，大模型角色扮演同样可能需要用一些小说、剧本类数据当作「课本」来学习一些潜在的能力。一个常见的做法是在这类数据上做continue pretrain，例如BaichuanNPC用了大概3T量级的tokens重新做了预训练。但基于海量数据的continue pretrain必然会消耗大量的计算资源，这对于大部分开发者并不友好；另一方面，大部分人能收集几B或者几十B就已经很难了，真的很难搞到那么多的数据。此外，但无论是基于我们的尝试结果还是【Continual Pre-Training of Large Language Models: How to (re)warm your model?】这篇论文的实验结论，使用少量数据进行continue pretrain不仅会发生在灾难性遗忘导致底座模型原有能力损失，亦有相当高的风险和概率在新数据集上表现出loss难收敛的情况。再从另一个维度想一下这个事儿。预训练的任务目标无非是给大模型「灌知识」，但是小说数据里到底有多少事实性知识呢？大部分小说都是杜撰的，其中大部分信息根本都是没用的，大模型根本不需要学习某个点击量只有几百条的网络小说某个扑街配角的人物生平信息，用户也根本不会问；用户可能会问一些热门小说比如「斗罗大陆」「斗破苍穹」，但这些知识pretrain模型本来就有啊，干嘛还要再做一次continue pretrain呢？因此对于角色扮演任务，我们更希望模型通过小说的描写手法和表述方式来学会如何将信息更好地表达和输出，这反而和SFT微调任务目标更接近。所以对于大多数开发者，如果你没有能力制作一个很好的预训练数据集和高超的炼丹技术，我可能不太建议走继续预训练这条路。但是可以尝试添加一些小说续写类数据，一方面增强模型的场景描写能力，另一方面写作能力和角色扮演能力确实强相关，也就是说：有角色扮演需求的用户大概率也会有小说创作的需求。关于通用对话数据大多数情况下，观众们真的不关心一个演员的数学好不好、会不会写代码。但很遗憾，如果我们把角色扮演模型类比成一个「演员」，那他首先是一个「人类」，对于一个人类来说，基本的知识能力和逻辑能力是必要的；可是要想让大模型具有一定的逻辑性和知识表达能力，不得不添加一些包括少量数学、代码任务在内的通用SFT问答数据。有一些角色扮演模型会直接从pretrain模型启动，仅使用少量角色扮演对话数据进行训练。从我个人的复现的结果来看，模型的表现更像是一个「努力假装自己是xxx角色的白痴」，下面是一条真实的infer结果：好的，我叫张飞，出身于涿郡，出生的时候父母不在身边，被母亲过继给关羽。后来曹操杀了他的母亲，与关羽失散，投奔刘备。在战场上，我率领部下多次打败敌军，得到了刘备的信任。是不是笑麻了？大模型明明具有相关知识，但就是因为缺乏SFT的激发，导致相关知识表达不出来，出现以上无比离谱的幻觉。所以综合前面continue pretrain的相关认知，我可能推荐添加少量通用SFT问答数据直接从一个训练好的pretrain模型启动，大概率就能得到一个还不错的baseline效果了。那么是否可以直接从SFT模型启动，不添加通用问答数据呢？答：可以，这取决于个人细节的任务目标、数据量、数据组织形式等等因素。直接从SFT模型启动，可以认为此时模型本身已经具有很好的通用能力了，甚至我们都不需要再添加额外的、重复的通用问答数据也能产生不错的效果。此时要考虑的问题反而变成了「如何在基础能力不退化的情况下迁移模型对话风格」。因为前面我们提到，通用问答模型具有很强的assistant语气，这必然会导致角色扮演任务的违和感。那太小的学习率，会导致模型更难以学到新知识；太大的学习率又容易灾难性遗忘。如何找到平衡点是一个很玄学的问题。如果你想采用一个更高的学习率，那这里可以参考Qwen技术报告，适当增大warmup ratio（但一定要保证数据量足够多），个人实验结果还不错。那么使用多大比例的通用问答数据？答：角色扮演任务和我们常谈到的一些垂域问答（如医疗金融法律等）还有一些区别：垂域问答可能需要的数据量更小，任务目标重点考察模型在专业领域的事实性知识能力，一些常见的数据比例可能是垂域数据占总数据量的10%到20%；但角色扮演任务不能单纯当作一个简单的垂域来看，我认为它不是底座大模型的子任务或某项能力，相反，基础的问答能力应该是角色扮演大模型的一项附属能力。因此我们推荐在全量数据当中，角色扮演数据占比更大，通用问答数据占比应该不超过全量数据的50%。我个人大概使用了1/4到1/3量级的通用问答数据，其中数学代码逻辑相关的数据用得非常少，只是通过一些简单的CoT数据来激发模型的逻辑能力或思考能力而已。一些零散的内容到此为止，我肚子里的东西基本都掏差不多了，下面是一些零散的小认知。进度条问题：一些游戏类角色扮演可能需要大模型实时输出一些角色状态信息，这本质上是一个复杂指令跟随能力，通常需要使用更大参数的模型去解决，13B以下规模的模型去做类似的事情肯定会更吃力一点。目前我个人的训练数据中没有特意构建这类数据，所以表现相对一般。小说抽取对话：我实验结论是这类数据会对最终结果有负面影响，具体现象是模型output变短，更容易给出只有一两字的output，这也就是之前我们提到的「信息缺失」的问题。我个人是不太推荐大量使用这种数据的，或者可能需要做更严格的清洗或改写。过于配合用户：也就是大模型不会拒绝用户提出的需求或问题，或很少表现出不耐烦等负面情绪。这个问题其实比较容易解决，只要在对话数据里包含这样类似的数据即可（最好是多轮），实在不行可以考虑使用DPO或RLHF来解决。生成数据多样性：使用GPT等生成对话数据肯定会有多样性问题，这里建议一定要对高频pattern做降采样，否则会导致模型过拟合。关于DPO：很多人也提到过，建议对模型本身的infer结果做自采样，我尝试下来的体验是：效果出奇的好安全问题：大模型角色扮演从提出来开始，就和越狱等内容相关，这里做对齐必然会导致效果损失，但没办法hhhOK啦，完结撒花。很多技术细节的东西不方便透露，所以只能给出一些比较抽象的认知性内容。感谢大家的关注和支持，欢迎随时私信交流～
679425777,6ed048bd3f2a45ef8dc7b6e7e4135b4f,海鸣,https://api.zhihu.com/people/6ed048bd3f2a45ef8dc7b6e7e4135b4f,people,公众号《AIGC小白入门记》,说说百度大模型算法工程师二面经历,article,https://api.zhihu.com/articles/679425777,算法工程师面试常考手撕题 搜狐<em>大模型</em>算法工程师 字节大模型算法实习生 理想汽车<em>大模型</em>算法实习生 百度大模型算法实习生面试题 腾讯<em>大模型</em>算法实习生面试题 阿里大模型算法工程师一面,64,1,232,1706012221,1709898428,"关注 个人 公众号《AIGC小白入门记》，了解 AIGC 面试资讯美团-大模型算法工程师面经小米-NLP算法工程师面试题好未来-NLP算法工程师面经百度大模型算法工程师面经算法工程师面试常考手撕题搜狐大模型算法工程师字节大模型算法实习生理想汽车大模型算法实习生百度大模型算法实习生面试题腾讯大模型算法实习生面试题阿里大模型算法工程师一面某大厂大模型算法工程师面试题说说百度大模型算法工程师二面经历昆仑天工大模型算法工程师百度大模型算法工程师面试题应聘岗位：百度大模型算法工程师 面试轮数：第二轮 整体面试感觉：偏简单个人公众号面试过程回顾1. 自我介绍在自我介绍环节，我清晰地阐述了个人基本信息、教育背景、工作经历和技能特长，展示了自信和沟通能力。2. Leetcode 题具体题意记不清了，但是类似 【208. 实现 Trie (前缀树)】题目内容Trie（发音类似 ""try""）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。请你实现 Trie 类：Trie() 初始化前缀树对象。void insert(String word) 向前缀树中插入字符串 word 。boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。示例：输入 [""Trie"", ""insert"", ""search"", ""search"", ""startsWith"", ""insert"", ""search""] [[], [""apple""], [""apple""], [""app""], [""app""], [""app""], [""app""]] 输出 [null, null, true, false, true, null, true] 解释 Trie trie = new Trie(); trie.insert(""apple""); trie.search(""apple""); // 返回 True trie.search(""app""); // 返回 False trie.startsWith(""app""); // 返回 True trie.insert(""app""); trie.search(""app""); // 返回 True 提示：1 <= word.length, prefix.length <= 2000word 和 prefix 仅由小写英文字母组成insert、search 和 startsWith 调用次数 总计 不超过 3 * 104 次题目解答class Trie(object): def __init__(self): self.children = [None]*26 self.is_end = False def insert(self, word): """""" :type word: str :rtype: None """""" node = self for c in word: c = ord(c)-ord(""a"") if not node.children[c]: node.children[c] = Trie() node = node.children[c] node.is_end = True def searchPrefix(self, prefix): node = self for c in prefix: c = ord(c)-ord(""a"") if not node.children[c]: return None node = node.children[c] return node def search(self, word): """""" :type word: str :rtype: bool """""" node = self.searchPrefix(word) return node is not None and node.is_end def startsWith(self, prefix): """""" :type prefix: str :rtype: bool """""" return self.searchPrefix(prefix) is not None # Your Trie object will be instantiated and called as such: # obj = Trie() # obj.insert(word) # param_2 = obj.search(word) # param_3 = obj.startsWith(prefix) 3. 技术问题回答3.1 结合 GNN 科研项目进行提问样本构建的流程是怎样的，并且为什么 GCN 相较于其他方法在效果上更胜一筹？节点特征指的是什么？3.2 结合 基于 RAG 的医学问答项目进行提问查询流程？使用什么向量数据库？介绍一下 RAG 原理？RAG 如何解决多实体提问问题？用户提问：感冒和咳嗽需要吃什么药？ 3.3 结合多模态科研项目进行提问Prompt是如何生成的，优化目标是什么，任务是什么？OCR 抽取效果不好，需要怎么排查问题？3.4 技术问题您是否使用过Pytorch提供的预训练模型，例如torchvision、transformers以及OpenAI开源的ClIP？对分布式训练有经验么？回答：学过但是没用过RNN与GNN之间有哪些区别，以及它们各自适用于哪些场景？回答：RNN与GNN的区别： 1. 数据类型： - RNN 设计用于处理序列数据，即数据点按时间顺序排列，如时间序列分析、语音识别和自然语言处理。 - GNN 专门用于处理图结构数据，图由节点和边组成，代表实体及其关系，如社交网络、交通网络和分子结构。 2. 结构和工作原理： - RNN 的核心是循环单元，它能够在序列的每个时间步上保持信息的状态，但是长序列会导致梯度消失或梯度爆炸问题，影响学习长期依赖。 - GNN 通过节点和边的特征以及图结构本身的信息，利用特殊的邻居节点更新机制来学习图中的特征表示，更好地捕捉节点间的依赖关系。 3. 长期依赖问题： - RNN 在处理长序列时存在长期依赖问题，虽然有LSTM（长短期记忆网络）等变体来缓解这一问题，但本质上是序列模型。 - GNN 通过图结构天然地能够捕捉节点间的依赖关系，因此在处理具有明确关系的数据时更为有效。 各自适用的场景： - RNN 适用于处理时间序列数据、文本序列等，如股票价格预测、语音识别、机器翻译（序列到序列的任务）。 - GNN 适用于处理结构化数据，如社交网络分析、推荐系统、生物信息学（如蛋白质结构预测）、地理信息系统等，其中实体和关系是数据的核心组成部分。 总的来说，RNN适合处理时间或顺序上的数据，而GNN适合处理具有明确结构关系的数据。两者各有优势，选择哪种模型取决于具体问题和数据的特点。 GPT和BERT在文本表征方面有哪些结构和工作原理上的差异？回答：BERT是Transformer Encoder，属于自监督训练方式，然后两大预训练任务，主要用于下游任务抽特征，GPT是Decoder，自回归训练，主要是预测下一个词的分布，依赖大语料库，GPT-3可以表现出Few-shot/zero-shot learning因为说了BERT好训练一些，问了为什么?回答：说了GPT任务对简单、比较依赖语料库的大小，BERT的MLM比较直觉且个人能训练，GPT只有openai等公司有成品说一说你对 Zero-shot和Few-shot的理解回答：Few-shot先给定任务范式描述，Zero-shot就是直接做怎么看待计算机网络和操作系统在DL中的作用回答：谈了DL研究一些计算机网路的问题，比如网络拓扑、交换机拓扑等，分布式训练时会有通信，也会用到进程相关知识你来调优一个BERT模型适应一个数据集或任务会怎么做回答：固定BERT，训练分类头或者使用Adapter训练完模型后准确率很低，怎么优化回答：首先检查代码结构和分类器的网络结构和BERT量级是否匹配，学习率+余弦退火调整，改为Adapter，检查数据集质量，验证阶段代码是否有误有一批文本数据，来源和质量不太一样，使用时如何处理回答：反问文本来源不同是否混合或完全分开，结合多模态融合的技术，增加一个学习任务，对不同来源的文本表示进行线性变换投影到相同的特征空间中个人本次面试总结百度的面试篇项目面一点，整体效果还行，面试官给人感觉比较温和。美团-大模型算法工程师面经小米-NLP算法工程师面试题好未来-NLP算法工程师面经百度大模型算法工程师面经算法工程师面试常考手撕题搜狐大模型算法工程师字节大模型算法实习生理想汽车大模型算法实习生百度大模型算法实习生面试题腾讯大模型算法实习生面试题阿里大模型算法工程师一面某大厂大模型算法工程师面试题说说百度大模型算法工程师二面经历昆仑天工大模型算法工程师"
643723202,c2026c09f73c27fcd3f4eb10354d5f7e,LinguaMind,https://api.zhihu.com/people/c2026c09f73c27fcd3f4eb10354d5f7e,people,NUS Research Fellow ,大语言模型外挂知识库缓解大模型幻觉问题,article,https://api.zhihu.com/articles/643723202,但并不是所有<em>模型</em>都能实现高性能。 键值检索任务的结果与多文档问答任务的结果有<em>很大</em>的相似趋势：当必须在输入上下文的中间访问键值对时，<em>模型</em>性能最低。模型性能通常也会在较长的输入上下文中降低,95,1,268,1689389935,1689422271,"当前基于知识库外挂方式来缓解大模型幻觉问题的方案逐步受到大家关注，关于这一块我们已经有多篇文章进行了论述。ACL2023也特意设置了《Retrieval-based Language Models and Applications》的讲习班课程。该课程讲述了如何基于检索来增强大模型的生成能力，其中涉及到架构、训练以及应用、多模态等多个方面的内容，下面给出了个每个section报告的地址，可供查看：Section1:Introduction地址：https://acl2023-retrieval-lm.github.io/slides/1-intro.pdfSection2:Definition Preliminaries地址：https://acl2023-retrieval-lm.github.io/slides/2-definition.pdfSection3:Retrieval-basedLMs:Architecture地址：https://acl2023-retrieval-lm.github.io/slides/3-architecture.pdfSection4:Retrieval-basedLMs:Training地址：https://acl2023-retrieval-lm.github.io/slides/4-training.pdfSection5:Retrieval-basedLMs:Applications地址：https://acl2023-retrieval-lm.github.io/slides/5-application.pdfSection6:Extension:Multilingual &Multimodal地址：https://acl2023-retrieval-lm.github.io/slides/6-extension.pdfSection7:Challenges & Opportunities地址：https://acl2023-retrieval-lm.github.io/slides/7-conclusion.pdf而外挂知识库的一个很重要的点就是是否能够提供准确地上下文，通常我们会受限于相似度阈值以及最长上下文地限制，从而影响上下文构造的数据效率。为了解决这个问题，做尽可能长的上下文，比如从2048增加到16k，或者增加到32k，甚至256k。例如，由于语言模型通常是用Transformer实现的，对长序列的扩展性很差，由于自我注意的复杂性与输入序列长度成二次方的关系。因此，语言模型通常用相对较小的上下文窗口进行训练，例如硬件（具有更多内存的更快的GPU）和算法如flash attention等改进让语言模型具有更大的上下文。具体的，可以见到的工作包括：设计具有比Trans-foormer更便宜伸缩性的性能语言模型有很多工作要做。许多工作都在追求具有注意力修正的Transformer变体，如递归、将注意力因子化为计算强度较低的近似或低阶近似。例如，Dao等人（2022）通过精心制作的IO感知CUDA内核提供了更快的准确注意力。又如通过卷积和/或线性RNN来消除注意力整体的二次序列长度复杂性，例如RWKV（Peng，2023）、S4（Gu等人，2022）或Hyena（Poli等人，2023年）的工作。但这又引来另一个问题，那就是上下文长了，是否就一定意味着有好的表现？语言模型在采用上下文答案时有什么选择机制？最近的文章《Lost in the Middle: How Language Models Use Long Contexts》值得关注。地址：https://arxiv.org/abs//2307.03172该工作分析了语言模型在两个需要识别输入上下文中相关信息的任务，即多文档问题回答和键值检索上的表现。该工作发现，当相关信息出现在输入上下文的开头或结尾时，性能通常是最高的，而当模型必须在长上下文的中间访问相关信息时，性能会明显下降。此外，随着输入语境的变长，性能也会逐步下降。如图1所示，改变相关信息的位置 (在这种情况下，回答输入问题的段落的位置）在语言模型的输入语境中会产生一个U型的性能曲线。例如，GPT-3.5-Turbo在多文档问题任务中，当相关信息被放置在其输入语境的中间时，其开卷性能低于在没有任何文档的情况下进行预测的性能。为了得到这个结论，该工作对开放式（MPT-30B-Instruct，LongChat-13B（16K））和封闭式（OpenAI的GPT-3.5-Turbo和Anthropic的Claude）语言模型在需要访问和使用输入上下文信息的情况下进行对照实验来进行研究。首先，第一个任务是多文档问题回答，这需要模型对所提供的文档进行推理，以找到相关的信息，并使用它来回答一个给定的问题，这项任务模仿了许多商业生成性搜索和问题回答应用（如hhBing Chat）背后的检索增强的生成机制。通过对输入上下文的大小和输入上下文相关信息的位置进行消融实验，并研究了它们对模型性能的影响。例如，通过在输入上下文中添加更多的文档来增加输入上下文的长度（类似于在检索增强生成中重新获取更多的文档），并通过改变输入上下文中文档的位置来修改相关信息在上下文中的位置，将相关文档放在上下文的开头、中间或结尾。 其次，键值对检索。既然语言模型在多文档问题回答任务中很难检索和使用相关信息，那么语言模型能在多大程度上从其输入的文本中检索出相关信息？该工作提出了一个合成的键值检索任务来研究这个问题，该任务被设计为一个小型的测试平台，即模型被给定一个JSON格式的键值对集合，并且必须返回与特定键相关的值，用于测试从输入语境中检索匹配标记的基本能力。在这个任务中，与多文档质量保证任务类似，键值检索任务也允许对输入上下文的长度（增加更多的键值对）和相关信息的位置进行改变。在这种情况下，也得到了类似的U型性能曲线，许多模型都在努力简单地检索出现在其输入上下文中间的匹配标记。此外，该工作发现，当从维基百科检索以回答NaturalQuestions-Open的查询时，模型的性能在检索器召回率达到一定水平之前就已经饱和了，这表明模型不能有效地使用额外的检索文件，使用超过20个检索文件只能略微提高性能（GPT-3.5-Turbo为1.5%，claude-1.3为1%）。关于该工作的代码和数据，可以进一步参考http://nelsonliu.me/papers/lost-in-the-middle，本文对该工作进行介绍，供大家一起参考，并思考。一、语言模块如何使用其输入上下文：多文档问答的实验分析多文档问答的目标是更好地理解语言模块如何使用其输入上下文。任务需要模型在输入上下文中找到相关信息，并使用它来回答问题。1、实验输入输出模型输入：要回答的问题和k个文档（例如，维基百科中的段落），其中恰好有一个文档包含问题的答案，而k−1个文档不包含，也就是干扰问答。模型输出：模型访问在其输入上下文中包含答案的文档，并使用它来回答问题。如图2表示多文档问题回答任务的例子，有一个输入上下文和期望的模型答案。在输入环境中，正确回答请求的相关文件被加粗。2、模型实验数据采用NaturalQuestions基准，该基准包含发布给谷歌搜索引擎的历史查询和从维基百科中提取的人工标注的答案。具体地，首先从NaturalQuestions Open中获取问题，使用维基百科中的使用段落（最多100个标记的块）作为输入文本中的文档。其中：为了获得一份回答问题的文档，使用维基百科的段落，其中包含NaturalQuestions注释中的答案。为了选择不包含答案的k−1个干扰物文档，使用Contriever检索系统来检索与问题最相关的但不包含任何NaturalQuestions答案的k–1维基百科块。{ ""question"": ""who got the first nobel prize in physics"", ""answers"": [ ""Wilhelm Conrad Röntgen"" ], ""ctxs"": [ ... { ""id"": <string id, e.g., ""71445"">, ""title"": <string title of the wikipedia article that this passage comes from>, ""text"": <string content of the passage>, ""score"": <string relevance score, e.g. ""1.0510446"">, ""hasanswer"": <boolean, whether any of the values in the `answers` key appears in the text>, ""original_retrieval_index"": <int indicating the original retrieval index. for example, a value of 0 indicates that this was the top retrieved document>, ""isgold"": <boolean, true or false indicating if this chunk is the gold answer from NaturalQuestions> }, ... ], ""nq_annotated_gold"": { ""title"": <string title of the wikipedia article containing the answer, as annotated in NaturalQuestions>, ""long_answer"": ""<string content of the paragraph element containing the answer, as annotated in NaturalQuestions>"", ""chunked_long_answer"": ""<string content of the paragraph element containing the answer, randomly chunked to approximately 100 words>"", ""short_answers"": [ <string short answers, as annootated in NaturalQuestions> ] }3、实验变量控制如图3所示，为了调节任务中的输入语境长度，该工作增加或减少不包含答案的检索文件的数量，添加不包含答案的额外文档会增加输入上下文的长度，但不会影响所需的输出。正确回答请求的相关文档对在输入上下文中被加粗。如图4所示，为了调节相关信息在输入上下文中的位置，调整输入语境中文件的顺序，以改变包含答案的文件的位置。在输入上下文中重新排序的文档不会影响到所需的输出。在输入上下文中，对请求做出正确回答的相关文档被加粗。4、实验评测指标对包含10个、20个和30个文档（每个文档2.7K个示例）的输入上下文进行了实验，使用准确性作为主要评价指标，判断是否有任何正确的答出现在预测的输出中。5、模型选用在开放式模型方面，使用MPT-30B-Instruction(8192)、LongChat-13B（16K）。在封闭式模型方面。使用OpenAI API扩展GPT-3.5-Turbo(4K)和GPT-3.5-Turbo（16K），claude-1.3(8K)，claude-1.3(100k)6、实验效果如图6所示，随着上下文长度的增长，模型对应的性能不断下降。其中有一些有趣的现象，例如，当包含答案的文档位于20个中的第10位时，GPT-3.5-Turbo在20个文档设置中的最低性能为52.9%设置30个文档的输入上下文对于GPT-3.5-Turbo来说太长，但使用其扩展上下文对应的GPT-3.5-Turbo（16K）也会导致性能下降当相关文档位于30个中的第10位时，性能下降49.5%。这也就说明，尽管扩展上下文模型可以处理更长的输入上下文，但它们可能不会更好地对其上下文窗口内的信息进行推理。二、语言模型从输入上下文中检索的效果如何：键值对检索实验分析考虑到语言模型在多文档问答任务中很难从其输入上下文的中间检索和使用信息，他们可以在多大程度上简单地从输入上下文中检索信息？该工作用一个综合的键值检索任务来研究这个问题。1、实验设置输入：一个具有k个键值对的字符串序列化JSON对象，其中每个键值都是唯一的、随机生成的UUID，以及上述JSON对象中的一个特殊键。输出：返回与指定键关联的值。因此，每个JSON对象都包含一个相关的键值对（其中要检索值），以及k−1个不相关的“distributor”键值对。具体数据样式如图7所示，提供了一个示例输入上下文及其相应的期望输出。2、模型选用及实验效果评测指标对包含75、140和300个键值对（每个500个示例）的输入上下文进行实验，在模型选用上使用与多文档问答实验相同的一组模型，使用准确性作为评估指标，评估预测输出中是否出现了正确的值。3、实验效果变量控制如图8所示，为了调整该任务中的输入上下文长度，通过添加或删除随机键来改变输入JSON键值对的数量k，从而改变干扰器键值对的数目。如图9所示，为了调整输入上下文中相关信息的位置，在序列化的JSON对象中更改要检索的键的位置。4、实验效果分析如图10所示，其显示了关键值检索性能。可以看到，有几个有趣的发现：尽管合成键值检索任务只需要识别输入上下文中的精确匹配，但并不是所有模型都能实现高性能。键值检索任务的结果与多文档问答任务的结果有很大的相似趋势：当必须在输入上下文的中间访问键值对时，模型性能最低。模型性能通常也会在较长的输入上下文中降低。特别的，LongChat-13B（16K）在相关信息位于输入上下文的开头时，倾向于生成代码来检索键，而不是输出值本身。三、为什么语言模型很难使用它们的整个输入上下文？我们的多文档问答和键值检索结果表明，当语言模型必须在长输入上下文中访问相关信息时，其性能会显著下降。为了更好地理解原因，该工作对模型架构的作用（例如，仅解码器与编码器-解码器）以及指令微调的影响进行了一些初步调查。1、模型架构的影响观察： Flan-UL2在其2048训练时间上下文窗口内的序列上进行评估，其性能对输入上下文中相关信息的位置变化相对稳健。当在序列长度超过2048个令牌的设置上进行评估时，当相关信息位于中间时，Flan-UL2性能开始下降。Flan-T5-XXL显示出类似的趋势，当将相关信息放在输入上下文的中间时，输入上下文越长，性能下降越大。机理推测：编码器-解码器模型可能会更好地利用其上下文窗口，因为它们的双向编码器允许在未来文档的上下文中处理每个文档，从而可能增强文档之间的相对重要性估计。2、指令调整的影响在这种监督指令调优数据中，任务指定和/或指令通常被放置在输入上下文的开头，这可能导致指令调优语言模型对输入上下文的开始给予更多的重视。为了更好地理解指令调整对语言模型如何使用长输入上下文的潜在影响，将MPT-30B-instruction的多文档问答性能与其在指令微调之前的基本型MPT-30B进行了比较，图13比较了MPT-30B和MPT-30B-Instruction的多文档QA性能。观察：MPT-30B和MPT-30B-Instruction都显示出U型性能曲线，当相关信息出现在上下文的最开始或最结束时，性能最高。尽管MPT-30B-Instruction的绝对性能一致高于MPT-30B，但它们的总体性能趋势非常相似。 这些观察结果补充了先前的工作，先前的工作发现语言模型偏向于最近的标记。这种近因偏倚通常表现在连续文本的下一个单词预测中，其中语言模型从长程信息中获益最小。四、思考：更多的背景总是更好吗？在实际环境中，通常需要权衡输入上下文长度的增加，为指令调整语言模型提供更多信息，这可能有助于提高下游任务性能，但也会增加模型必须推理的内容量。那么？即使一个语言模型可以接受16K个标记，提供16K个上下文标记实际上有益么？这个问题的答案是特定于下游任务的，因为它取决于添加上下文的边际值和模型有效使用长输入上下文的能力。为了研究这个问题，该工作在NaturalQuestions-open上使用开放域问答进行案例研究。图14显示了开放域QA结果，可以看到，在检索器检索文档逐步提升后，模型性能的提升趋缓，这表明模型很难利用额外检索到的文档。使用20多个检索到的文档只能略微提高读取器性能，其中GPT-3.5-Turbo为～1.5%，Claude为～1%，同时显著增加了输入上下文长度，从而增加了延迟和成本。这些结果，再加上观察到模型在输入上下文的开始或结束时更善于检索和使用信息，对检索到的文档进行有效的重新排序，将相关信息推向输入上下文的开头或排序列表截断，必要时返回更少的文档，这个是值得深入的方向。总结本文主要针对《Lost in the Middle: How Language Models Use Long Contexts》这一文章进行了介绍。该工作通过使用多文档问答和键值检索两个任务进行验证，实证研究了语言模型如何使用长输入上下文。研究的最终结论为：语言模型在长输入上下文的中间使用信息时会带来性能下降，并且随着输入上下文的增长，性能会进一步恶化，其更偏向于头尾两个位置。这个工作对我们可以有个较好的建议，即对检索到的文档进行有效的重新排序，将相关信息推向输入上下文的开头或排序列表截断，必要时返回更少的文档，这个是值得深入的方向。最后，关于ACL2023的材料，也可以多看看，相信会有更多发现。参考文献1、https://arxiv.org/abs//2307.031722、https://acl2023-retrieval-lm.github.io/原文链接：也看ACL2023外挂知识库大模型开放报告及大模型使用输入长文本的有趣发现：关注头尾位置而忽视中间位置的选择偏置研究"
641885645,36f69162230003d316d0b8a6d8da20ba,量子位,https://api.zhihu.com/people/36f69162230003d316d0b8a6d8da20ba,people,,华为大模型登Nature正刊！审稿人：让人们重新审视预报模型的未来,article,https://api.zhihu.com/articles/641885645,最后，华为云盘古气象<em>大模型</em>团队如何看待AI气象预报的未来？ 答案是三大关键： 首先，大数据。庞大的气象数据是AI模型的基石，当前盘古气象<em>大模型</em>仅使用部分ERA5再分析数据,177,38,77,1688626423,1688626423,允中 发自 凹非寺量子位 | 公众号 QbitAI比传统方法快1万倍、只需要1.4秒就能完成24小时全球气象预报——它就是来自华为云的盘古气象大模型。今天，它登上了Nature，据称还是近年来首篇以中国科技公司作为唯一署名单位发表的Nature正刊论文（也就是华为云独作）。审稿人给予它高度评价，这一模型使人类得以重新审视气象预报模型的未来。言外之意，就是有了它，原来的传统方法都不香了。那么，它究竟是如何被开发出来的？解决了哪些关键难题？又有何具体成效和应用？顺着这篇论文带你一文看尽。破解现有AI气象预报模型精度不足问题上世纪20年代以来，特别是近三十年随着算力的迅速发展，传统的数值天气预报在每日天气预报、极端灾害预警、气候变化预测等领域取得了巨大的成功。但是随着算力增长的趋缓和物理模型的逐渐复杂化，这一方式的瓶颈日益突出。于是研究者们开始挖掘新的气象预报范式如使用深度学习方法预测未来天气。华为云研发团队于2年前开始这方面的研究。他们发现，在数值方法应用最广泛的领域如中长期预报中，现有的AI预报方法精度仍然显著低于数值预报方法，并受到可解释性欠缺，极端天气预测不准等问题的制约。而造成AI气象预报模型的精度不足主要有两个原因：第一，现有的AI气象预报模型都是基于2D神经网络，无法很好地处理不均匀的3D气象数据；第二，AI方法缺少数学物理机理约束，因此在迭代的过程中会不断积累迭代误差。在此，华为云的研究人员提出了3D Earth-Specific Transformer（3DEST）来处理复杂的不均匀3D气象数据，从而打造了盘古气象大模型。其主要思想是使用一个视觉transformer的3D变种来处理复杂的不均匀的气象要素，并且使用层次化时域聚合策略，训练了4个不同预报间隔的模型（分别为1小时间隔、3小时间隔、6小时间隔、24小时间隔），使得预测特定时间气象状况的迭代次数最小，从而减少迭代误差，也避免了由递归训练带来的训练资源消耗。为了训练每个模型，研究人员使用1979-2021年的气象数据，以小时为单位采样，训练了100个epoch。每个模型需要在192块V100显卡上训练16天。事实上，即使经历100个epoch，这些模型依旧没有完全收敛。也就是说，在计算资源更加充足的情况下，AI预报的精度还能够进一步提升。最终推理时，盘古气象大模型仅需在一张V100显卡上运行1.4秒，即可完成24小时全球气象预报，包括位势、湿度、风速、温度、海平面气压等，其中水平空间分辨率达到 0.25∘×0.25∘ ，时间分辨率为1小时，覆盖13层垂直高度，可以精准地预测细粒度气象特征。而作为首个精度超过传统数值预报方法的AI方法，它的计算速度相比传统数值预报提升超过10000倍。可直接应用于多个下游场景今年5月，台风“玛娃”走向受到广泛关注。中央气象局表示，华为云盘古大模型在“玛娃”的路径预报中表现优异，提前五天预报出其将在台湾岛东部海域转向路径。在第19届世界气象大会上，欧洲气象局也指出，华为云盘古气象大模型在精度上有不可否认的能力，纯数据驱动的AI天气预报模型，展现出了可与欧洲中期天气预报中心业务数值模式媲美的预报实力。欧洲中期天气预报中心主任弗洛伦斯·哈比耶详细地展示了华为云盘古气象大模型与欧洲中期天气预报中心的实时运行检验对比情况：为了探索AI捕获极端天气的能力，我们研究了今年2月芬兰的一个案例，当时观测到了-29℃的寒潮，我们发现盘古较早认识到了这一事件的严重性。弗洛伦斯·哈比耶还强调，AI预测方法资源消耗少，为发展中国家提供了重要机遇，因为它不再需要大规模的超算资源，还为提升全球预报能力提供了难得的机遇。至于华为云选择AI气象预报领域作为一个“突破口”，一方面，气象预报尤其是对极端天气如暴雨、台风、干旱、寒潮的精准预测关乎国际民生，另一方面，气象预测问题非常复杂，AI可以从海量数据中挖掘出新的大气演变规律，在精度和速度上具有巨大的提升潜力。据了解，世界气象组织（WMO）即将发布的WMO2024—2027年战略计划吸收了人工智能元素，使其成为推动气象科技发展的重要力量。WMO还将积极推动AI在临近预报及数值天气预报等领域的示范应用，创建人工智能产品应用国际比对平台，制定AI气象应用标准和指南，推动人工智能数据集共享等相关工作，探索并发挥AI在气象领域的应用潜力，有效支撑全民早期预警倡议。未来三大关键最后，华为云盘古气象大模型团队如何看待AI气象预报的未来？答案是三大关键：首先，大数据。庞大的气象数据是AI模型的基石，当前盘古气象大模型仅使用部分ERA5再分析数据，未来的AI模型将基于海量的、更精细的全球观测数据。其次，大算力。气象数据超高的分辨率对AI模型的训练造成了巨大的挑战，盘古气象大模型现在的输入分辨率为1440×720×14×5，相比计算视觉任务常用的分辨率224×224×3大约500倍，随着分辨率的进一步增加和模型的增大，需要的算力资源也会迅速增加。最后，大模型。复杂的气象规律，超高的分辨率与庞大的数据量都决定了AI气象预报需要使用计算量极高的AI模型。同时，想要不断迭代领先的AI气象预报模型，稳定的云上环境、工作套件和对应的运维也是必不可少的。论文地址：https://www.nature.com/articles/s41586-023-06185-3—完—@量子位 · 追踪AI技术和产品新动态深有感触的朋友，欢迎赞同、关注、分享三连վ'ᴗ' ի ❤
613239726,95fcc8df269eb6c3d68ac55b6c2c6341,nghuyong,https://api.zhihu.com/people/95fcc8df269eb6c3d68ac55b6c2c6341,people,略懂些NLP｜非典型程序员,中文开源1B以上大模型汇总,article,https://api.zhihu.com/articles/613239726,本文对开源的中文<em>大模型</em>进行汇总，这里汇总的大模型满足两个条件： 模型参数量大于1B（10亿）参数 可用原生的huggingface进行直接加载 下面的这个模型就不属于，因为需要引入额外的`fengshen`库from,64,11,205,1678548228,1699085048,背景相比BERT类的“小模型”，大模型在文本生成，文本理解方面能展现出更多惊喜的效果。大模型的架构上可以分成Encoder-Decoder和Decoder-only两种类型。虽然随着GPT-3的大火，但这也不表明大模型只能是Decoder-only类型，可以参考下面“元语智能”对架构问题的回应。https://mp.weixin.qq.com/s/UkUhxX5LcZr33SRU4h9IBw本文对开源的中文大模型进行汇总，这里汇总的大模型满足两个条件：模型参数量大于1B（10亿）参数可用原生的huggingface进行直接加载下面的这个模型就不属于，因为需要引入额外的`fengshen`库from fengshen.models.transfo_xl_denoise.tokenization_transfo_xl_denoise import TransfoXLDenoiseTokenizer from fengshen.models.transfo_xl_denoise.modeling_transfo_xl_denoise import TransfoXLDenoiseModel tokenizer = TransfoXLDenoiseTokenizer.from_pretrained('IDEA-CCNL/Randeng-Transformer-1.1B-Denoise') model = TransfoXLDenoiseModel.from_pretrained('IDEA-CCNL/Randeng-Transformer-1.1B-Denoise')Encoder-Decoder架构暂无纯中文的1B大模型，ClueAI/ChatYuan-large-v1的参数量也只是783M。所以只能使用开源的多语言版本。google/mt5-largehttps://huggingface.co/google/mt5-large基于C4语料训练，涵盖101种语言，参数量1.2B。该系列还包括google/mt5-XL（3.7B），google/mt5-XXL(13B)。thudm/GLMGitHub - THUDM/GLM: GLM (General Language Model)https://github.com/THUDM/ChatGLM-6B为前缀语言模型（类似UniLM），在NLU和NLG任务上都有SOTA的效果。中文开源的有10B的版本，以及加入指令微调以及人工反馈强化学习后的ChatGLM-6B。Decoder-only架构IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese https://huggingface.co/IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese基于悟道数据集预训练的中文版的GPT2，参数量3.5BTsinghuaAI/CPM-Generate https://huggingface.co/TsinghuaAI/CPM-Generate基于100G数据（新闻，网页，对话等）训练的中文预训练模型，参数量2.6BLangboat/bloom-1b4-zhhttps://huggingface.co/Langboat/bloom-1b4-zh基于bigscience/bloom-1b7 对词表进行了裁剪，保留中文部分。参数量1.4B。 该系列还包括Langboat/bloom-2b5-zh，Langboat/bloom-6b4-zh。LianjiaTech/BELLEhttps://github.com/LianjiaTech/BELLE基于BLOOMZ-7B1-mt进行了中文指令的调优，参数为7B[update] 开源的类ChatGPT大模型可以参见这里: nghuyong：LLM基座模型横向对比: 数据/模型/训练
642117003,4338c2384a026f9c1f430d81771f0e0a,刘聪NLP,https://api.zhihu.com/people/4338c2384a026f9c1f430d81771f0e0a,people,LLM is all you need！,如何评估大模型-LLMs的好坏？,article,https://api.zhihu.com/articles/642117003,我们的口号是“生命不止，学习不停”。 往期回顾 刘聪NLP：<em>大模型</em>时代-不进则退 刘聪NLP：大模型LLM-微调经验分享&amp;总结 刘聪NLP：ChatGPT-所见、所闻、所感,140,0,349,1688711792,1688955558,"写在前面大家好，我是刘聪NLP。今天刷到了一篇大型语言模型评估综述的Paper-《A Survey on Evaluation of Large Language Models》，只感叹，不仅大型语言模型卷的厉害，评估体系也是同步在卷。个人觉得这是个好事情，不同评估系统侧重点不用，可以充分挖掘LLM的潜在能力，从多种角度来分析LLM的优劣，但请不要以打榜为目的来训练LLM。Paper: https://arxiv.org/pdf/2307.03109.pdf Github: https://github.com/MLGroupJLU/LLM-eval-survey该篇综述主要汇总了大型语言模型在进行评估时，一般评估什么内容、在哪儿进行评估和如何评估。下面内容均参考该论文的Github，并且作者也表示会持续更新，请感兴趣的同学同步关注。评估什么内容Natural language processing「自然语言理解」「推理」 「自然语言生成」 「多语言任务」 「事实性」 Robustness, ethics, biases, and trustworthiness「鲁棒性」 「伦理与偏见」 「可信性」 Social scienceNatural science and engineering「数学」 「科学」 「工程」 Medical application「医学问答」 「医学考试」 「医学教育」 「医学助手」 Agent applicationsOther applications「教育」 「搜索推荐」 「性格测试」 「其他任务」 去哪儿进行评估如何进行评估自动评估人工评估欢迎多多转发，点赞，关注，有问题的朋友也欢迎加我微信「logCong」私聊，交个朋友吧，一起学习，一起进步。我们的口号是“生命不止，学习不停”。往期回顾刘聪NLP：大模型时代-不进则退刘聪NLP：大模型LLM-微调经验分享&总结刘聪NLP：ChatGPT-所见、所闻、所感刘聪NLP：中文NER数据集整理刘聪NLP：ACL2022 | DCSR：一种面向开放域段落检索的句子感知的对比学习方法刘聪NLP：ACL2022 | NoisyTune：微调前加入少量噪音可能会有意想不到的效果刘聪NLP：ACL2022论文分类汇总-Prompt、句子表征、检索排序&摘要刘聪NLP：总结|Prompt在NER场景的应用刘聪NLP：PERT：一种基于乱序语言模型的预训练模型刘聪NLP：SIGIR2022论文筛选刘聪NLP：常用预训练语言模型（PTMs）总结刘聪NLP：ACL2021论文之ChineseBERT：融合字形与拼音信息的中文预训练模型刘聪NLP：Sentence-Bert论文笔记"
660722382,cb470a6ed2def441c7d2fd36458a2c96,爱吃牛油果的璐璐,https://api.zhihu.com/people/cb470a6ed2def441c7d2fd36458a2c96,people,8年+推荐算法、开发、架构实战经验，AIGC、LLM技术研究,GraphRAG：知识图谱+大模型,article,https://api.zhihu.com/articles/660722382,chatgpt训练三步走阶段总结： 书籍推荐 <em>大模型</em>相关资料 爱吃牛油果的璐璐：大模型LLM推理框架整理 爱吃牛油果的璐璐：联邦+大语言模型案例 爱吃牛油果的璐璐：<em>大模型</em>中的思维链、思维树、思维图,105,7,466,1700535322,1703227591,"大模型局限性1、领域信息不足虽然 LLM 仅基于公开数据集进行训练，但这也意味着它缺乏领域特定或专有等非公开的信息。这可能导致在某些情况下， LLM 无法提供更准确或更全面的回答。2、有可能产生误导尽管 LLM 尽力根据已有的数据提供信息和答案，但在超出其范围的情况下，它可能会给出不正确或是虚构的信息。这是因为 LLM 无法直接获取新的信息或与实时数据同步。3、无法获取实时信息由于 LLM 的训练成本非常高，它无法实时更新其知识库。因此，LLM 可能无法提供最新的信息或跟上快速变化的情况。4、预训练数据不可更改LLM 使用的预训练数据可能包含错误或过时的信息，并且无法进行更正或删除。这意味着 LLM 可能会基于不准确或过时的数据进行回答。5、缺乏长期记忆LLM 设计的目标是根据输入的数据给出准确的回答，但它没有真正的长期记忆能力。这意味着 LLM 可能在处理复杂的问题或需要上下文理解的情况下表现不佳什么是Graph RAGRAG（Retrieval Argumented Generation）指的是通过 RAG 模型来对搜索结果进行增强的过程。具体来说，它是将检索技术和语言生成技术相结合来增强生成过程的一种技术，可以帮助传统搜索引擎生成更加准确、相关和多样化的信息来满足用户的需求。Graph RAG是一种基于知识图谱的检索增强技术，通过构建图模型的知识表达，将实体和关系之间的联系用图的形式进行展示，然后利用大语言模型 LLM进行检索增强。Graph RAG 将知识图谱等价于一个超大规模的词汇表，而实体和关系则对应于单词。通过这种方式，Graph RAG 在检索时能够将实体和关系作为单元进行联合建模。一个简单的 Graph RAG 思想在于，对用户输入的query提取实体，然后构造子图形成上下文，最后送入大模型完成生成。代码示例：def simple_graph_rag(query_str, nebulagraph_store, llm): entities = _get_key_entities(query_str, llm) graph_rag_context = _retrieve_subgraph_context(entities) return _synthesize_answer(query_str, graph_rag_context, llm)首先，使用LLM(或其他)模型从问题中提取关键实体。def _get_key_entities(query_str, llm=None ,with_llm=True): ... return _expand_synonyms(entities)其次，根据这些实体检索子图，深入到一定的深度，例如可以是2度甚至更多。def _retrieve_subgraph_context(entities, depth=2, limit=30): ... return nebulagraph_store.get_relations(entities, depth, limit)最后，利用获得的上下文利用LLM产生答案。def _synthesize_answer(query_str, graph_rag_context, llm): return llm.predict(PROMPT_SYNTHESIZE_AND_REFINE, query_str, graph_rag_context)传统检索增强技术的瓶颈：缺少训练数据，文本理解不足总结：图探索的 7 种方式图探索的方法 1：KG 基于向量的检索query_engine = kg_index.as_query_engine()这种方法通过向量相似性查找 KG 实体，获取连接的文本块，并选择性探索关系。是 LlamaIndex 基于索引构建的默认查询方式。它非常简单、开箱即用，不用额外的参数。图探索的方法 2：KG 基于关键词的检索kg_keyword_query_engine = kg_index.as_query_engine( # setting to false uses the raw triplets instead of adding the text from the corresponding nodes include_text=False, retriever_mode=""keyword"", response_mode=""tree_summarize"", )这个查询用了关键词来检索相关的 KG 实体，来获取连接的文本块，并选择性地探索关系以获取更多的上下文。而参数retriever_mode=""keyword"" 指定了本次检索采用关键词形式。include_text=False：查询引擎只用原生三元组进行查询，查询不包含对应节点的文本信息；response_mode=""tree_summarize""：返回结果（响应形式）是知识图谱的树结构的总结。这个树以递归方式构建，查询作为根节点，最相关的答案作为叶节点。tree_summarize 响应模式对于总结性任务非常有用，比如：提供某个话题的高度概括，或是回答某个需要考虑周全的问题。当然，它还可以生成更复杂的响应，比如：解释某个事物发生的真实原因，或者解释某个过程涉及了哪些步骤。图探索方法 3：KG 混合检索kg_hybrid_query_engine = kg_index.as_query_engine( include_text=True, response_mode=""tree_summarize"", embedding_mode=""hybrid"", similarity_top_k=3, explore_global_knowledge=True, )通过设定 embedding_mode=""hybrid""，指定查询引擎为基于向量的检索和基于关键词的检索二者的混合方式，从知识图谱中检索信息，并进行去重。KG 混合检索方式不仅使用关键词找到相关的三元组，它也使用基于向量的检索来找到基于语义相似性的相似三元组。所以，本质上，混合模式结合了关键词搜索和语义搜索，并利用这两种方法的优势来提高搜索结果的准确性和相关性。include_text=True：同上文的字段一样，用来指定是否包含节点的文本信息；similarity_top_k=3：Top K 设定，它将根据 Embedding 检索出最相似结果的前三个结果。你可以根据你的使用场景弹性地调整这个值；explore_global_knowledge=True：指定查询引擎是否要考虑知识图谱的全局上下文来检索信息。当设置 explore_global_knowledge=True时，查询引擎不会将其搜索限制在本地上下文（即，一个节点的直接邻居），而是会考虑知识图谱的更广泛的全局上下文。当你想检索与查询不直接相关，但在该知识图谱的更大上下文中有关的信息时，这可能很有用。基于关键词的检索和混合检索二者主要区别，在于我们从知识图谱中检索信息的方法：基于关键词的检索使用关键词方法，而混合检索使用结合 Embedding 和关键词的混合方法。图探索方法 4：原生向量索引检索vector_index = VectorStoreIndex.from_documents(wiki_documents + youtube_documents) vector_query_engine = vector_index.as_query_engine()这种方式完全不处理知识图谱。它基于向量索引，会先构建文档的向量索引，再从向量索引构建向量查询引擎。图探索方法 5：自定义组合查询引擎（KG 检索和向量索引检索的组合）from llama_index import QueryBundle from llama_index.schema import NodeWithScore from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever from typing import List class CustomRetriever(BaseRetriever): def __init__( self, vector_retriever: VectorIndexRetriever, kg_retriever: KGTableRetriever, mode: str = ""OR"", ) -> None: """"""Init params."""""" self._vector_retriever = vector_retriever self._kg_retriever = kg_retriever if mode not in (""AND"", ""OR""): raise ValueError(""Invalid mode."") self._mode = mode def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]: """"""Retrieve nodes given query."""""" vector_nodes = self._vector_retriever.retrieve(query_bundle) kg_nodes = self._kg_retriever.retrieve(query_bundle) vector_ids = {n.node.node_id for n in vector_nodes} kg_ids = {n.node.node_id for n in kg_nodes} combined_dict = {n.node.node_id: n for n in vector_nodes} combined_dict.update({n.node.node_id: n for n in kg_nodes}) if self._mode == ""AND"": retrieve_ids = vector_ids.intersection(kg_ids) else: retrieve_ids = vector_ids.union(kg_ids) retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids] return retrieve_nodes from llama_index import get_response_synthesizer from llama_index.query_engine import RetrieverQueryEngine from llama_index.retrievers import VectorIndexRetriever, KGTableRetriever # create custom retriever vector_retriever = VectorIndexRetriever(index=vector_index) kg_retriever = KGTableRetriever( index=kg_index, retriever_mode=""keyword"", include_text=False ) custom_retriever = CustomRetriever(vector_retriever, kg_retriever) # create response synthesizer response_synthesizer = get_response_synthesizer( service_context=service_context, response_mode=""tree_summarize"", ) custom_query_engine = RetrieverQueryEngine( retriever=custom_retriever, response_synthesizer=response_synthesizer, )LlamaIndex 构建了一个 CustomRetriever。如上所示，你可以看到它的具体实现。它用来进行知识图谱搜索和向量搜索。默认的 mode OR 保证了两种搜索结果的并集，结果是包含了这两个搜索方式的结果，且进行了结果去重：从知识图谱搜索（KGTableRetriever）获得的细节；从向量索引搜索（VectorIndexRetriever）获得的语义相似性搜索的详情。图探索方法 6：KnowledgeGraphQueryEngine到目前为止，我们已经探索了使用 KG 索引构建的不同查询引擎。现在，来看看另一个由 LlamaIndex 构建的知识图谱查询引擎——KnowledgeGraphQueryEngine。看下面的代码片段：query_engine = KnowledgeGraphQueryEngine( storage_context=storage_context, service_context=service_context, llm=llm, verbose=True, )KnowledgeGraphQueryEngine 是一个可让我们用自然语言查询知识图谱的查询引擎。它使用 LLM 生成 Cypher 查询语句，再在知识图谱上执行这些查询。这样，我们可以在不学习 Cypher 或任何其他查询语言的情况下查询知识图谱。KnowledgeGraphQueryEngine 接收 storage_context，service_context 和 llm，并构建一个知识图谱查询引擎，其中 NebulaGraphStore 作为 storage_context.graph_store。图探索方法 7：KnowledgeGraphRAGRetrieverKnowledgeGraphRAGRetriever 是 LlamaIndex 中的一个 RetrieverQueryEngine，它在知识图谱上执行 Graph RAG 查询。它接收一个问题或任务作为输入，并执行以下步骤：使用关键词在知识图谱中提取或 Embedding 搜索相关实体；从知识图谱中获取那些实体的子图，默认深度为 2；基于子图构建上下文。一个下游任务，如：LLM，可以使用这个上下文生成一个反馈。看下下面的代码片段是如何构建一个 KnowledgeGraphRAGRetriever：graph_rag_retriever = KnowledgeGraphRAGRetriever( storage_context=storage_context, service_context=service_context, llm=llm, verbose=True, ) kg_rag_query_engine = RetrieverQueryEngine.from_args( graph_rag_retriever, service_context=service_context )7 个查询引擎的优点和缺点chatgpt训练三步走阶段总结：书籍推荐大模型相关资料爱吃牛油果的璐璐：大模型LLM推理框架整理爱吃牛油果的璐璐：联邦+大语言模型案例爱吃牛油果的璐璐：大模型中的思维链、思维树、思维图爱吃牛油果的璐璐：如何从头训练一个大模型爱吃牛油果的璐璐：LLM大语言模型能力与开源框架选型参考资料：NebulaGraph：https://github.com/vesoft-inc/nebulaGraph RAG LlamaIndex Workshop：https://colab.research.google.com"
656646499,17636fdafe0e68df8736c2e242edce1a,产品经理大群,https://api.zhihu.com/people/17636fdafe0e68df8736c2e242edce1a,people,点赞就好，十年AI策略产品经验 持续分享 公众号同名,小白入门大模型：LangChain,article,https://api.zhihu.com/articles/656646499,Langchain 竞品 （个人认为）在商业化上，基于<em>大模型</em>业务分为三个层次： 基础设施层：通用的<em>大模型</em>底座 垂直领域层：基于大模型底座+领域场景数据微调形成更强垂直能力 应用层,174,6,585,1694852165,1711261359,"自己学习了，分享一下给大家1、LangChain介绍LangChain 就是一个 LLM 编程框架，你想开发一个基于 LLM 应用，需要什么组件它都有，直接使用就行；甚至针对常规的应用流程，它利用链(LangChain中Chain的由来)这个概念已经内置标准化方案了。下面我们从新兴的大语言模型（LLM）技术栈的角度来看看为何它的理念这么受欢迎。其官方的定义 LangChain是一个基于语言模型开发应用程序的框架。它可以实现以下应用程序：数据感知：将语言模型连接到其他数据源自主性：允许语言模型与其环境进行交互 LangChain的主要价值在于：组件化：为使用语言模型提供抽象层，以及每个抽象层的一组实现。组件是模块化且易于使用的，无论您是否使用LangChain框架的其余部分。现成的链：结构化的组件集合，用于完成特定的高级任务 现成的链使得入门变得容易。对于更复杂的应用程序和微妙的用例，组件化使得定制现有链或构建新链变得更容易。新兴 LLM 技术栈大语言模型技术栈由四个主要部分组成：数据预处理流程（data preprocessing pipeline）嵌入端点（embeddings endpoint ）+向量存储（vector store）LLM 终端（LLM endpoints）LLM 编程框架（LLM programming framework）数据预处理流程该步骤包括与数据源连接的连接器（例如S3存储桶或CRM）、数据转换层以及下游连接器（例如向矢量数据库）。通常，输入到LLM中的最有价值的信息也是最难处理的（如PDF、PPTX、HTML等），但同时，易于访问文本的文档（例如.DOCX）中也包含用户不希望发送到推理终端的信息（例如广告、法律条款等）。因为涉及的数据源繁杂（数千个PDF、PPTX、聊天记录、抓取的HTML等），这步也存在大量的 dirty work，使用OCR模型、Python脚本和正则表达式等方式来自动提取、清理和转换关键文档元素（例如标题、正文、页眉/页脚、列表等），最终向外部以API的方式提供JSON数据，以便嵌入终端和存储在向量数据库中。嵌入端点和向量存储使用嵌入端点（用于生成和返回诸如词向量、文档向量等嵌入向量的 API 端点）和向量存储（用于存储和检索向量的数据库或数据存储系统）代表了数据存储和访问方式的重大演变。以前，嵌入主要用于诸如文档聚类之类的特定任务，在新的架构中，将文档及其嵌入存储在向量数据库中，可以通过LLM端点实现关键的交互模式。直接存储原始嵌入，意味着数据可以以其自然格式存储，从而实现更快的处理时间和更高效的数据检索。此外，这种方法可以更容易地处理大型数据集，因为它可以减少训练和推理过程中需要处理的数据量。LLM终端LLM终端是接收输入数据并生成LLM输出的终端。LLM终端负责管理模型的资源，包括内存和计算资源，并提供可扩展和容错的接口，用于向下游应用程序提供LLM输出。LLM编程框架LLM编程框架提供了一套工具和抽象，用于使用语言模型构建应用程序。在现代技术栈中出现了各种类型的组件，包括：LLM提供商、嵌入模型、向量存储、文档加载器、其他外部工具（谷歌搜索等），这些框架的一个重要功能是协调各种组件。关键组件解释PromptsPrompts用来管理 LLM 输入的工具，在从 LLM 获得所需的输出之前需要对提示进行相当多的调整，最终的Promps可以是单个句子或多个句子的组合，它们可以包含变量和条件语句。Chains是一种将LLM和其他多个组件连接在一起的工具，以实现复杂的任务。Agents是一种使用LLM做出决策的工具，它们可以执行特定的任务并生成文本输出。Agents通常由三个部分组成：Action、Observation和Decision。Action是代理执行的操作，Observation是代理接收到的信息，Decision是代理基于Action和Observation做出的决策。Memory是一种用于存储数据的工具，由于LLM 没有任何长期记忆，它有助于在多次调用之间保持状态。典型应用场景特定文档的问答：从Notion数据库中提取信息并回答用户的问题。聊天机器人：使用Chat-LangChain模块创建一个与用户交流的机器人。代理：使用GPT和WolframAlpha结合，创建一个能够执行数学计算和其他任务的代理。文本摘要：使用外部数据源来生成特定文档的摘要。Langchain 竞品（个人认为）在商业化上，基于大模型业务分为三个层次：基础设施层：通用的大模型底座垂直领域层：基于大模型底座+领域场景数据微调形成更强垂直能力应用层：基于前两者，瘦前端的方式提供多样化应用体验类似 LangChain 这种工具框架可以做到整合各个层能力，具备加速应用开发和落地验证的优势，因此也出现了很多竞争者。名称语言特点LangChainPython/JS优点：提供了标准的内存接口和内存实现，支持自定义大模型的封装。缺点：评估生成模型的性能比较困难。Dust.ttRust/TS优点：提供了简单易用的API，可以让开发者快速构建自己的LLM应用程序。缺点：文档不够完善。Semantic-kernelTypeScript优点：轻量级SDK，可将AI大型语言模型（LLMs）与传统编程语言集成在一起。缺点：文档不够完善。Fixie.aiPython优点：开放、免费、简单，多模态（images, audio, video...）缺点：PaaS平台，需要在平台部署Brancher AIPython/JS优点：链接所有大模型，无代码快速生成应用, Langchain产品）缺点：-LangChain 主体分为 6 个模块，分别是对（大语言）模型输入输出的管理、外部数据接入、链的概念、（上下文记忆）存储管理、智能代理以及回调系统，通过文档的组织结构，你可以清晰了解到 LangChain的侧重点，以及在大语言模型开发生态中对自己的定位。2、LLM输入输出管理Model I/O这部分包括对大语言模型输入输出的管理，输入环节的提示词管理（包含模板化提示词和提示词动态选择等），处理环节的语言模型（包括所有LLMs的通用接口，以及常用的LLMs工具；Chat模型是一种与LLMs不同的API，用来处理消息），输出环节包括从模型输出中提取信息。提示词管理提示模板 动态提示词=提示模板+变量，通过引入给提示词引入变量的方式，一方面保证了灵活性，一方面又能保证Prompt内容结构达到最佳系统设置了三种模板:固定模板不需要参数,单参数模板可以替换单个词,双参数模板可以替换两个词。 比如单参数模板可以提问""告诉一个形容词笑话""。运行时传入""有意思""作为参数,就变成""告诉一个有意思的笑话""。 双参数模板可以提问""某个形容词关于某内容的笑话""。运行时传入""有趣"",""鸡""作为参数,就变成""告诉一个有趣的关于鸡的笑话""。 通过设置不同模板和参数,一套系统就可以自动生成各种问题,实现智能对话与用户互动。聊天提示模板 聊天场景中，消息可以与AI、人类或系统角色相关联，模型应该更加密切地遵循系统聊天消息的指示。这个是对 OpenAI gpt-3.5-tubor API中role字段（role 的属性用于显式定义角色，其中 system 用于系统预设，比如”你是一个翻译家“，“你是一个写作助手”，user 表示用户的输入， assistant 表示模型的输出）的一种抽象，以便应用于其他大语言模型。SystemMessage对应系统预设，HumanMessage用户输入，AIMessage表示模型输出，使用 ChatMessagePromptTemplate 可以使用任意角色接收聊天消息。系统会先提示“我是一个翻译助手,可以把英文翻译成中文”这样的系统预设消息。 然后用户输入需要翻译的内容,比如“我喜欢大语言模型”。 系统会根据预先定义的对话模板,自动接受翻译语种和用户输入作为参数,并生成对应的用户输入消息。 最后,系统回复翻译结果给用户。通过定制不同对话角色和动态输入参数,实现了模型自动回复用户需求的翻译对话。其他 基于 StringPromptTemplate 自定义提示模板StringPromptTemplate将Prompt输入与特征存储关联起来(FeaturePromptTemplate)少样本提示模板（FewShotPromptTemplate）从示例中动态提取提示词✍️LLMsLLMs 将文本字符串作为输入并返回文本字符串的模型（纯文本补全模型），这里重点说下做项目尽量用异步的方式，体验会更好，下面的例子连续10个请求，时间相差接近5s。该系统使用语言模型为用户提供文本输出功能。代码测试了串行和并行请求两个模式。 串行模式一个一个发起10个请求,响应时间总计需要5秒。 并行模式通过asyncio模块同时发起10个请求任务。 tasks列表存放每个请求,await等待它们都结束。 与串行只有1.4秒响应时间相比,并行模式大大提升了效率。缓存 如果多次请求的返回一样，就可以考虑使用缓存，一方面可以减少对API调用次数节省token消耗，一方面可以加快应用程序的速度。首先定义一个内存缓存,用来临时保存请求结果。 然后对语言模型发送同一条""告诉我一个笑话""的请求两次。 第一次请求耗时2.18秒,结果和响应通过缓存保存起来。 第二次请求就直接从缓存中获取结果。流式传输功能,能即时逐字返回生成内容,还原聊天过程回调追踪token使用,了解模型耗费情况ChatModel支持聊天消息作为输入,生成回应完成对话配置管理可以读取保存LLM设置,方便重复使用提供模拟工具代替真实模型,在测试中减少成本与其他AI基础设施无缝融合输出解析器输出解析器用于构造大语言模型的响应格式，具体通过格式化指令和自定义方法两种方式。需要对机器人回复进行结构化处理。代码测试了两种输出解析方式。 一是使用内置的解析器,识别指定格式如逗号分隔列表的输出。 二是通过定义ResponseSchema来定制输出结构,比如需要包含答案和来源两个字段。 它会提示模型按照格式要求进行回复,比如“答案,来源”。 解析器可以分析模型输出,判断是否符合预期结构。 smallCoder明白了,输出解析器可以规范化模型响应,一个是使用内置规则,一个可以全自定义,都有助于结构化对话与后续处理。Data Connection打通外部数据的管道，包含文档加载，文档转换，文本嵌入，向量存储几个环节。 文档加载重点包括了csv（CSVLoader），html（UnstructuredHTMLLoader），json（JSONLoader），markdown（UnstructuredMarkdownLoader）以及pdf（因为pdf的格式比较复杂，提供了PyPDFLoader、MathpixPDFLoader、UnstructuredPDFLoader，PyMuPDF等多种形式的加载引擎）几种常用格式的内容解析，但是在实际的项目中，数据来源一般比较多样，格式也比较复杂，重点推荐按需去查看与各种数据源 集成的章节说明，Discord、Notion、Joplin，Word等数据源。文档拆分重点关注按照字符递归拆分的方式 RecursiveCharacterTextSplitter ，这种方式会将语义最相关的文本片段放在一起。文本嵌入嵌入包含两个方法，一个用于嵌入文档，接受多个文本作为输入；一个用于嵌入查询，接受单个文本。文档中示例使用了OpenAI的嵌入模型text-embedding-ada-002，但提供了很多第三方嵌入模型集成可以按需查看。需要根据文本内容进行相似匹配查找。它利用了语言嵌入技术来实现。 首先定义好嵌入模型,这里使用OpenAI提供的文本嵌入模型。 然后有两种方法可以获取文本向量: 传入多篇文本,同时获取所有文本的嵌入向量表示。 仅传入单篇文本,获取其嵌入向量。 嵌入向量可以用于计算文本间的相似程度,从而实现内容查找。向量存储这个就是对常用矢量数据库（FAISS，Milvus，Pinecone，PGVector等）封装接口的说明，详细的可以前往嵌入专题查看。大概流程都一样：初始化数据库连接信息——>建立索引——>存储矢量——>相似性查询，下面以 Pinecone为例：文本搜索系统需要对大量文档进行索引,以实现相关性搜索。 它首先使用文本加载器读取文本内容,然后用分词器将长文本分割成短语。 接着调用嵌入模型为每段文本生成向量表示。 系统利用Pinecone这类向量数据库创建索引,并存入所有文本向量。 后续只需传入查询词语,调用相似性搜索接口,就可以快速找到与查询最相关的文本片段。数据查询这节重点关注数据压缩，目的是获得相关性最高的文本带入prompt上下文，这样既可以减少token消耗，也可以保证LLM的输出质量。问答系统需要从大量文本快速检索与用户问题相关的内容。 它先采用向量检索技术获取初步文档,然后利用LLM迭代提取相关段落进行数据压缩。 另外,系统也可以在压缩结果上再进行向量相似度过滤,进一步优化结果。 同时,为提升效率,系统还实现了基于结构化metadata和概要进行主动查询,而不是索引所有文本内容。针对基础检索得到的文档再做一次向量相似性搜索进行过滤，也可以取得不错的效果。最后一点就是自查询（SelfQueryRetriever）的概念，其实就是结构化查询元数据，因为对文档的元信息查询和文档内容的概要描述部分查询效率肯定是高于全部文档的。MemoryChain和Agent是无状态的，只能独立地处理每个传入的查询，Memory 可以管理和操作历史消息。一个带存储的Agent例子如下：一个基于问答历史记录的聊天agent。 它首先定义了搜索工具,并使用ZeroShotAgent生成 prompts。 然后创建一个ConversationBufferMemory对象保存历史消息。 将agent、搜索工具和内存对象封装成AgentExecutor。 每次处理用户问题时,都会查询内存中的历史记录,充当聊天上下文。 同时问题响应也会添加到内存,形成持续互动。3、数据接入层Data Connection打通外部数据的管道，包含文档加载，文档转换，文本嵌入，向量存储几个环节。 文档加载重点包括了csv（CSVLoader），html（UnstructuredHTMLLoader），json（JSONLoader），markdown（UnstructuredMarkdownLoader）以及pdf（因为pdf的格式比较复杂，提供了PyPDFLoader、MathpixPDFLoader、UnstructuredPDFLoader，PyMuPDF等多种形式的加载引擎）几种常用格式的内容解析，但是在实际的项目中，数据来源一般比较多样，格式也比较复杂，重点推荐按需去查看与各种数据源 集成的章节说明，Discord、Notion、Joplin，Word等数据源。文档拆分重点关注按照字符递归拆分的方式 RecursiveCharacterTextSplitter ，这种方式会将语义最相关的文本片段放在一起。文本嵌入嵌入包含两个方法，一个用于嵌入文档，接受多个文本作为输入；一个用于嵌入查询，接受单个文本。文档中示例使用了OpenAI的嵌入模型text-embedding-ada-002，但提供了很多第三方嵌入模型集成可以按需查看。需要根据文本内容进行相似匹配与提取。它利用多种NLP技术来实现。 首先使用递归字符拆分器将长文本根据语义语境分割成短段。 然后采用文本嵌入模型,为每个文本和查询获得其向量表示。 嵌入模型可以为系统提供多篇文本和单篇文本的向量,为后续匹配检索打下基础。 系统通过计算和比对这些向量,可以找到与查询最相似的文本片段。向量存储这个就是对常用矢量数据库（FAISS，Milvus，Pinecone，PGVector等）封装接口的说明，详细的可以前往嵌入专题查看。大概流程都一样：初始化数据库连接信息——>建立索引——>存储矢量——>相似性查询，下面以 Pinecone为例：问答系统需要对大量文本进行索引,实现快速相关搜索。 它先用文本加载器读取文本,字符级分词器切分为短段。 然后调用嵌入模型为每段文本生成矢量表示。 利用Pinecone向量数据库创建索引,将所有文本向量存入。 只需传入查询,调用相似搜索接口,即可快速找到与查询最相关的文本段。数据查询这节重点关注数据压缩，目的是获得相关性最高的文本带入prompt上下文，这样既可以减少token消耗，也可以保证LLM的输出质量。问答系统需要从大量文档快速检索与询问相关内容。 它先通过向量检索获取初步文档,然后利用深度学习模型从中取出最相关文本进行数据压缩。 同时,系统还采用向量相似度过滤进一步优化检索结果。 此外,通过建立在结构化元数据和概要上的主动查询,而非全文匹配,也可以提升检索效率。针对基础检索得到的文档再做一次向量相似性搜索进行过滤，也可以取得不错的效果。最后一点就是自查询（SelfQueryRetriever）的概念，其实就是结构化查询元数据，因为对文档的元信息查询和文档内容的概要描述部分查询效率肯定是高于全部文档的。4、Embedding专题文本嵌入是什么向量是一个有方向和长度的量，可以用数学中的坐标来表示。例如，可以用二维坐标系中的向量表示一个平面上的点，也可以用三维坐标系中的向量表示一个空间中的点。在机器学习中，向量通常用于表示数据的特征。而文本嵌入是一种将文本这种离散数据映射到连续向量空间的方法，嵌入技术可以将高维的离散数据降维到低维的连续空间中，并保留数据之间的语义关系，从而方便进行机器学习和深度学习的任务。例如： ""机器学习""表示为 [1,2,3]""深度学习""表示为[2,3,3]""英雄联盟""表示为[9,1,3] 使用余弦相似度（余弦相似度是一种用于衡量向量之间相似度的指标，可以用于文本嵌入之间的相似度）在计算机中来判断文本之间的距离： “机器学习”与“深度学习”的距离： ""机器学习”与“英雄联盟“的距离""： “机器学习”与“深度学习”两个文本之间的余弦相似度更高，表示它们在语义上更相似。 文本嵌入算法文本嵌入算法是指将文本数据转化为向量表示的具体算法，通常包括以下几个步骤：分词：将文本划分成一个个单词或短语。构建词汇表：将分词后的单词或短语建立词汇表，并为每个单词或短语赋予一个唯一的编号。计算词嵌入：使用预训练的模型或自行训练的模型，将每个单词或短语映射到向量空间中。计算文本嵌入：将文本中每个单词或短语的向量表示取平均或加权平均，得到整个文本的向量表示。常见的文本嵌入算法包括 Word2Vec、GloVe、FastText 等。这些算法通过预训练或自行训练的方式，将单词或短语映射到低维向量空间中，从而能够在计算机中方便地处理文本数据。文本嵌入用途文本嵌入用于测量文本字符串的相关性，通常用于：搜索（结果按与查询字符串的相关性排序）聚类（其中文本字符串按相似性分组）推荐（推荐具有相关文本字符串的项目）异常检测（识别出相关性很小的异常值）多样性测量（分析相似性分布）分类（其中文本字符串按其最相似的标签分类）使用文本嵌入模型可以使用 HuggingFace上能够处理文本嵌入的开源模型，例如：uer/sbert-base-chinese-nli问答系统利用文本嵌入技术将输入内容转换为数字向量表示。 它使用了开源库SentenceTransformer,选择预训练模型'uer/sbert-base-chinese-nli'进行中文文本向量化。 这个模型可以高效对一批文本进行编码,输出每个文本的嵌入向量。 系统也可以使用OpenAI提供的文本嵌入API,选用text-embedding-ada-002模型进行处理。 OpenAI的API支持多种预训练模型,不同模型在处理效果和性能上会有差异。 通过文本向量化,系统可以实现内容的深层理解,比如对文本进行分类、相似度计算等,为后续问答提供技术支撑。使用之前介绍的 OpenAI 文本嵌入API 可以将文本转换为向量，OpenAI API提供了多个文本嵌入模型，这篇博客对它们的性能进行了比较，这里是性能最好的text-embedding-ada-002说明： 模型名称价格分词器最大输入 token输出text-embedding-ada-002$0.000/1k tokenscl100k_base81911536支持文本嵌入的其他模型nghuyong/ernie-3.0-nano-zhshibing624/text2vec-base-chineseGanymedeNil/text2vec-large-chinesemoka-ai/m3e-base用于句子、文本和图像嵌入的Python库矢量数据库为了快速搜索多个矢量，建议使用矢量数据库，下面是一些可选的矢量数据库： Pinecone，一个完全托管的矢量数据库Weaviate，一个开源的矢量搜索引擎Redis作为矢量数据库Qdrant，一个矢量搜索引擎Milvus，一个为可扩展的相似性搜索而构建的矢量数据库Chroma，一个开源嵌入式商店Typesense，快速的开源矢量搜索引擎Zilliz，数据基础设施，由Milvus提供技术支持FAISS 是Meta开源的用于高效搜索大规模矢量数据集的库性能优化：和传统数据库一样，可以使用工程手段优化矢量数据库搜索性能，最直接的就是更新索引算法 ，对索引数据进行分区优化。平面索引（FLAT）：将向量简单地存储在一个平面结构中，最基本的向量索引方法。欧式距离（Euclidean Distance）余弦相似度（Cosine Similarity）分区索引（IVF）：将向量分配到不同的分区中，每个分区建立一个倒排索引结构，最终通过倒排索引实现相似度搜索。欧式距离（Euclidean Distance）余弦相似度（Cosine Similarity）量化索引（PQ）：将高维向量划分成若干子向量，将每个子向量量化为一个编码，最终将编码存储在倒排索引中，利用倒排索引进行相似度搜索。欧式距离（Euclidean Distance）汉明距离（Hamming Distance）HNSW (Hierarchical Navigable Small World)：通过构建一棵层次化的图结构，从而实现高效的相似度搜索。内积（Inner Product）欧式距离（Euclidean Distance）NSG (Navigating Spreading-out Graph)：通过构建一个分层的无向图来实现快速的相似度搜索。欧式距离（Euclidean Distance）Annoy (Approximate Nearest Neighbors Oh Yeah)：通过将高维空间的向量映射到低维空间，并构建一棵二叉树来实现高效的近似最近邻搜索。欧式距离（Euclidean Distance）曼哈顿距离（Manhattan Distance）LSH (Locality-Sensitive Hashing)：通过使用哈希函数将高维的向量映射到低维空间，并在低维空间中比较哈希桶之间的相似度，实现高效的相似度搜索。内积（Inner Product）欧式距离（Euclidean Distance）5、Chain模块Chain链定义链定义为对组件的一系列调用，也可以包括其他链，这种在链中将组件组合在一起的想法很简单但功能强大，极大地简化了复杂应用程序的实现并使其更加模块化，这反过来又使调试、维护和改进应用程序变得更加容易。 Chain基类是所有chain对象的基本入口，与用户程序交互，处理用户的输入，准备其他模块的输入，提供内存能力，chain的回调能力，其他所有的 Chain 类都继承自这个基类，并根据需要实现特定的功能。Chain定义和自定义Chain的主要作用: Chain定义了一系列组件的调用顺序,可以包含子Chain,实现复杂应用的模块化。 Chain基类是所有Chain对象的起点,处理输入、输出、内存和回调等功能。 自定义Chain需要继承Chain基类,实现_call/_acall方法定义调用逻辑。 例如,根据prompt生成文字会使用语言模型生成响应,同时支持回调处理。 Chain支持同步和异步调用,内部组件也可以通过回调进行交互。 这极大简化了应用开发,通过组合小组件构建复杂系统。 模块化设计也促进了代码维护、扩展和重用等。继承Chain的子类主要有两种类型：通用工具 chain: 控制chain的调用顺序， 是否调用，他们可以用来合并构造其他的chain。 专门用途 chain: 和通用chain比较来说，他们承担了具体的某项任务，可以和通用的chain组合起来使用，也可以直接使用。有些 Chain 类可能用于处理文本数据，有些可能用于处理图像数据，有些可能用于处理音频数据等。从 LangChainHub 加载链LangChainHub 托管了一些高质量Prompt、Agent和Chain，可以直接在langchain中使用。LangChainHub加载链主要作用: LangChainHub提供了量大质精的Prompt、Agent和Chain模板,可以直接在langchain中使用。 例如加载""math""链,它是一个基于LLM实现的计算器链。运行 LLM 链的五种方式运行LLM链的主要作用: LangChainHub提供了量大质精的Prompt、Agent和Chain模板,可以直接在langchain中使用。 例如加载""math""链,它是一个基于LLM实现的计算器链。 LLM链可以通过5种方式运行: 1.调用对象 2. run方法 3. apply方法 generate方法 5. predict方法 它们接受格式不同的输入参数,但都可以输出响应结果。 这大大简化了开发流程 - 直接使用高质量模板,只需少量代码即可构建应用。 运行LLM链也提供便利的接口。通用工具chain以下概述几种通用工具链的主要功能:MultiPromptChain通过Embedding匹配选择最相关提示回答问题。EmbeddingRouterChain使用嵌入判断选择下一链。LLMRouterChain使用LLM判断选择下一链。SimpleSequentialChain/SequentialChain将多个链组成流水线传递上下文。TransformChain通过自定义函数转换输入,动态改变数据流向。它们提供了动态路由、数据转换、链串联等功能,大大提升了链的灵活性。例如,通过TransformChain截取文本,SequentialChain联合链对文本进行摘要。这些通用工具chains可以帮助开发者快速构建复杂应用,实现不同业务需求。只需进行少量定制就可以满足个性需求,真正实现“取而代之”的开发模式。 合并文档的链（专门用途chain）BaseCombineDocumentsChain 有四种不同的模式以下概括合并文档链的主要功能: 该链专门用于问答场景下从多篇文档中提取答案。 它提供四种模式来合并多个文档: 1. stuff模式:直接将所有文档合成一段文字 2. map_reduce模式:每个文档提取答案候选,最后整合答案 3. map_rerank模式:每个文档提取答案候选,通过LLM进行重新排序 4. refine模式:首先用LLM优化每个文档,然后进行上述处理 开发者只需选择合适模式,便可以搭建出问答链。 四种模式分别注重不同因素,如效率、质量等。 这大大简化了定制问答类应用的难度。StuffDocumentsChain获取一个文档列表，带入提示上下文，传递给LLM（适合小文档）StuffDocumentsChain的主要功能是: 接受一个包含多个文档的列表作为输入 将所有文档使用特定分割符连接成一个长文本 将长文本通过prompt模板格式化,传入到LLM生成响应 它适用于文档数量和大小比较小的场景。RefineDocumentsChain在Studff方式上进一步优化，循环输入文档并迭代更新其答案，以获得最好的最终结果。具体做法是将所有非文档输入、当前文档和最新的中间答案组合传递给LLM。（适合LLM上下文大小不能容纳的小文档）系统需从多个文档中为用户提供准确答案。 它先采用修正链模式: 将问题和当前文档及初步答案输入语言模型校验 模型根据上下文回复更精准答案 系统循环每篇文档,迭代优化答案质量 结束后获得最优答案输出给用户MapReduceDocumentsChain将LLM链应用于每个单独的文档（Map步骤），将链的输出视为新文档。然后，将所有新文档传递给单独的合并文档链以获得单一输出（Reduce步骤）。在执行Map步骤前也可以对每个单独文档进行压缩或合并映射，以确保它们适合合并文档链；可以将这个步骤递归执行直到满足要求。（适合大规模文档的情况）为提升问答效率,系统采取如下流程: 将问题输入每个文件,语言模型给出初步答案 系统自动收集各文件答案,形成新文件集 将答案文件集输入语言模型,整合产出最终答案 如文件体积过大,还可分批处理缩小文件体积 通过分布式“映射-规约”模式,系统高效 parallel 地从海量文件中找到答案: 映射工作使每个文件独立处理,提高效率; 规约环节再整合答案,精度不恨损失。 这样既利用了分布技术,又保证了答案质量,帮助用户快速解决问题。MapRerankDocumentsChain每个文档上运行一个初始提示，再给对应输出给一个分数，返回得分最高的回答。为能从海量文件中快速给出最好答案,系统采用“映射-重新排序”流程: 将问题输入每个文件,语言模型给出多个答案候选 系统针对每个候选答案给出一个匹配分数 按分数从高到低重新排序所有文件答案 取排在前列的高分答案作为最优答案 这使系统从每个文件中高效挖掘答案,并利用匹配度快速定位最好答案。获取领域知识的链（专门用途chain）APIChain使得可以使用LLMs与API进行交互，以检索相关信息。通过提供与所提供的API文档相关的问题来构建链。 下面是与播客查询相关的为帮助用户更全面解答问题,系统利用外部API获取关联知识: 系统将问题输入到基于API文档构建的链中 链自动调用外部播客API搜索相关节目 根据用户需求筛选超30分钟的一条播客结果 系统返回相关节目信息,丰富了解答内容 通过对接第三方API,系统解答能力得到极大扩展: API提供传输知识,系统转化为答案; 用户只需简单问题,即可获取相关外部资源。 这有效弥补了单一模型不足,助用户全面解决问题需求。合并文档的链的高频使用场景举例对话场景（最广泛）ConversationalRetrievalChain 对话式检索链的工作原理：将聊天历史记录（显式传入或从提供的内存中检索）和问题合并到一个独立的问题中，然后从检索器查找相关文档，最后将这些文档和问题传递给问答链以返回响应。用户在系统提问时,往往与之前对话上下文相关。 为给出最合理答复,系统采用以下对话式处理流程: 保存所有历史对话为内存 将新问题和历史对话合成一个问题 从大量文档中寻找相关段落 将答卷和问题输入语言模型 获得兼顾上下文的最佳答案 这样系统可以深入理解用户意图,结合历史信息给出情境化的响应。基于数据库问答场景通过数据库链将结构化数据连接到语言模型,实现问答功能。 此外,它还支持: 实体链接 知识图谱问答 文档分类聚类 对象检测机器翻译等总结场景对输入文本进行总结,比如提取关键信息并连接成简介。问答场景读取文件内容,利用文档搜索及联合链从多个文档中给出最佳答案。6、代理模块某些应用程序需要基于用户输入的对LLM和其他工具的灵活调用链。Agents为此类应用程序提供了灵活性。代理可以访问单一工具，并根据用户输入确定要使用的工具。代理可以使用多个工具，并使用一个工具的输出作为下一个工具的输入。主要有两种类型的代理：Plan-and-Execute Agents 用于制定动作计划；Action Agents 决定实施何种动作。Agents模块还包含配合代理执行的工具（代理可以执行的操作。为代理提供哪些工具在很大程度上取决于希望代理做什么）和工具包（一套工具集合，这些工具可以与特定用例一起使用。例如，为了使代理与SQL数据库进行交互，它可能需要一个工具来执行查询，另一个工具来检查表）。下面对不同的Agent类型进行说明CONVERSATIONAL_REACT_DESCRIPTION针对对话场景优化的代理为优化对话系统的智能化能力,langchain采用代理模式: 系统定义了不同工具函数,如搜索引擎查询接口 用户提问时,代理首先检查是否需要调用工具函数 如果调用搜索查询,就利用接口搜索答案 否则将问题通过语言模型进行自然对话 全过程与历史对话上下文保持一致 通过这种设计: 系统可以动态选择调用内外部功能 回答不仅限于语言模型,更加智能 用户获得更全面更自动化的对话体验Agent执行过程系统处理流程如下: 用户提问系统总人口数量时,系统开始分析 系统判断是否需要调用外部工具寻找答案 系统决定调用搜索工具,搜索“中国人口数量” 搜索结果告知中国2020人口数据及民族构成 系统分析搜索结果已得到需要答案 所以不需要调用其他工具,直接据搜索结果回复用户 整个流程从问题理解到答案回复都是逻辑清晰的CHAT_CONVERSATIONAL_REACT_DESCRIPTION 针对聊天场景优化的代理OpenAI Functions Agent这个是 LangChain对 OpenAI Function Call 的封装。关于 Function Calling的能力，可以看我这篇文章：OpenAI Function Calling 特性有什么用OpenAI Functions Agent的工作流程: 1. 用户提问,语言模型判断是否需要调用功能函数 2. 如果需要,调用定义好的Calculator函数 3. Calculator函数通过LLM计算公式结果 4. 将公式执行结果返回给语言模型 5. 语言模型将结果翻译成自然语言给用户 通过这种模式: - 语言模型可以调用外部函数完成更复杂任务 - 用户提问范围不限于纯对话,可以求解数学等问题 - whole process是一体化的,用户体验更好计划和执行代理计划和执行代理通过首先计划要做什么，然后执行子任务来实现目标。这个想法很大程度上受到BabyAGI的启发。计划与执行代理通过分步实现: 1. 首先使用聊天规划器对问题进行解析分解,得到执行计划 2. 计划内容可能包括调用不同工具函数获取子结果: - Search接口查询美国GDP - Calculator计算日本GDP - Calculator计算两国GDP差值 3. 然后执行器依次调用相关函数运行计划任务 4. 将子结果整合返回给用户 通过这种分层设计: - 规划器可以针对不同类型问题制定个性化计划 - 执行器负责统一调用运行子任务 - 用户问题可以一步到位高效解决ZERO_SHOT_REACT_DESCRIPTION给LLM提供一个工具名称列表，包括它们的效用描述以及有关预期输入/输出的详细信息。指示LLM在必要时使用提供的工具来回答用户给出的提示。指令建议模型遵循ReAct格式：思考、行动、行动输入、观察，下面是一个例子：这个流程描述了系统利用外部工具高效解答用户问题的体验过程: 1. 系统提前定义好各种常用工具名称和功能 2. 当用户提问时,系统会理解问题所需信息 3. 然后选择调用合适工具,明确工具名称和预期输出 4. 通过工具搜索相关数据,观察结果 5. 根据观察得出确切答案 6. 整个流程分步可追踪,结果一目了然其他AgentType.SELF_ASK_WITH_SEARCH：自我进行对话迭代的代理REACT_DOCSTORE：基于文档做ReAct的代理STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION：在聊天过程中接入工具性代理，相当于OpenAI Plugin7、Callback模块回调模块允许接到LLM应用程序的各个阶段，鉴于LLM的幻觉问题，这对于日志记录、监视、流式处理和其他任务非常有用，现在也有专用的工具Helicone，Arize AI等产品可用，下面我们开始看代码：自定义回调对象所有的回调对象都是基于这个基类来声明的系统定义了统一的回调处理基类: 基类定义了各种标准回调接口如开始/结束回调 这些回调适用于不同子模块如NLP模型/链/工具等 开发者只需扩展基类,实现自己的回调处理逻辑 然后在运行过程中注册这些回调对象 系统就能在各个节点调用对应的回调方法 例如: 开发日志记录回调类 extend 基类 注册到 chains/models 日志就能自动记录运行细节 这样设计优点很明显: 统一标准化回调定义 高扩展性,任何子模块均支持定制回调 开发维护成本低使用回调的两种方式构造函数时定义回调：在构造函数中定义，例如LLMChain(callbacks=[handler], tags=['a-tag'])，它将被用于对该对象的所有调用，并且将只针对该对象，例如，如果你向LLMChain构造函数传递一个handler，它将不会被附属于该链的Model使用。请求函数时传入回调：定义在用于发出请求的call()/run()/apply()方法中，例如chain.call(inputs, callbacks=[handler])，它将仅用于该特定请求，以及它所包含的所有子请求（例如，对LLMChain的调用会触发对Model的调用，Model会使用call()方法中传递的相同 handler）。下面这是采用构造函数定义回调的例子：两种方式的区别在于: - 构造函数回调作用于对象所有调用 - 请求函数回调只针对单次调用 两者都可以实现回调功能,选择时就看追求统一还是 situational 了。执行效果LLM调用开始.... Hi! I just woke up. Your llm is starting 同步回调被调用: token: 同步回调被调用: token: 好 同步回调被调用: token: 的 同步回调被调用: token: ， 同步回调被调用: token: 我 同步回调被调用: token: 来 同步回调被调用: token: 给 同步回调被调用: token: 你 同步回调被调用: token: 讲 同步回调被调用: token: 个 同步回调被调用: token: 笑 同步回调被调用: token: 话 同步回调被调用: token: ： 同步回调被调用: token: 有 同步回调被调用: token: 一 同步回调被调用: token: 天 同步回调被调用: token: ， 同步回调被调用: token: 小 同步回调被调用: token: 明 同步回调被调用: token: 上 同步回调被调用: token: 学 同步回调被调用: token: 迟 同步回调被调用: token: 到 同步回调被调用: token: 了 同步回调被调用: token: LLM调用结束.... Hi! I just woke up. Your llm is ending参考链接：LangChain指南：打造LLM的垂域AI框架一文了解：打造垂域的大模型应用ChatGPT爱吃牛油果的璐璐：（万字长文）手把手教你认识学会LangChain更多细节内容参考官方文档： ️ Langchain"
684657376,97912a6a0e8cce1699bfd7ecd35a301d,Neviimm,https://api.zhihu.com/people/97912a6a0e8cce1699bfd7ecd35a301d,people,计算机&amp;金融双硕士,金融突破检测大模型——BreakGPT,article,https://api.zhihu.com/articles/684657376,是一个真突破。 2.3 计算机领域：<em>大</em>语言<em>模型</em> 大语言模型是一种大规模的深度学习<em>模型</em>，它是AI领域中具有革命意义的研究成果。通常而言，<em>大</em>语言<em>模型</em>，例如Bert和GPT，使用复杂的神经网络,43,11,108,1709218751,1711696294,"简单分享一个我们最近针对金融领域大语言模型的工作：""BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection ""论文链接如下：BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection (arxiv.org)代码如下：Neviim96/BreakGPT: BreakGPT (github.com)一.背景基本面分析和技术面分析是金融投资领域的两个基本方法。在技术面分析中，突破交易法受到交易员的青睐，在股票、期货和外汇等金融市场中被广泛使用。然而，区分真假突破并提供正确的理由是该领域的重大挑战。最近，大语言模型在各种下游领域中取得了成功，但它们在金融突破检测领域的有效性处于较低水准。具体表现在三方面：1.缺乏专业知识；2.回答中出现错误，在该任务上的准确率低；3.模型输出不稳定，对于同一输出，反复输出会给出不同的判断结果。针对以上问题，我们引入了BreakGPT，具体贡献有以下三方面：这是第一个金融突破检测模型。它具有突破交易领域的专业知识。创新性地为该大型语言模型建立了一个新的框架，即多级结构（Multi-stage Structure），有效减少下游应用出现的错误并改善了输出的稳定性。在准确性方面，相比ChatGPT-3.5提升了44%，相比ChatGPT-4提升了42.07%。建立了第一个金融突破检测领域的数据集，它可以为后续研究提供帮助。BreakGPT的任务样例如下，在我们的任务中，输入是经过处理的数据表格 (金融产品足迹图的历史数据）和问题，输出是判断真假突破的结果和判断的依据。A sample fo the BreakGPT task二.相关知识由于本文涉及计算机和金融两个领域，为方便来自不同领域的读者理解，我们简单介绍一下涉及这两个领域的基本知识。2.1 金融领域：技术面交易技术面分析指的是以历史图表数据作为分析手段，从而预测市场价格未来走势的研究。在传统金融学家看来，金融产品的当前价格已经反应了市场供需关系，而未来的走势如同“随机漫步”，没有规律可循。在这个观点下，股价的波动是不可预测的。有效市场假说的出现论证了技术面分析在理论上的不可靠性，进一步挤压了技术面分析的生存空间。然而随着时间的流逝，有效市场假说的弊端逐渐彰显。有效市场假说前提是市场上的投资者都是理性人并且信息的变动可以很快反应在价格上。在现实世界中并非所有投资者都是理性的。此外，信息的传播速度也有限，无法迅速反应在股价上。由于有效市场假说的失效，技术面分析重新回到学者的视线里，并且验证了技术面分析在市场中的获利能力。2.2 金融领域：突破交易法突破交易法是技术面分析中的重要手段。它指的多空双方会在某一价格存在争夺，当价格突破争夺价位后，该价位就会形成有力支撑，保证该金融产品短时间内不会回落。在现实市场中，我们可以经常观测到一只股票会在一段价格区间波动，当该股票突破原先的价格区间后，它会保持继续上涨的态势。在使用突破交易技术时，我们需要判断该金融产品的突破方向和阻力位。突破方向是其历史走势，而阻力位则是买卖双方的争夺价位，它通常是历史价位的最高点或者最低点。此外，根据订单流，阻力位上下的买卖双方力量能预示着突破的真假性。例如对于一个向上的突破，如果在阻力位之上买方订单多于卖方订单，则该产品突破该价位后短时间内不会跌落，是一个真突破。2.3 计算机领域：大语言模型大语言模型是一种大规模的深度学习模型，它是AI领域中具有革命意义的研究成果。通常而言，大语言模型，例如Bert和GPT，使用复杂的神经网络，并通过海量文本数据训练数亿甚至上千亿参数量从而可以处理诸多自然语言任务。随着技术的发展，大语言模型对下游领域的影响业逐渐加深。他们会被微调并已经在医疗，教育，金融和自然科学等下游领域获得成功。2.4 计算机领域：思维链在大模型的应用中，模型答案出现错误的情况，而思维链(Chain of thought)是该问题解决方案之一. 思维链的核心在于让大语言模型在回答问题之前，先给出推理过程，再给出结果。在该领域开山之作中，学者通过手动设计了一些样例，让大语言模型学习样例中的推理过程，从而在回答所需问题时给出正确理由和答案。在此之后，学者们也在寻求新的范式，例如zero shot cot和 Auto-CoT. 然而思维链的使用仍会造成大语言模型回答错误的情况。对于这个问题，一些学者尝试通过引入新的模态（例如视觉）来解决。三.方法模型整体框架如下图所示： Overview of BreakGPT with Multi-stage structure模型采用Multi-stage structure，Multi-stage structure 有两部分组成: Part A: 任务链。 这是有多个特殊子任务组成的任务链，同时包含我们训练的大语言模型BreakGPT。任务链的作用是将整个任务分成多个子任务，逐个输出子任务的结果，而非一次性输出完整任务的结果。宏观来看，整个任务链的输入是模型的输入，即数据表格与问题提示语，任务链输出是各个特殊子任务的结果。Part B：Report generator。Report generator的作用是将任务链的各个输出拟合在一起，并给出真假突破的答案，从而最终输出带有合理理由和答案的报告。3.1 任务链任务链主要由三部分组成Stage 1(S1)task, Stage 2(S2) task, Stage 3(S3) task，在每个部分我们使用BreakGPT来获取任务的结果。 S1:判断整个突破的突破方向; S2: 识别突破的阻力位；S3: 对比阻力位之上或者之下的买卖双方力量。3.2 BreakGPTBreakGPT主要负责完成Stage1-3的任务，它可以分别输出用于判断真假的突破的关键依据。Base Model: BreakGPT的base model 采用gpt-3.5-turbo-1106, 它相比众多开源大模型有着更好的性能。我们使用API微调gpt-3.5。Training data: 为了训练BreakGPT,我们创建了第一个用于金融breakout检测的数据集。整个数据集包含S1, S2和S3三个任务，每个任务有10组训练数据，一共30组数据。在每个任务的10组训练数据中，真突破与假突破各5组。每组训练数据由system，user提问和assistant回应三部分组成。System指的是对模型的介绍。例如""BreakGPT is an expert in stock breakout detection."" User提问指的是需要让模型做的任务。他是在任务中，模型的输入.例如在S1任务中，User部分的content为经过处理的数据表格与针对S1任务的提问。 Assistant指的是模型对提问的答案，他是模型的输出。以S1 task举例而言，Assistant回应即为突破方向判断的依据与突破方向的答案。3.3 Report generatorReport generator是我们研究中的另一个大语言模型。他的功能在于聚合BreakGPT在各个子任务的答案，并输出专业的分析报告与真假突破的答案。他和breakGPT在功能上有本质的差别，所以我们进行单独训练。Base model: Report generator的base model与BreakGPT一致，都是选取gpt-3.5-turbo-1106，并使用API继续微调。Training data: Report generator的训练数据主要包含10组训练数据，每组数据与BreakGPT的格式一致，包含System, Assistant, User三部分。User主要涉及BreakGPT在S1-3的答案与报告生成的提示词。Assistant是根据S1-3的答案生成的突破分析报告以及真假突破的答案。这一部分是有人类标注的。我们通过专业人士的评估确保了标注的专业性，同时根据真实市场真假突破的信息保证了训练数据中真假突破的准确性。四.实验结果4.1 实验结果highlight实验结果highlights如下图： Result highlightsHighlight中，带有Multi-stage的BreakGPT的表现明显好于其他模型，其中包括ChatGPT-4。需要注意的是，在S2（突破位检测）任务上的提升尤为明显，而我们认为S2任务上的提升更能揭示模型的优越性。原因在于，S1与S3的答案的情况是十分有限的。例如S1（突破方向检测）的答案只有：向上或者向下或者无法检测。在这种情况下，S1与S3任务中，模型容易出现猜测导致准确率虚高的情况。4.2 主试验4.2.1 专业性对比 Output samples of LLMs for professional knowledge comparison. Green: Valuable domain knowledge. Red: Wrong domain knowledge. 上图是完整任务的输出结果。绿色部分是有价值的专业领域知识，红色部分是错误的专业领域知识。我们可以看的ChatGPT3.5和4的绿色部分较少，白色部分比较多，说明回答中有价值的信息较少。4.2.2 准确性对比Error bar. ChatGPT-3.5 VS BreakGPT由误差棒可见，在S1-S3任务中，BreakGPT的准确性明显高于ChatGPT-3.5。4.2.3 输出稳定性对比Output stability pie chart for main experiment输出稳定性我们主要用两方面来展示：1.反复输出答案的标准差。标准差越大，则输出越不稳定；2.输出稳定性饼图。稳定性饼图主要展示在样本10次反复输出中，10次输出完全一致情况的占比。占比（深蓝色）越大，模型输出稳定性越高，模型越可靠。从标准差来看，BreakGPT在3个任务上的标准差均显著小于ChatGPT-3.5。S1任务：0 VS 0.5676 ; S2任务 0.0771 VS 224.5708； S3任务： 0.0716 VS 0.6504.从稳定性饼图来看，结论与上述一致。以S1任务为例，BreakGPT中20组测试样本每组测试样本反复输出10次，10次结果完全一致的样本数是20，完全稳定的情况占比是100%，而在ChatGPT-3.5中，10次反复输出完全一致的测试样本仅为一列，占比仅为5%。4.3 消融实验消融实验主要对比带有Multi-stage的BreakGPT与不带有Multi-stage的BreakGPT在3个子任务上的表现，以彰显Multi-stage的作用。4.3.1 准确性对比Error bar. BreakGPT w/o multistage VS BreakGPT w/ multistage由误差棒可见，Multi-stage在S2任务上带来明显提升，而S2任务是3个子任务中最重要的任务。在S1与S3任务中，也减少了准确性的标准差。4.3.2 输出稳定性对比Output stability pie chart for ablation study从标准差来看，带有Multi-stage的BreakGPT在3个任务上的标准差均显著小于不带有Multi-stage的BreakGPT。S1任务：0 VS 0.68; S2任务： 0.0771 VS 0.6162； S3任务： 0.0716 VS 0.347.从稳定性饼图来看，结论与上述一致。以S2任务为例，BreakGPT中20组测试样本每组测试样本反复输出10次，10次结果完全一致的样本数是17，完全稳定的情况占比是85%，而在不带有Multi-satge的BreakGPT中，10次反复输出完全一致的测试样本仅为一列，占比仅为35%。"
695389300,36f69162230003d316d0b8a6d8da20ba,量子位,https://api.zhihu.com/people/36f69162230003d316d0b8a6d8da20ba,people,,神秘大模型一夜刷屏，能力太强被疑GPT4.5，奥特曼避而不答打哑谜,article,https://api.zhihu.com/articles/695389300,量子位 | 公众号 QbitAI 一夜之间，<em>大模型</em>话题王，再次易主。 一个神秘模型突然杀入众人视野，能力超越一众开源模型，甚至包括GPT-4。几乎所有人都在谈论它,133,20,79,1714441557,1714441563,梦晨 明敏 发自 凹非寺量子位 | 公众号 QbitAI一夜之间，大模型话题王，再次易主。一个神秘模型突然杀入众人视野，能力超越一众开源模型，甚至包括GPT-4。几乎所有人都在谈论它，服务器都被挤爆了。它就是“gpt2-chatbot”。（注意啊，是gpt2不是GPT-2）它有多强？IMO国际数学奥林匹克竞赛的题目，一次答对。一次在PyOpenGL中生成旋转3D立方体。直接输出如下结果：在GPT-4标志性能力“画独角兽”上，还能轻松秒杀LLaMA-3-70B（左）。推理方面更是表现出了惊艳效果，常见逻辑陷阱可以轻松绕过，而且回答时还带有合适的语气。提问：1公斤羽毛和1公斤铅块，哪个更重？回答：一公斤羽毛和一公斤铅的重量是一样的——它们都是一公斤。两者之间的关键区别在于它们的体积和密度:一公斤的羽毛会占据更多的空间，看起来更大，而一公斤的铅会紧凑而小。这通常会导致一个经典的问题，但就重量而言，一公斤就是一公斤!如此好的表现，但是又没明说身份……这不是更让大伙兴奋了！难道是传说中的GPT-4.5？要知道，名震江湖的AlphaGo就曾使用“Master”化名在网上大杀四方，轰动围棋界。这回的gpt2-chatbot，也有这感觉了。而且名字上也很值得深究，gpt2更像是gpt2.0，似乎是ChatGPT层级的版本迭代。奥特曼看热闹不嫌事大，还发帖说：我确实对gpt2情有独钟。现在，随着冲去试玩的人还在不断增加，试玩限制也在加大。具体咋样？我们已经上手实测。中文能力也很到位如果想亲手测试这只神秘AI，目前唯一已知方式是在LMSYS大模型竞技场。首先打开竞技场网页，进入Direct Chat，就可以在模型选项里找到gpt2-chatbot。要注意每人每天有8条消息的限制，全局也有每小时3000条消息的限制，所以测试机会非常有限。如果看到下面的错误提示，就只能去竞技场排位模式看运气能不能匹配到它了。只要抓到它一次，就可以继续多轮对话。在短暂的测试中，我们发现gpt2-chatbot中文能力也很到位。只要问题是中文的，无需特别强调就可以默认用中文回答，至少可以排除是Llama 3微调了。针对一个充满误导的经典问题，可以看出gpt2-chatbot的回答条理分明，仿佛自带CoT思维链提示（“让我们一步一步地想”），识别出了所有陷阱。并且准确提供了非常细节的知识，如北京到青岛距离、男子女子跳远世界纪录、农夫山泉在国内的价格等。而大多数其他AI模型，最多只能模糊的判断出15米超出人类能力，或按美元算矿泉水价格。那么这只超强神秘AI到底是何方神圣，我们也用破解GPTs的祖传手艺“拷问”了一把。OpenAI开发的GPT系列聊天机器人，那么系统提示词的开头不出意外应该是“You are ChatGPT……”，但为了防止它看到“ChatGPT”一词后产生幻觉，我们在问题中把ChatGPT去掉。清除所有上下文信息，再让它复述“前面的单词”，就会出现系统提示词了。果然，它自曝是由OpenAI训练的大模型，基于GPT-4架构，还可以接受图像输入。最关键一点在最后一部分“人格：v2”。并且gpt2-chatbot对这一问题的回答，在不同时间不同地点尝试都是一致的。另外如果尝试让它重复Claude系列以“The assistant is”开头的系统提示词，它也不会上当，会在开头后面重复一遍完整的问题。△这样答也不算错虽然就算这样也不能排除是幻觉的可能性，或非GPT模型使用了ChatGPT生成的数据微调，但至少是稳定的。神秘AI身份的几种主流猜测有网友组织了更详细的测试，有如下发现：它使用OpenAI的tokenizer，对OpenAI使用的特殊token有反应，且对Claude/Llama/Gemini使用的特殊token没有影响。当咨询紧急情况/法律相关问题时，它会给出OpenAI的联系方式。针对OpenAI模型的提示词注入攻击有效，且它从未声称自己来自OpenAI之外的组织。……基于以上种种信息，不少人猜测它就是匿名发布的GPT-4.5，或GPT-4原始版本经过不同的对齐训练。不过也有迹象表明，它可能是LMSYS组织基于2019年的GPT-2架构训练的模型。理由为最近发表的一篇论文声称，GPT-2在某些情况下比多个现代模型能力更强。并且这篇论文的作者之一与LMSYS的赞助商MBZUAI（阿联酋人工智能大学）相关。假设它确实是古老的GPT-2架构（只有1.5B参数），也有人怀疑可能是结合了OpenAI守口如瓶的Q*技术。最后一种猜测（狗头）便是失踪的OpenAI首席科学家Ilya Sutskever藏在里面了。最后，面对神秘新模型搅起来的种种风波，奥特曼本人还被发现来搅浑水，修改了他的推文细节。这样一下子，是OpenAI匿名发布新模型炒作的可能性更大了一些。试玩地址：https://chat.lmsys.org/参考链接：[1]https://twitter.com/i/trending/1785009023609397580[2]https://rentry.org/gpt2— 完 —@量子位 · 追踪AI技术和产品新动态深有感触的朋友，欢迎赞同、关注、分享三连վ'ᴗ' ի ❤
684833988,444ab03711bf1f324b6a10eb00da8a2b,ecosystem,https://api.zhihu.com/people/444ab03711bf1f324b6a10eb00da8a2b,people,努力成为一名民间科学家,大模型与物理模型的结合,article,https://api.zhihu.com/articles/684833988,有人喜欢笔记，有人喜欢记忆…… <em>大模型</em>是对已有信息的压缩，借此衍生出问题的答案。物理模型是对已有认识的压缩，借此推演出问题的答案。 <em>大模型</em>和物理模型的结合将构成人工智能——有记忆和推理能力的综合体,1,0,1,1709347827,1709368085,普通的历史教科书会罗列时间、地点和人物，似乎历史发展是由一些毫不相干的人在一夜之前完成的，人类的进步源于少量人的发明和壮举。当你有了些社会阅历就会发现，这种写法的问题：历史中的时间哪里去了？每次人类思想和技术的进步都始于对“时间”这个概念的新理解，牛顿在绝对时空观中解决无穷小概念，爱因斯坦在相对时空观中解决变与不变的关系。不要期待工业革命一夜之间到来，我们一直处于工业革命的历史中。不同人的思维对“数”和“象”的理解存在偏好：有人喜欢代数，有人喜欢几何；有人喜欢科技论文，有人喜欢文艺小说；有人喜欢笔记，有人喜欢记忆……大模型是对已有信息的压缩，借此衍生出问题的答案。物理模型是对已有认识的压缩，借此推演出问题的答案。大模型和物理模型的结合将构成人工智能——有记忆和推理能力的综合体。
631941039,9be855b4a2249a812f08e2bdd65e4cb7,AI时代社,https://api.zhihu.com/people/9be855b4a2249a812f08e2bdd65e4cb7,people,AI时代的自由探索者,stable diffusion 常用大模型解释和推荐（持续更新ing）,article,https://api.zhihu.com/articles/631941039,也就是<em>大模型</em>/底模型/主模型对于AI绘画的影响是最大的。后台也有很多小伙伴问我，有没有什么模型可以推荐的，我这边就给大家简单介绍几个我自己觉得还不错的<em>模型</em>。同时,313,34,898,1684936404,1684936404,"之前的文章里，我跟大家介绍了常见的模型以及详细的说明：AI时代社：Stable Diffusion 常用模型下载与说明（保姆级）从文章中我们可以知道，Checkpoint，也就是大模型/底模型/主模型对于AI绘画的影响是最大的。后台也有很多小伙伴问我，有没有什么模型可以推荐的，我这边就给大家简单介绍几个我自己觉得还不错的模型。同时，也给大家避避雷，有的模型又大又慢，就不用下载了。基础模型（不推荐）SD系列比如sd-v1-4、sd-v1-5、sd-v2（简写成SD1.5、SD2.0）之类的大模型，这些都是Stable-Diffusion自带的大模型，分别是1.4、1.5和2.0版本。目前的话1.5版本更好用一些，但是基本没有人会用自带的大模型，因为效果很差。但是，大家如果想自己练大模型的话，SD系列是不错的基础模型，因为他们比较庞杂，什么风格都有，属于中性模型。顺便说一句，大家在下载模型的时候，会看到pruned、emaonly（ema）之类的后缀，pruned是完整版，emaonly是剪枝版。剪枝版比完整版通常小很多，方便下载。如果只是使用的话，两者差别不大，如果是想要自己练模型的话，需要下载完整版。不过有的剪枝版也不带后缀，有的剪枝版后缀也是prune，目前大家命名相对还比较随意，你可以具体看相应的版本说明（一般来说完整版都比较大7G左右）。但如果只是自己使用的话，不用太在意这些。final-prune、animefull-latest系列NovelAI出的大模型，final-prune是剪枝版，animefull-latest是完整版，不自己练模型的话，没有太大差别。NovelAI的大模型已经比Stable-Diffusion自带的大模型好很多了，至少可以出图了。什么都能画，但是没有什么特色，也没有什么专长。属于中规中矩的中性基础模型。二次元模型Anything系列Anything系列是一个以二次元漫画为主打的模型，一直也比较火。版本也比较多，包括Anything-V3.5，Anything-V4.0，Anything-V4.5，Anything-V5.0等等，不同版本下还有各类变形版本和剪枝版本。推荐下载4.0或者更新的版本，不过目前4.0版本已经能做到非常不错的效果。（秋叶整合包是默认自带了Anything5.0版本）值得注意的是，Anything系列出图效果好，但是风格比较单一，对关键词的要求不太高。我随手用Anything-V4.0跑了一些图，大家可以看下它的风格。提示词：solo, 1girl, portrait, looking at viewer, masterpiece, best quality, 8k,反向提示词: (worst quality, low quality:1.4), (bad-image-v2-39000:0.75), (bad_prompt_v2:0.85), (censored, bar censor), cropped, mature,葡萄柚、Hassaku系列葡萄柚和Hassaku系列都拥有明亮、清晰的动漫风格的模型，都是同一个作者。葡萄柚模型相对小清新一些，Hassaku系列非常容易出NSFW的图像，请谨慎使用（不过，Hassaku在C站下载量非常高，懂的都懂）。不需要额外的VAE。建议带上""masterpiece""、""best quality""，负面提示词：(worst quality, low quality:1.4)。模型：hassaku提示词：masterpiece, best quality, 1girl, red eyes, white hair, mechanical legs, raincoat, black gloves, black coat, raining, sky, ruins, rifle, headset, rubble, android, long hair, halo, detailed eyes反向提示词：(worst quality, low quality:1.4),Cetus-Mix这是一个二次元的混合模型，融合了很多二次元的模型，实际使用效果还不错。对提示词的要求不高。提示词：1girl,cetus,coda,ocean反向提示词： lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry写实Chilloutmix/ChikmixChilloutmix就是大名鼎鼎的亚洲美女模型。市面上你看到的大量的AI美女，基本上都是这个模型生成的。当时最火的图应该下面这个系列。 也正是这个模型，让AI绘画彻底出圈。 发布不到1个月的时间，这个模型就已经拥有了11万的下载量。目前已经成为C站（https://civitai.com/）最火的模型。然而，由于有人利用该模型从事网络诈骗行为，已经有骗子被网警抓进去了，因此吓得该模型的作者删库跑路。后来该模型免费赠送给了C站。目前该模型在C站仍可以继续下载，为18+，但由于该模型争议较大，因此不建议大家轻易下载使用，如果要使用，也请谨慎使用，不要用于非法的目的。顺便说一句，C站需要科学上网，而且经常被屏蔽。最近有一个新模型，叫做chikmix，融合了Chilloutmix等多个模型，实际使用效果也还不错。Deliberate系列目前最新版本是deliberate_v2，这个模型是一个万能模型，可以画能任何你想要的东西。风格上偏油画和数绘风格。值得注意的是，这个模型的提示词必须非常详细地填写。提示词：end of the world, epic realistic, (hdr:1.4), (muted colors:1.4), apocalypse, freezing, abandoned, neutral colors, night, screen space refractions, (intricate details), (intricate details, hyperdetailed:1.2), artstation, cinematic shot, vignette, complex background, buildings, snowy反向提示词: poorly drawn提示词：a 42 yo milkman, smiling, (milk bar:1.2), (gray apron:0.9), cook hat, artstation, (epic realistic:1.2), (hdr:1.3), (dark shot:0.7), intricate details, [[rutkowski]], intricate, cinematic, detailed反向提示词: (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputationRealistic Vision系列Realistic Vision系列也是写实风格的模型，比较适合人物和动物，不过相对也比较万金油。目前最新版是realisticVisionV20。模型：realisticVisionV2.0提示词：b&w photo of 42 y.o man in black clothes, bald, face, half body, body, high detailed skin, skin pores, coastline, overcast weather, wind, waves, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3反向提示词: (semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck 模型：realisticVisionV2.0提示词：close up photo of a rabbit, forest, haze, halation, bloom, dramatic atmosphere, centred, rule of thirds, 200mm 1.4f macro shot反向提示词: (semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck2.5D/数绘风格PerfectWorld欧美版的Chilloutmix，主要绘制欧美风格的美女，不过偏2.5D，介于动漫和写实之间。产图基本默认NSFW，需要谨慎使用。国风系列这是一个中国华丽古风风格模型，也可以说是一个古风游戏角色模型，具有2.5D的质感。目前最新的版本是GuoFeng3.3，但是我还是更喜欢用3.2版本。模型：GuoFeng3.2提示词：best quality, masterpiece, highres, 1girl,blush,(seductive smile:0.8),star-shaped pupils, china hanfu, hair ornament, necklace, jewelry, Beautiful face,upon_body, tyndall effect, photorealistic, dark studio, rim lighting, two tone lighting,(high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, volumetric lighting, candid, Photograph, high resolution, 4k, 8k, Bokeh反向提示词: (((simple background))),monochrome ,lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, lowres, bad anatomy, bad hands, text, error, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, ugly, pregnant, vore, duplicate, morbid, mut ilated, tran nsexual, hermaphrodite, long neck, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, bad anatomy, bad proportions, malformed limbs, extra limbs, cloned face, disfigured, gross proportions, (((missing arms))),(((missing legs))), (((extra arms))),(((extra legs))),pubic hair, plump,bad legs, error legs, username, blurry, bad feet特定风格明快 CrispMix这个模型主要强调一种明快清爽的画风，比起油画和厚涂类模型，表现阴影的色块面积较为控制，画面整洁干净，非常适合时尚服装的展示。linecakeA1杂志风格，配合Graphic design这个LORA一起使用，效果会更好。风格类似于上图中的八重神子，营造了一种时尚杂志的感觉，很容易出片。"
689653975,59915ce281b6623b6d399c8e4c469a1b,Python与数据挖掘,https://api.zhihu.com/people/59915ce281b6623b6d399c8e4c469a1b,people,,火爆出圈！《大模型实战宝典》内容大升级！,article,https://api.zhihu.com/articles/689653975,之后新的生成式 AI 标杆。 关于<em>大模型</em>的话题不断涌现，令人应接不暇，截至到目前，国内已发布的<em>大模型</em>超过200个。 去年我们就推出《<em>大模型</em>实战宝典》（以下简称《实战宝典》）,7,0,7,1711675045,1711675045,欢迎关注 @Python与数据挖掘 ，专注 Python、数据分析、算法、大模型、好玩工具！2024 年刚开年，OpenAI 推出的文生视频工具 Sora 再次风靡全球，成为 OpenAI 继 ChatGPT 之后新的生成式 AI 标杆。关于大模型的话题不断涌现，令人应接不暇，截至到目前，国内已发布的大模型超过200个。去年我们就推出《大模型实战宝典》（以下简称《实战宝典》），很多小伙伴订阅后，反馈内容通俗易懂，收获了很多。今年年初开始，就计划内容大版本升级，前期做了很多准备工作。《实战宝典》内容更新，有几点考虑：以 ChatGLM 为例，目前已升级到 ChatGLM3 了(开源版)，模型已采用全新 Prompt 格式，支持代码执行和 Agent 任务等复杂场景；大模型应用框架 Langchain、LlamaIndex 升级了很多内容；案例有待丰富，大模型基础知识有待补齐；补充多模态的相关内容；新版的《大模型实战宝典》（2024版） 共计22w+字，内容比去年增加了100%。 与市面上大模型相关资料相比，具备如下优点：一次订阅，终身可用。后续内容升级，依然可以免费阅读；内容根据大模型最新发展趋势，不断丰富，与时俱进；提供技术社群，随时进行答疑沟通；相信读完后，无论你是学生还是在职人员，在求职和工作实践方面一定能会有所收获。如有兴趣，可以随时与我交流。内容概况篇幅有限，部分内容如上。与上一个版本相比：NLP相关内容进行内容，更容易学习；大模型必备知识增加RAG、多模态相关内容；实战案例20+；增加大模型实践经验分享；文档适合人群 在校学生，想学习AI相关内容去公司实习或者找工作，用大模型为简历增加亮点； 刚参加工作同学不久，想学习大模型相关内容升职加薪或者跳槽； 想“偷懒”省事，想获取一些大模型实战相关资料、阅读整理好的信息； 想近距离交流，获得更多经验和第一手信息； 以下情况，不适合： 有强大自我学习能力，不需要额外帮助； 不准备进入AI相关领域或者不愿意学习AI；获取方式本资料耗费了大量时间和精力，想要获取，加微信获取：mlc2060，备注：大模型实战宝典文章推荐47w+字！我写了一本《大模型面试宝典》！太强了！5个很酷的 Jupyter Notebook 技巧推荐收藏！使用 Transformer 进行时间序列预测！强烈推荐！7个强大的无代码数据科学工具！学习数据分析之前，先搞懂标签与指标体系！推荐收藏！小米数据指标体系建设实践！推荐收藏！一文讲透数据分析和指标体系！学习数据分析之前，先搞懂标签与指标体系！推荐收藏！一文讲透数据分析和指标体系！网易：如何做到数据指标异常发现和诊断分析？PyGWalker ：数据分析中最优秀工具库！36个顶级数据分析方法与模型！Jupyter notebook 两个炸裂的神操作！Jupyter Lab 十大高效生产力插件！基于 Toad 的评分卡模型全流程详解（含 Python 源码）Python有哪些杀手级超厉害框架或库或应用？盘点10个让工作效率倍增且有趣的 Python库！推荐收藏！机器学习建模调参方法总结！推荐收藏！23个机器学习最佳入门项目（附源代码没看完这11 条，别说你精通 Python 装饰器!20个数据分析师必会的数据模型，建议收藏！机器学习模型验证，这3个 Python 包可轻松解决95%的需求！精选 30 个炫酷的可视化大屏模板，拿走就用！深度盘点：8000字详细介绍 Python 中的 7 种交叉验证方法
674607019,18b69f05c9e0df6bc601b51bbdace517,chhzh123,https://api.zhihu.com/people/18b69f05c9e0df6bc601b51bbdace517,people,PLxCompiler-Cornell CS PhD,剖析 FPGA 加速大模型推理的潜力,article,https://api.zhihu.com/articles/674607019,我们选择了一些数据点进行实现和验证，最终实现的 FPGA 加速器在特定<em>模型</em>上已经可以达到比 A100 更低的延时和更低的能耗。 背景知识 <em>大</em>语言<em>模型</em>推理一般分为预填充（prefill）和解码（decode）两个阶段[2],137,33,232,1703842108,1703842186,TLDR: 用 FPGA 加速 LLM 推理在特定场景下能够达到比 NVIDIA A100 GPU 更高的性能和更低的能耗，但并非所有场景都适合用 FPGA。论文链接：https://arxiv.org/abs/2312.15159原文链接：https://chhzh123.github.io/blogs/2023-12-28-fpga-llm/这一年生成式大语言模型（large language models，LLMs）的爆火直接促进了 NVIDIA 卖卡的进程，股价直接翻了几番，现在高价求卡都还一卡难求。NVIDIA 的 GPU 虽好，它高额的成本和产生的电费让很多人望而却步。为了实现更加高效的 LLM 部署，最近几个月来互联网厂商一方面继续买 N 卡，另外一方面也在寻求替代品。所有人都知道需要寻找一种更加高效且低功耗的加速器，但究竟什么硬件才能实现这个目的。一些厂商直接开始了造芯进程（Google 的 TPU，Amazon 的 Inferentia，和 Microsoft 刚出的 Maia 等等），但是造芯的周期太长，一旦流片也很难跟上瞬息万变的模型发展（Maia 尴尬的内存带宽就是个例子）。而在通用处理器 CPU/GPU 和专用加速器 ASIC 中间其实还有一种硬件，既能做到一定的可编程性，又能达到比通用处理器更高的性能，这就是可编程门阵列 FPGA（Field-Programmable Gate Array）。目前最大的 FPGA 厂商是 Xilinx（已经被 AMD 收购），其芯片也陆续被集成进现在高端的 SoC 里面。我们最近的工作正是要探讨利用可重构硬件 FPGA 到底能不能在 LLM 这波浪潮下占据有利地位，以及 FPGA 在大语言模型推理上究竟能够释放多大的潜力。找对 FPGA 的应用场景其实很重要，要发挥其最大功力一定要考虑其重构性（reconfigurability）。之前很多工作[1]都尝试在 FPGA 上搭指令集架构（或称覆盖 overlay），但其实这是对 FPGA 的一种极大误用，因为一旦做成专用电路，将对应的架构做成 ASIC 总是可以比 FPGA 更加高效，那 FPGA 不过是一个实验性 prototype，它可重构的特点基本上就没有办法发挥出来。指令集架构如 CPU 最大的性能和能耗开销正是在指令解码和多级内存访问上，如果能够将这些额外开销消除，那实现的硬件将非常高效。我们希望实现的是针对模型定制化的（model-specific）完全展平的数据流架构（dataflow architecture），一方面能够最大程度减少片下内存（offchip memory/DRAM）访问从而降低能耗，同时因为没有 cache 的影响，实现的加速器也能具有确定性的时延（latency）。相比已经固化的 ASIC 电路，这种方式可以对新兴的复杂多变的网络结构进行重构，更加具有灵活性。上述两种方案在我们 paper 里又被归类成 temporal 和 spatial 两种不同的 architecture，前者指在时间上复用同一个大而全计算单元，后者则是直接将不同算子的计算单元用 FIFO 进行相连形成一个流水线型的数据流架构。PE是不同的处理引擎（Processing Engine），f1-f4是模型中不同的算子与之前的工作不同，我们并非要提出一个 state-of-the-art 的加速器；相反，我们提供了一个理论分析的框架，可以方便且准确地预测不同的 FPGA 设计能够达到的性能，同时我们也可以知道 FPGA 所能达到的理论上限以及与 GPU 的差距，从而来引导我们设计真正高效的加速器。基于这个理论分析，我们选择了一些数据点进行实现和验证，最终实现的 FPGA 加速器在特定模型上已经可以达到比 A100 更低的延时和更低的能耗。背景知识大语言模型推理一般分为预填充（prefill）和解码（decode）两个阶段[2]。预填充主要负责将用户输入的 prompt 读进来，然后生成第一个 token，而解码阶段则是所谓的自回归（autoregressive）过程，即根据上一个 token 生成下一个 token，直到生成结尾单词（EOS）为止。单从这个流程就可以发现，其实两个阶段的需求并不一样，第一个阶段由于读入多个 token，序列长度大于1，因此核心的计算是通用矩阵乘（General Matrix Multiply，GEMM）；而后一个阶段是一个一个 token 进行处理，因此核心计算转换为通用矩阵向量乘（General Matrix-Vector Multiply，GEMV）。通常来讲 GPU 在预填充阶段由于有巨大的算力，所以可以达到比较高的性能；但是在解码阶段由于没有足够的并行度倒是 GPU 利用率低，所以性能并不高，这在后面的案例分析中也会有讨论。我们在这里主要讨论单批次（single-batch）推理的场景，看最终的延时能够减少到多少。两阶段大模型推断理论分析框架我们主要考虑 Transformer 里面的线性层（linear layer），因为这些层占据了主要的计算和内存资源，而非线性层实际上在 FPGA 上用组合逻辑可以非常高效地实现，因此不在这里进行讨论。算力需求算力需求算是做 LLMSys 一个最基本的分析了，类似于 @陈乐群 的 剖析GPT推断中的批处理效应，我们也可以将两个阶段每个线性层所需的计算量统计成表格，注意这里我们并没有采用大 O 记号，这是 GPU 与 FPGA 估计的一大区别。有了每个算子精确的计算量，我们就可以对 FPGA 加速器的整体延时做精确估计。l是序列长度，d是隐含层维度为了最大程度发挥定制化硬件的性能，我们假设每个线性层都已经进行了量化（quantization），即里面的计算都可以高效地用整数乘加进行实现，这样每个乘加运算（Multiply-Accumulates，MACs）都可以在一个硬件周期内完成。同时我们假设 是分配给每个线性层的算力，即每秒能够执行的乘加运算，以 MACs/cycle 为单位，因此比如 对应的线性层就可以在 个周期内完成。这个算力是有上限的，其上限即为硬件所能提供的最大算力，在 FPGA 上也就是 DSP 的数目（通常一个 DSP 可以在一个周期内实现一个整数乘加）。我们可以得到下面的限制条件 其中 是在一块 FPGA 上部署的 Transformer 层数， 是 FPGA 上所有等效 DSP 的总数目，可以通过查每个厂商的 FPGA 芯片手册得到。内存容量限制这个限制比较显然，将所有参数需要的存储容量加起来，只要不超过 FPGA 总的 DRAM 和 BRAM 容量就可以了。这里我们将参数存在 DRAM 上，要求所有参数量小于 DRAM 的容量 而片上内存主要存储分片（tile）后的参数、KV cache 和中间存储激活值的 FIFO，可以得到 内存端口限制这个问题其实更加严重，也是 FPGA 独有的问题。虽然 FPGA 有充分的可重构性，但是其可重构性并不是针对任意的内存访问带宽都成立的，比如对于 AMD UltraScale+ FPGA，就只有 这几个位宽可以选，因此如果位宽不够的话就得做 padding，从而造成 BRAM 的浪费。由于不同的计算单元需要对数据同时进行读取，为了避免内存访问冲突，我们需要将对应的数据分配到不同的端口上。假设线性层 的参数量为 ，并且有 个 MAC 计算单元并行执行，其中 个计算单元会分享同一个加载的参数，因此权重缓存（weight buffer）需要被划分为 个分片。如果权重缓存存储在片上，且每个元素的位宽为 ，那么每个分片得到的元素个数为 。又由于 不能占据 BRAM 的所有位宽，我们引入 为比 大的最小合法 BRAM 位宽， 为一个内存单元的总容量（比如 AMD FPGA 的 BRAM 容量为 36Kb），那么我们可以得到对于一个线性层来说需要的 BRAM 数目为 但明显这种方式对于内存的使用是非常糟糕的，我们可以通过将多个元素进行打包（pack）从而实现更高效的内存使用。假设我们打包 个元素，即 ，那么我们可以得到新的内存需求 通过将所有线性层的内存需求相加，我们可以最终得到总的内存需求，这个值也需要小于对应的 BRAM 内存容量。内存带宽限制最后是内存带宽的限制，由于每个周期需要从 DRAM 中加载 个参数喂给 个计算单元，因此需要的带宽是 其中 是 FPGA 的频率，最终 的总和也需要小于总的 DRAM 带宽。延迟估计在 FPGA 上实现数据流就可以做到非常高效，大部分的算子都可以被重叠到一起，即不需要等上一个算子的结果全部输出就可以进行下一个算子的运算。唯一需要等待的是 q/k/v 线性层和后面的注意力模块，因为这里需要计算 K 和 V 的值，并且存储/合并 KV cache，所以流水线在这里需要等前面的结果全部算完后再进行下一步的计算。因此我们可以得到下面这幅两阶段的流水线图，每个阶段的时间都大致为 。（Note：这里也可以发现 FPGA 数据流实现的一大好处——某种意义上这些算子都已经自动实现了算子融合，因为上一个算子的输出数据可以直接传到下一个算子作为输入，而不需要写回内存，因此也不需要像 GPU 一样搞各种 FlashAttention 的优化来进行算子融合。）多个算子的流水线基于我们上述定义的算力 和每个算子的计算量，我们可以估计出最终的延迟 其中 是总的 Transformer 模型层数， 是 DRAM 访问的延时， 是最长的序列长度。如果考虑负载均衡，上述公式还可以进一步简化成 可以看到最终预填充阶段的延时跟序列长度成正比，而解码阶段的延时则几乎为常数（因为 ）。结合前面的限制条件，我们可以利用简单的线性搜索解得到最优的 值。多卡的分析在此不再赘述，详情可以参见我们的论文。案例分析接下来我们用上述框架预测一下现有 FPGA 在 LLM 推断时的性能，这里我们主要考虑 BERT、GPT2 以及 LLaMA2 几个模型。下面表格列举了我们进行比较的 FPGA 和 GPU 的核心参数。稍微对比一下 A100 与其他设备的数据就可以发现，A100 实在是一骑绝尘，浮点峰值性能都远超 FPGA 的整数峰值性能，同时内存带宽也达到了惊人的 2TB/s。现有的 FPGA 明显还是上一代的产品，跟 A100 的参数完全没有办法对标。但即便如此，我们还是分析了差距有多大以及怎么去弥补这个差距。目前常用的 FPGA 和 GPU 设备参数首先我们来看一下整体的延时，这里我们实测了 A100 和 2080 Ti 两块 GPU 的 fp16 性能，同时估计不同 FPGA 设备的理论最优延时，结果如下图所示。事实证明 GPU 在预填充阶段由于算力充足，改变序列长度基本不影响延时，而 FPGA 在预填充阶段想要达到 A100 的性能还是难度很大的（其延时是随序列长度线性增长的）。本来像 VCK5000 等专门为 AI 应用优化过的 FPGA 应该能够达到 GPU 的性能，但是比较迷的是 Xilinx 当时并没有给它加上 HBM，导致内存带宽严重受限。相反，在解码阶段，FPGA 完全有可能达到比 GPU 更优的延迟，这主要是因为 GPU 在这个阶段没有办法充分利用其并行度，所以对于 GPU 来说一个常见的优化就是做各种 batching 去尽可能提升资源利用率。在 BERT 和 GPT2 模型上的延迟估计，GPU 的数据是实测值在 LLaMA2 模型上的数据更加说明这一点，FPGA 需要开到非常大的并行度才有办法在预填充阶段与 GPU 抗衡，而在解码阶段 FPGA 只需比较小的并行度即可达到 GPU 的性能。当然，最近很多 LLM 优化的框架也在不断推进 GPU 的性能（参见GPT-fast），目前优化到最好的 LLaMA2 模型大约能够达到 200 tokens/sec 的性能。对于 FPGA 来说，利用拥有最新 HBM 的设备（如 VHK158）还是能够跟 A100 拼一拼的。我们也可以通过 GPU 的性能指标，来反推 FPGA 的并行度需要达到多大才能与 GPU 持平。在 LLaMA2 模型上的延迟估计，GPU 的数据是实测值我们也进一步分析了量化和数据打包对延时的影响，这里直接给出结论：权重量化非常重要，一方面可以降低 BRAM 资源占用，另外一方面也是减缓 DRAM 的带宽压力。激活值量化对内存占用影响不大，因为数据流架构基本不需要存储中间的激活值。数据打包对减少 BRAM 端口数目，降低资源占用至关重要，这也有利于后端的布局布线。最近的一些量化工作（如 UW 的 Atom[3]和 我校的 QuIP[4]）都开始往 4-bit 甚至 2-bit 推进，他们的实验结构都证明了低位宽的 LLM 是完全有可能达到 fp16 模型的准确率的，因此这也给高效的 FPGA 实现提供了可能性。量化及数据打包对性能的影响实验结果仅仅理论分析是不够的，我们利用高层次综合（high-level synthesis，HLS）实现了一系列高性能的算子。对于线性层我们采用了脉动阵列（systolic array）进行实现，然后将多个脉动阵列用 FIFO 进行串联构成完整的 Transformer 层，这种串联方式也是之前的工作从未尝试过的。我们选取了 的数据点进行实现，即脉动阵列的大小为 （我们采用了 DSP packing，因此算力值可以翻倍），如果并行度再往上涨就会出现比较严重的 routing 问题。同时我们手动将模型划分到不同的逻辑块上（super logic region，SLR），这样可以进一步降低走线的压力，从而提升频率。最终的加速器架构这里我们只放主要结果，在小的 GPT2 model 上，我们用 16nm 的 U280 FPGA 已经可以实现超过 7nm A100 GPU 接近1.9倍性能的推断加速比，同时比其能耗低 5.7 倍。FPGA 加速器在 GPT2 模型上的延时与能耗实测对于更大的 model，我们预计于明年上半年与我们的编译框架一同放出。事实上清华汪玉老师组的无问芯穹也已经做出了对应的 FPGA 加速器并且在 LLaMA 上呈现了比 A100 更好的 performance，大家之后也可以多加关注。讨论从延时的公式中，我们可以看到，要提升总体性能，其实有一个直接的方式是提升频率 。之前的工作大部分都是在 200MHz 左右，而我们的设计已经可以推到 250MHz，但是更往上推就会遇到 routing 的问题。将并行度 给 scale up 的过程也会导致更大的走线压力，我们之前尝试使用 AutoBridge[5]等框架来优化布线，但是这些框架对于复杂的数据流架构还是非常麻烦，需要进行大量的程序改写，并且多层级的数据流也没有良好的支持。另外一方面我们也看到了 FPGA 自定义数据流在解码阶段的巨大潜力，一个直接的想法则是利用 GPU 加速预填充阶段，用 FPGA 加速解码阶段，这样可以充分发挥两者的优势。作为 NVIDIA 的对家，AMD 其实下了非常大的赌注在异构计算上，这也决定了到底能不能 AMD yes。其收购了 Xilinx 也是看中了FPGA的场景，再加上最近发布的 AIE 更加表明 AMD 想要充分利用异构硬件的优势，实现更高效的加速计算。事实上 AMD 已经将 FPGA 集成进他们的 CPU 里面了（参见 XDNA-AI Engine），所以异构计算也提供了弯道超车的可能性。正如前文所说，仅仅提出一个高效的专用加速器架构并不是我们的最终目的，我们更希望能够有完整的框架适用于不同的应用不同的模型，并且快速生成高效的加速器架构。这也是我们同时进行的编译器项目想要实现的事情，上述的 HLS 算子也都直接由我们的编译器生成，对应的算子库及编译器都将在明年一起放出。致谢这个 project 从 23 年的春季学期就开始做了，期间 LLMSys 相关的工作层出不穷，我们对 LLM 性能瓶颈和优化的认识也提升了不少。特别感谢 Jiahao 带飞整个加速器设计，他是我至今见到最 productive 的 undergrad，从零开始搭建搭建起每个 LLM 的模块并组合成完整的加速器，初版的设计他可能用不到一周时间就写完了，组里的 PhD 得知后都非常震惊（Jiahao 今年也在申请 PhD，感兴趣的老师可以联系他 :)）。也特别感谢 Yixiao 带我深入剖析加速器结构并且搭起了理论分析的框架，这是我做过最 architecture 的 project 了，没有他的协助我可能真的没法搞清楚后端综合出现的各种问题。还有其他的合作者 Shaojie/Zichao/Niansong/Yaohui 也对这个 project 的模型量化、实验、理论分析等方面做出了很大的贡献，一并在此感谢！另外特别鸣谢叶老板 @叶子豪 在 LLM 相关 project 上的深入讨论，以及介绍 UW 的朋友一起探讨，本文的写作受到了很多启发。
662619689,cfa6e32b0290b6b42d86fcf46a78b8fb,helloai,https://api.zhihu.com/people/cfa6e32b0290b6b42d86fcf46a78b8fb,people,深度学习，深度思考,个人学习Notes-大模型SFT介绍,article,https://api.zhihu.com/articles/662619689,fine-tuning (PEFT) 全参数微调的挑战主要在于<em>大模型</em>训练过程中GPU需要加载的参数量巨大，除了<em>大模型</em>本身的参数外，还包括：每个参数对应优化器的参数(X2)、每个参数对应的梯度(x1)和一些中间缓存结果,92,1,254,1697885873,1704988785,"以下是个人学习大模型SFT相关知识的笔记整理，主要参考课程：Generative AI with Large Language Models，相关错误欢迎指正～～Fine-tuning on single task相比于预训练阶段，单个任务上微调的数据量一般在500-1000个样本就可以达到不错的效果；在单个任务上对大模型进行微调带来的问题，容易造成灾难性遗忘（catastrophic forgetting）灾难性遗忘的发生是因为完整的微调过程修改了原始LLM的权重。虽然这会在单个微调任务上带来出色的性能，但它可能会降低其他任务的性能；如何避免灾难性遗忘首先重要的是要确定灾难性遗忘是否真的会影响你的场景。如果您所需要的只是在微调的单个任务上的可靠性能，那么模型无法推广到其他任务可能不是问题。multiple tasks fine-tuning：如果希望模型保持多任务泛化能力，可以一次性对多个任务执行微调。良好的多任务微调可能需要包含许多任务的50-100,000个示例。考虑PEFT的方法：也就是保留了原始LLM的权重，不采用全参数微调的方法。通过训练少量特定于任务的适配器层和参数。PEFT对灾难性遗忘表现出更大的鲁棒性，因为大多数预训练的权重保持不变。Multi-task instruction fine-tuning主要的要点：收集不同任务类型的数据，比如总结、翻译、代码编写等任务，每种任务需要收集50-100,000个示例；2. 每种任务尽量包含不同形式的Prompt，以便于模型更方便的理解人类指令，比如对于总结任务，可以设计如下不同形式的Prompt模版；Parameter efficient fine-tuning (PEFT)全参数微调的挑战主要在于大模型训练过程中GPU需要加载的参数量巨大，除了大模型本身的参数外，还包括：每个参数对应优化器的参数(X2)、每个参数对应的梯度(x1)和一些中间缓存结果，整体的参数量约等于12-20倍的模型自身参数。【分析transformer模型的参数量、计算量、中间激活、KV cache】因此可以通过引入部分参数微调的技术PEFT，冻结LLM的部分参数，可以加快模型的训练速度，且能够保留原始LLM的通用能力，从而减少灾难性遗忘的发生。PEFT参数微调主要几个方向：策略1：选择LLM的部分参数作为微调的参数；策略2：利用LoRA技术对模型的参数重新表示；策略3：添加额外的模型层；LoRA1，冻结原始LLM的权重参数不参与训练；2，在原始的LLM权重参数基础上，注入两个秩为r的低秩分解矩阵A、和B；3，分解矩阵A、B参与模型训练；LoRA微调可以采用不同的低秩矩阵适配不同的任务类型，且LLM的原始权重不用变化；整体上LoRA微调效果相对于基座模型有较大的提升，但是相对于全参数微调方式来说效果上还是低一点。Full fine-tune>LoRA>base modelPrompt Tuning with soft prompts在输入层增加可训练的Soft Prompt参数，参数长度一般在20-100个，每个参数的embedding维度和词表token的embedding维度相同，如下图所示：相较于全参数微调，Prompt Tuning也是通过冻结LLM的原始参数，添加少量额外的训练参数以达到加速训练的目的；Prompt Tuning的实际效果可以从下面图中看出：1，当模型参数不够大的时候，Prompt Tuning比全参数微调的效果差一点，但是高于单纯的Prompt工程；2，当模型参数在100亿时，Prompt Tuning的效果和全参数微调的效果一样好；Prompt Tuning的可解释性说明：对于已完成训练的prompt embedding来说，是无法与词表中任何token表示对应的（Trained soft-prompt embedding does not correspond to a known token）；但是观察其邻域范围内的token表示可以看出其具有相同的语义，能够表示相同的意思（but nearest neighbors form a semantic group with similar meanings）；参考文献以上Slides图片均来自于 DeepLearning.AI附录LLM大模型之top P、top K和Temperature解释Hello0709：LLM Pre-Training和SFT阶段的Loss剖析"
640042827,88900abf2751adf884150d566ec927f8,汇智网,https://api.zhihu.com/people/88900abf2751adf884150d566ec927f8,people,学习软件编程,运行Llama大模型的PC配置要求,article,https://api.zhihu.com/articles/640042827,LLaMA <em>模型</em>需要比运行模型更多的 VRAM。 这是因为训练过程需要将<em>模型</em>以及训练数据存储在 VRAM 中。 训练所需的 VRAM 量取决于<em>模型</em>的大小和训练数据量,142,12,416,1687913402,1687913402,"大型语言模型 (LLM) 是强大的工具，可以为各种任务和领域生成自然语言文本。 最先进的LLM之一是 LLaMA（大型语言模型 Meta AI），这是由 Facebook 的研究部门 Meta AI 开发的一个包含 650 亿个参数的模型要在家运行 LLaMA 模型，你需要一台配备强大 GPU 的计算机，能够处理推理所需的大量数据和计算。 在本文中，我们将讨论本地运行 LLaMA 的一些硬件要求。在消费类硬件上运行 LLaMA 模型有多种不同的方法。 最常见的方法是使用单个 NVIDIA GeForce RTX 3090 GPU。 该 GPU 具有 24 GB 内存，足以运行 LLaMA 模型。 RTX 3090 可以运行 4 位量化的 LLaMA 30B 模型，每秒大约 4 到 10 个令牌。 24GB VRAM 似乎是在消费类台式电脑上使用单个 GPU 的最佳选择。推荐：用NSDT设计器快速搭建可编程3D场景。但是，如果你想运行更大的模型，则必须使用双 GPU 设置。 这将允许你将模型权重放入 VRAM 中。 你还可以使用高级 GPU，例如 NVIDIA A100。 这个GPU非常昂贵，但有40GB内存，可以更好地运行模型。你还可以在 CPU 上运行 LLaMA 模型。 必须使用模型的 GGML 版本（LLaMA、Vicuna、Alpaca 和 GPT4All）以及名为 llama.cpp 的软件才能使用CPU。 运行 LLaMA 的合适 CPU 是 Core i7 12900K 和 Ryzen 9 5900X。 有关此主题的更多信息，请查看 CPU 部分。请记住，训练或微调 LLaMA 模型需要比运行模型更多的 VRAM。 这是因为训练过程需要将模型以及训练数据存储在 VRAM 中。 训练所需的 VRAM 量取决于模型的大小和训练数据量。为了在台式电脑上使用 LLaMA 模型，请查看需要满足的一些硬件要求：1、运行 LLaMA 的 GPU要求在消费级机器上运行 LLaMA 时，GPU 是最重要的计算机硬件，因为它负责运行模型所需的大部分处理。 GPU的性能将直接影响推理的速度和准确性。模型的不同变体和实现可能需要功能较弱的硬件。 不过，GPU 仍将是系统中最重要的部分。4 位量化 LLaMA 模型的 GPU 要求：LLaMA ModelMinimum VRAM RequirementRecommended GPU ExamplesLLaMA-7B6GBRTX 3060, GTX 1660, 2060, AMD 5700 XT, RTX 3050LLaMA-13B10GBAMD 6900 XT, RTX 2060 12GB, 3060 12GB, 3080, A2000LLaMA-30B20GBRTX 3080 20GB, A4500, A5000, 3090, 4090, 6000, Tesla V100, Tesla P40LLaMA-65B40GBA100 40GB, 2x3090, 2x4090, A40, RTX A6000, 8000LLama-7B为了有效运行 LLaMA-7B，建议使用至少具有 6GB VRAM 的 GPU。 适合此模型的 GPU 示例是 RTX 3060，它提供 8GB VRAM 版本。 其他 GPU（例如 GTX 1660、2060、AMD 5700 XT 或 RTX 3050）也具有 6GB VRAM，可以作为支持 LLaMA-7B 的良好选择。LLaMA-13B为了获得 LLaMA-13B 的最佳性能，建议使用至少具有 10GB VRAM 的 GPU。 满足此要求的 GPU 示例包括 AMD 6900 XT、RTX 2060 12GB、3060 12GB、3080 或 A2000。 这些 GPU 提供必要的 VRAM 容量来有效处理 LLaMA-13B 的计算需求。LLaMA-30B为确保 LLaMA-30B 顺利运行，建议使用至少 20GB VRAM 的 GPU。 RTX 3080 20GB、A4500、A5000、3090、4090、6000 或 Tesla V100 是提供所需 VRAM 容量的 GPU 示例。 这些 GPU 可实现 LLaMA-30B 的高效处理和内存管理。LLaMA-65BLLaMA-65B 与至少具有 40GB VRAM 的 GPU 配合使用时，性能最佳。 适用于此型号的 GPU 示例包括 A100 40GB、2x3090、2x4090、A40、RTX A6000 或 8000。这些 GPU 提供充足的 VRAM 容量来处理与 LLaMA-65B 相关的密集计算任务。每个 LLaMA 模型都有特定的 VRAM 要求，建议的 GPU 是根据其满足或超过这些要求的能力来选择的，以确保相应的 LLaMA 模型平稳高效的性能。2、运行LLaMA 的 CPU要求除了 GPU 之外，你还需要一个可以支持 GPU 并处理其他任务（例如数据加载和预处理）的 CPU。 基于 GPQT (GPU) 的模型对 CPU 的要求低于针对 CPU 优化的模型。适合 LLaMA 的 CPU 是 Intel Core i9-10900K、i7-12700K 或 Ryzen 9 5900x。 但是，为了获得更好的性能，你可能需要使用更强大的 CPU，例如具有 64 核和 128 线程的 AMD Ryzen Threadripper 3990X。 最后，真正重要的是 CPU 的速度。 这才是真正的力量所在。 当在昂贵的服务器 CPU 和高端游戏 CPU 之间进行选择时，后者占据主导地位。我们必须注意，本文讨论的模型是针对 GPU 的，但也有针对 CPU 的 LLaMa 模型优化器。 例如，GGML 是一种解决方案，可以解决处理大型模型时 GPU 内存带来的限制。 如果你更喜欢使用 CPU，建议运行 GGML 格式的模型文件。你可以使用名为 llama.cpp（LLaMA 模型的接口）的软件来利用你的 CPU。 llama.cpp 最近的更新引入了新的增强功能，使用户能够在 CPU 和 GPU 之间分配模型的工作负载。 这不仅有利于加载更大的模型，而且还提高了令牌的速度。这是使用 Ryzen 7 3700X 和 128GB RAM 运行 llama.cpp 的示例。GGML ModelMemory per TokenLoad TimeSample TimePredict TimeTotal TimeLLaMA-7B 4-bit14434244 bytes1270.15 ms325.76 ms15147.15 ms / 117.42 ms per token17077.88 msLLaMA-13B 4-bit22439492 bytes2946.00 ms86.11 ms7358.48 ms / 216.43 ms per token11019.28 msLLaMA-30B 4-bit43387780 bytes6666.53 ms332.71 ms68779.27 ms / 533.17 ms per token77333.97 msLLaMA-65B 4-bit70897348 bytes14010.35 ms335.09 ms140527.48 ms / 1089.36 ms per token157951.48 ms3、运行LLaMA 的内存要求除了GPU和CPU之外，你还需要足够的RAM（随机存取存储器）和存储空间来存储模型参数和数据。 4 位 LLaMA-30B 的最低 RAM 要求为 32 GB，可以将整个模型保存在内存中，而无需交换到磁盘。 但是，对于较大的数据集或较长的文本，你可能需要使用更多 RAM，例如 64 GB 或 128 GB。CPU 和内存之间的带宽是一个关键因素，我想强调它的重要性。 当生成单个 token 时，整个模型需要从内存中读取一次。 假设你有 Core i9-10900X（4 通道支持）和 DDR4-3600 内存，这意味着吞吐量为 115 GB/s，而你的型号大小为 13 GB。 在这种情况下，理论限制约为每秒 8.8 个令牌，无论你的 CPU 有多快或有多少个并行核心。RAM 的大小取决于 GGML 量化的类型和你使用的模型（LLaMA、Alpaca、Wizard、Vicuna 等）。这些是 在CPU上使用 LLaMA 模型的内存 (RAM) 要求：GGML ModelOriginal sizeQuantized size (4-bit)Quantized size (5-bit)Quantized size (8-bit)7B13 GB3.9 – 7.5 GB7.5 – 8.5 GB8.5 – 10.0 GB13B24 GB7.8 – 11 GB11.5 – 13.5 GB13.5 – 17.5 GB30B60 GB19.5 – 23.0 GB23.5 – 27.5 GB28.5 – 38.5 GB65B120 GB38.5 – 47.0 GB47.0 – 52.0 GB71.0 – 80.0 GB在 CPU 上运行时基于内存 (RAM) 速度的模型 (8GB) 推理速度：RAM speedCPUCPU channelsBandwidth*InferenceDDR4-3600Ryzen 5 3600256 GB/s7 tokens/sDDR4-3200Ryzen 5 5600X251 GB/s6.3 tokens/sDDR5-5600Core i9-13900K289.6 GB/s11.2 tokens/sDDR4-2666Core i5-10400f241.6 GB/s5.1 tokens/s速度为理论最大值，取决于操作系统和系统负载。4、运行LLaMA的存储要求LLaMA的最低存储要求是1TB NVMe SSD，可以存储模型文件和数据文件，读写速度很快。 但是，为了更多数据或备份目的，你可能需要使用更多存储空间，例如 2 TB 或 4 TB SSD。选择高速存储。 选择具有出色顺序速度的 PCIe 4.0 NVMe SSD，以促进存储和系统 RAM 之间的快速数据传输。5、模型量化如何影响 GPU 的选择？量化 LLM使用更少的位数来存储和处理模型的权重和激活。 这使得它们的 GPU 部署更快、更高效。4 位量化 LLM 每个权重或激活仅使用 4 位。 这意味着它们比全精度模型占用更少的内存和计算时间。 它们可以在 VRAM 容量较低的 GPU 上平稳运行。8 位量化 LLM 每个权重或激活使用 8 位。 与全精度模型相比，这仍然减少了内存和计算成本，但不如 4 位量化那么多。 它们需要更多的 GPU 内存和计算能力才能良好运行。 它们更适合具有高 VRAM 容量和计算能力的 GPU。总而言之，4 位量化 LLM 效率更高，并且可以在 VRAM 容量较低的 GPU 上运行。 8 位量化 LLM 的效率稍低，需要具有高 VRAM 容量和计算能力的 GPU。LLaMA PrecisionGPU Memory RequirementsComputational DemandsSuitable GPUNative (32-bit)Higher requirementsHigher computational demandsGPUs with larger VRAM capacities and high computational capabilities16-bit QuantizedModerate requirementsModerate computational demandsGPUs with moderate VRAM capacities and good computational capabilities8-bit QuantizedRelatively higher requirementsSlightly higher computational demandsGPUs with larger VRAM capacities and higher computational capabilities4-bit QuantizedLower requirementsLower computational demandsGPUs with limited VRAM capacities正如你所看到的，LLaMA 的精度对其 GPU 内存需求和计算需求有直接影响。 原生（32 位）LLM 需要最多的 GPU 内存和计算能力，而 4 位量化 LLM 需要最少。适用于 LLaMA 的 GPU 取决于其精度以及您想要使用它执行的特定任务。 如果您需要在各种任务上运行大型 LLaMA，那么您将需要具有大 VRAM 容量和高计算能力的 GPU。 如果您只需要在几个特定任务上运行小型 LLaMA，那么您可以使用具有较小 VRAM 容量和较低计算能力的 GPU。需要注意的是，随着量化级别的降低，模型的准确性也会降低。 这是因为精度降低可能会导致模型预测出现错误。最适合你的量化级别取决于你的具体需求和要求。 如果需要一个小而高效的模型，那么你可能需要考虑使用 4 位或 8 位量化模型。 但是，如果你需要高度准确的模型，那么可能需要使用 16 位模型。6、双GPU是否有效提升 LLaMA性能？添加第二个 GPU 可能不会像预期那样加快文本生成速度。 瓶颈似乎阻碍了增加更多计算能力的简单解决方案。 一些测试显示出令人惊讶的结果，低端 GPU 每秒生成令牌的速度比高端 GPU 更快。 其原因尚不清楚，文本生成程序可能需要更好的优化才能很好地使用双 GPU 设置。双 GPU 设置总共具有更多 VRAM，但每个 GPU 仍然有其自己的 VRAM 限制。 30B LLaMA 需要大约 20GB VRAM，因此两个 RTX 3090 GPU（每个都有 24GB VRAM）仍然只有 24GB VRAM 可用。 该模型应适合一个 GPU 的 VRAM 才能正常运行。但是，如果模型太大而无法容纳单个 GPU 的 VRAM 并且需要利用系统 RAM，则使用多个 GPU 确实可以加快该过程。 在这种情况下，每个 GPU 可以处理模型的一部分，并且计算负载在它们之间分配。 这种并行化可以提高超过单个 GPU 的 VRAM 容量的大型模型的速度。因此，在处理具有高 VRAM 要求的大型模型时，通常会采用多个 GPU。 它可以有效利用资源并加速训练或推理过程。将像 65B LLaMA 这样的大型语言模型拆分到具有模型并行性的多个 GPU 上可能会很困难，并且可能会导致通信延迟。 通过 GPU 拆分和同步模型的参数和计算需要仔细编码，并且可能并不总是能大幅提高性能。双 GPU 设置可能不适用于某些软件。 某些机器学习框架或库可能无法完全使用多个 GPU，并且可能需要额外的工作来设置和优化系统以使用双 GPU。这些限制意味着，将双 GPU 设置用于 30B LLaMA 的可能优势与难度和潜在问题进行比较非常重要。 有时，获得更强的单GPU或尝试其他优化方法可能是更好的方法。7、为 LLaMA 选择 PC 硬件的技巧围绕 GPU 构建创建一个包含主板、CPU 和 RAM 的平台。 GPU 处理训练和推理，而 CPU、RAM 和存储管理数据加载。 选择支持 PCIe 4.0（或 5.0）、多个 NVMe 驱动器插槽、x16 GPU 插槽和充足内存 DIMM 的主板。 建议使用单线程速度较高的 CPU，例如 Ryzen 5000 或 Intel 第 12/13 代。型号选择和 VRAM为了在响应质量方面获得最佳性能，建议在具有至少 20GB VRAM 的 GPU 上运行 8 位 13B 模型或 4 位 30B 模型。 两种型号都提供相似的质量响应，VRAM 可用性应该是决定因素。 投资具有张量核心的 Nvidia GPU 以增强性能。 考虑 RTX 30 系列或 RTX 40 系列等选项，例如 RTX 3090 24GB、RTX 4090 24GB，以获得最佳性能。速度比较就每秒生成的令牌而言，13B 模型通常比 30B 模型运行得更快。 虽然确切的速度差异可能有所不同，但与 30B 模型相比，13B 模型往往会在生成速度方面提供显着的改进。内存要求目标是至少 1.5 倍 VRAM 容量或两倍 VRAM 以获得最佳性能。 当使用 128GB 或更多 RAM 时，主板和 CPU 的选择变得至关重要。PCIe 4.0 NVMe 固态硬盘高顺序速度 PCIe 4.0 NVMe SSD 的重要性主要在于将初始模型加载到 VRAM 中。 模型加载后，SSD 对生成速度（令牌/秒）的影响很小。足够的常规 RAM拥有足够的常规 RAM（最好是 VRAM 容量的两倍）对于初始模型加载至关重要。 模型一旦加载，对实际生成速度的影响是有限的。 确保初始加载期间有足够的常规 RAM 对于流畅的体验至关重要。CPU单线程速度CPU 的单线程速度主要对于初始模型加载非常重要，而不是在生成期间运行模型。 CPU的作用在数据预处理、模型加载和其他不依赖GPU的操作等任务中更加突出。扩展以提高速度如果你需要将文本生成速度从 15 个令牌/秒提高到 30 个令牌/秒，设置整个 PC 的文字克隆可能比添加第二个 3090 卡更有效。 将整体系统资源（包括 CPU 和 RAM）加倍可能会在提高文本生成速度方面产生更好的结果。单GPU性能由于 GPU 本身的内部带宽优势，单个 GPU 通常比多 GPU 设置提供更快的性能。电源及机箱投资具有足够容量为所有组件供电的高质量电源。 选择通风良好的宽敞机箱以获得最佳散热效果。DDR5 和未来平台虽然 DDR5 和 Zen 4 或 AM5 等未来平台具有优势，但稳定性和兼容性可能会有所不同。 考虑投资具有良好 PCIe 插槽布局和内存支持的高端主板，以实现未来的升级。请记住，虽然这些提示和技巧提供了基于经验的见解，但各个系统配置和性能可能会有所不同。 始终建议对不同的设置进行试验和基准测试，以找到最适合你的特定需求的解决方案。原文链接：Llama消费级硬件要求 - BimAnt"
623543497,6ab5d9817bdcadd5eeaff0ccba56c61e,绝密伏击,https://api.zhihu.com/people/6ab5d9817bdcadd5eeaff0ccba56c61e,people,《推荐系统技术原理与实践》作者，欢迎知友京东购买。,LORA：大模型轻量级微调,article,https://api.zhihu.com/articles/623543497,PLAM-540B，华为的盘古<em>大模型</em>，阿里的通义千问，等等。 图1：清华开源的ChatGLM-130B<em>大模型</em> 这些大公司或者研究机构，都是有足够资源的来开发<em>大模型</em>，但是对于一般的小公司或者个人来说,628,37,1201,1682071082,1704976359,"最近大模型高效微调成为业界关注的焦点，如何通过轻量微调变成各个不同领域的专业模型，成为讨论的热点。今天我们介绍一种轻量级的微调方法：LORA。LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS背景随着 ChatGPT 的爆火，很多机构都开源了自己的大模型，比如清华的 ChatGLM-6B/ChatGLM-10B/ChatGLM-130B，HuggingFace 的 BLOOM-176B。当然还有很多没有开源的，比如 OpenAI 的 ChatGPT/GPT-4，百度的文心一言，谷歌的 PLAM-540B，华为的盘古大模型，阿里的通义千问，等等。图1：清华开源的ChatGLM-130B大模型这些大公司或者研究机构，都是有足够资源的来开发大模型，但是对于一般的小公司或者个人来说，要想开发自己的大模型几乎不可能，要知道像 ChatGPT 这样的大模型，一次训练的成本就在上千亿美元。那么那些小公司或者个人，又怎么能够利用这些开源的大模型，在自己的数据上继续训练，从而应用于自己的业务场景？有没有低成本的方法微调大模型？答案是有的。目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2。这些方法都有各自的特点，从个人使用情况来说，LORA 的效果会好于其它几种方法。其它方法都有各自的一些问题：Adapter Tuning 增加了模型层数，引入了额外的推理延迟Prefix-Tuning 难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能P-tuning v2 很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出低秩自适应（LoRA）方法。LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。接下来我们介绍下细节。LORALoRA 的思想很简单:在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsic rank。训练的时候固定 PLM 的参数，只训练降维矩阵 与升维矩阵 。而模型的输入输出维度不变，输出时将 与 PLM 的参数叠加。用随机高斯分布初始化 ，用 0 矩阵初始化 ，保证训练的开始此旁路矩阵依然是 0 矩阵。假设要在下游任务微调一个预训练语言模型（如 GPT-3），则需要更新预训练模型参数，公式表示如下： 是预训练模型初始化的参数， 就是需要更新的参数。如果是全参数微调，则它的参数量 （如果是 GPT-3，则 ）。从这可以看出要全参数微调大语言模型，代价是非常高的。而对于 LORA 来说，只需要微调 。具体来看，假设预训练的矩阵为 ，它的更新可表示为： 其中秩 。在 LoRA 的训练过程中， 是固定不变的，只有 和 是训练参数。在前向过程中， 与 都会乘以相同的输入 ，最后相加：LORA 的这种思想有点类似于残差连接，同时使用这个旁路的更新来模拟 Full Fine-Tuning的过程。并且，Full Fine-Tuning可以被看做是 LoRA 的特例（当 等于 时）。在推理过程中，LoRA 也几乎未引入额外的 Inference Latency，只需要计算 即可。LoRA 与 Transformer 的结合也很简单，仅在 QKV Attention 的计算中增加一个旁路。LORA 轻量级微调实验效果基准内容理解上的效果下面是在各个任务上的效果：MNLI、SST-2 、MRPC、CoLA、QNLI、QQP、RTE、STS-B表示各项任务：MNLI（Multi-Genre Natural Language Inference）：该任务是一个自然语言推理任务，要求模型根据给定的前提和假设来判断它们之间的关系（蕴含、中立或矛盾）。数据集中包含来自不同文体（新闻、文学等）的句子对。SST-2（Stanford Sentiment Treebank）：该任务是一个情感分析任务，要求模型判断给定句子的情感是正面还是负面。MRPC（Microsoft Research Paraphrase Corpus）：该任务是一个语义相似度任务，要求模型判断给定句子对是否具有语义相似性。CoLA（Corpus of Linguistic Acceptability）：该任务是一个语言可接受性任务，要求模型判断给定句子是否符合语法规则和语言习惯。QNLI（Question NLI）：该任务是一个自然语言推理任务，要求模型根据给定的问题和前提，判断问题是否可以从前提中推导出来。QQP（Quora Question Pairs）：该任务是一个语义相似度任务，要求模型判断给定问题对是否具有语义相似性。RTE（Recognizing Textual Entailment）：该任务是一个自然语言推理任务，要求模型根据给定的前提和假设来判断它们之间的关系（蕴含或不蕴含）。STS-B（Semantic Textual Similarity Benchmark）：该任务是一个语义相似度任务，要求模型判断给定句子对是否具有语义相似性，但是与MRPC不同的是，STS-B中的句子对具有连续的语义相似性等级。可以看到 LORA 相比其它微调方法，可训练参数最少，但是整体上效果最好。结论：参数量较全参数微调（Fine-Tuning）显著降低，参数量和现有高效参数微调方法持平或更低。性能优于其它参数高效微调方法，和全参数微调（Fine-Tuning）基本持平甚至更高。生成任务上的效果和前面内容理解的结论基本一致，LORA 参数量更少，但是性能和全参数微调相当。备注：2E NLG Challenge（End-to-End Natural Language Generation Challenge）是一个年度举办的自然语言生成比赛。该比赛旨在推动和促进自然语言生成技术的发展，并为该领域的研究人员和从业者提供一个交流和展示的平台。比赛的任务是将给定的结构化数据转换为自然语言文本。比赛的数据集包括不同领域（如餐饮、酒店、娱乐等）的数据，其中包括用于描述餐厅、酒店等的数据，例如名称、地址、菜单、评论等。比赛的评估标准是BLEU（Bilingual Evaluation Understudy），它是一种常用于机器翻译和自然语言生成任务的评估指标。除此之外，评估还包括生成文本的流畅性、可读性和准确性等方面的考虑。E2E NLG Challenge为研究人员和从业者提供了一个展示自然语言生成技术的平台，并促进了该领域的研究和发展。该比赛已经成为自然语言生成领域中备受关注的比赛之一。GPT-3 上的效果前面 ROBbase 和 GPT-2 都是参数量比较小的模型，而在大模型 GPT-3 175B 上，LORA 也取得了不错的效果（见上图）。当增加微调方法的可训练参数量时，其它微调方法都出现了性能下降的现象，只有 LORA 的性能保持了稳定，见下图：结论：LORA 相比其它微调方法，增加参数量不会导致性能的下降。性能上与全参数微调持平甚至超过。秩的选择实验结果显示，对于一般的任务， 就足够了。而一些领域差距比较大的任务可能需要更大的 。同时，增加 值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料。LORA 开源实现目前 LORA 已经被 HuggingFace 集成在了 PEFT（Parameter-Efficient Fine-Tuning） 代码库里。使用也非常简单，比如我们使用 LORA 微调 BigScience 机器翻译模型：from transformers import AutoModelForSeq2SeqLM from peft import get_peft_config, get_peft_model, LoraConfig, TaskType model_name_or_path = ""bigscience/mt0-large"" tokenizer_name_or_path = ""bigscience/mt0-large"" peft_config = LoraConfig( task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1 ) model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path) model = get_peft_model(model, peft_config) model.print_trainable_parameters() # output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282模型微调好之后，加载也非常简单，只需要额外加载 LORA 参数：model.load_state_dict(torch.load(peft_path), strict=False)补充：为什么矩阵B、A不能同时为0在前面我们介绍了，用随机高斯分布初始化 ，用 0 矩阵初始化 ，矩阵 为什么不也用0初始化？这主要是因为如果矩阵 也用0初始化，那么矩阵 梯度就始终为0，无法更新参数，导致 。这里简单推理一下。对于 ，设 ，则：因此：如果矩阵 也用0初始化，那么上面的梯度就变成了0。所以矩阵 不能用0初始化。同样，我们看一下矩阵 初始化0的影响。由于矩阵 的参数会发生更新，而 矩阵又不是0矩阵，因此后面 ，所以矩阵 可以用0初始化。总结基于大模型的内在低秩特性，增加旁路矩阵来模拟全参数微调，LoRA 通过简单有效的方案来达成轻量微调的目的。它的应用自不必提，可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型。此外，考虑 OpenAI 对 GPT 模型的认知，GPT 的本质是对训练数据的有效压缩，从而发现数据内部的逻辑与联系，LoRA 的思想与之有相通之处，原模型虽大，但起核心作用的参数是低秩的，通过增加旁路，达到四两拨千斤的效果。参考LoRA: Low-Rank Adaptation of Large Language Models 简读我是阿豪啊：LoRA论文回顾Johnson7788：LoRA:大型语言模型的低秩适配器小虎AI珏爷：论文阅读：LORA-大型语言模型的低秩适应通用 AI 模型参数量越来越高，模型大小的标准是否正在发生变化？多大才算大模型？https://github.com/huggingface/pefthttps://arxiv.org/pdf/2106.09685.pdf书籍推荐"
630832593,91a079d6e279bb4f0af93f0bfac8d434,Young,https://api.zhihu.com/people/91a079d6e279bb4f0af93f0bfac8d434,people,HPC高性能计算,大模型推理性能优化之KV Cache解读,article,https://api.zhihu.com/articles/630832593,生成式generative<em>模型</em>的推理过程很有特点，我们给一个输入文本，<em>模型</em>会输出一个回答（长度为N），其实该过程中执行了N次推理过程。即GPT类<em>模型</em>一次推理只输出一个token,718,84,1152,1684570415,1700127304,"0. 引言做大模型性能优化的一定对KV Cache不陌生，那么我们对这个技术了解到什么程度呢？请尝试回答如下问题：KV Cache节省了Self-Attention层中哪部分的计算？KV Cache对MLP层的计算量有影响吗？KV Cache对block间的数据传输量有影响吗？本文打算剖析该技术并给出上面问题的答案。1. KV Cache是啥？大模型推理性能优化的一个常用技术是KV Cache，该技术可以在不影响任何计算精度的前提下，通过空间换时间思想，提高推理性能。网上有一些关于该技术的分析博客，但读过后仍然会很迷糊，甚至可能会被带偏，认为这个Cache过程和数据库读取或CPU Cache加速类似的荒谬结论。刚开始我也有类似误解，直到逐行查阅并运行源码，才清楚了解到其Cache了啥，以及如何节省计算的。2. 背景生成式generative模型的推理过程很有特点，我们给一个输入文本，模型会输出一个回答（长度为N），其实该过程中执行了N次推理过程。即GPT类模型一次推理只输出一个token，输出token会与输入tokens 拼接在一起，然后作为下一次推理的输入，这样不断反复直到遇到终止符。如上描述是我们通常认知的GPT推理过程。代码描述如下：import torch from transformers import GPT2LMHeadModel, GPT2Tokenizer model = GPT2LMHeadModel.from_pretrained(""/WORK/Test/gpt"", torchscript=True).eval() # tokenizer tokenizer = GPT2Tokenizer.from_pretrained(""/WORK/Test/gpt"") in_text = ""Lionel Messi is a"" in_tokens = torch.tensor(tokenizer.encode(in_text)) # inference token_eos = torch.tensor([198]) # line break symbol out_token = None i = 0 with torch.no_grad(): while out_token != token_eos: logits, _ = model(in_tokens) out_token = torch.argmax(logits[-1, :], dim=0, keepdim=True) in_tokens = torch.cat((in_tokens, out_token), 0) text = tokenizer.decode(in_tokens) print(f'step {i} input: {text}', flush=True) i += 1 out_text = tokenizer.decode(in_tokens) print(f' Input: {in_text}') print(f'Output: {out_text}')输出：step 0 input: Lionel Messi is a player step 1 input: Lionel Messi is a player who step 2 input: Lionel Messi is a player who has step 3 input: Lionel Messi is a player who has been step 4 input: Lionel Messi is a player who has been a step 5 input: Lionel Messi is a player who has been a key step 6 input: Lionel Messi is a player who has been a key part step 7 input: Lionel Messi is a player who has been a key part of step 8 input: Lionel Messi is a player who has been a key part of the step 9 input: Lionel Messi is a player who has been a key part of the team step 10 input: Lionel Messi is a player who has been a key part of the team's step 11 input: Lionel Messi is a player who has been a key part of the team's success step 12 input: Lionel Messi is a player who has been a key part of the team's success. step 13 input: Lionel Messi is a player who has been a key part of the team's success. Input: Lionel Messi is a Output: Lionel Messi is a player who has been a key part of the team's success.可以看出如上计算的问题吗？每次推理过程的输入tokens都变长了，导致推理FLOPs随之增大。有方法实现推理过程的FLOPs基本恒定不变或变小吗？（埋个伏笔，注意是基本恒定）。3. 原理在上面的推理过程中，每 step 内，输入一个 token序列，经过Embedding层将输入token序列变为一个三维张量[b, s, h]，经过一通计算，最后经logits层将计算结果映射至词表空间，输出张量维度为[b, s, vocab_size]。当前轮输出token与输入tokens拼接，并作为下一轮的输入tokens，反复多次。可以看出第 轮输入数据只比第轮输入数据新增了一个token，其他全部相同！因此第轮推理时必然包含了第 轮的部分计算。KV Cache的出发点就在这里，缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果，就是这么简单，不存在什么Cache miss问题。4. 实现细节目前各大模型推理都实现了KV Cache，下面就看如何使用了。我们可以在上面代码基础上修改，主要改动：在推理时新增了 past_key_values 参数，该参数就会以追加方式保存每一轮的K V值。kvcache变量内容为((k,v), (k,v), ..., (k,v))，即有 个 k,v 组成的一个元组，其中 k 和 v 的维度均为 [b, n_head, s, head_dims]。这里可以顺带计算出每轮推理对应的 cache 数据量为 ，这里 值等于当前轮次值。以GPT3-175B为例，假设以 float16 来保存 KV cache，senquence长度为100，batchsize=1，则 KV cache占用显存为 2×100×12288×96×2 Byte= 472MB。推理输出的token直接作为下一轮的输入，不再拼接，因为上文信息已经在 kvcache 中。代码示例：import torch from transformers import GPT2LMHeadModel, GPT2Tokenizer model = GPT2LMHeadModel.from_pretrained(""/WORK/Test/gpt"", torchscript=True).eval() # tokenizer tokenizer = GPT2Tokenizer.from_pretrained(""/WORK/Test/gpt"") in_text = ""Lionel Messi is a"" in_tokens = torch.tensor(tokenizer.encode(in_text)) # inference token_eos = torch.tensor([198]) # line break symbol out_token = None kvcache = None out_text = in_text i = 0 with torch.no_grad(): while out_token != token_eos: logits, kvcache = model(in_tokens, past_key_values=kvcache) # 增加了一个 past_key_values 的参数 out_token = torch.argmax(logits[-1, :], dim=0, keepdim=True) in_tokens = out_token # 输出 token 直接作为下一轮的输入，不再拼接 text = tokenizer.decode(in_tokens) print(f'step {i} input: {text}', flush=True) i += 1 out_text += text print(f' Input: {in_text}') print(f'Output: {out_text}')通过上面代码只能看到调用层面的变化，实现细节还需看各框架的底层实现，例如Hugging Face的transformers库代码实现就比较清爽，在modeling_gpt2.py中Attention部分相关代码如下： query = self._split_heads(query, self.num_heads, self.head_dim) key = self._split_heads(key, self.num_heads, self.head_dim) value = self._split_heads(value, self.num_heads, self.head_dim) if layer_past is not None: # 当输出第一个token后，layer_past就是非None了 past_key, past_value = layer_past # 取出之前计算好的 key, value key = torch.cat((past_key, key), dim=-2) # past_key 与当前 token 对应的 key 拼接 value = torch.cat((past_value, value), dim=-2) # past_value 与当前 token 对应的 value 拼接 if use_cache is True: present = (key, value) else: present = None在 block 层面也有相关代码，大家有空细品吧。还是那句话，说一千道一万不如阅读并运行源码一次。其实，KV Cache 配置开启后，推理过程可以分为2个阶段：预填充阶段：发生在计算第一个输出token过程中，这时Cache是空的，计算时需要为每个 transformer layer 计算并保存key cache和value cache，在输出token时Cache完成填充；FLOPs同KV Cache关闭一致，存在大量gemm操作，推理速度慢。使用KV Cache阶段：发生在计算第二个输出token至最后一个token过程中，这时Cache是有值的，每轮推理只需读取Cache，同时将当前轮计算出的新的Key、Value追加写入至Cache；FLOPs降低，gemm变为gemv操作，推理速度相对第一阶段变快，这时属于Memory-bound类型计算。这里用图可能更有助理解，下图是一个Decoder Block（仅以MHA为例），含有Self-Attention和MLP，标红部分为KV Cache影响到的内容，即KV Cache开启后，标红的序列长度 变为 1，当batch_size=1时，Self-Attention中的2个dense全都变为gemv操作，MLP中的dense也全都变为gemv操作。看懂这个图就可以答对上面的3个问题啦。图中数据维度相关字母的含义：b: batchsizes: sequence length，序列长度h: hidden_state 维度 = n * dn: head 个数d: head 维度如下链接也有这方面的定量分析，写的很棒，推荐大家看看。回旋托马斯x：分析transformer模型的参数量、计算量、中间激活、KV cache5. 总结KV Cache是Transformer推理性能优化的一项重要工程化技术，各大推理框架都已实现并将其进行了封装（例如 transformers库 generate 函数已经将其封装，用户不需要手动传入past_key_values）并默认开启（config.json文件中use_cache=True）。本文尝试打开封装分析该技术内部实现，希望对大家有所帮助，文中如有纰漏，欢迎指正。"
